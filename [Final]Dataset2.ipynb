{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/21020718/KLTN_2025_TrangNTT/blob/main/%5BFinal%5DDataset2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bộ dữ liệu thứ hai**"
      ],
      "metadata": {
        "id": "H4ub86oNH4xG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "iFjIlJQ8h1L7",
        "outputId": "5f49ab4d-08c5-4118-aa60-e97122dbf135"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "IPython.notebook.set_autosave_interval(60000)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Autosaving every 60 seconds\n"
          ]
        }
      ],
      "source": [
        "%autosave 60\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "W0eXsw8rHewM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsLZNOScHwro"
      },
      "outputs": [],
      "source": [
        "# Set random seeds for reproducibility\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Define a fixed seed value\n",
        "SEED = 42\n",
        "\n",
        "# Set random seeds for all libraries\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)  # For GPU if available\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "import sklearn.preprocessing\n",
        "import sklearn.preprocessing._data as data\n",
        "import sys\n",
        "sys.modules[\"sklearn.preprocessing.data\"] = data\n",
        "\n",
        "import pandas as pd\n",
        "from numpy import concatenate\n",
        "from math import sqrt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.metrics import normalized_mutual_info_score\n",
        "\n",
        "def Scaler(data):\n",
        "\n",
        "    \"\"\"\n",
        "        Scaler all feature to range(0,1)\n",
        "        Arguments:\n",
        "          data:  Pandas DataFrame of data\n",
        "        Return:\n",
        "          scaler: scaler\n",
        "          scaledDf:Pandas DataFrame of scaled data\n",
        "    \"\"\"\n",
        "\n",
        "    values = data.values\n",
        "    values = values.astype('float32')\n",
        "    scaler = MinMaxScaler(feature_range=(0,1))\n",
        "    scaled = scaler.fit_transform(values)\n",
        "    scaledDf = pd.DataFrame(scaled,columns=data.columns)\n",
        "    return scaler,scaledDf\n",
        "\n",
        "\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    \"\"\"\n",
        "    Frame a time series as a supervised learning dataset.\n",
        "    Arguments:\n",
        "        data: Sequence of observations as a list or NumPy array.\n",
        "        n_in: Number of lag observations as input (X).\n",
        "        n_out: Number of observations as output (y).\n",
        "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
        "    Returns:\n",
        "        Pandas DataFrame of series framed for supervised learning.\n",
        "    \"\"\"\n",
        "\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = pd.DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # put it all together\n",
        "    agg = pd.concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "\n",
        "    agg.drop(agg.columns[-(df.shape[1]-1):],axis = 1,inplace=True)\n",
        "    return agg\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # Load data with date column preserved\n",
        "    # data = pd.read_csv(\"/content/drive/MyDrive/KLTN/data/VN2008-2020/VN2008-2020.csv\", encoding='utf-8-sig')\n",
        "    data = pd.read_csv(\"/content/drive/MyDrive/VN2008-2025.csv\", encoding='utf-8-sig')\n",
        "\n",
        "    # Store dates separately before dropping the column\n",
        "    dates = data[\"date\"].copy()\n",
        "    data.drop(columns=[\"date\"], inplace=True)\n",
        "    data.fillna(0, inplace=True)\n",
        "\n",
        "    scaler, scaledDf = Scaler(data)\n",
        "\n",
        "    # Add dates back to scaled data for splitting\n",
        "    scaledDf_with_dates = scaledDf.copy()\n",
        "    scaledDf_with_dates['date'] = dates\n",
        "\n",
        "    # Create time series features\n",
        "    reframed = series_to_supervised(scaledDf, n_in=12)\n",
        "\n",
        "    # Add dates back to reframed data (dates correspond to the target time)\n",
        "    # Since we're using 12 months of history, the date for each row should be the last month in the sequence\n",
        "    reframed_dates = dates.reset_index(drop=True)\n",
        "    reframed_with_dates = reframed.copy()\n",
        "    reframed_with_dates['date'] = reframed_dates"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build model"
      ],
      "metadata": {
        "id": "LqU8GngdHofX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzMY1DDXiBRl",
        "outputId": "a45105f4-18bc-41c3-cf07-baa1cb194992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 92/100, Train Loss: 0.0523, Val Loss: 0.0898\n",
            "Epoch 93/100, Train Loss: 0.0534, Val Loss: 0.0898\n",
            "Epoch 94/100, Train Loss: 0.0538, Val Loss: 0.0898\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0517, Val Loss: 0.0898\n",
            "Epoch 96/100, Train Loss: 0.0519, Val Loss: 0.0898\n",
            "Epoch 97/100, Train Loss: 0.0538, Val Loss: 0.0898\n",
            "Epoch 98/100, Train Loss: 0.0537, Val Loss: 0.0898\n",
            "Epoch 99/100, Train Loss: 0.0526, Val Loss: 0.0898\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0517, Val Loss: 0.0898\n",
            "\n",
            "Test iteration 2/3\n",
            "Current training set size: 181 samples\n",
            "Epoch 1/100, Train Loss: 0.1775, Val Loss: 0.3198\n",
            "Epoch 2/100, Train Loss: 0.1536, Val Loss: 0.3267\n",
            "Epoch 3/100, Train Loss: 0.1221, Val Loss: 0.0891\n",
            "Epoch 4/100, Train Loss: 0.0792, Val Loss: 0.0945\n",
            "Epoch 5/100, Train Loss: 0.0696, Val Loss: 0.0914\n",
            "Epoch 6/100, Train Loss: 0.0654, Val Loss: 0.0953\n",
            "Epoch 7/100, Train Loss: 0.0703, Val Loss: 0.0877\n",
            "Epoch 8/100, Train Loss: 0.0652, Val Loss: 0.0886\n",
            "Epoch 9/100, Train Loss: 0.0641, Val Loss: 0.0895\n",
            "Epoch 10/100, Train Loss: 0.0571, Val Loss: 0.0937\n",
            "Epoch 11/100, Train Loss: 0.0558, Val Loss: 0.0837\n",
            "Epoch 12/100, Train Loss: 0.0555, Val Loss: 0.0832\n",
            "Epoch 13/100, Train Loss: 0.0568, Val Loss: 0.0923\n",
            "Epoch 14/100, Train Loss: 0.0583, Val Loss: 0.0833\n",
            "Epoch 15/100, Train Loss: 0.0539, Val Loss: 0.0814\n",
            "Epoch 16/100, Train Loss: 0.0530, Val Loss: 0.0815\n",
            "Epoch 17/100, Train Loss: 0.0543, Val Loss: 0.0750\n",
            "Epoch 18/100, Train Loss: 0.0502, Val Loss: 0.0772\n",
            "Epoch 19/100, Train Loss: 0.0483, Val Loss: 0.0804\n",
            "Epoch 20/100, Train Loss: 0.0529, Val Loss: 0.0682\n",
            "Epoch 21/100, Train Loss: 0.0485, Val Loss: 0.0740\n",
            "Epoch 22/100, Train Loss: 0.0528, Val Loss: 0.1098\n",
            "Epoch 23/100, Train Loss: 0.0502, Val Loss: 0.0723\n",
            "Epoch 24/100, Train Loss: 0.0515, Val Loss: 0.0661\n",
            "Epoch 25/100, Train Loss: 0.0466, Val Loss: 0.0669\n",
            "Epoch 26/100, Train Loss: 0.0452, Val Loss: 0.0722\n",
            "Epoch 27/100, Train Loss: 0.0445, Val Loss: 0.0620\n",
            "Epoch 28/100, Train Loss: 0.0422, Val Loss: 0.0651\n",
            "Epoch 29/100, Train Loss: 0.0425, Val Loss: 0.0689\n",
            "Epoch 30/100, Train Loss: 0.0399, Val Loss: 0.0647\n",
            "Epoch 31/100, Train Loss: 0.0411, Val Loss: 0.0652\n",
            "Epoch 32/100, Train Loss: 0.0424, Val Loss: 0.0592\n",
            "Epoch 33/100, Train Loss: 0.0403, Val Loss: 0.0838\n",
            "Epoch 34/100, Train Loss: 0.0397, Val Loss: 0.0653\n",
            "Epoch 35/100, Train Loss: 0.0415, Val Loss: 0.0616\n",
            "Epoch 36/100, Train Loss: 0.0391, Val Loss: 0.0741\n",
            "Epoch 37/100, Train Loss: 0.0390, Val Loss: 0.0563\n",
            "Epoch 38/100, Train Loss: 0.0377, Val Loss: 0.0563\n",
            "Epoch 39/100, Train Loss: 0.0395, Val Loss: 0.0631\n",
            "Epoch 40/100, Train Loss: 0.0382, Val Loss: 0.0792\n",
            "Epoch 41/100, Train Loss: 0.0363, Val Loss: 0.0563\n",
            "Epoch 42/100, Train Loss: 0.0387, Val Loss: 0.0566\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0339, Val Loss: 0.0563\n",
            "Epoch 44/100, Train Loss: 0.0327, Val Loss: 0.0563\n",
            "Epoch 45/100, Train Loss: 0.0332, Val Loss: 0.0564\n",
            "Epoch 46/100, Train Loss: 0.0330, Val Loss: 0.0559\n",
            "Epoch 47/100, Train Loss: 0.0332, Val Loss: 0.0555\n",
            "Epoch 48/100, Train Loss: 0.0338, Val Loss: 0.0555\n",
            "Epoch 49/100, Train Loss: 0.0322, Val Loss: 0.0555\n",
            "Epoch 50/100, Train Loss: 0.0347, Val Loss: 0.0553\n",
            "Epoch 51/100, Train Loss: 0.0346, Val Loss: 0.0552\n",
            "Epoch 52/100, Train Loss: 0.0320, Val Loss: 0.0550\n",
            "Epoch 53/100, Train Loss: 0.0326, Val Loss: 0.0549\n",
            "Epoch 54/100, Train Loss: 0.0343, Val Loss: 0.0545\n",
            "Epoch 55/100, Train Loss: 0.0333, Val Loss: 0.0545\n",
            "Epoch 56/100, Train Loss: 0.0336, Val Loss: 0.0547\n",
            "Epoch 57/100, Train Loss: 0.0330, Val Loss: 0.0544\n",
            "Epoch 58/100, Train Loss: 0.0350, Val Loss: 0.0542\n",
            "Epoch 59/100, Train Loss: 0.0335, Val Loss: 0.0539\n",
            "Epoch 60/100, Train Loss: 0.0339, Val Loss: 0.0538\n",
            "Epoch 61/100, Train Loss: 0.0317, Val Loss: 0.0540\n",
            "Epoch 62/100, Train Loss: 0.0337, Val Loss: 0.0539\n",
            "Epoch 63/100, Train Loss: 0.0332, Val Loss: 0.0534\n",
            "Epoch 64/100, Train Loss: 0.0341, Val Loss: 0.0535\n",
            "Epoch 65/100, Train Loss: 0.0339, Val Loss: 0.0535\n",
            "Epoch 66/100, Train Loss: 0.0338, Val Loss: 0.0536\n",
            "Epoch 67/100, Train Loss: 0.0312, Val Loss: 0.0536\n",
            "Epoch 68/100, Train Loss: 0.0328, Val Loss: 0.0532\n",
            "Epoch 69/100, Train Loss: 0.0309, Val Loss: 0.0529\n",
            "Epoch 70/100, Train Loss: 0.0315, Val Loss: 0.0530\n",
            "Epoch 71/100, Train Loss: 0.0315, Val Loss: 0.0527\n",
            "Epoch 72/100, Train Loss: 0.0327, Val Loss: 0.0526\n",
            "Epoch 73/100, Train Loss: 0.0350, Val Loss: 0.0524\n",
            "Epoch 74/100, Train Loss: 0.0330, Val Loss: 0.0522\n",
            "Epoch 75/100, Train Loss: 0.0318, Val Loss: 0.0522\n",
            "Epoch 76/100, Train Loss: 0.0311, Val Loss: 0.0525\n",
            "Epoch 77/100, Train Loss: 0.0310, Val Loss: 0.0523\n",
            "Epoch 78/100, Train Loss: 0.0317, Val Loss: 0.0521\n",
            "Epoch 79/100, Train Loss: 0.0326, Val Loss: 0.0519\n",
            "Epoch 80/100, Train Loss: 0.0327, Val Loss: 0.0520\n",
            "Epoch 81/100, Train Loss: 0.0342, Val Loss: 0.0517\n",
            "Epoch 82/100, Train Loss: 0.0321, Val Loss: 0.0517\n",
            "Epoch 83/100, Train Loss: 0.0335, Val Loss: 0.0515\n",
            "Epoch 84/100, Train Loss: 0.0318, Val Loss: 0.0512\n",
            "Epoch 85/100, Train Loss: 0.0336, Val Loss: 0.0513\n",
            "Epoch 86/100, Train Loss: 0.0321, Val Loss: 0.0513\n",
            "Epoch 87/100, Train Loss: 0.0323, Val Loss: 0.0513\n",
            "Epoch 88/100, Train Loss: 0.0318, Val Loss: 0.0513\n",
            "Epoch 89/100, Train Loss: 0.0354, Val Loss: 0.0514\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0310, Val Loss: 0.0514\n",
            "Epoch 91/100, Train Loss: 0.0323, Val Loss: 0.0514\n",
            "Epoch 92/100, Train Loss: 0.0319, Val Loss: 0.0514\n",
            "Epoch 93/100, Train Loss: 0.0320, Val Loss: 0.0514\n",
            "Epoch 94/100, Train Loss: 0.0328, Val Loss: 0.0514\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0322, Val Loss: 0.0514\n",
            "Epoch 96/100, Train Loss: 0.0326, Val Loss: 0.0514\n",
            "Epoch 97/100, Train Loss: 0.0332, Val Loss: 0.0514\n",
            "Epoch 98/100, Train Loss: 0.0334, Val Loss: 0.0514\n",
            "Epoch 99/100, Train Loss: 0.0325, Val Loss: 0.0514\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0312, Val Loss: 0.0514\n",
            "\n",
            "Test iteration 3/3\n",
            "Current training set size: 182 samples\n",
            "Epoch 1/100, Train Loss: 0.1948, Val Loss: 0.3851\n",
            "Epoch 2/100, Train Loss: 0.1564, Val Loss: 0.3181\n",
            "Epoch 3/100, Train Loss: 0.1108, Val Loss: 0.1040\n",
            "Epoch 4/100, Train Loss: 0.0702, Val Loss: 0.0975\n",
            "Epoch 5/100, Train Loss: 0.0729, Val Loss: 0.0946\n",
            "Epoch 6/100, Train Loss: 0.0666, Val Loss: 0.0899\n",
            "Epoch 7/100, Train Loss: 0.0618, Val Loss: 0.0885\n",
            "Epoch 8/100, Train Loss: 0.0582, Val Loss: 0.0903\n",
            "Epoch 9/100, Train Loss: 0.0591, Val Loss: 0.0887\n",
            "Epoch 10/100, Train Loss: 0.0564, Val Loss: 0.0889\n",
            "Epoch 11/100, Train Loss: 0.0592, Val Loss: 0.0856\n",
            "Epoch 12/100, Train Loss: 0.0555, Val Loss: 0.0874\n",
            "Epoch 13/100, Train Loss: 0.0556, Val Loss: 0.0833\n",
            "Epoch 14/100, Train Loss: 0.0533, Val Loss: 0.0789\n",
            "Epoch 15/100, Train Loss: 0.0523, Val Loss: 0.0788\n",
            "Epoch 16/100, Train Loss: 0.0545, Val Loss: 0.0843\n",
            "Epoch 17/100, Train Loss: 0.0499, Val Loss: 0.0941\n",
            "Epoch 18/100, Train Loss: 0.0448, Val Loss: 0.0745\n",
            "Epoch 19/100, Train Loss: 0.0455, Val Loss: 0.0759\n",
            "Epoch 20/100, Train Loss: 0.0437, Val Loss: 0.0786\n",
            "Epoch 21/100, Train Loss: 0.0435, Val Loss: 0.0737\n",
            "Epoch 22/100, Train Loss: 0.0448, Val Loss: 0.0699\n",
            "Epoch 23/100, Train Loss: 0.0470, Val Loss: 0.0652\n",
            "Epoch 24/100, Train Loss: 0.0455, Val Loss: 0.0881\n",
            "Epoch 25/100, Train Loss: 0.0415, Val Loss: 0.0638\n",
            "Epoch 26/100, Train Loss: 0.0417, Val Loss: 0.0831\n",
            "Epoch 27/100, Train Loss: 0.0428, Val Loss: 0.0569\n",
            "Epoch 28/100, Train Loss: 0.0459, Val Loss: 0.0554\n",
            "Epoch 29/100, Train Loss: 0.0429, Val Loss: 0.0649\n",
            "Epoch 30/100, Train Loss: 0.0376, Val Loss: 0.0757\n",
            "Epoch 31/100, Train Loss: 0.0424, Val Loss: 0.0630\n",
            "Epoch 32/100, Train Loss: 0.0402, Val Loss: 0.0746\n",
            "Epoch 33/100, Train Loss: 0.0379, Val Loss: 0.0496\n",
            "Epoch 34/100, Train Loss: 0.0386, Val Loss: 0.0593\n",
            "Epoch 35/100, Train Loss: 0.0468, Val Loss: 0.0606\n",
            "Epoch 36/100, Train Loss: 0.0391, Val Loss: 0.0513\n",
            "Epoch 37/100, Train Loss: 0.0382, Val Loss: 0.0723\n",
            "Epoch 38/100, Train Loss: 0.0425, Val Loss: 0.0542\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0354, Val Loss: 0.0547\n",
            "Epoch 40/100, Train Loss: 0.0370, Val Loss: 0.0554\n",
            "Epoch 41/100, Train Loss: 0.0335, Val Loss: 0.0563\n",
            "Epoch 42/100, Train Loss: 0.0339, Val Loss: 0.0564\n",
            "Epoch 43/100, Train Loss: 0.0327, Val Loss: 0.0565\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0340, Val Loss: 0.0565\n",
            "Epoch 45/100, Train Loss: 0.0329, Val Loss: 0.0565\n",
            "Epoch 46/100, Train Loss: 0.0326, Val Loss: 0.0565\n",
            "Epoch 47/100, Train Loss: 0.0347, Val Loss: 0.0565\n",
            "Epoch 48/100, Train Loss: 0.0353, Val Loss: 0.0565\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0341, Val Loss: 0.0565\n",
            "Epoch 50/100, Train Loss: 0.0347, Val Loss: 0.0565\n",
            "Epoch 51/100, Train Loss: 0.0336, Val Loss: 0.0565\n",
            "Epoch 52/100, Train Loss: 0.0342, Val Loss: 0.0565\n",
            "Epoch 53/100, Train Loss: 0.0337, Val Loss: 0.0565\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0336, Val Loss: 0.0565\n",
            "Epoch 55/100, Train Loss: 0.0356, Val Loss: 0.0565\n",
            "Epoch 56/100, Train Loss: 0.0332, Val Loss: 0.0565\n",
            "Epoch 57/100, Train Loss: 0.0333, Val Loss: 0.0565\n",
            "Epoch 58/100, Train Loss: 0.0342, Val Loss: 0.0565\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0337, Val Loss: 0.0565\n",
            "Epoch 60/100, Train Loss: 0.0338, Val Loss: 0.0565\n",
            "Epoch 61/100, Train Loss: 0.0327, Val Loss: 0.0565\n",
            "Epoch 62/100, Train Loss: 0.0331, Val Loss: 0.0565\n",
            "Epoch 63/100, Train Loss: 0.0330, Val Loss: 0.0565\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0329, Val Loss: 0.0565\n",
            "Epoch 65/100, Train Loss: 0.0344, Val Loss: 0.0565\n",
            "Epoch 66/100, Train Loss: 0.0351, Val Loss: 0.0565\n",
            "Epoch 67/100, Train Loss: 0.0341, Val Loss: 0.0565\n",
            "Epoch 68/100, Train Loss: 0.0349, Val Loss: 0.0565\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0335, Val Loss: 0.0565\n",
            "Epoch 70/100, Train Loss: 0.0331, Val Loss: 0.0565\n",
            "Epoch 71/100, Train Loss: 0.0329, Val Loss: 0.0565\n",
            "Epoch 72/100, Train Loss: 0.0335, Val Loss: 0.0565\n",
            "Epoch 73/100, Train Loss: 0.0349, Val Loss: 0.0565\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0351, Val Loss: 0.0565\n",
            "Epoch 75/100, Train Loss: 0.0312, Val Loss: 0.0565\n",
            "Epoch 76/100, Train Loss: 0.0338, Val Loss: 0.0565\n",
            "Epoch 77/100, Train Loss: 0.0330, Val Loss: 0.0565\n",
            "Epoch 78/100, Train Loss: 0.0337, Val Loss: 0.0565\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0350, Val Loss: 0.0565\n",
            "Epoch 80/100, Train Loss: 0.0348, Val Loss: 0.0565\n",
            "Epoch 81/100, Train Loss: 0.0337, Val Loss: 0.0565\n",
            "Epoch 82/100, Train Loss: 0.0346, Val Loss: 0.0565\n",
            "Epoch 83/100, Train Loss: 0.0340, Val Loss: 0.0565\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0321, Val Loss: 0.0565\n",
            "Epoch 85/100, Train Loss: 0.0327, Val Loss: 0.0565\n",
            "Epoch 86/100, Train Loss: 0.0340, Val Loss: 0.0565\n",
            "Epoch 87/100, Train Loss: 0.0333, Val Loss: 0.0565\n",
            "Epoch 88/100, Train Loss: 0.0341, Val Loss: 0.0565\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0339, Val Loss: 0.0565\n",
            "Epoch 90/100, Train Loss: 0.0343, Val Loss: 0.0565\n",
            "Epoch 91/100, Train Loss: 0.0332, Val Loss: 0.0565\n",
            "Epoch 92/100, Train Loss: 0.0336, Val Loss: 0.0565\n",
            "Epoch 93/100, Train Loss: 0.0339, Val Loss: 0.0565\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0340, Val Loss: 0.0565\n",
            "Epoch 95/100, Train Loss: 0.0342, Val Loss: 0.0565\n",
            "Epoch 96/100, Train Loss: 0.0324, Val Loss: 0.0565\n",
            "Epoch 97/100, Train Loss: 0.0336, Val Loss: 0.0565\n",
            "Epoch 98/100, Train Loss: 0.0330, Val Loss: 0.0565\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0339, Val Loss: 0.0565\n",
            "Epoch 100/100, Train Loss: 0.0346, Val Loss: 0.0565\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: hybrid, Units: L128_G8, Dropout: 0.2, Dense Units: 64\n",
            "Batch Size: 4, Epochs: 100\n",
            "Device: cuda\n",
            "Total test samples: 3\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/3\n",
            "Current training set size: 180 samples\n",
            "Epoch 1/100, Train Loss: 0.1720, Val Loss: 0.3567\n",
            "Epoch 2/100, Train Loss: 0.1395, Val Loss: 0.2762\n",
            "Epoch 3/100, Train Loss: 0.0965, Val Loss: 0.0935\n",
            "Epoch 4/100, Train Loss: 0.0737, Val Loss: 0.0940\n",
            "Epoch 5/100, Train Loss: 0.0665, Val Loss: 0.0879\n",
            "Epoch 6/100, Train Loss: 0.0622, Val Loss: 0.0890\n",
            "Epoch 7/100, Train Loss: 0.0719, Val Loss: 0.0899\n",
            "Epoch 8/100, Train Loss: 0.0547, Val Loss: 0.0896\n",
            "Epoch 9/100, Train Loss: 0.0599, Val Loss: 0.0878\n",
            "Epoch 10/100, Train Loss: 0.0495, Val Loss: 0.0863\n",
            "Epoch 11/100, Train Loss: 0.0553, Val Loss: 0.0894\n",
            "Epoch 12/100, Train Loss: 0.0582, Val Loss: 0.0828\n",
            "Epoch 13/100, Train Loss: 0.0502, Val Loss: 0.0819\n",
            "Epoch 14/100, Train Loss: 0.0513, Val Loss: 0.0855\n",
            "Epoch 15/100, Train Loss: 0.0452, Val Loss: 0.0798\n",
            "Epoch 16/100, Train Loss: 0.0484, Val Loss: 0.0804\n",
            "Epoch 17/100, Train Loss: 0.0527, Val Loss: 0.0741\n",
            "Epoch 18/100, Train Loss: 0.0437, Val Loss: 0.1041\n",
            "Epoch 19/100, Train Loss: 0.0495, Val Loss: 0.0738\n",
            "Epoch 20/100, Train Loss: 0.0452, Val Loss: 0.0702\n",
            "Epoch 21/100, Train Loss: 0.0427, Val Loss: 0.0731\n",
            "Epoch 22/100, Train Loss: 0.0420, Val Loss: 0.0868\n",
            "Epoch 23/100, Train Loss: 0.0427, Val Loss: 0.0683\n",
            "Epoch 24/100, Train Loss: 0.0472, Val Loss: 0.0705\n",
            "Epoch 25/100, Train Loss: 0.0425, Val Loss: 0.0874\n",
            "Epoch 26/100, Train Loss: 0.0434, Val Loss: 0.0607\n",
            "Epoch 27/100, Train Loss: 0.0428, Val Loss: 0.0627\n",
            "Epoch 28/100, Train Loss: 0.0445, Val Loss: 0.0720\n",
            "Epoch 29/100, Train Loss: 0.0417, Val Loss: 0.0684\n",
            "Epoch 30/100, Train Loss: 0.0396, Val Loss: 0.0604\n",
            "Epoch 31/100, Train Loss: 0.0392, Val Loss: 0.0666\n",
            "Epoch 32/100, Train Loss: 0.0438, Val Loss: 0.0744\n",
            "Epoch 33/100, Train Loss: 0.0400, Val Loss: 0.0642\n",
            "Epoch 34/100, Train Loss: 0.0410, Val Loss: 0.0593\n",
            "Epoch 35/100, Train Loss: 0.0424, Val Loss: 0.0534\n",
            "Epoch 36/100, Train Loss: 0.0405, Val Loss: 0.0560\n",
            "Epoch 37/100, Train Loss: 0.0416, Val Loss: 0.0641\n",
            "Epoch 38/100, Train Loss: 0.0370, Val Loss: 0.0829\n",
            "Epoch 39/100, Train Loss: 0.0393, Val Loss: 0.0554\n",
            "Epoch 40/100, Train Loss: 0.0380, Val Loss: 0.0640\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0482, Val Loss: 0.0662\n",
            "Epoch 42/100, Train Loss: 0.0412, Val Loss: 0.0686\n",
            "Epoch 43/100, Train Loss: 0.0380, Val Loss: 0.0694\n",
            "Epoch 44/100, Train Loss: 0.0373, Val Loss: 0.0684\n",
            "Epoch 45/100, Train Loss: 0.0349, Val Loss: 0.0670\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0349, Val Loss: 0.0670\n",
            "Epoch 47/100, Train Loss: 0.0347, Val Loss: 0.0670\n",
            "Epoch 48/100, Train Loss: 0.0354, Val Loss: 0.0669\n",
            "Epoch 49/100, Train Loss: 0.0346, Val Loss: 0.0669\n",
            "Epoch 50/100, Train Loss: 0.0341, Val Loss: 0.0669\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0357, Val Loss: 0.0669\n",
            "Epoch 52/100, Train Loss: 0.0336, Val Loss: 0.0669\n",
            "Epoch 53/100, Train Loss: 0.0352, Val Loss: 0.0669\n",
            "Epoch 54/100, Train Loss: 0.0362, Val Loss: 0.0669\n",
            "Epoch 55/100, Train Loss: 0.0354, Val Loss: 0.0669\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0346, Val Loss: 0.0669\n",
            "Epoch 57/100, Train Loss: 0.0356, Val Loss: 0.0669\n",
            "Epoch 58/100, Train Loss: 0.0363, Val Loss: 0.0669\n",
            "Epoch 59/100, Train Loss: 0.0362, Val Loss: 0.0669\n",
            "Epoch 60/100, Train Loss: 0.0354, Val Loss: 0.0669\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0355, Val Loss: 0.0669\n",
            "Epoch 62/100, Train Loss: 0.0350, Val Loss: 0.0669\n",
            "Epoch 63/100, Train Loss: 0.0357, Val Loss: 0.0669\n",
            "Epoch 64/100, Train Loss: 0.0332, Val Loss: 0.0669\n",
            "Epoch 65/100, Train Loss: 0.0339, Val Loss: 0.0669\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0337, Val Loss: 0.0669\n",
            "Epoch 67/100, Train Loss: 0.0356, Val Loss: 0.0669\n",
            "Epoch 68/100, Train Loss: 0.0347, Val Loss: 0.0669\n",
            "Epoch 69/100, Train Loss: 0.0371, Val Loss: 0.0669\n",
            "Epoch 70/100, Train Loss: 0.0340, Val Loss: 0.0669\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0345, Val Loss: 0.0669\n",
            "Epoch 72/100, Train Loss: 0.0342, Val Loss: 0.0669\n",
            "Epoch 73/100, Train Loss: 0.0353, Val Loss: 0.0669\n",
            "Epoch 74/100, Train Loss: 0.0358, Val Loss: 0.0669\n",
            "Epoch 75/100, Train Loss: 0.0349, Val Loss: 0.0669\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0359, Val Loss: 0.0669\n",
            "Epoch 77/100, Train Loss: 0.0338, Val Loss: 0.0669\n",
            "Epoch 78/100, Train Loss: 0.0355, Val Loss: 0.0669\n",
            "Epoch 79/100, Train Loss: 0.0371, Val Loss: 0.0669\n",
            "Epoch 80/100, Train Loss: 0.0357, Val Loss: 0.0669\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0342, Val Loss: 0.0669\n",
            "Epoch 82/100, Train Loss: 0.0353, Val Loss: 0.0669\n",
            "Epoch 83/100, Train Loss: 0.0346, Val Loss: 0.0669\n",
            "Epoch 84/100, Train Loss: 0.0347, Val Loss: 0.0669\n",
            "Epoch 85/100, Train Loss: 0.0365, Val Loss: 0.0669\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0349, Val Loss: 0.0669\n",
            "Epoch 87/100, Train Loss: 0.0348, Val Loss: 0.0669\n",
            "Epoch 88/100, Train Loss: 0.0349, Val Loss: 0.0669\n",
            "Epoch 89/100, Train Loss: 0.0343, Val Loss: 0.0669\n",
            "Epoch 90/100, Train Loss: 0.0370, Val Loss: 0.0669\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0362, Val Loss: 0.0669\n",
            "Epoch 92/100, Train Loss: 0.0354, Val Loss: 0.0669\n",
            "Epoch 93/100, Train Loss: 0.0350, Val Loss: 0.0669\n",
            "Epoch 94/100, Train Loss: 0.0345, Val Loss: 0.0669\n",
            "Epoch 95/100, Train Loss: 0.0350, Val Loss: 0.0669\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0335, Val Loss: 0.0669\n",
            "Epoch 97/100, Train Loss: 0.0363, Val Loss: 0.0669\n",
            "Epoch 98/100, Train Loss: 0.0355, Val Loss: 0.0669\n",
            "Epoch 99/100, Train Loss: 0.0344, Val Loss: 0.0669\n",
            "Epoch 100/100, Train Loss: 0.0336, Val Loss: 0.0669\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 2/3\n",
            "Current training set size: 181 samples\n",
            "Epoch 1/100, Train Loss: 0.1759, Val Loss: 0.3065\n",
            "Epoch 2/100, Train Loss: 0.1389, Val Loss: 0.1814\n",
            "Epoch 3/100, Train Loss: 0.0947, Val Loss: 0.1059\n",
            "Epoch 4/100, Train Loss: 0.0747, Val Loss: 0.0902\n",
            "Epoch 5/100, Train Loss: 0.0690, Val Loss: 0.0898\n",
            "Epoch 6/100, Train Loss: 0.0621, Val Loss: 0.0904\n",
            "Epoch 7/100, Train Loss: 0.0670, Val Loss: 0.0879\n",
            "Epoch 8/100, Train Loss: 0.0695, Val Loss: 0.0883\n",
            "Epoch 9/100, Train Loss: 0.0589, Val Loss: 0.0869\n",
            "Epoch 10/100, Train Loss: 0.0553, Val Loss: 0.0981\n",
            "Epoch 11/100, Train Loss: 0.0521, Val Loss: 0.0828\n",
            "Epoch 12/100, Train Loss: 0.0569, Val Loss: 0.0837\n",
            "Epoch 13/100, Train Loss: 0.0569, Val Loss: 0.0813\n",
            "Epoch 14/100, Train Loss: 0.0507, Val Loss: 0.0790\n",
            "Epoch 15/100, Train Loss: 0.0516, Val Loss: 0.0907\n",
            "Epoch 16/100, Train Loss: 0.0488, Val Loss: 0.0958\n",
            "Epoch 17/100, Train Loss: 0.0525, Val Loss: 0.0692\n",
            "Epoch 18/100, Train Loss: 0.0445, Val Loss: 0.0707\n",
            "Epoch 19/100, Train Loss: 0.0427, Val Loss: 0.0738\n",
            "Epoch 20/100, Train Loss: 0.0423, Val Loss: 0.0647\n",
            "Epoch 21/100, Train Loss: 0.0433, Val Loss: 0.0637\n",
            "Epoch 22/100, Train Loss: 0.0482, Val Loss: 0.0763\n",
            "Epoch 23/100, Train Loss: 0.0471, Val Loss: 0.0621\n",
            "Epoch 24/100, Train Loss: 0.0469, Val Loss: 0.0674\n",
            "Epoch 25/100, Train Loss: 0.0429, Val Loss: 0.0639\n",
            "Epoch 26/100, Train Loss: 0.0443, Val Loss: 0.0796\n",
            "Epoch 27/100, Train Loss: 0.0408, Val Loss: 0.0589\n",
            "Epoch 28/100, Train Loss: 0.0388, Val Loss: 0.0668\n",
            "Epoch 29/100, Train Loss: 0.0406, Val Loss: 0.0664\n",
            "Epoch 30/100, Train Loss: 0.0384, Val Loss: 0.0707\n",
            "Epoch 31/100, Train Loss: 0.0377, Val Loss: 0.0620\n",
            "Epoch 32/100, Train Loss: 0.0421, Val Loss: 0.0608\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0426, Val Loss: 0.0617\n",
            "Epoch 34/100, Train Loss: 0.0373, Val Loss: 0.0630\n",
            "Epoch 35/100, Train Loss: 0.0366, Val Loss: 0.0638\n",
            "Epoch 36/100, Train Loss: 0.0342, Val Loss: 0.0643\n",
            "Epoch 37/100, Train Loss: 0.0335, Val Loss: 0.0649\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0339, Val Loss: 0.0649\n",
            "Epoch 39/100, Train Loss: 0.0341, Val Loss: 0.0649\n",
            "Epoch 40/100, Train Loss: 0.0348, Val Loss: 0.0649\n",
            "Epoch 41/100, Train Loss: 0.0347, Val Loss: 0.0649\n",
            "Epoch 42/100, Train Loss: 0.0338, Val Loss: 0.0649\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0332, Val Loss: 0.0649\n",
            "Epoch 44/100, Train Loss: 0.0349, Val Loss: 0.0649\n",
            "Epoch 45/100, Train Loss: 0.0338, Val Loss: 0.0649\n",
            "Epoch 46/100, Train Loss: 0.0340, Val Loss: 0.0649\n",
            "Epoch 47/100, Train Loss: 0.0332, Val Loss: 0.0649\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0360, Val Loss: 0.0649\n",
            "Epoch 49/100, Train Loss: 0.0346, Val Loss: 0.0649\n",
            "Epoch 50/100, Train Loss: 0.0348, Val Loss: 0.0649\n",
            "Epoch 51/100, Train Loss: 0.0353, Val Loss: 0.0649\n",
            "Epoch 52/100, Train Loss: 0.0330, Val Loss: 0.0649\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0330, Val Loss: 0.0649\n",
            "Epoch 54/100, Train Loss: 0.0343, Val Loss: 0.0649\n",
            "Epoch 55/100, Train Loss: 0.0341, Val Loss: 0.0649\n",
            "Epoch 56/100, Train Loss: 0.0336, Val Loss: 0.0649\n",
            "Epoch 57/100, Train Loss: 0.0340, Val Loss: 0.0649\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0356, Val Loss: 0.0649\n",
            "Epoch 59/100, Train Loss: 0.0346, Val Loss: 0.0649\n",
            "Epoch 60/100, Train Loss: 0.0341, Val Loss: 0.0649\n",
            "Epoch 61/100, Train Loss: 0.0337, Val Loss: 0.0649\n",
            "Epoch 62/100, Train Loss: 0.0342, Val Loss: 0.0649\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0348, Val Loss: 0.0649\n",
            "Epoch 64/100, Train Loss: 0.0347, Val Loss: 0.0649\n",
            "Epoch 65/100, Train Loss: 0.0348, Val Loss: 0.0649\n",
            "Epoch 66/100, Train Loss: 0.0359, Val Loss: 0.0649\n",
            "Epoch 67/100, Train Loss: 0.0325, Val Loss: 0.0649\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0347, Val Loss: 0.0649\n",
            "Epoch 69/100, Train Loss: 0.0331, Val Loss: 0.0649\n",
            "Epoch 70/100, Train Loss: 0.0328, Val Loss: 0.0649\n",
            "Epoch 71/100, Train Loss: 0.0330, Val Loss: 0.0649\n",
            "Epoch 72/100, Train Loss: 0.0348, Val Loss: 0.0649\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0360, Val Loss: 0.0649\n",
            "Epoch 74/100, Train Loss: 0.0338, Val Loss: 0.0649\n",
            "Epoch 75/100, Train Loss: 0.0348, Val Loss: 0.0649\n",
            "Epoch 76/100, Train Loss: 0.0324, Val Loss: 0.0649\n",
            "Epoch 77/100, Train Loss: 0.0331, Val Loss: 0.0649\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0337, Val Loss: 0.0649\n",
            "Epoch 79/100, Train Loss: 0.0349, Val Loss: 0.0649\n",
            "Epoch 80/100, Train Loss: 0.0338, Val Loss: 0.0649\n",
            "Epoch 81/100, Train Loss: 0.0360, Val Loss: 0.0649\n",
            "Epoch 82/100, Train Loss: 0.0343, Val Loss: 0.0649\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0337, Val Loss: 0.0649\n",
            "Epoch 84/100, Train Loss: 0.0331, Val Loss: 0.0649\n",
            "Epoch 85/100, Train Loss: 0.0362, Val Loss: 0.0649\n",
            "Epoch 86/100, Train Loss: 0.0335, Val Loss: 0.0649\n",
            "Epoch 87/100, Train Loss: 0.0346, Val Loss: 0.0649\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0333, Val Loss: 0.0649\n",
            "Epoch 89/100, Train Loss: 0.0370, Val Loss: 0.0649\n",
            "Epoch 90/100, Train Loss: 0.0338, Val Loss: 0.0649\n",
            "Epoch 91/100, Train Loss: 0.0344, Val Loss: 0.0649\n",
            "Epoch 92/100, Train Loss: 0.0339, Val Loss: 0.0649\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0352, Val Loss: 0.0649\n",
            "Epoch 94/100, Train Loss: 0.0352, Val Loss: 0.0649\n",
            "Epoch 95/100, Train Loss: 0.0331, Val Loss: 0.0649\n",
            "Epoch 96/100, Train Loss: 0.0336, Val Loss: 0.0649\n",
            "Epoch 97/100, Train Loss: 0.0348, Val Loss: 0.0649\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0347, Val Loss: 0.0649\n",
            "Epoch 99/100, Train Loss: 0.0343, Val Loss: 0.0649\n",
            "Epoch 100/100, Train Loss: 0.0330, Val Loss: 0.0649\n",
            "\n",
            "Test iteration 3/3\n",
            "Current training set size: 182 samples\n",
            "Epoch 1/100, Train Loss: 0.1894, Val Loss: 0.3696\n",
            "Epoch 2/100, Train Loss: 0.1513, Val Loss: 0.2522\n",
            "Epoch 3/100, Train Loss: 0.1011, Val Loss: 0.0941\n",
            "Epoch 4/100, Train Loss: 0.0708, Val Loss: 0.1003\n",
            "Epoch 5/100, Train Loss: 0.0688, Val Loss: 0.0981\n",
            "Epoch 6/100, Train Loss: 0.0668, Val Loss: 0.0898\n",
            "Epoch 7/100, Train Loss: 0.0631, Val Loss: 0.0878\n",
            "Epoch 8/100, Train Loss: 0.0573, Val Loss: 0.0877\n",
            "Epoch 9/100, Train Loss: 0.0606, Val Loss: 0.0872\n",
            "Epoch 10/100, Train Loss: 0.0515, Val Loss: 0.0858\n",
            "Epoch 11/100, Train Loss: 0.0587, Val Loss: 0.0826\n",
            "Epoch 12/100, Train Loss: 0.0502, Val Loss: 0.0837\n",
            "Epoch 13/100, Train Loss: 0.0547, Val Loss: 0.0823\n",
            "Epoch 14/100, Train Loss: 0.0515, Val Loss: 0.0769\n",
            "Epoch 15/100, Train Loss: 0.0463, Val Loss: 0.0741\n",
            "Epoch 16/100, Train Loss: 0.0480, Val Loss: 0.0867\n",
            "Epoch 17/100, Train Loss: 0.0460, Val Loss: 0.0779\n",
            "Epoch 18/100, Train Loss: 0.0451, Val Loss: 0.0751\n",
            "Epoch 19/100, Train Loss: 0.0442, Val Loss: 0.0778\n",
            "Epoch 20/100, Train Loss: 0.0413, Val Loss: 0.0841\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0454, Val Loss: 0.0784\n",
            "Epoch 22/100, Train Loss: 0.0409, Val Loss: 0.0750\n",
            "Epoch 23/100, Train Loss: 0.0404, Val Loss: 0.0739\n",
            "Epoch 24/100, Train Loss: 0.0409, Val Loss: 0.0728\n",
            "Epoch 25/100, Train Loss: 0.0390, Val Loss: 0.0724\n",
            "Epoch 26/100, Train Loss: 0.0403, Val Loss: 0.0724\n",
            "Epoch 27/100, Train Loss: 0.0394, Val Loss: 0.0723\n",
            "Epoch 28/100, Train Loss: 0.0413, Val Loss: 0.0727\n",
            "Epoch 29/100, Train Loss: 0.0398, Val Loss: 0.0723\n",
            "Epoch 30/100, Train Loss: 0.0391, Val Loss: 0.0730\n",
            "Epoch 31/100, Train Loss: 0.0404, Val Loss: 0.0736\n",
            "Epoch 32/100, Train Loss: 0.0391, Val Loss: 0.0723\n",
            "Epoch 33/100, Train Loss: 0.0392, Val Loss: 0.0731\n",
            "Epoch 34/100, Train Loss: 0.0383, Val Loss: 0.0727\n",
            "Epoch 35/100, Train Loss: 0.0389, Val Loss: 0.0723\n",
            "Epoch 36/100, Train Loss: 0.0394, Val Loss: 0.0725\n",
            "Epoch 37/100, Train Loss: 0.0423, Val Loss: 0.0722\n",
            "Epoch 38/100, Train Loss: 0.0412, Val Loss: 0.0721\n",
            "Epoch 39/100, Train Loss: 0.0388, Val Loss: 0.0720\n",
            "Epoch 40/100, Train Loss: 0.0397, Val Loss: 0.0720\n",
            "Epoch 41/100, Train Loss: 0.0385, Val Loss: 0.0722\n",
            "Epoch 42/100, Train Loss: 0.0396, Val Loss: 0.0725\n",
            "Epoch 43/100, Train Loss: 0.0381, Val Loss: 0.0721\n",
            "Epoch 44/100, Train Loss: 0.0402, Val Loss: 0.0718\n",
            "Epoch 45/100, Train Loss: 0.0381, Val Loss: 0.0718\n",
            "Epoch 46/100, Train Loss: 0.0368, Val Loss: 0.0718\n",
            "Epoch 47/100, Train Loss: 0.0397, Val Loss: 0.0717\n",
            "Epoch 48/100, Train Loss: 0.0400, Val Loss: 0.0717\n",
            "Epoch 49/100, Train Loss: 0.0394, Val Loss: 0.0717\n",
            "Epoch 50/100, Train Loss: 0.0388, Val Loss: 0.0714\n",
            "Epoch 51/100, Train Loss: 0.0387, Val Loss: 0.0714\n",
            "Epoch 52/100, Train Loss: 0.0382, Val Loss: 0.0716\n",
            "Epoch 53/100, Train Loss: 0.0386, Val Loss: 0.0715\n",
            "Epoch 54/100, Train Loss: 0.0374, Val Loss: 0.0715\n",
            "Epoch 55/100, Train Loss: 0.0383, Val Loss: 0.0712\n",
            "Epoch 56/100, Train Loss: 0.0391, Val Loss: 0.0713\n",
            "Epoch 57/100, Train Loss: 0.0413, Val Loss: 0.0712\n",
            "Epoch 58/100, Train Loss: 0.0387, Val Loss: 0.0712\n",
            "Epoch 59/100, Train Loss: 0.0391, Val Loss: 0.0710\n",
            "Epoch 60/100, Train Loss: 0.0387, Val Loss: 0.0708\n",
            "Epoch 61/100, Train Loss: 0.0381, Val Loss: 0.0708\n",
            "Epoch 62/100, Train Loss: 0.0377, Val Loss: 0.0708\n",
            "Epoch 63/100, Train Loss: 0.0379, Val Loss: 0.0705\n",
            "Epoch 64/100, Train Loss: 0.0386, Val Loss: 0.0706\n",
            "Epoch 65/100, Train Loss: 0.0398, Val Loss: 0.0705\n",
            "Epoch 66/100, Train Loss: 0.0398, Val Loss: 0.0703\n",
            "Epoch 67/100, Train Loss: 0.0382, Val Loss: 0.0703\n",
            "Epoch 68/100, Train Loss: 0.0389, Val Loss: 0.0703\n",
            "Epoch 69/100, Train Loss: 0.0384, Val Loss: 0.0703\n",
            "Epoch 70/100, Train Loss: 0.0383, Val Loss: 0.0705\n",
            "Epoch 71/100, Train Loss: 0.0382, Val Loss: 0.0705\n",
            "Epoch 72/100, Train Loss: 0.0371, Val Loss: 0.0701\n",
            "Epoch 73/100, Train Loss: 0.0397, Val Loss: 0.0701\n",
            "Epoch 74/100, Train Loss: 0.0408, Val Loss: 0.0700\n",
            "Epoch 75/100, Train Loss: 0.0364, Val Loss: 0.0699\n",
            "Epoch 76/100, Train Loss: 0.0385, Val Loss: 0.0700\n",
            "Epoch 77/100, Train Loss: 0.0388, Val Loss: 0.0697\n",
            "Epoch 78/100, Train Loss: 0.0378, Val Loss: 0.0698\n",
            "Epoch 79/100, Train Loss: 0.0379, Val Loss: 0.0698\n",
            "Epoch 80/100, Train Loss: 0.0393, Val Loss: 0.0696\n",
            "Epoch 81/100, Train Loss: 0.0381, Val Loss: 0.0695\n",
            "Epoch 82/100, Train Loss: 0.0393, Val Loss: 0.0697\n",
            "Epoch 83/100, Train Loss: 0.0387, Val Loss: 0.0697\n",
            "Epoch 84/100, Train Loss: 0.0365, Val Loss: 0.0697\n",
            "Epoch 85/100, Train Loss: 0.0367, Val Loss: 0.0696\n",
            "Epoch 86/100, Train Loss: 0.0388, Val Loss: 0.0697\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0360, Val Loss: 0.0697\n",
            "Epoch 88/100, Train Loss: 0.0378, Val Loss: 0.0697\n",
            "Epoch 89/100, Train Loss: 0.0381, Val Loss: 0.0697\n",
            "Epoch 90/100, Train Loss: 0.0377, Val Loss: 0.0697\n",
            "Epoch 91/100, Train Loss: 0.0381, Val Loss: 0.0697\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0380, Val Loss: 0.0697\n",
            "Epoch 93/100, Train Loss: 0.0388, Val Loss: 0.0697\n",
            "Epoch 94/100, Train Loss: 0.0378, Val Loss: 0.0697\n",
            "Epoch 95/100, Train Loss: 0.0384, Val Loss: 0.0697\n",
            "Epoch 96/100, Train Loss: 0.0373, Val Loss: 0.0697\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0377, Val Loss: 0.0697\n",
            "Epoch 98/100, Train Loss: 0.0384, Val Loss: 0.0697\n",
            "Epoch 99/100, Train Loss: 0.0379, Val Loss: 0.0697\n",
            "Epoch 100/100, Train Loss: 0.0378, Val Loss: 0.0697\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: hybrid, Units: L128_G16, Dropout: 0.1, Dense Units: 32\n",
            "Batch Size: 4, Epochs: 100\n",
            "Device: cuda\n",
            "Total test samples: 3\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/3\n",
            "Current training set size: 180 samples\n",
            "Epoch 1/100, Train Loss: 0.1787, Val Loss: 0.3990\n",
            "Epoch 2/100, Train Loss: 0.1515, Val Loss: 0.3058\n",
            "Epoch 3/100, Train Loss: 0.0956, Val Loss: 0.0881\n",
            "Epoch 4/100, Train Loss: 0.0803, Val Loss: 0.0898\n",
            "Epoch 5/100, Train Loss: 0.0678, Val Loss: 0.0886\n",
            "Epoch 6/100, Train Loss: 0.0625, Val Loss: 0.0896\n",
            "Epoch 7/100, Train Loss: 0.0652, Val Loss: 0.0893\n",
            "Epoch 8/100, Train Loss: 0.0636, Val Loss: 0.0905\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 9/100, Train Loss: 0.0530, Val Loss: 0.0902\n",
            "Epoch 10/100, Train Loss: 0.0520, Val Loss: 0.0897\n",
            "Epoch 11/100, Train Loss: 0.0524, Val Loss: 0.0896\n",
            "Epoch 12/100, Train Loss: 0.0509, Val Loss: 0.0896\n",
            "Epoch 13/100, Train Loss: 0.0513, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 14/100, Train Loss: 0.0520, Val Loss: 0.0895\n",
            "Epoch 15/100, Train Loss: 0.0507, Val Loss: 0.0895\n",
            "Epoch 16/100, Train Loss: 0.0513, Val Loss: 0.0895\n",
            "Epoch 17/100, Train Loss: 0.0517, Val Loss: 0.0895\n",
            "Epoch 18/100, Train Loss: 0.0518, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0513, Val Loss: 0.0895\n",
            "Epoch 20/100, Train Loss: 0.0514, Val Loss: 0.0895\n",
            "Epoch 21/100, Train Loss: 0.0517, Val Loss: 0.0895\n",
            "Epoch 22/100, Train Loss: 0.0512, Val Loss: 0.0895\n",
            "Epoch 23/100, Train Loss: 0.0511, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0508, Val Loss: 0.0895\n",
            "Epoch 25/100, Train Loss: 0.0508, Val Loss: 0.0895\n",
            "Epoch 26/100, Train Loss: 0.0513, Val Loss: 0.0895\n",
            "Epoch 27/100, Train Loss: 0.0519, Val Loss: 0.0895\n",
            "Epoch 28/100, Train Loss: 0.0515, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0517, Val Loss: 0.0895\n",
            "Epoch 30/100, Train Loss: 0.0507, Val Loss: 0.0895\n",
            "Epoch 31/100, Train Loss: 0.0517, Val Loss: 0.0895\n",
            "Epoch 32/100, Train Loss: 0.0512, Val Loss: 0.0895\n",
            "Epoch 33/100, Train Loss: 0.0517, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0513, Val Loss: 0.0895\n",
            "Epoch 35/100, Train Loss: 0.0518, Val Loss: 0.0895\n",
            "Epoch 36/100, Train Loss: 0.0518, Val Loss: 0.0895\n",
            "Epoch 37/100, Train Loss: 0.0517, Val Loss: 0.0895\n",
            "Epoch 38/100, Train Loss: 0.0511, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0507, Val Loss: 0.0895\n",
            "Epoch 40/100, Train Loss: 0.0509, Val Loss: 0.0895\n",
            "Epoch 41/100, Train Loss: 0.0514, Val Loss: 0.0895\n",
            "Epoch 42/100, Train Loss: 0.0514, Val Loss: 0.0895\n",
            "Epoch 43/100, Train Loss: 0.0515, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0511, Val Loss: 0.0895\n",
            "Epoch 45/100, Train Loss: 0.0520, Val Loss: 0.0895\n",
            "Epoch 46/100, Train Loss: 0.0518, Val Loss: 0.0895\n",
            "Epoch 47/100, Train Loss: 0.0513, Val Loss: 0.0895\n",
            "Epoch 48/100, Train Loss: 0.0510, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0513, Val Loss: 0.0895\n",
            "Epoch 50/100, Train Loss: 0.0511, Val Loss: 0.0895\n",
            "Epoch 51/100, Train Loss: 0.0511, Val Loss: 0.0895\n",
            "Epoch 52/100, Train Loss: 0.0512, Val Loss: 0.0895\n",
            "Epoch 53/100, Train Loss: 0.0517, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0512, Val Loss: 0.0895\n",
            "Epoch 55/100, Train Loss: 0.0510, Val Loss: 0.0895\n",
            "Epoch 56/100, Train Loss: 0.0512, Val Loss: 0.0895\n",
            "Epoch 57/100, Train Loss: 0.0514, Val Loss: 0.0895\n",
            "Epoch 58/100, Train Loss: 0.0513, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0511, Val Loss: 0.0895\n",
            "Epoch 60/100, Train Loss: 0.0510, Val Loss: 0.0895\n",
            "Epoch 61/100, Train Loss: 0.0510, Val Loss: 0.0895\n",
            "Epoch 62/100, Train Loss: 0.0512, Val Loss: 0.0895\n",
            "Epoch 63/100, Train Loss: 0.0503, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0515, Val Loss: 0.0895\n",
            "Epoch 65/100, Train Loss: 0.0519, Val Loss: 0.0895\n",
            "Epoch 66/100, Train Loss: 0.0514, Val Loss: 0.0895\n",
            "Epoch 67/100, Train Loss: 0.0514, Val Loss: 0.0895\n",
            "Epoch 68/100, Train Loss: 0.0511, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0511, Val Loss: 0.0895\n",
            "Epoch 70/100, Train Loss: 0.0525, Val Loss: 0.0895\n",
            "Epoch 71/100, Train Loss: 0.0509, Val Loss: 0.0895\n",
            "Epoch 72/100, Train Loss: 0.0509, Val Loss: 0.0895\n",
            "Epoch 73/100, Train Loss: 0.0514, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0509, Val Loss: 0.0895\n",
            "Epoch 75/100, Train Loss: 0.0519, Val Loss: 0.0895\n",
            "Epoch 76/100, Train Loss: 0.0523, Val Loss: 0.0895\n",
            "Epoch 77/100, Train Loss: 0.0512, Val Loss: 0.0895\n",
            "Epoch 78/100, Train Loss: 0.0520, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0513, Val Loss: 0.0895\n",
            "Epoch 80/100, Train Loss: 0.0513, Val Loss: 0.0895\n",
            "Epoch 81/100, Train Loss: 0.0518, Val Loss: 0.0895\n",
            "Epoch 82/100, Train Loss: 0.0517, Val Loss: 0.0895\n",
            "Epoch 83/100, Train Loss: 0.0510, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0513, Val Loss: 0.0895\n",
            "Epoch 85/100, Train Loss: 0.0520, Val Loss: 0.0895\n",
            "Epoch 86/100, Train Loss: 0.0515, Val Loss: 0.0895\n",
            "Epoch 87/100, Train Loss: 0.0515, Val Loss: 0.0895\n",
            "Epoch 88/100, Train Loss: 0.0515, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0502, Val Loss: 0.0895\n",
            "Epoch 90/100, Train Loss: 0.0506, Val Loss: 0.0895\n",
            "Epoch 91/100, Train Loss: 0.0515, Val Loss: 0.0895\n",
            "Epoch 92/100, Train Loss: 0.0506, Val Loss: 0.0895\n",
            "Epoch 93/100, Train Loss: 0.0512, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0519, Val Loss: 0.0895\n",
            "Epoch 95/100, Train Loss: 0.0515, Val Loss: 0.0895\n",
            "Epoch 96/100, Train Loss: 0.0517, Val Loss: 0.0895\n",
            "Epoch 97/100, Train Loss: 0.0510, Val Loss: 0.0895\n",
            "Epoch 98/100, Train Loss: 0.0519, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0512, Val Loss: 0.0895\n",
            "Epoch 100/100, Train Loss: 0.0513, Val Loss: 0.0895\n",
            "\n",
            "Test iteration 2/3\n",
            "Current training set size: 181 samples\n",
            "Epoch 1/100, Train Loss: 0.1788, Val Loss: 0.3195\n",
            "Epoch 2/100, Train Loss: 0.1525, Val Loss: 0.2723\n",
            "Epoch 3/100, Train Loss: 0.1205, Val Loss: 0.0980\n",
            "Epoch 4/100, Train Loss: 0.0726, Val Loss: 0.1097\n",
            "Epoch 5/100, Train Loss: 0.0746, Val Loss: 0.0903\n",
            "Epoch 6/100, Train Loss: 0.0616, Val Loss: 0.0938\n",
            "Epoch 7/100, Train Loss: 0.0698, Val Loss: 0.0901\n",
            "Epoch 8/100, Train Loss: 0.0618, Val Loss: 0.0884\n",
            "Epoch 9/100, Train Loss: 0.0612, Val Loss: 0.0886\n",
            "Epoch 10/100, Train Loss: 0.0548, Val Loss: 0.0955\n",
            "Epoch 11/100, Train Loss: 0.0611, Val Loss: 0.0852\n",
            "Epoch 12/100, Train Loss: 0.0496, Val Loss: 0.0833\n",
            "Epoch 13/100, Train Loss: 0.0525, Val Loss: 0.0829\n",
            "Epoch 14/100, Train Loss: 0.0534, Val Loss: 0.0810\n",
            "Epoch 15/100, Train Loss: 0.0490, Val Loss: 0.0817\n",
            "Epoch 16/100, Train Loss: 0.0465, Val Loss: 0.0900\n",
            "Epoch 17/100, Train Loss: 0.0458, Val Loss: 0.0741\n",
            "Epoch 18/100, Train Loss: 0.0462, Val Loss: 0.0789\n",
            "Epoch 19/100, Train Loss: 0.0432, Val Loss: 0.0707\n",
            "Epoch 20/100, Train Loss: 0.0450, Val Loss: 0.0708\n",
            "Epoch 21/100, Train Loss: 0.0426, Val Loss: 0.0602\n",
            "Epoch 22/100, Train Loss: 0.0446, Val Loss: 0.0808\n",
            "Epoch 23/100, Train Loss: 0.0461, Val Loss: 0.0726\n",
            "Epoch 24/100, Train Loss: 0.0434, Val Loss: 0.0722\n",
            "Epoch 25/100, Train Loss: 0.0433, Val Loss: 0.0669\n",
            "Epoch 26/100, Train Loss: 0.0413, Val Loss: 0.0686\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0356, Val Loss: 0.0658\n",
            "Epoch 28/100, Train Loss: 0.0350, Val Loss: 0.0647\n",
            "Epoch 29/100, Train Loss: 0.0361, Val Loss: 0.0638\n",
            "Epoch 30/100, Train Loss: 0.0373, Val Loss: 0.0629\n",
            "Epoch 31/100, Train Loss: 0.0352, Val Loss: 0.0622\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0355, Val Loss: 0.0622\n",
            "Epoch 33/100, Train Loss: 0.0356, Val Loss: 0.0622\n",
            "Epoch 34/100, Train Loss: 0.0346, Val Loss: 0.0622\n",
            "Epoch 35/100, Train Loss: 0.0351, Val Loss: 0.0622\n",
            "Epoch 36/100, Train Loss: 0.0358, Val Loss: 0.0622\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0349, Val Loss: 0.0622\n",
            "Epoch 38/100, Train Loss: 0.0354, Val Loss: 0.0622\n",
            "Epoch 39/100, Train Loss: 0.0357, Val Loss: 0.0622\n",
            "Epoch 40/100, Train Loss: 0.0355, Val Loss: 0.0622\n",
            "Epoch 41/100, Train Loss: 0.0352, Val Loss: 0.0622\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0356, Val Loss: 0.0622\n",
            "Epoch 43/100, Train Loss: 0.0356, Val Loss: 0.0622\n",
            "Epoch 44/100, Train Loss: 0.0367, Val Loss: 0.0622\n",
            "Epoch 45/100, Train Loss: 0.0369, Val Loss: 0.0622\n",
            "Epoch 46/100, Train Loss: 0.0354, Val Loss: 0.0622\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0348, Val Loss: 0.0622\n",
            "Epoch 48/100, Train Loss: 0.0350, Val Loss: 0.0622\n",
            "Epoch 49/100, Train Loss: 0.0367, Val Loss: 0.0622\n",
            "Epoch 50/100, Train Loss: 0.0355, Val Loss: 0.0622\n",
            "Epoch 51/100, Train Loss: 0.0362, Val Loss: 0.0622\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0354, Val Loss: 0.0622\n",
            "Epoch 53/100, Train Loss: 0.0357, Val Loss: 0.0622\n",
            "Epoch 54/100, Train Loss: 0.0349, Val Loss: 0.0622\n",
            "Epoch 55/100, Train Loss: 0.0356, Val Loss: 0.0622\n",
            "Epoch 56/100, Train Loss: 0.0350, Val Loss: 0.0622\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0352, Val Loss: 0.0622\n",
            "Epoch 58/100, Train Loss: 0.0358, Val Loss: 0.0622\n",
            "Epoch 59/100, Train Loss: 0.0363, Val Loss: 0.0622\n",
            "Epoch 60/100, Train Loss: 0.0346, Val Loss: 0.0622\n",
            "Epoch 61/100, Train Loss: 0.0349, Val Loss: 0.0622\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0358, Val Loss: 0.0622\n",
            "Epoch 63/100, Train Loss: 0.0352, Val Loss: 0.0622\n",
            "Epoch 64/100, Train Loss: 0.0369, Val Loss: 0.0622\n",
            "Epoch 65/100, Train Loss: 0.0358, Val Loss: 0.0622\n",
            "Epoch 66/100, Train Loss: 0.0356, Val Loss: 0.0622\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0351, Val Loss: 0.0622\n",
            "Epoch 68/100, Train Loss: 0.0352, Val Loss: 0.0622\n",
            "Epoch 69/100, Train Loss: 0.0359, Val Loss: 0.0622\n",
            "Epoch 70/100, Train Loss: 0.0351, Val Loss: 0.0622\n",
            "Epoch 71/100, Train Loss: 0.0363, Val Loss: 0.0622\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0363, Val Loss: 0.0622\n",
            "Epoch 73/100, Train Loss: 0.0364, Val Loss: 0.0622\n",
            "Epoch 74/100, Train Loss: 0.0357, Val Loss: 0.0622\n",
            "Epoch 75/100, Train Loss: 0.0359, Val Loss: 0.0622\n",
            "Epoch 76/100, Train Loss: 0.0348, Val Loss: 0.0622\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0349, Val Loss: 0.0622\n",
            "Epoch 78/100, Train Loss: 0.0361, Val Loss: 0.0622\n",
            "Epoch 79/100, Train Loss: 0.0350, Val Loss: 0.0622\n",
            "Epoch 80/100, Train Loss: 0.0352, Val Loss: 0.0622\n",
            "Epoch 81/100, Train Loss: 0.0360, Val Loss: 0.0622\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0365, Val Loss: 0.0622\n",
            "Epoch 83/100, Train Loss: 0.0363, Val Loss: 0.0622\n",
            "Epoch 84/100, Train Loss: 0.0358, Val Loss: 0.0622\n",
            "Epoch 85/100, Train Loss: 0.0366, Val Loss: 0.0622\n",
            "Epoch 86/100, Train Loss: 0.0355, Val Loss: 0.0622\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0353, Val Loss: 0.0622\n",
            "Epoch 88/100, Train Loss: 0.0352, Val Loss: 0.0622\n",
            "Epoch 89/100, Train Loss: 0.0372, Val Loss: 0.0622\n",
            "Epoch 90/100, Train Loss: 0.0352, Val Loss: 0.0622\n",
            "Epoch 91/100, Train Loss: 0.0361, Val Loss: 0.0622\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0354, Val Loss: 0.0622\n",
            "Epoch 93/100, Train Loss: 0.0358, Val Loss: 0.0622\n",
            "Epoch 94/100, Train Loss: 0.0372, Val Loss: 0.0622\n",
            "Epoch 95/100, Train Loss: 0.0370, Val Loss: 0.0622\n",
            "Epoch 96/100, Train Loss: 0.0354, Val Loss: 0.0622\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0356, Val Loss: 0.0622\n",
            "Epoch 98/100, Train Loss: 0.0351, Val Loss: 0.0622\n",
            "Epoch 99/100, Train Loss: 0.0353, Val Loss: 0.0622\n",
            "Epoch 100/100, Train Loss: 0.0362, Val Loss: 0.0622\n",
            "\n",
            "Test iteration 3/3\n",
            "Current training set size: 182 samples\n",
            "Epoch 1/100, Train Loss: 0.1958, Val Loss: 0.3545\n",
            "Epoch 2/100, Train Loss: 0.1582, Val Loss: 0.3344\n",
            "Epoch 3/100, Train Loss: 0.1138, Val Loss: 0.0878\n",
            "Epoch 4/100, Train Loss: 0.0858, Val Loss: 0.1378\n",
            "Epoch 5/100, Train Loss: 0.0755, Val Loss: 0.0970\n",
            "Epoch 6/100, Train Loss: 0.0702, Val Loss: 0.0900\n",
            "Epoch 7/100, Train Loss: 0.0632, Val Loss: 0.0883\n",
            "Epoch 8/100, Train Loss: 0.0583, Val Loss: 0.0897\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 9/100, Train Loss: 0.0540, Val Loss: 0.0895\n",
            "Epoch 10/100, Train Loss: 0.0546, Val Loss: 0.0894\n",
            "Epoch 11/100, Train Loss: 0.0529, Val Loss: 0.0894\n",
            "Epoch 12/100, Train Loss: 0.0527, Val Loss: 0.0894\n",
            "Epoch 13/100, Train Loss: 0.0528, Val Loss: 0.0894\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 14/100, Train Loss: 0.0519, Val Loss: 0.0894\n",
            "Epoch 15/100, Train Loss: 0.0527, Val Loss: 0.0894\n",
            "Epoch 16/100, Train Loss: 0.0538, Val Loss: 0.0894\n",
            "Epoch 17/100, Train Loss: 0.0519, Val Loss: 0.0894\n",
            "Epoch 18/100, Train Loss: 0.0529, Val Loss: 0.0894\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0522, Val Loss: 0.0894\n",
            "Epoch 20/100, Train Loss: 0.0521, Val Loss: 0.0894\n",
            "Epoch 21/100, Train Loss: 0.0527, Val Loss: 0.0894\n",
            "Epoch 22/100, Train Loss: 0.0521, Val Loss: 0.0894\n",
            "Epoch 23/100, Train Loss: 0.0526, Val Loss: 0.0894\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0561, Val Loss: 0.0894\n",
            "Epoch 25/100, Train Loss: 0.0527, Val Loss: 0.0894\n",
            "Epoch 26/100, Train Loss: 0.0534, Val Loss: 0.0894\n",
            "Epoch 27/100, Train Loss: 0.0524, Val Loss: 0.0894\n",
            "Epoch 28/100, Train Loss: 0.0525, Val Loss: 0.0894\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0525, Val Loss: 0.0894\n",
            "Epoch 30/100, Train Loss: 0.0523, Val Loss: 0.0894\n",
            "Epoch 31/100, Train Loss: 0.0524, Val Loss: 0.0894\n",
            "Epoch 32/100, Train Loss: 0.0525, Val Loss: 0.0894\n",
            "Epoch 33/100, Train Loss: 0.0524, Val Loss: 0.0894\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0525, Val Loss: 0.0894\n",
            "Epoch 35/100, Train Loss: 0.0522, Val Loss: 0.0894\n",
            "Epoch 36/100, Train Loss: 0.0522, Val Loss: 0.0894\n",
            "Epoch 37/100, Train Loss: 0.0545, Val Loss: 0.0894\n",
            "Epoch 38/100, Train Loss: 0.0536, Val Loss: 0.0894\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0530, Val Loss: 0.0894\n",
            "Epoch 40/100, Train Loss: 0.0526, Val Loss: 0.0894\n",
            "Epoch 41/100, Train Loss: 0.0527, Val Loss: 0.0894\n",
            "Epoch 42/100, Train Loss: 0.0514, Val Loss: 0.0894\n",
            "Epoch 43/100, Train Loss: 0.0520, Val Loss: 0.0894\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0523, Val Loss: 0.0894\n",
            "Epoch 45/100, Train Loss: 0.0519, Val Loss: 0.0894\n",
            "Epoch 46/100, Train Loss: 0.0525, Val Loss: 0.0894\n",
            "Epoch 47/100, Train Loss: 0.0546, Val Loss: 0.0894\n",
            "Epoch 48/100, Train Loss: 0.0531, Val Loss: 0.0894\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0542, Val Loss: 0.0894\n",
            "Epoch 50/100, Train Loss: 0.0526, Val Loss: 0.0894\n",
            "Epoch 51/100, Train Loss: 0.0520, Val Loss: 0.0894\n",
            "Epoch 52/100, Train Loss: 0.0525, Val Loss: 0.0894\n",
            "Epoch 53/100, Train Loss: 0.0522, Val Loss: 0.0894\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0524, Val Loss: 0.0894\n",
            "Epoch 55/100, Train Loss: 0.0524, Val Loss: 0.0894\n",
            "Epoch 56/100, Train Loss: 0.0526, Val Loss: 0.0894\n",
            "Epoch 57/100, Train Loss: 0.0552, Val Loss: 0.0894\n",
            "Epoch 58/100, Train Loss: 0.0519, Val Loss: 0.0894\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0530, Val Loss: 0.0894\n",
            "Epoch 60/100, Train Loss: 0.0531, Val Loss: 0.0894\n",
            "Epoch 61/100, Train Loss: 0.0523, Val Loss: 0.0894\n",
            "Epoch 62/100, Train Loss: 0.0526, Val Loss: 0.0894\n",
            "Epoch 63/100, Train Loss: 0.0520, Val Loss: 0.0894\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0527, Val Loss: 0.0894\n",
            "Epoch 65/100, Train Loss: 0.0534, Val Loss: 0.0894\n",
            "Epoch 66/100, Train Loss: 0.0532, Val Loss: 0.0894\n",
            "Epoch 67/100, Train Loss: 0.0520, Val Loss: 0.0894\n",
            "Epoch 68/100, Train Loss: 0.0524, Val Loss: 0.0894\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0524, Val Loss: 0.0894\n",
            "Epoch 70/100, Train Loss: 0.0528, Val Loss: 0.0894\n",
            "Epoch 71/100, Train Loss: 0.0525, Val Loss: 0.0894\n",
            "Epoch 72/100, Train Loss: 0.0535, Val Loss: 0.0894\n",
            "Epoch 73/100, Train Loss: 0.0523, Val Loss: 0.0894\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0555, Val Loss: 0.0894\n",
            "Epoch 75/100, Train Loss: 0.0515, Val Loss: 0.0894\n",
            "Epoch 76/100, Train Loss: 0.0520, Val Loss: 0.0894\n",
            "Epoch 77/100, Train Loss: 0.0561, Val Loss: 0.0894\n",
            "Epoch 78/100, Train Loss: 0.0531, Val Loss: 0.0894\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0535, Val Loss: 0.0894\n",
            "Epoch 80/100, Train Loss: 0.0526, Val Loss: 0.0894\n",
            "Epoch 81/100, Train Loss: 0.0527, Val Loss: 0.0894\n",
            "Epoch 82/100, Train Loss: 0.0523, Val Loss: 0.0894\n",
            "Epoch 83/100, Train Loss: 0.0526, Val Loss: 0.0894\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0526, Val Loss: 0.0894\n",
            "Epoch 85/100, Train Loss: 0.0540, Val Loss: 0.0894\n",
            "Epoch 86/100, Train Loss: 0.0521, Val Loss: 0.0894\n",
            "Epoch 87/100, Train Loss: 0.0532, Val Loss: 0.0894\n",
            "Epoch 88/100, Train Loss: 0.0524, Val Loss: 0.0894\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0529, Val Loss: 0.0894\n",
            "Epoch 90/100, Train Loss: 0.0532, Val Loss: 0.0894\n",
            "Epoch 91/100, Train Loss: 0.0520, Val Loss: 0.0894\n",
            "Epoch 92/100, Train Loss: 0.0528, Val Loss: 0.0894\n",
            "Epoch 93/100, Train Loss: 0.0529, Val Loss: 0.0894\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0530, Val Loss: 0.0894\n",
            "Epoch 95/100, Train Loss: 0.0533, Val Loss: 0.0894\n",
            "Epoch 96/100, Train Loss: 0.0527, Val Loss: 0.0894\n",
            "Epoch 97/100, Train Loss: 0.0526, Val Loss: 0.0894\n",
            "Epoch 98/100, Train Loss: 0.0527, Val Loss: 0.0894\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0529, Val Loss: 0.0894\n",
            "Epoch 100/100, Train Loss: 0.0530, Val Loss: 0.0894\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: hybrid, Units: L128_G16, Dropout: 0.1, Dense Units: 64\n",
            "Batch Size: 4, Epochs: 100\n",
            "Device: cuda\n",
            "Total test samples: 3\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/3\n",
            "Current training set size: 180 samples\n",
            "Epoch 1/100, Train Loss: 0.1748, Val Loss: 0.3725\n",
            "Epoch 2/100, Train Loss: 0.1471, Val Loss: 0.2767\n",
            "Epoch 3/100, Train Loss: 0.0858, Val Loss: 0.1015\n",
            "Epoch 4/100, Train Loss: 0.0739, Val Loss: 0.0906\n",
            "Epoch 5/100, Train Loss: 0.0745, Val Loss: 0.0901\n",
            "Epoch 6/100, Train Loss: 0.0720, Val Loss: 0.0924\n",
            "Epoch 7/100, Train Loss: 0.0690, Val Loss: 0.0937\n",
            "Epoch 8/100, Train Loss: 0.0550, Val Loss: 0.0891\n",
            "Epoch 9/100, Train Loss: 0.0550, Val Loss: 0.0917\n",
            "Epoch 10/100, Train Loss: 0.0523, Val Loss: 0.0869\n",
            "Epoch 11/100, Train Loss: 0.0481, Val Loss: 0.0871\n",
            "Epoch 12/100, Train Loss: 0.0521, Val Loss: 0.0862\n",
            "Epoch 13/100, Train Loss: 0.0652, Val Loss: 0.0827\n",
            "Epoch 14/100, Train Loss: 0.0504, Val Loss: 0.0808\n",
            "Epoch 15/100, Train Loss: 0.0454, Val Loss: 0.0784\n",
            "Epoch 16/100, Train Loss: 0.0462, Val Loss: 0.0804\n",
            "Epoch 17/100, Train Loss: 0.0494, Val Loss: 0.0731\n",
            "Epoch 18/100, Train Loss: 0.0441, Val Loss: 0.1050\n",
            "Epoch 19/100, Train Loss: 0.0470, Val Loss: 0.0704\n",
            "Epoch 20/100, Train Loss: 0.0429, Val Loss: 0.0667\n",
            "Epoch 21/100, Train Loss: 0.0443, Val Loss: 0.0651\n",
            "Epoch 22/100, Train Loss: 0.0412, Val Loss: 0.0631\n",
            "Epoch 23/100, Train Loss: 0.0407, Val Loss: 0.0653\n",
            "Epoch 24/100, Train Loss: 0.0404, Val Loss: 0.0712\n",
            "Epoch 25/100, Train Loss: 0.0423, Val Loss: 0.0829\n",
            "Epoch 26/100, Train Loss: 0.0390, Val Loss: 0.0681\n",
            "Epoch 27/100, Train Loss: 0.0403, Val Loss: 0.0654\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0508, Val Loss: 0.0655\n",
            "Epoch 29/100, Train Loss: 0.0425, Val Loss: 0.0660\n",
            "Epoch 30/100, Train Loss: 0.0368, Val Loss: 0.0662\n",
            "Epoch 31/100, Train Loss: 0.0357, Val Loss: 0.0649\n",
            "Epoch 32/100, Train Loss: 0.0342, Val Loss: 0.0646\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0343, Val Loss: 0.0646\n",
            "Epoch 34/100, Train Loss: 0.0342, Val Loss: 0.0646\n",
            "Epoch 35/100, Train Loss: 0.0347, Val Loss: 0.0646\n",
            "Epoch 36/100, Train Loss: 0.0349, Val Loss: 0.0646\n",
            "Epoch 37/100, Train Loss: 0.0354, Val Loss: 0.0646\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0349, Val Loss: 0.0646\n",
            "Epoch 39/100, Train Loss: 0.0340, Val Loss: 0.0646\n",
            "Epoch 40/100, Train Loss: 0.0335, Val Loss: 0.0646\n",
            "Epoch 41/100, Train Loss: 0.0341, Val Loss: 0.0646\n",
            "Epoch 42/100, Train Loss: 0.0341, Val Loss: 0.0646\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0352, Val Loss: 0.0646\n",
            "Epoch 44/100, Train Loss: 0.0349, Val Loss: 0.0646\n",
            "Epoch 45/100, Train Loss: 0.0353, Val Loss: 0.0646\n",
            "Epoch 46/100, Train Loss: 0.0344, Val Loss: 0.0646\n",
            "Epoch 47/100, Train Loss: 0.0342, Val Loss: 0.0646\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0342, Val Loss: 0.0646\n",
            "Epoch 49/100, Train Loss: 0.0349, Val Loss: 0.0646\n",
            "Epoch 50/100, Train Loss: 0.0337, Val Loss: 0.0646\n",
            "Epoch 51/100, Train Loss: 0.0343, Val Loss: 0.0646\n",
            "Epoch 52/100, Train Loss: 0.0351, Val Loss: 0.0646\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0352, Val Loss: 0.0646\n",
            "Epoch 54/100, Train Loss: 0.0345, Val Loss: 0.0646\n",
            "Epoch 55/100, Train Loss: 0.0344, Val Loss: 0.0646\n",
            "Epoch 56/100, Train Loss: 0.0343, Val Loss: 0.0646\n",
            "Epoch 57/100, Train Loss: 0.0346, Val Loss: 0.0646\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0346, Val Loss: 0.0646\n",
            "Epoch 59/100, Train Loss: 0.0346, Val Loss: 0.0646\n",
            "Epoch 60/100, Train Loss: 0.0344, Val Loss: 0.0646\n",
            "Epoch 61/100, Train Loss: 0.0345, Val Loss: 0.0646\n",
            "Epoch 62/100, Train Loss: 0.0350, Val Loss: 0.0646\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0344, Val Loss: 0.0646\n",
            "Epoch 64/100, Train Loss: 0.0353, Val Loss: 0.0646\n",
            "Epoch 65/100, Train Loss: 0.0358, Val Loss: 0.0646\n",
            "Epoch 66/100, Train Loss: 0.0350, Val Loss: 0.0646\n",
            "Epoch 67/100, Train Loss: 0.0351, Val Loss: 0.0646\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0349, Val Loss: 0.0646\n",
            "Epoch 69/100, Train Loss: 0.0349, Val Loss: 0.0646\n",
            "Epoch 70/100, Train Loss: 0.0346, Val Loss: 0.0646\n",
            "Epoch 71/100, Train Loss: 0.0339, Val Loss: 0.0646\n",
            "Epoch 72/100, Train Loss: 0.0335, Val Loss: 0.0646\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0352, Val Loss: 0.0646\n",
            "Epoch 74/100, Train Loss: 0.0344, Val Loss: 0.0646\n",
            "Epoch 75/100, Train Loss: 0.0357, Val Loss: 0.0646\n",
            "Epoch 76/100, Train Loss: 0.0350, Val Loss: 0.0646\n",
            "Epoch 77/100, Train Loss: 0.0348, Val Loss: 0.0646\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0344, Val Loss: 0.0646\n",
            "Epoch 79/100, Train Loss: 0.0353, Val Loss: 0.0646\n",
            "Epoch 80/100, Train Loss: 0.0345, Val Loss: 0.0646\n",
            "Epoch 81/100, Train Loss: 0.0354, Val Loss: 0.0646\n",
            "Epoch 82/100, Train Loss: 0.0342, Val Loss: 0.0646\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0353, Val Loss: 0.0646\n",
            "Epoch 84/100, Train Loss: 0.0349, Val Loss: 0.0646\n",
            "Epoch 85/100, Train Loss: 0.0353, Val Loss: 0.0646\n",
            "Epoch 86/100, Train Loss: 0.0345, Val Loss: 0.0646\n",
            "Epoch 87/100, Train Loss: 0.0341, Val Loss: 0.0646\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0356, Val Loss: 0.0646\n",
            "Epoch 89/100, Train Loss: 0.0345, Val Loss: 0.0646\n",
            "Epoch 90/100, Train Loss: 0.0350, Val Loss: 0.0646\n",
            "Epoch 91/100, Train Loss: 0.0343, Val Loss: 0.0646\n",
            "Epoch 92/100, Train Loss: 0.0340, Val Loss: 0.0646\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0352, Val Loss: 0.0646\n",
            "Epoch 94/100, Train Loss: 0.0351, Val Loss: 0.0646\n",
            "Epoch 95/100, Train Loss: 0.0348, Val Loss: 0.0646\n",
            "Epoch 96/100, Train Loss: 0.0348, Val Loss: 0.0646\n",
            "Epoch 97/100, Train Loss: 0.0341, Val Loss: 0.0646\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0353, Val Loss: 0.0646\n",
            "Epoch 99/100, Train Loss: 0.0341, Val Loss: 0.0646\n",
            "Epoch 100/100, Train Loss: 0.0346, Val Loss: 0.0646\n",
            "\n",
            "Test iteration 2/3\n",
            "Current training set size: 181 samples\n",
            "Epoch 1/100, Train Loss: 0.1754, Val Loss: 0.3218\n",
            "Epoch 2/100, Train Loss: 0.1422, Val Loss: 0.1486\n",
            "Epoch 3/100, Train Loss: 0.1001, Val Loss: 0.0938\n",
            "Epoch 4/100, Train Loss: 0.0672, Val Loss: 0.0966\n",
            "Epoch 5/100, Train Loss: 0.0631, Val Loss: 0.0884\n",
            "Epoch 6/100, Train Loss: 0.0582, Val Loss: 0.0919\n",
            "Epoch 7/100, Train Loss: 0.0687, Val Loss: 0.0876\n",
            "Epoch 8/100, Train Loss: 0.0624, Val Loss: 0.0860\n",
            "Epoch 9/100, Train Loss: 0.0598, Val Loss: 0.0859\n",
            "Epoch 10/100, Train Loss: 0.0513, Val Loss: 0.1087\n",
            "Epoch 11/100, Train Loss: 0.0483, Val Loss: 0.0814\n",
            "Epoch 12/100, Train Loss: 0.0474, Val Loss: 0.0788\n",
            "Epoch 13/100, Train Loss: 0.0452, Val Loss: 0.0774\n",
            "Epoch 14/100, Train Loss: 0.0455, Val Loss: 0.0780\n",
            "Epoch 15/100, Train Loss: 0.0538, Val Loss: 0.0737\n",
            "Epoch 16/100, Train Loss: 0.0518, Val Loss: 0.0816\n",
            "Epoch 17/100, Train Loss: 0.0434, Val Loss: 0.0728\n",
            "Epoch 18/100, Train Loss: 0.0436, Val Loss: 0.0689\n",
            "Epoch 19/100, Train Loss: 0.0434, Val Loss: 0.0672\n",
            "Epoch 20/100, Train Loss: 0.0414, Val Loss: 0.0722\n",
            "Epoch 21/100, Train Loss: 0.0425, Val Loss: 0.0615\n",
            "Epoch 22/100, Train Loss: 0.0440, Val Loss: 0.0648\n",
            "Epoch 23/100, Train Loss: 0.0412, Val Loss: 0.0696\n",
            "Epoch 24/100, Train Loss: 0.0415, Val Loss: 0.0695\n",
            "Epoch 25/100, Train Loss: 0.0384, Val Loss: 0.0665\n",
            "Epoch 26/100, Train Loss: 0.0424, Val Loss: 0.1015\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0400, Val Loss: 0.0937\n",
            "Epoch 28/100, Train Loss: 0.0356, Val Loss: 0.0883\n",
            "Epoch 29/100, Train Loss: 0.0363, Val Loss: 0.0845\n",
            "Epoch 30/100, Train Loss: 0.0374, Val Loss: 0.0817\n",
            "Epoch 31/100, Train Loss: 0.0351, Val Loss: 0.0795\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0359, Val Loss: 0.0794\n",
            "Epoch 33/100, Train Loss: 0.0356, Val Loss: 0.0794\n",
            "Epoch 34/100, Train Loss: 0.0344, Val Loss: 0.0794\n",
            "Epoch 35/100, Train Loss: 0.0352, Val Loss: 0.0794\n",
            "Epoch 36/100, Train Loss: 0.0348, Val Loss: 0.0793\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0350, Val Loss: 0.0793\n",
            "Epoch 38/100, Train Loss: 0.0356, Val Loss: 0.0793\n",
            "Epoch 39/100, Train Loss: 0.0352, Val Loss: 0.0793\n",
            "Epoch 40/100, Train Loss: 0.0351, Val Loss: 0.0793\n",
            "Epoch 41/100, Train Loss: 0.0341, Val Loss: 0.0793\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0353, Val Loss: 0.0793\n",
            "Epoch 43/100, Train Loss: 0.0350, Val Loss: 0.0793\n",
            "Epoch 44/100, Train Loss: 0.0363, Val Loss: 0.0793\n",
            "Epoch 45/100, Train Loss: 0.0371, Val Loss: 0.0793\n",
            "Epoch 46/100, Train Loss: 0.0353, Val Loss: 0.0793\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0346, Val Loss: 0.0793\n",
            "Epoch 48/100, Train Loss: 0.0349, Val Loss: 0.0793\n",
            "Epoch 49/100, Train Loss: 0.0358, Val Loss: 0.0793\n",
            "Epoch 50/100, Train Loss: 0.0344, Val Loss: 0.0793\n",
            "Epoch 51/100, Train Loss: 0.0354, Val Loss: 0.0793\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0347, Val Loss: 0.0793\n",
            "Epoch 53/100, Train Loss: 0.0358, Val Loss: 0.0793\n",
            "Epoch 54/100, Train Loss: 0.0346, Val Loss: 0.0793\n",
            "Epoch 55/100, Train Loss: 0.0347, Val Loss: 0.0793\n",
            "Epoch 56/100, Train Loss: 0.0344, Val Loss: 0.0793\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0346, Val Loss: 0.0793\n",
            "Epoch 58/100, Train Loss: 0.0352, Val Loss: 0.0793\n",
            "Epoch 59/100, Train Loss: 0.0356, Val Loss: 0.0793\n",
            "Epoch 60/100, Train Loss: 0.0347, Val Loss: 0.0793\n",
            "Epoch 61/100, Train Loss: 0.0345, Val Loss: 0.0793\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0352, Val Loss: 0.0793\n",
            "Epoch 63/100, Train Loss: 0.0344, Val Loss: 0.0793\n",
            "Epoch 64/100, Train Loss: 0.0367, Val Loss: 0.0793\n",
            "Epoch 65/100, Train Loss: 0.0360, Val Loss: 0.0793\n",
            "Epoch 66/100, Train Loss: 0.0351, Val Loss: 0.0793\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0352, Val Loss: 0.0793\n",
            "Epoch 68/100, Train Loss: 0.0350, Val Loss: 0.0793\n",
            "Epoch 69/100, Train Loss: 0.0354, Val Loss: 0.0793\n",
            "Epoch 70/100, Train Loss: 0.0350, Val Loss: 0.0793\n",
            "Epoch 71/100, Train Loss: 0.0357, Val Loss: 0.0793\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0357, Val Loss: 0.0793\n",
            "Epoch 73/100, Train Loss: 0.0354, Val Loss: 0.0793\n",
            "Epoch 74/100, Train Loss: 0.0356, Val Loss: 0.0793\n",
            "Epoch 75/100, Train Loss: 0.0354, Val Loss: 0.0793\n",
            "Epoch 76/100, Train Loss: 0.0340, Val Loss: 0.0793\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0352, Val Loss: 0.0793\n",
            "Epoch 78/100, Train Loss: 0.0360, Val Loss: 0.0793\n",
            "Epoch 79/100, Train Loss: 0.0350, Val Loss: 0.0793\n",
            "Epoch 80/100, Train Loss: 0.0355, Val Loss: 0.0793\n",
            "Epoch 81/100, Train Loss: 0.0362, Val Loss: 0.0793\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0359, Val Loss: 0.0793\n",
            "Epoch 83/100, Train Loss: 0.0362, Val Loss: 0.0793\n",
            "Epoch 84/100, Train Loss: 0.0352, Val Loss: 0.0793\n",
            "Epoch 85/100, Train Loss: 0.0366, Val Loss: 0.0793\n",
            "Epoch 86/100, Train Loss: 0.0346, Val Loss: 0.0793\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0346, Val Loss: 0.0793\n",
            "Epoch 88/100, Train Loss: 0.0348, Val Loss: 0.0793\n",
            "Epoch 89/100, Train Loss: 0.0367, Val Loss: 0.0793\n",
            "Epoch 90/100, Train Loss: 0.0349, Val Loss: 0.0793\n",
            "Epoch 91/100, Train Loss: 0.0354, Val Loss: 0.0793\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0348, Val Loss: 0.0793\n",
            "Epoch 93/100, Train Loss: 0.0355, Val Loss: 0.0793\n",
            "Epoch 94/100, Train Loss: 0.0362, Val Loss: 0.0793\n",
            "Epoch 95/100, Train Loss: 0.0365, Val Loss: 0.0793\n",
            "Epoch 96/100, Train Loss: 0.0348, Val Loss: 0.0793\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0348, Val Loss: 0.0793\n",
            "Epoch 98/100, Train Loss: 0.0350, Val Loss: 0.0793\n",
            "Epoch 99/100, Train Loss: 0.0353, Val Loss: 0.0793\n",
            "Epoch 100/100, Train Loss: 0.0359, Val Loss: 0.0793\n",
            "\n",
            "Test iteration 3/3\n",
            "Current training set size: 182 samples\n",
            "Epoch 1/100, Train Loss: 0.1917, Val Loss: 0.4027\n",
            "Epoch 2/100, Train Loss: 0.1528, Val Loss: 0.2732\n",
            "Epoch 3/100, Train Loss: 0.1072, Val Loss: 0.0962\n",
            "Epoch 4/100, Train Loss: 0.0669, Val Loss: 0.1203\n",
            "Epoch 5/100, Train Loss: 0.0697, Val Loss: 0.0994\n",
            "Epoch 6/100, Train Loss: 0.0630, Val Loss: 0.0943\n",
            "Epoch 7/100, Train Loss: 0.0583, Val Loss: 0.0899\n",
            "Epoch 8/100, Train Loss: 0.0542, Val Loss: 0.0895\n",
            "Epoch 9/100, Train Loss: 0.0513, Val Loss: 0.0886\n",
            "Epoch 10/100, Train Loss: 0.0504, Val Loss: 0.0866\n",
            "Epoch 11/100, Train Loss: 0.0524, Val Loss: 0.0815\n",
            "Epoch 12/100, Train Loss: 0.0474, Val Loss: 0.0816\n",
            "Epoch 13/100, Train Loss: 0.0500, Val Loss: 0.0816\n",
            "Epoch 14/100, Train Loss: 0.0503, Val Loss: 0.0773\n",
            "Epoch 15/100, Train Loss: 0.0529, Val Loss: 0.0748\n",
            "Epoch 16/100, Train Loss: 0.0490, Val Loss: 0.1064\n",
            "Epoch 17/100, Train Loss: 0.0446, Val Loss: 0.0910\n",
            "Epoch 18/100, Train Loss: 0.0455, Val Loss: 0.0776\n",
            "Epoch 19/100, Train Loss: 0.0406, Val Loss: 0.1267\n",
            "Epoch 20/100, Train Loss: 0.0442, Val Loss: 0.0747\n",
            "Epoch 21/100, Train Loss: 0.0408, Val Loss: 0.0674\n",
            "Epoch 22/100, Train Loss: 0.0377, Val Loss: 0.0660\n",
            "Epoch 23/100, Train Loss: 0.0389, Val Loss: 0.0605\n",
            "Epoch 24/100, Train Loss: 0.0406, Val Loss: 0.0920\n",
            "Epoch 25/100, Train Loss: 0.0416, Val Loss: 0.0615\n",
            "Epoch 26/100, Train Loss: 0.0390, Val Loss: 0.0590\n",
            "Epoch 27/100, Train Loss: 0.0389, Val Loss: 0.0582\n",
            "Epoch 28/100, Train Loss: 0.0391, Val Loss: 0.0564\n",
            "Epoch 29/100, Train Loss: 0.0388, Val Loss: 0.0935\n",
            "Epoch 30/100, Train Loss: 0.0367, Val Loss: 0.0624\n",
            "Epoch 31/100, Train Loss: 0.0375, Val Loss: 0.0643\n",
            "Epoch 32/100, Train Loss: 0.0371, Val Loss: 0.0582\n",
            "Epoch 33/100, Train Loss: 0.0368, Val Loss: 0.0539\n",
            "Epoch 34/100, Train Loss: 0.0414, Val Loss: 0.0651\n",
            "Epoch 35/100, Train Loss: 0.0419, Val Loss: 0.0579\n",
            "Epoch 36/100, Train Loss: 0.0347, Val Loss: 0.0751\n",
            "Epoch 37/100, Train Loss: 0.0365, Val Loss: 0.0557\n",
            "Epoch 38/100, Train Loss: 0.0391, Val Loss: 0.0577\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0353, Val Loss: 0.0583\n",
            "Epoch 40/100, Train Loss: 0.0338, Val Loss: 0.0590\n",
            "Epoch 41/100, Train Loss: 0.0314, Val Loss: 0.0595\n",
            "Epoch 42/100, Train Loss: 0.0310, Val Loss: 0.0599\n",
            "Epoch 43/100, Train Loss: 0.0315, Val Loss: 0.0610\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0315, Val Loss: 0.0610\n",
            "Epoch 45/100, Train Loss: 0.0311, Val Loss: 0.0610\n",
            "Epoch 46/100, Train Loss: 0.0318, Val Loss: 0.0610\n",
            "Epoch 47/100, Train Loss: 0.0319, Val Loss: 0.0610\n",
            "Epoch 48/100, Train Loss: 0.0331, Val Loss: 0.0610\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0325, Val Loss: 0.0610\n",
            "Epoch 50/100, Train Loss: 0.0311, Val Loss: 0.0610\n",
            "Epoch 51/100, Train Loss: 0.0315, Val Loss: 0.0610\n",
            "Epoch 52/100, Train Loss: 0.0319, Val Loss: 0.0610\n",
            "Epoch 53/100, Train Loss: 0.0318, Val Loss: 0.0610\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0313, Val Loss: 0.0610\n",
            "Epoch 55/100, Train Loss: 0.0327, Val Loss: 0.0610\n",
            "Epoch 56/100, Train Loss: 0.0316, Val Loss: 0.0610\n",
            "Epoch 57/100, Train Loss: 0.0325, Val Loss: 0.0610\n",
            "Epoch 58/100, Train Loss: 0.0319, Val Loss: 0.0610\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0322, Val Loss: 0.0610\n",
            "Epoch 60/100, Train Loss: 0.0314, Val Loss: 0.0610\n",
            "Epoch 61/100, Train Loss: 0.0314, Val Loss: 0.0610\n",
            "Epoch 62/100, Train Loss: 0.0321, Val Loss: 0.0610\n",
            "Epoch 63/100, Train Loss: 0.0308, Val Loss: 0.0610\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0316, Val Loss: 0.0610\n",
            "Epoch 65/100, Train Loss: 0.0321, Val Loss: 0.0610\n",
            "Epoch 66/100, Train Loss: 0.0327, Val Loss: 0.0610\n",
            "Epoch 67/100, Train Loss: 0.0315, Val Loss: 0.0610\n",
            "Epoch 68/100, Train Loss: 0.0328, Val Loss: 0.0610\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0309, Val Loss: 0.0610\n",
            "Epoch 70/100, Train Loss: 0.0312, Val Loss: 0.0610\n",
            "Epoch 71/100, Train Loss: 0.0326, Val Loss: 0.0610\n",
            "Epoch 72/100, Train Loss: 0.0329, Val Loss: 0.0610\n",
            "Epoch 73/100, Train Loss: 0.0311, Val Loss: 0.0610\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0344, Val Loss: 0.0610\n",
            "Epoch 75/100, Train Loss: 0.0307, Val Loss: 0.0610\n",
            "Epoch 76/100, Train Loss: 0.0315, Val Loss: 0.0610\n",
            "Epoch 77/100, Train Loss: 0.0323, Val Loss: 0.0610\n",
            "Epoch 78/100, Train Loss: 0.0323, Val Loss: 0.0610\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0336, Val Loss: 0.0610\n",
            "Epoch 80/100, Train Loss: 0.0316, Val Loss: 0.0610\n",
            "Epoch 81/100, Train Loss: 0.0316, Val Loss: 0.0610\n",
            "Epoch 82/100, Train Loss: 0.0313, Val Loss: 0.0610\n",
            "Epoch 83/100, Train Loss: 0.0319, Val Loss: 0.0610\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0322, Val Loss: 0.0610\n",
            "Epoch 85/100, Train Loss: 0.0327, Val Loss: 0.0610\n",
            "Epoch 86/100, Train Loss: 0.0312, Val Loss: 0.0610\n",
            "Epoch 87/100, Train Loss: 0.0320, Val Loss: 0.0610\n",
            "Epoch 88/100, Train Loss: 0.0321, Val Loss: 0.0610\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0320, Val Loss: 0.0610\n",
            "Epoch 90/100, Train Loss: 0.0325, Val Loss: 0.0610\n",
            "Epoch 91/100, Train Loss: 0.0315, Val Loss: 0.0610\n",
            "Epoch 92/100, Train Loss: 0.0320, Val Loss: 0.0610\n",
            "Epoch 93/100, Train Loss: 0.0321, Val Loss: 0.0610\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0313, Val Loss: 0.0610\n",
            "Epoch 95/100, Train Loss: 0.0322, Val Loss: 0.0610\n",
            "Epoch 96/100, Train Loss: 0.0311, Val Loss: 0.0610\n",
            "Epoch 97/100, Train Loss: 0.0312, Val Loss: 0.0610\n",
            "Epoch 98/100, Train Loss: 0.0328, Val Loss: 0.0610\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0321, Val Loss: 0.0610\n",
            "Epoch 100/100, Train Loss: 0.0326, Val Loss: 0.0610\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: hybrid, Units: L128_G16, Dropout: 0.2, Dense Units: 32\n",
            "Batch Size: 4, Epochs: 100\n",
            "Device: cuda\n",
            "Total test samples: 3\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/3\n",
            "Current training set size: 180 samples\n",
            "Epoch 1/100, Train Loss: 0.1773, Val Loss: 0.3846\n",
            "Epoch 2/100, Train Loss: 0.1520, Val Loss: 0.3485\n",
            "Epoch 3/100, Train Loss: 0.1023, Val Loss: 0.0870\n",
            "Epoch 4/100, Train Loss: 0.0864, Val Loss: 0.0938\n",
            "Epoch 5/100, Train Loss: 0.0685, Val Loss: 0.0877\n",
            "Epoch 6/100, Train Loss: 0.0653, Val Loss: 0.0889\n",
            "Epoch 7/100, Train Loss: 0.0646, Val Loss: 0.0886\n",
            "Epoch 8/100, Train Loss: 0.0661, Val Loss: 0.0885\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 9/100, Train Loss: 0.0544, Val Loss: 0.0890\n",
            "Epoch 10/100, Train Loss: 0.0525, Val Loss: 0.0892\n",
            "Epoch 11/100, Train Loss: 0.0529, Val Loss: 0.0893\n",
            "Epoch 12/100, Train Loss: 0.0520, Val Loss: 0.0894\n",
            "Epoch 13/100, Train Loss: 0.0521, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 14/100, Train Loss: 0.0530, Val Loss: 0.0895\n",
            "Epoch 15/100, Train Loss: 0.0507, Val Loss: 0.0895\n",
            "Epoch 16/100, Train Loss: 0.0514, Val Loss: 0.0895\n",
            "Epoch 17/100, Train Loss: 0.0526, Val Loss: 0.0895\n",
            "Epoch 18/100, Train Loss: 0.0532, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0511, Val Loss: 0.0895\n",
            "Epoch 20/100, Train Loss: 0.0515, Val Loss: 0.0895\n",
            "Epoch 21/100, Train Loss: 0.0519, Val Loss: 0.0895\n",
            "Epoch 22/100, Train Loss: 0.0514, Val Loss: 0.0895\n",
            "Epoch 23/100, Train Loss: 0.0516, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0505, Val Loss: 0.0895\n",
            "Epoch 25/100, Train Loss: 0.0509, Val Loss: 0.0895\n",
            "Epoch 26/100, Train Loss: 0.0519, Val Loss: 0.0895\n",
            "Epoch 27/100, Train Loss: 0.0519, Val Loss: 0.0895\n",
            "Epoch 28/100, Train Loss: 0.0522, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0532, Val Loss: 0.0895\n",
            "Epoch 30/100, Train Loss: 0.0512, Val Loss: 0.0895\n",
            "Epoch 31/100, Train Loss: 0.0518, Val Loss: 0.0895\n",
            "Epoch 32/100, Train Loss: 0.0519, Val Loss: 0.0895\n",
            "Epoch 33/100, Train Loss: 0.0532, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0509, Val Loss: 0.0895\n",
            "Epoch 35/100, Train Loss: 0.0523, Val Loss: 0.0895\n",
            "Epoch 36/100, Train Loss: 0.0520, Val Loss: 0.0895\n",
            "Epoch 37/100, Train Loss: 0.0527, Val Loss: 0.0895\n",
            "Epoch 38/100, Train Loss: 0.0516, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0509, Val Loss: 0.0895\n",
            "Epoch 40/100, Train Loss: 0.0509, Val Loss: 0.0895\n",
            "Epoch 41/100, Train Loss: 0.0516, Val Loss: 0.0895\n",
            "Epoch 42/100, Train Loss: 0.0529, Val Loss: 0.0895\n",
            "Epoch 43/100, Train Loss: 0.0526, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0528, Val Loss: 0.0895\n",
            "Epoch 45/100, Train Loss: 0.0523, Val Loss: 0.0895\n",
            "Epoch 46/100, Train Loss: 0.0529, Val Loss: 0.0895\n",
            "Epoch 47/100, Train Loss: 0.0518, Val Loss: 0.0895\n",
            "Epoch 48/100, Train Loss: 0.0517, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0515, Val Loss: 0.0895\n",
            "Epoch 50/100, Train Loss: 0.0513, Val Loss: 0.0895\n",
            "Epoch 51/100, Train Loss: 0.0528, Val Loss: 0.0895\n",
            "Epoch 52/100, Train Loss: 0.0515, Val Loss: 0.0895\n",
            "Epoch 53/100, Train Loss: 0.0525, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0518, Val Loss: 0.0895\n",
            "Epoch 55/100, Train Loss: 0.0513, Val Loss: 0.0895\n",
            "Epoch 56/100, Train Loss: 0.0519, Val Loss: 0.0895\n",
            "Epoch 57/100, Train Loss: 0.0510, Val Loss: 0.0895\n",
            "Epoch 58/100, Train Loss: 0.0526, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0525, Val Loss: 0.0895\n",
            "Epoch 60/100, Train Loss: 0.0513, Val Loss: 0.0895\n",
            "Epoch 61/100, Train Loss: 0.0516, Val Loss: 0.0895\n",
            "Epoch 62/100, Train Loss: 0.0522, Val Loss: 0.0895\n",
            "Epoch 63/100, Train Loss: 0.0513, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0531, Val Loss: 0.0895\n",
            "Epoch 65/100, Train Loss: 0.0529, Val Loss: 0.0895\n",
            "Epoch 66/100, Train Loss: 0.0511, Val Loss: 0.0895\n",
            "Epoch 67/100, Train Loss: 0.0512, Val Loss: 0.0895\n",
            "Epoch 68/100, Train Loss: 0.0519, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0524, Val Loss: 0.0895\n",
            "Epoch 70/100, Train Loss: 0.0527, Val Loss: 0.0895\n",
            "Epoch 71/100, Train Loss: 0.0521, Val Loss: 0.0895\n",
            "Epoch 72/100, Train Loss: 0.0510, Val Loss: 0.0895\n",
            "Epoch 73/100, Train Loss: 0.0519, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0514, Val Loss: 0.0895\n",
            "Epoch 75/100, Train Loss: 0.0523, Val Loss: 0.0895\n",
            "Epoch 76/100, Train Loss: 0.0529, Val Loss: 0.0895\n",
            "Epoch 77/100, Train Loss: 0.0523, Val Loss: 0.0895\n",
            "Epoch 78/100, Train Loss: 0.0522, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0520, Val Loss: 0.0895\n",
            "Epoch 80/100, Train Loss: 0.0522, Val Loss: 0.0895\n",
            "Epoch 81/100, Train Loss: 0.0528, Val Loss: 0.0895\n",
            "Epoch 82/100, Train Loss: 0.0519, Val Loss: 0.0895\n",
            "Epoch 83/100, Train Loss: 0.0521, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0522, Val Loss: 0.0895\n",
            "Epoch 85/100, Train Loss: 0.0523, Val Loss: 0.0895\n",
            "Epoch 86/100, Train Loss: 0.0518, Val Loss: 0.0895\n",
            "Epoch 87/100, Train Loss: 0.0525, Val Loss: 0.0895\n",
            "Epoch 88/100, Train Loss: 0.0521, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0518, Val Loss: 0.0895\n",
            "Epoch 90/100, Train Loss: 0.0519, Val Loss: 0.0895\n",
            "Epoch 91/100, Train Loss: 0.0518, Val Loss: 0.0895\n",
            "Epoch 92/100, Train Loss: 0.0511, Val Loss: 0.0895\n",
            "Epoch 93/100, Train Loss: 0.0508, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0526, Val Loss: 0.0895\n",
            "Epoch 95/100, Train Loss: 0.0522, Val Loss: 0.0895\n",
            "Epoch 96/100, Train Loss: 0.0511, Val Loss: 0.0895\n",
            "Epoch 97/100, Train Loss: 0.0509, Val Loss: 0.0895\n",
            "Epoch 98/100, Train Loss: 0.0530, Val Loss: 0.0895\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0523, Val Loss: 0.0895\n",
            "Epoch 100/100, Train Loss: 0.0521, Val Loss: 0.0895\n",
            "\n",
            "Test iteration 2/3\n",
            "Current training set size: 181 samples\n",
            "Epoch 1/100, Train Loss: 0.1784, Val Loss: 0.3291\n",
            "Epoch 2/100, Train Loss: 0.1585, Val Loss: 0.3869\n",
            "Epoch 3/100, Train Loss: 0.1359, Val Loss: 0.1061\n",
            "Epoch 4/100, Train Loss: 0.0815, Val Loss: 0.0969\n",
            "Epoch 5/100, Train Loss: 0.0689, Val Loss: 0.0906\n",
            "Epoch 6/100, Train Loss: 0.0619, Val Loss: 0.0939\n",
            "Epoch 7/100, Train Loss: 0.0644, Val Loss: 0.0891\n",
            "Epoch 8/100, Train Loss: 0.0628, Val Loss: 0.0904\n",
            "Epoch 9/100, Train Loss: 0.0665, Val Loss: 0.0896\n",
            "Epoch 10/100, Train Loss: 0.0549, Val Loss: 0.1028\n",
            "Epoch 11/100, Train Loss: 0.0598, Val Loss: 0.0850\n",
            "Epoch 12/100, Train Loss: 0.0541, Val Loss: 0.0837\n",
            "Epoch 13/100, Train Loss: 0.0530, Val Loss: 0.0846\n",
            "Epoch 14/100, Train Loss: 0.0513, Val Loss: 0.0818\n",
            "Epoch 15/100, Train Loss: 0.0550, Val Loss: 0.0804\n",
            "Epoch 16/100, Train Loss: 0.0457, Val Loss: 0.0889\n",
            "Epoch 17/100, Train Loss: 0.0498, Val Loss: 0.0731\n",
            "Epoch 18/100, Train Loss: 0.0472, Val Loss: 0.0724\n",
            "Epoch 19/100, Train Loss: 0.0462, Val Loss: 0.0693\n",
            "Epoch 20/100, Train Loss: 0.0443, Val Loss: 0.0762\n",
            "Epoch 21/100, Train Loss: 0.0457, Val Loss: 0.0650\n",
            "Epoch 22/100, Train Loss: 0.0449, Val Loss: 0.0648\n",
            "Epoch 23/100, Train Loss: 0.0477, Val Loss: 0.0752\n",
            "Epoch 24/100, Train Loss: 0.0443, Val Loss: 0.0663\n",
            "Epoch 25/100, Train Loss: 0.0427, Val Loss: 0.0604\n",
            "Epoch 26/100, Train Loss: 0.0411, Val Loss: 0.0769\n",
            "Epoch 27/100, Train Loss: 0.0409, Val Loss: 0.0603\n",
            "Epoch 28/100, Train Loss: 0.0405, Val Loss: 0.0678\n",
            "Epoch 29/100, Train Loss: 0.0390, Val Loss: 0.0588\n",
            "Epoch 30/100, Train Loss: 0.0402, Val Loss: 0.0616\n",
            "Epoch 31/100, Train Loss: 0.0385, Val Loss: 0.0587\n",
            "Epoch 32/100, Train Loss: 0.0423, Val Loss: 0.0591\n",
            "Epoch 33/100, Train Loss: 0.0380, Val Loss: 0.0732\n",
            "Epoch 34/100, Train Loss: 0.0359, Val Loss: 0.0635\n",
            "Epoch 35/100, Train Loss: 0.0409, Val Loss: 0.0588\n",
            "Epoch 36/100, Train Loss: 0.0351, Val Loss: 0.0735\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0389, Val Loss: 0.0681\n",
            "Epoch 38/100, Train Loss: 0.0348, Val Loss: 0.0626\n",
            "Epoch 39/100, Train Loss: 0.0349, Val Loss: 0.0602\n",
            "Epoch 40/100, Train Loss: 0.0326, Val Loss: 0.0590\n",
            "Epoch 41/100, Train Loss: 0.0326, Val Loss: 0.0587\n",
            "Epoch 42/100, Train Loss: 0.0347, Val Loss: 0.0585\n",
            "Epoch 43/100, Train Loss: 0.0327, Val Loss: 0.0581\n",
            "Epoch 44/100, Train Loss: 0.0331, Val Loss: 0.0581\n",
            "Epoch 45/100, Train Loss: 0.0333, Val Loss: 0.0580\n",
            "Epoch 46/100, Train Loss: 0.0334, Val Loss: 0.0575\n",
            "Epoch 47/100, Train Loss: 0.0333, Val Loss: 0.0574\n",
            "Epoch 48/100, Train Loss: 0.0333, Val Loss: 0.0574\n",
            "Epoch 49/100, Train Loss: 0.0344, Val Loss: 0.0572\n",
            "Epoch 50/100, Train Loss: 0.0326, Val Loss: 0.0571\n",
            "Epoch 51/100, Train Loss: 0.0337, Val Loss: 0.0571\n",
            "Epoch 52/100, Train Loss: 0.0334, Val Loss: 0.0570\n",
            "Epoch 53/100, Train Loss: 0.0329, Val Loss: 0.0567\n",
            "Epoch 54/100, Train Loss: 0.0322, Val Loss: 0.0566\n",
            "Epoch 55/100, Train Loss: 0.0331, Val Loss: 0.0565\n",
            "Epoch 56/100, Train Loss: 0.0326, Val Loss: 0.0569\n",
            "Epoch 57/100, Train Loss: 0.0332, Val Loss: 0.0567\n",
            "Epoch 58/100, Train Loss: 0.0328, Val Loss: 0.0566\n",
            "Epoch 59/100, Train Loss: 0.0322, Val Loss: 0.0566\n",
            "Epoch 60/100, Train Loss: 0.0335, Val Loss: 0.0568\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0333, Val Loss: 0.0567\n",
            "Epoch 62/100, Train Loss: 0.0334, Val Loss: 0.0567\n",
            "Epoch 63/100, Train Loss: 0.0315, Val Loss: 0.0567\n",
            "Epoch 64/100, Train Loss: 0.0334, Val Loss: 0.0567\n",
            "Epoch 65/100, Train Loss: 0.0326, Val Loss: 0.0567\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0342, Val Loss: 0.0567\n",
            "Epoch 67/100, Train Loss: 0.0329, Val Loss: 0.0567\n",
            "Epoch 68/100, Train Loss: 0.0329, Val Loss: 0.0567\n",
            "Epoch 69/100, Train Loss: 0.0344, Val Loss: 0.0567\n",
            "Epoch 70/100, Train Loss: 0.0322, Val Loss: 0.0567\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0334, Val Loss: 0.0567\n",
            "Epoch 72/100, Train Loss: 0.0333, Val Loss: 0.0567\n",
            "Epoch 73/100, Train Loss: 0.0333, Val Loss: 0.0567\n",
            "Epoch 74/100, Train Loss: 0.0320, Val Loss: 0.0567\n",
            "Epoch 75/100, Train Loss: 0.0336, Val Loss: 0.0567\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0332, Val Loss: 0.0567\n",
            "Epoch 77/100, Train Loss: 0.0326, Val Loss: 0.0567\n",
            "Epoch 78/100, Train Loss: 0.0343, Val Loss: 0.0567\n",
            "Epoch 79/100, Train Loss: 0.0329, Val Loss: 0.0567\n",
            "Epoch 80/100, Train Loss: 0.0333, Val Loss: 0.0567\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0339, Val Loss: 0.0567\n",
            "Epoch 82/100, Train Loss: 0.0328, Val Loss: 0.0567\n",
            "Epoch 83/100, Train Loss: 0.0335, Val Loss: 0.0567\n",
            "Epoch 84/100, Train Loss: 0.0333, Val Loss: 0.0567\n",
            "Epoch 85/100, Train Loss: 0.0327, Val Loss: 0.0567\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0332, Val Loss: 0.0567\n",
            "Epoch 87/100, Train Loss: 0.0334, Val Loss: 0.0567\n",
            "Epoch 88/100, Train Loss: 0.0326, Val Loss: 0.0567\n",
            "Epoch 89/100, Train Loss: 0.0355, Val Loss: 0.0567\n",
            "Epoch 90/100, Train Loss: 0.0327, Val Loss: 0.0567\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0341, Val Loss: 0.0567\n",
            "Epoch 92/100, Train Loss: 0.0328, Val Loss: 0.0567\n",
            "Epoch 93/100, Train Loss: 0.0326, Val Loss: 0.0567\n",
            "Epoch 94/100, Train Loss: 0.0346, Val Loss: 0.0567\n",
            "Epoch 95/100, Train Loss: 0.0338, Val Loss: 0.0567\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0334, Val Loss: 0.0567\n",
            "Epoch 97/100, Train Loss: 0.0328, Val Loss: 0.0567\n",
            "Epoch 98/100, Train Loss: 0.0333, Val Loss: 0.0567\n",
            "Epoch 99/100, Train Loss: 0.0328, Val Loss: 0.0567\n",
            "Epoch 100/100, Train Loss: 0.0317, Val Loss: 0.0567\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 3/3\n",
            "Current training set size: 182 samples\n",
            "Epoch 1/100, Train Loss: 0.1960, Val Loss: 0.3896\n",
            "Epoch 2/100, Train Loss: 0.1588, Val Loss: 0.3438\n",
            "Epoch 3/100, Train Loss: 0.1267, Val Loss: 0.1324\n",
            "Epoch 4/100, Train Loss: 0.0735, Val Loss: 0.0989\n",
            "Epoch 5/100, Train Loss: 0.0748, Val Loss: 0.1007\n",
            "Epoch 6/100, Train Loss: 0.0676, Val Loss: 0.1011\n",
            "Epoch 7/100, Train Loss: 0.0628, Val Loss: 0.0895\n",
            "Epoch 8/100, Train Loss: 0.0600, Val Loss: 0.0908\n",
            "Epoch 9/100, Train Loss: 0.0573, Val Loss: 0.0898\n",
            "Epoch 10/100, Train Loss: 0.0544, Val Loss: 0.0892\n",
            "Epoch 11/100, Train Loss: 0.0526, Val Loss: 0.0855\n",
            "Epoch 12/100, Train Loss: 0.0482, Val Loss: 0.0830\n",
            "Epoch 13/100, Train Loss: 0.0526, Val Loss: 0.0833\n",
            "Epoch 14/100, Train Loss: 0.0493, Val Loss: 0.0775\n",
            "Epoch 15/100, Train Loss: 0.0522, Val Loss: 0.0764\n",
            "Epoch 16/100, Train Loss: 0.0508, Val Loss: 0.0809\n",
            "Epoch 17/100, Train Loss: 0.0476, Val Loss: 0.0893\n",
            "Epoch 18/100, Train Loss: 0.0429, Val Loss: 0.0773\n",
            "Epoch 19/100, Train Loss: 0.0437, Val Loss: 0.1111\n",
            "Epoch 20/100, Train Loss: 0.0449, Val Loss: 0.0734\n",
            "Epoch 21/100, Train Loss: 0.0436, Val Loss: 0.0689\n",
            "Epoch 22/100, Train Loss: 0.0415, Val Loss: 0.0796\n",
            "Epoch 23/100, Train Loss: 0.0422, Val Loss: 0.0601\n",
            "Epoch 24/100, Train Loss: 0.0418, Val Loss: 0.0589\n",
            "Epoch 25/100, Train Loss: 0.0413, Val Loss: 0.0539\n",
            "Epoch 26/100, Train Loss: 0.0414, Val Loss: 0.1001\n",
            "Epoch 27/100, Train Loss: 0.0403, Val Loss: 0.0618\n",
            "Epoch 28/100, Train Loss: 0.0429, Val Loss: 0.0567\n",
            "Epoch 29/100, Train Loss: 0.0432, Val Loss: 0.0977\n",
            "Epoch 30/100, Train Loss: 0.0433, Val Loss: 0.0669\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0346, Val Loss: 0.0661\n",
            "Epoch 32/100, Train Loss: 0.0356, Val Loss: 0.0650\n",
            "Epoch 33/100, Train Loss: 0.0349, Val Loss: 0.0641\n",
            "Epoch 34/100, Train Loss: 0.0351, Val Loss: 0.0639\n",
            "Epoch 35/100, Train Loss: 0.0357, Val Loss: 0.0634\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0336, Val Loss: 0.0634\n",
            "Epoch 37/100, Train Loss: 0.0367, Val Loss: 0.0634\n",
            "Epoch 38/100, Train Loss: 0.0357, Val Loss: 0.0634\n",
            "Epoch 39/100, Train Loss: 0.0364, Val Loss: 0.0634\n",
            "Epoch 40/100, Train Loss: 0.0358, Val Loss: 0.0634\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0347, Val Loss: 0.0634\n",
            "Epoch 42/100, Train Loss: 0.0346, Val Loss: 0.0634\n",
            "Epoch 43/100, Train Loss: 0.0346, Val Loss: 0.0634\n",
            "Epoch 44/100, Train Loss: 0.0345, Val Loss: 0.0634\n",
            "Epoch 45/100, Train Loss: 0.0340, Val Loss: 0.0634\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0348, Val Loss: 0.0634\n",
            "Epoch 47/100, Train Loss: 0.0347, Val Loss: 0.0634\n",
            "Epoch 48/100, Train Loss: 0.0369, Val Loss: 0.0634\n",
            "Epoch 49/100, Train Loss: 0.0357, Val Loss: 0.0634\n",
            "Epoch 50/100, Train Loss: 0.0345, Val Loss: 0.0634\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0340, Val Loss: 0.0634\n",
            "Epoch 52/100, Train Loss: 0.0343, Val Loss: 0.0634\n",
            "Epoch 53/100, Train Loss: 0.0351, Val Loss: 0.0634\n",
            "Epoch 54/100, Train Loss: 0.0348, Val Loss: 0.0634\n",
            "Epoch 55/100, Train Loss: 0.0352, Val Loss: 0.0634\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0347, Val Loss: 0.0634\n",
            "Epoch 57/100, Train Loss: 0.0357, Val Loss: 0.0634\n",
            "Epoch 58/100, Train Loss: 0.0346, Val Loss: 0.0634\n",
            "Epoch 59/100, Train Loss: 0.0356, Val Loss: 0.0634\n",
            "Epoch 60/100, Train Loss: 0.0351, Val Loss: 0.0634\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0335, Val Loss: 0.0634\n",
            "Epoch 62/100, Train Loss: 0.0350, Val Loss: 0.0634\n",
            "Epoch 63/100, Train Loss: 0.0356, Val Loss: 0.0634\n",
            "Epoch 64/100, Train Loss: 0.0341, Val Loss: 0.0634\n",
            "Epoch 65/100, Train Loss: 0.0357, Val Loss: 0.0634\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0352, Val Loss: 0.0634\n",
            "Epoch 67/100, Train Loss: 0.0346, Val Loss: 0.0634\n",
            "Epoch 68/100, Train Loss: 0.0346, Val Loss: 0.0634\n",
            "Epoch 69/100, Train Loss: 0.0351, Val Loss: 0.0634\n",
            "Epoch 70/100, Train Loss: 0.0339, Val Loss: 0.0634\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0357, Val Loss: 0.0634\n",
            "Epoch 72/100, Train Loss: 0.0354, Val Loss: 0.0634\n",
            "Epoch 73/100, Train Loss: 0.0340, Val Loss: 0.0634\n",
            "Epoch 74/100, Train Loss: 0.0369, Val Loss: 0.0634\n",
            "Epoch 75/100, Train Loss: 0.0341, Val Loss: 0.0634\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0346, Val Loss: 0.0634\n",
            "Epoch 77/100, Train Loss: 0.0362, Val Loss: 0.0634\n",
            "Epoch 78/100, Train Loss: 0.0351, Val Loss: 0.0634\n",
            "Epoch 79/100, Train Loss: 0.0370, Val Loss: 0.0634\n",
            "Epoch 80/100, Train Loss: 0.0344, Val Loss: 0.0634\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0350, Val Loss: 0.0634\n",
            "Epoch 82/100, Train Loss: 0.0338, Val Loss: 0.0634\n",
            "Epoch 83/100, Train Loss: 0.0349, Val Loss: 0.0634\n",
            "Epoch 84/100, Train Loss: 0.0350, Val Loss: 0.0634\n",
            "Epoch 85/100, Train Loss: 0.0361, Val Loss: 0.0634\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0335, Val Loss: 0.0634\n",
            "Epoch 87/100, Train Loss: 0.0354, Val Loss: 0.0634\n",
            "Epoch 88/100, Train Loss: 0.0358, Val Loss: 0.0634\n",
            "Epoch 89/100, Train Loss: 0.0354, Val Loss: 0.0634\n",
            "Epoch 90/100, Train Loss: 0.0354, Val Loss: 0.0634\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0345, Val Loss: 0.0634\n",
            "Epoch 92/100, Train Loss: 0.0335, Val Loss: 0.0634\n",
            "Epoch 93/100, Train Loss: 0.0347, Val Loss: 0.0634\n",
            "Epoch 94/100, Train Loss: 0.0347, Val Loss: 0.0634\n",
            "Epoch 95/100, Train Loss: 0.0362, Val Loss: 0.0634\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0346, Val Loss: 0.0634\n",
            "Epoch 97/100, Train Loss: 0.0349, Val Loss: 0.0634\n",
            "Epoch 98/100, Train Loss: 0.0353, Val Loss: 0.0634\n",
            "Epoch 99/100, Train Loss: 0.0351, Val Loss: 0.0634\n",
            "Epoch 100/100, Train Loss: 0.0359, Val Loss: 0.0634\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: hybrid, Units: L128_G16, Dropout: 0.2, Dense Units: 64\n",
            "Batch Size: 4, Epochs: 100\n",
            "Device: cuda\n",
            "Total test samples: 3\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/3\n",
            "Current training set size: 180 samples\n",
            "Epoch 1/100, Train Loss: 0.1740, Val Loss: 0.3640\n",
            "Epoch 2/100, Train Loss: 0.1469, Val Loss: 0.3016\n",
            "Epoch 3/100, Train Loss: 0.0886, Val Loss: 0.0984\n",
            "Epoch 4/100, Train Loss: 0.0763, Val Loss: 0.0903\n",
            "Epoch 5/100, Train Loss: 0.0707, Val Loss: 0.0894\n",
            "Epoch 6/100, Train Loss: 0.0615, Val Loss: 0.0908\n",
            "Epoch 7/100, Train Loss: 0.0667, Val Loss: 0.0939\n",
            "Epoch 8/100, Train Loss: 0.0608, Val Loss: 0.0886\n",
            "Epoch 9/100, Train Loss: 0.0568, Val Loss: 0.0904\n",
            "Epoch 10/100, Train Loss: 0.0559, Val Loss: 0.0874\n",
            "Epoch 11/100, Train Loss: 0.0501, Val Loss: 0.0879\n",
            "Epoch 12/100, Train Loss: 0.0511, Val Loss: 0.0844\n",
            "Epoch 13/100, Train Loss: 0.0509, Val Loss: 0.0846\n",
            "Epoch 14/100, Train Loss: 0.0520, Val Loss: 0.0807\n",
            "Epoch 15/100, Train Loss: 0.0478, Val Loss: 0.0766\n",
            "Epoch 16/100, Train Loss: 0.0467, Val Loss: 0.0794\n",
            "Epoch 17/100, Train Loss: 0.0527, Val Loss: 0.0708\n",
            "Epoch 18/100, Train Loss: 0.0487, Val Loss: 0.0823\n",
            "Epoch 19/100, Train Loss: 0.0472, Val Loss: 0.0685\n",
            "Epoch 20/100, Train Loss: 0.0425, Val Loss: 0.0682\n",
            "Epoch 21/100, Train Loss: 0.0427, Val Loss: 0.0656\n",
            "Epoch 22/100, Train Loss: 0.0429, Val Loss: 0.0799\n",
            "Epoch 23/100, Train Loss: 0.0421, Val Loss: 0.0638\n",
            "Epoch 24/100, Train Loss: 0.0399, Val Loss: 0.0616\n",
            "Epoch 25/100, Train Loss: 0.0425, Val Loss: 0.0683\n",
            "Epoch 26/100, Train Loss: 0.0402, Val Loss: 0.0644\n",
            "Epoch 27/100, Train Loss: 0.0432, Val Loss: 0.0623\n",
            "Epoch 28/100, Train Loss: 0.0397, Val Loss: 0.0592\n",
            "Epoch 29/100, Train Loss: 0.0395, Val Loss: 0.0639\n",
            "Epoch 30/100, Train Loss: 0.0390, Val Loss: 0.0592\n",
            "Epoch 31/100, Train Loss: 0.0394, Val Loss: 0.0602\n",
            "Epoch 32/100, Train Loss: 0.0374, Val Loss: 0.0585\n",
            "Epoch 33/100, Train Loss: 0.0367, Val Loss: 0.0570\n",
            "Epoch 34/100, Train Loss: 0.0369, Val Loss: 0.0573\n",
            "Epoch 35/100, Train Loss: 0.0369, Val Loss: 0.0518\n",
            "Epoch 36/100, Train Loss: 0.0359, Val Loss: 0.0550\n",
            "Epoch 37/100, Train Loss: 0.0356, Val Loss: 0.0606\n",
            "Epoch 38/100, Train Loss: 0.0370, Val Loss: 0.0665\n",
            "Epoch 39/100, Train Loss: 0.0376, Val Loss: 0.0568\n",
            "Epoch 40/100, Train Loss: 0.0402, Val Loss: 0.0586\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0367, Val Loss: 0.0586\n",
            "Epoch 42/100, Train Loss: 0.0347, Val Loss: 0.0582\n",
            "Epoch 43/100, Train Loss: 0.0331, Val Loss: 0.0581\n",
            "Epoch 44/100, Train Loss: 0.0340, Val Loss: 0.0576\n",
            "Epoch 45/100, Train Loss: 0.0326, Val Loss: 0.0574\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0329, Val Loss: 0.0574\n",
            "Epoch 47/100, Train Loss: 0.0313, Val Loss: 0.0574\n",
            "Epoch 48/100, Train Loss: 0.0323, Val Loss: 0.0574\n",
            "Epoch 49/100, Train Loss: 0.0313, Val Loss: 0.0574\n",
            "Epoch 50/100, Train Loss: 0.0310, Val Loss: 0.0574\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0328, Val Loss: 0.0574\n",
            "Epoch 52/100, Train Loss: 0.0311, Val Loss: 0.0574\n",
            "Epoch 53/100, Train Loss: 0.0325, Val Loss: 0.0574\n",
            "Epoch 54/100, Train Loss: 0.0319, Val Loss: 0.0574\n",
            "Epoch 55/100, Train Loss: 0.0321, Val Loss: 0.0574\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0320, Val Loss: 0.0574\n",
            "Epoch 57/100, Train Loss: 0.0324, Val Loss: 0.0574\n",
            "Epoch 58/100, Train Loss: 0.0321, Val Loss: 0.0574\n",
            "Epoch 59/100, Train Loss: 0.0319, Val Loss: 0.0574\n",
            "Epoch 60/100, Train Loss: 0.0338, Val Loss: 0.0574\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0323, Val Loss: 0.0574\n",
            "Epoch 62/100, Train Loss: 0.0324, Val Loss: 0.0574\n",
            "Epoch 63/100, Train Loss: 0.0317, Val Loss: 0.0574\n",
            "Epoch 64/100, Train Loss: 0.0339, Val Loss: 0.0574\n",
            "Epoch 65/100, Train Loss: 0.0328, Val Loss: 0.0574\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0319, Val Loss: 0.0574\n",
            "Epoch 67/100, Train Loss: 0.0328, Val Loss: 0.0574\n",
            "Epoch 68/100, Train Loss: 0.0318, Val Loss: 0.0574\n",
            "Epoch 69/100, Train Loss: 0.0329, Val Loss: 0.0574\n",
            "Epoch 70/100, Train Loss: 0.0333, Val Loss: 0.0574\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0318, Val Loss: 0.0574\n",
            "Epoch 72/100, Train Loss: 0.0307, Val Loss: 0.0574\n",
            "Epoch 73/100, Train Loss: 0.0325, Val Loss: 0.0574\n",
            "Epoch 74/100, Train Loss: 0.0320, Val Loss: 0.0574\n",
            "Epoch 75/100, Train Loss: 0.0320, Val Loss: 0.0574\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0324, Val Loss: 0.0574\n",
            "Epoch 77/100, Train Loss: 0.0321, Val Loss: 0.0574\n",
            "Epoch 78/100, Train Loss: 0.0311, Val Loss: 0.0574\n",
            "Epoch 79/100, Train Loss: 0.0329, Val Loss: 0.0574\n",
            "Epoch 80/100, Train Loss: 0.0322, Val Loss: 0.0574\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0334, Val Loss: 0.0574\n",
            "Epoch 82/100, Train Loss: 0.0324, Val Loss: 0.0574\n",
            "Epoch 83/100, Train Loss: 0.0331, Val Loss: 0.0574\n",
            "Epoch 84/100, Train Loss: 0.0326, Val Loss: 0.0574\n",
            "Epoch 85/100, Train Loss: 0.0326, Val Loss: 0.0574\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0319, Val Loss: 0.0574\n",
            "Epoch 87/100, Train Loss: 0.0322, Val Loss: 0.0574\n",
            "Epoch 88/100, Train Loss: 0.0329, Val Loss: 0.0574\n",
            "Epoch 89/100, Train Loss: 0.0329, Val Loss: 0.0574\n",
            "Epoch 90/100, Train Loss: 0.0342, Val Loss: 0.0574\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0316, Val Loss: 0.0574\n",
            "Epoch 92/100, Train Loss: 0.0313, Val Loss: 0.0574\n",
            "Epoch 93/100, Train Loss: 0.0311, Val Loss: 0.0574\n",
            "Epoch 94/100, Train Loss: 0.0330, Val Loss: 0.0574\n",
            "Epoch 95/100, Train Loss: 0.0336, Val Loss: 0.0574\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0310, Val Loss: 0.0574\n",
            "Epoch 97/100, Train Loss: 0.0306, Val Loss: 0.0574\n",
            "Epoch 98/100, Train Loss: 0.0320, Val Loss: 0.0574\n",
            "Epoch 99/100, Train Loss: 0.0316, Val Loss: 0.0574\n",
            "Epoch 100/100, Train Loss: 0.0324, Val Loss: 0.0574\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 2/3\n",
            "Current training set size: 181 samples\n",
            "Epoch 1/100, Train Loss: 0.1740, Val Loss: 0.3446\n",
            "Epoch 2/100, Train Loss: 0.1388, Val Loss: 0.1232\n",
            "Epoch 3/100, Train Loss: 0.0921, Val Loss: 0.0929\n",
            "Epoch 4/100, Train Loss: 0.0694, Val Loss: 0.0944\n",
            "Epoch 5/100, Train Loss: 0.0669, Val Loss: 0.0901\n",
            "Epoch 6/100, Train Loss: 0.0564, Val Loss: 0.0935\n",
            "Epoch 7/100, Train Loss: 0.0693, Val Loss: 0.0881\n",
            "Epoch 8/100, Train Loss: 0.0599, Val Loss: 0.0905\n",
            "Epoch 9/100, Train Loss: 0.0620, Val Loss: 0.0879\n",
            "Epoch 10/100, Train Loss: 0.0532, Val Loss: 0.0944\n",
            "Epoch 11/100, Train Loss: 0.0489, Val Loss: 0.0839\n",
            "Epoch 12/100, Train Loss: 0.0498, Val Loss: 0.0832\n",
            "Epoch 13/100, Train Loss: 0.0511, Val Loss: 0.0809\n",
            "Epoch 14/100, Train Loss: 0.0463, Val Loss: 0.0804\n",
            "Epoch 15/100, Train Loss: 0.0563, Val Loss: 0.0811\n",
            "Epoch 16/100, Train Loss: 0.0499, Val Loss: 0.0743\n",
            "Epoch 17/100, Train Loss: 0.0459, Val Loss: 0.0738\n",
            "Epoch 18/100, Train Loss: 0.0456, Val Loss: 0.0689\n",
            "Epoch 19/100, Train Loss: 0.0437, Val Loss: 0.0874\n",
            "Epoch 20/100, Train Loss: 0.0472, Val Loss: 0.0692\n",
            "Epoch 21/100, Train Loss: 0.0411, Val Loss: 0.0600\n",
            "Epoch 22/100, Train Loss: 0.0429, Val Loss: 0.0636\n",
            "Epoch 23/100, Train Loss: 0.0444, Val Loss: 0.0659\n",
            "Epoch 24/100, Train Loss: 0.0415, Val Loss: 0.0582\n",
            "Epoch 25/100, Train Loss: 0.0447, Val Loss: 0.0674\n",
            "Epoch 26/100, Train Loss: 0.0421, Val Loss: 0.0887\n",
            "Epoch 27/100, Train Loss: 0.0409, Val Loss: 0.0594\n",
            "Epoch 28/100, Train Loss: 0.0420, Val Loss: 0.0674\n",
            "Epoch 29/100, Train Loss: 0.0393, Val Loss: 0.0616\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0384, Val Loss: 0.0615\n",
            "Epoch 31/100, Train Loss: 0.0369, Val Loss: 0.0616\n",
            "Epoch 32/100, Train Loss: 0.0363, Val Loss: 0.0618\n",
            "Epoch 33/100, Train Loss: 0.0356, Val Loss: 0.0616\n",
            "Epoch 34/100, Train Loss: 0.0340, Val Loss: 0.0615\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0352, Val Loss: 0.0615\n",
            "Epoch 36/100, Train Loss: 0.0343, Val Loss: 0.0615\n",
            "Epoch 37/100, Train Loss: 0.0339, Val Loss: 0.0615\n",
            "Epoch 38/100, Train Loss: 0.0345, Val Loss: 0.0615\n",
            "Epoch 39/100, Train Loss: 0.0351, Val Loss: 0.0615\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0355, Val Loss: 0.0615\n",
            "Epoch 41/100, Train Loss: 0.0350, Val Loss: 0.0615\n",
            "Epoch 42/100, Train Loss: 0.0360, Val Loss: 0.0615\n",
            "Epoch 43/100, Train Loss: 0.0346, Val Loss: 0.0615\n",
            "Epoch 44/100, Train Loss: 0.0345, Val Loss: 0.0615\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0349, Val Loss: 0.0615\n",
            "Epoch 46/100, Train Loss: 0.0357, Val Loss: 0.0615\n",
            "Epoch 47/100, Train Loss: 0.0352, Val Loss: 0.0615\n",
            "Epoch 48/100, Train Loss: 0.0337, Val Loss: 0.0615\n",
            "Epoch 49/100, Train Loss: 0.0356, Val Loss: 0.0615\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0346, Val Loss: 0.0615\n",
            "Epoch 51/100, Train Loss: 0.0356, Val Loss: 0.0615\n",
            "Epoch 52/100, Train Loss: 0.0363, Val Loss: 0.0615\n",
            "Epoch 53/100, Train Loss: 0.0356, Val Loss: 0.0615\n",
            "Epoch 54/100, Train Loss: 0.0338, Val Loss: 0.0615\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0348, Val Loss: 0.0615\n",
            "Epoch 56/100, Train Loss: 0.0347, Val Loss: 0.0615\n",
            "Epoch 57/100, Train Loss: 0.0343, Val Loss: 0.0615\n",
            "Epoch 58/100, Train Loss: 0.0352, Val Loss: 0.0615\n",
            "Epoch 59/100, Train Loss: 0.0346, Val Loss: 0.0615\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0341, Val Loss: 0.0615\n",
            "Epoch 61/100, Train Loss: 0.0346, Val Loss: 0.0615\n",
            "Epoch 62/100, Train Loss: 0.0353, Val Loss: 0.0615\n",
            "Epoch 63/100, Train Loss: 0.0353, Val Loss: 0.0615\n",
            "Epoch 64/100, Train Loss: 0.0343, Val Loss: 0.0615\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0352, Val Loss: 0.0615\n",
            "Epoch 66/100, Train Loss: 0.0368, Val Loss: 0.0615\n",
            "Epoch 67/100, Train Loss: 0.0353, Val Loss: 0.0615\n",
            "Epoch 68/100, Train Loss: 0.0337, Val Loss: 0.0615\n",
            "Epoch 69/100, Train Loss: 0.0359, Val Loss: 0.0615\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0344, Val Loss: 0.0615\n",
            "Epoch 71/100, Train Loss: 0.0350, Val Loss: 0.0615\n",
            "Epoch 72/100, Train Loss: 0.0353, Val Loss: 0.0615\n",
            "Epoch 73/100, Train Loss: 0.0353, Val Loss: 0.0615\n",
            "Epoch 74/100, Train Loss: 0.0354, Val Loss: 0.0615\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0353, Val Loss: 0.0615\n",
            "Epoch 76/100, Train Loss: 0.0338, Val Loss: 0.0615\n",
            "Epoch 77/100, Train Loss: 0.0350, Val Loss: 0.0615\n",
            "Epoch 78/100, Train Loss: 0.0348, Val Loss: 0.0615\n",
            "Epoch 79/100, Train Loss: 0.0355, Val Loss: 0.0615\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0350, Val Loss: 0.0615\n",
            "Epoch 81/100, Train Loss: 0.0355, Val Loss: 0.0615\n",
            "Epoch 82/100, Train Loss: 0.0342, Val Loss: 0.0615\n",
            "Epoch 83/100, Train Loss: 0.0348, Val Loss: 0.0615\n",
            "Epoch 84/100, Train Loss: 0.0351, Val Loss: 0.0615\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0355, Val Loss: 0.0615\n",
            "Epoch 86/100, Train Loss: 0.0347, Val Loss: 0.0615\n",
            "Epoch 87/100, Train Loss: 0.0345, Val Loss: 0.0615\n",
            "Epoch 88/100, Train Loss: 0.0344, Val Loss: 0.0615\n",
            "Epoch 89/100, Train Loss: 0.0370, Val Loss: 0.0615\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0351, Val Loss: 0.0615\n",
            "Epoch 91/100, Train Loss: 0.0365, Val Loss: 0.0615\n",
            "Epoch 92/100, Train Loss: 0.0351, Val Loss: 0.0615\n",
            "Epoch 93/100, Train Loss: 0.0344, Val Loss: 0.0615\n",
            "Epoch 94/100, Train Loss: 0.0359, Val Loss: 0.0615\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0350, Val Loss: 0.0615\n",
            "Epoch 96/100, Train Loss: 0.0346, Val Loss: 0.0615\n",
            "Epoch 97/100, Train Loss: 0.0353, Val Loss: 0.0615\n",
            "Epoch 98/100, Train Loss: 0.0347, Val Loss: 0.0615\n",
            "Epoch 99/100, Train Loss: 0.0356, Val Loss: 0.0615\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0345, Val Loss: 0.0615\n",
            "\n",
            "Test iteration 3/3\n",
            "Current training set size: 182 samples\n",
            "Epoch 1/100, Train Loss: 0.1912, Val Loss: 0.4021\n",
            "Epoch 2/100, Train Loss: 0.1517, Val Loss: 0.2620\n",
            "Epoch 3/100, Train Loss: 0.1064, Val Loss: 0.0908\n",
            "Epoch 4/100, Train Loss: 0.0789, Val Loss: 0.1426\n",
            "Epoch 5/100, Train Loss: 0.0726, Val Loss: 0.0971\n",
            "Epoch 6/100, Train Loss: 0.0651, Val Loss: 0.0932\n",
            "Epoch 7/100, Train Loss: 0.0578, Val Loss: 0.0915\n",
            "Epoch 8/100, Train Loss: 0.0587, Val Loss: 0.0904\n",
            "Epoch 9/100, Train Loss: 0.0540, Val Loss: 0.0887\n",
            "Epoch 10/100, Train Loss: 0.0541, Val Loss: 0.0876\n",
            "Epoch 11/100, Train Loss: 0.0519, Val Loss: 0.0828\n",
            "Epoch 12/100, Train Loss: 0.0479, Val Loss: 0.0821\n",
            "Epoch 13/100, Train Loss: 0.0511, Val Loss: 0.0815\n",
            "Epoch 14/100, Train Loss: 0.0465, Val Loss: 0.0747\n",
            "Epoch 15/100, Train Loss: 0.0463, Val Loss: 0.0742\n",
            "Epoch 16/100, Train Loss: 0.0491, Val Loss: 0.1046\n",
            "Epoch 17/100, Train Loss: 0.0491, Val Loss: 0.0909\n",
            "Epoch 18/100, Train Loss: 0.0433, Val Loss: 0.0690\n",
            "Epoch 19/100, Train Loss: 0.0428, Val Loss: 0.1267\n",
            "Epoch 20/100, Train Loss: 0.0477, Val Loss: 0.0722\n",
            "Epoch 21/100, Train Loss: 0.0407, Val Loss: 0.0672\n",
            "Epoch 22/100, Train Loss: 0.0396, Val Loss: 0.0665\n",
            "Epoch 23/100, Train Loss: 0.0403, Val Loss: 0.0584\n",
            "Epoch 24/100, Train Loss: 0.0410, Val Loss: 0.0672\n",
            "Epoch 25/100, Train Loss: 0.0381, Val Loss: 0.0561\n",
            "Epoch 26/100, Train Loss: 0.0397, Val Loss: 0.0878\n",
            "Epoch 27/100, Train Loss: 0.0384, Val Loss: 0.0579\n",
            "Epoch 28/100, Train Loss: 0.0376, Val Loss: 0.0549\n",
            "Epoch 29/100, Train Loss: 0.0422, Val Loss: 0.1129\n",
            "Epoch 30/100, Train Loss: 0.0406, Val Loss: 0.0769\n",
            "Epoch 31/100, Train Loss: 0.0397, Val Loss: 0.0612\n",
            "Epoch 32/100, Train Loss: 0.0382, Val Loss: 0.0571\n",
            "Epoch 33/100, Train Loss: 0.0372, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0341, Val Loss: 0.0560\n",
            "Epoch 35/100, Train Loss: 0.0353, Val Loss: 0.0554\n",
            "Epoch 36/100, Train Loss: 0.0332, Val Loss: 0.0554\n",
            "Epoch 37/100, Train Loss: 0.0359, Val Loss: 0.0553\n",
            "Epoch 38/100, Train Loss: 0.0340, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0336, Val Loss: 0.0559\n",
            "Epoch 40/100, Train Loss: 0.0343, Val Loss: 0.0559\n",
            "Epoch 41/100, Train Loss: 0.0335, Val Loss: 0.0559\n",
            "Epoch 42/100, Train Loss: 0.0326, Val Loss: 0.0559\n",
            "Epoch 43/100, Train Loss: 0.0333, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0325, Val Loss: 0.0559\n",
            "Epoch 45/100, Train Loss: 0.0318, Val Loss: 0.0559\n",
            "Epoch 46/100, Train Loss: 0.0339, Val Loss: 0.0559\n",
            "Epoch 47/100, Train Loss: 0.0328, Val Loss: 0.0559\n",
            "Epoch 48/100, Train Loss: 0.0353, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0347, Val Loss: 0.0559\n",
            "Epoch 50/100, Train Loss: 0.0329, Val Loss: 0.0559\n",
            "Epoch 51/100, Train Loss: 0.0333, Val Loss: 0.0559\n",
            "Epoch 52/100, Train Loss: 0.0341, Val Loss: 0.0559\n",
            "Epoch 53/100, Train Loss: 0.0327, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0331, Val Loss: 0.0559\n",
            "Epoch 55/100, Train Loss: 0.0344, Val Loss: 0.0559\n",
            "Epoch 56/100, Train Loss: 0.0338, Val Loss: 0.0559\n",
            "Epoch 57/100, Train Loss: 0.0348, Val Loss: 0.0559\n",
            "Epoch 58/100, Train Loss: 0.0332, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0342, Val Loss: 0.0559\n",
            "Epoch 60/100, Train Loss: 0.0336, Val Loss: 0.0559\n",
            "Epoch 61/100, Train Loss: 0.0335, Val Loss: 0.0559\n",
            "Epoch 62/100, Train Loss: 0.0329, Val Loss: 0.0559\n",
            "Epoch 63/100, Train Loss: 0.0326, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0333, Val Loss: 0.0559\n",
            "Epoch 65/100, Train Loss: 0.0336, Val Loss: 0.0559\n",
            "Epoch 66/100, Train Loss: 0.0335, Val Loss: 0.0559\n",
            "Epoch 67/100, Train Loss: 0.0338, Val Loss: 0.0559\n",
            "Epoch 68/100, Train Loss: 0.0341, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0338, Val Loss: 0.0559\n",
            "Epoch 70/100, Train Loss: 0.0333, Val Loss: 0.0559\n",
            "Epoch 71/100, Train Loss: 0.0334, Val Loss: 0.0559\n",
            "Epoch 72/100, Train Loss: 0.0341, Val Loss: 0.0559\n",
            "Epoch 73/100, Train Loss: 0.0332, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0365, Val Loss: 0.0559\n",
            "Epoch 75/100, Train Loss: 0.0326, Val Loss: 0.0559\n",
            "Epoch 76/100, Train Loss: 0.0338, Val Loss: 0.0559\n",
            "Epoch 77/100, Train Loss: 0.0343, Val Loss: 0.0559\n",
            "Epoch 78/100, Train Loss: 0.0331, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0354, Val Loss: 0.0559\n",
            "Epoch 80/100, Train Loss: 0.0332, Val Loss: 0.0559\n",
            "Epoch 81/100, Train Loss: 0.0327, Val Loss: 0.0559\n",
            "Epoch 82/100, Train Loss: 0.0331, Val Loss: 0.0559\n",
            "Epoch 83/100, Train Loss: 0.0328, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0330, Val Loss: 0.0559\n",
            "Epoch 85/100, Train Loss: 0.0348, Val Loss: 0.0559\n",
            "Epoch 86/100, Train Loss: 0.0331, Val Loss: 0.0559\n",
            "Epoch 87/100, Train Loss: 0.0336, Val Loss: 0.0559\n",
            "Epoch 88/100, Train Loss: 0.0339, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0332, Val Loss: 0.0559\n",
            "Epoch 90/100, Train Loss: 0.0339, Val Loss: 0.0559\n",
            "Epoch 91/100, Train Loss: 0.0331, Val Loss: 0.0559\n",
            "Epoch 92/100, Train Loss: 0.0328, Val Loss: 0.0559\n",
            "Epoch 93/100, Train Loss: 0.0342, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0334, Val Loss: 0.0559\n",
            "Epoch 95/100, Train Loss: 0.0341, Val Loss: 0.0559\n",
            "Epoch 96/100, Train Loss: 0.0321, Val Loss: 0.0559\n",
            "Epoch 97/100, Train Loss: 0.0327, Val Loss: 0.0559\n",
            "Epoch 98/100, Train Loss: 0.0340, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0346, Val Loss: 0.0559\n",
            "Epoch 100/100, Train Loss: 0.0334, Val Loss: 0.0559\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: hybrid, Units: L256_G8, Dropout: 0.1, Dense Units: 32\n",
            "Batch Size: 4, Epochs: 100\n",
            "Device: cuda\n",
            "Total test samples: 3\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/3\n",
            "Current training set size: 180 samples\n",
            "Epoch 1/100, Train Loss: 0.1771, Val Loss: 0.3643\n",
            "Epoch 2/100, Train Loss: 0.1465, Val Loss: 0.2674\n",
            "Epoch 3/100, Train Loss: 0.0895, Val Loss: 0.0883\n",
            "Epoch 4/100, Train Loss: 0.0721, Val Loss: 0.0891\n",
            "Epoch 5/100, Train Loss: 0.0705, Val Loss: 0.0845\n",
            "Epoch 6/100, Train Loss: 0.0680, Val Loss: 0.0859\n",
            "Epoch 7/100, Train Loss: 0.0673, Val Loss: 0.0841\n",
            "Epoch 8/100, Train Loss: 0.0688, Val Loss: 0.0833\n",
            "Epoch 9/100, Train Loss: 0.0578, Val Loss: 0.0834\n",
            "Epoch 10/100, Train Loss: 0.0491, Val Loss: 0.0797\n",
            "Epoch 11/100, Train Loss: 0.0477, Val Loss: 0.0795\n",
            "Epoch 12/100, Train Loss: 0.0487, Val Loss: 0.0779\n",
            "Epoch 13/100, Train Loss: 0.0532, Val Loss: 0.0738\n",
            "Epoch 14/100, Train Loss: 0.0468, Val Loss: 0.0694\n",
            "Epoch 15/100, Train Loss: 0.0489, Val Loss: 0.0697\n",
            "Epoch 16/100, Train Loss: 0.0521, Val Loss: 0.0662\n",
            "Epoch 17/100, Train Loss: 0.0442, Val Loss: 0.0645\n",
            "Epoch 18/100, Train Loss: 0.0430, Val Loss: 0.0706\n",
            "Epoch 19/100, Train Loss: 0.0475, Val Loss: 0.0568\n",
            "Epoch 20/100, Train Loss: 0.0410, Val Loss: 0.0596\n",
            "Epoch 21/100, Train Loss: 0.0395, Val Loss: 0.0544\n",
            "Epoch 22/100, Train Loss: 0.0398, Val Loss: 0.0605\n",
            "Epoch 23/100, Train Loss: 0.0374, Val Loss: 0.0582\n",
            "Epoch 24/100, Train Loss: 0.0392, Val Loss: 0.0579\n",
            "Epoch 25/100, Train Loss: 0.0389, Val Loss: 0.0783\n",
            "Epoch 26/100, Train Loss: 0.0399, Val Loss: 0.0570\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0368, Val Loss: 0.0571\n",
            "Epoch 28/100, Train Loss: 0.0349, Val Loss: 0.0572\n",
            "Epoch 29/100, Train Loss: 0.0353, Val Loss: 0.0569\n",
            "Epoch 30/100, Train Loss: 0.0335, Val Loss: 0.0568\n",
            "Epoch 31/100, Train Loss: 0.0337, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0338, Val Loss: 0.0564\n",
            "Epoch 33/100, Train Loss: 0.0336, Val Loss: 0.0564\n",
            "Epoch 34/100, Train Loss: 0.0335, Val Loss: 0.0564\n",
            "Epoch 35/100, Train Loss: 0.0344, Val Loss: 0.0564\n",
            "Epoch 36/100, Train Loss: 0.0336, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0337, Val Loss: 0.0564\n",
            "Epoch 38/100, Train Loss: 0.0337, Val Loss: 0.0564\n",
            "Epoch 39/100, Train Loss: 0.0340, Val Loss: 0.0564\n",
            "Epoch 40/100, Train Loss: 0.0342, Val Loss: 0.0564\n",
            "Epoch 41/100, Train Loss: 0.0341, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0334, Val Loss: 0.0564\n",
            "Epoch 43/100, Train Loss: 0.0333, Val Loss: 0.0564\n",
            "Epoch 44/100, Train Loss: 0.0337, Val Loss: 0.0564\n",
            "Epoch 45/100, Train Loss: 0.0341, Val Loss: 0.0564\n",
            "Epoch 46/100, Train Loss: 0.0335, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0343, Val Loss: 0.0564\n",
            "Epoch 48/100, Train Loss: 0.0337, Val Loss: 0.0564\n",
            "Epoch 49/100, Train Loss: 0.0329, Val Loss: 0.0564\n",
            "Epoch 50/100, Train Loss: 0.0334, Val Loss: 0.0564\n",
            "Epoch 51/100, Train Loss: 0.0339, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0343, Val Loss: 0.0564\n",
            "Epoch 53/100, Train Loss: 0.0338, Val Loss: 0.0564\n",
            "Epoch 54/100, Train Loss: 0.0335, Val Loss: 0.0564\n",
            "Epoch 55/100, Train Loss: 0.0326, Val Loss: 0.0564\n",
            "Epoch 56/100, Train Loss: 0.0342, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0337, Val Loss: 0.0564\n",
            "Epoch 58/100, Train Loss: 0.0337, Val Loss: 0.0564\n",
            "Epoch 59/100, Train Loss: 0.0334, Val Loss: 0.0564\n",
            "Epoch 60/100, Train Loss: 0.0333, Val Loss: 0.0564\n",
            "Epoch 61/100, Train Loss: 0.0331, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0342, Val Loss: 0.0564\n",
            "Epoch 63/100, Train Loss: 0.0337, Val Loss: 0.0564\n",
            "Epoch 64/100, Train Loss: 0.0331, Val Loss: 0.0564\n",
            "Epoch 65/100, Train Loss: 0.0331, Val Loss: 0.0564\n",
            "Epoch 66/100, Train Loss: 0.0339, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0343, Val Loss: 0.0564\n",
            "Epoch 68/100, Train Loss: 0.0329, Val Loss: 0.0564\n",
            "Epoch 69/100, Train Loss: 0.0340, Val Loss: 0.0564\n",
            "Epoch 70/100, Train Loss: 0.0337, Val Loss: 0.0564\n",
            "Epoch 71/100, Train Loss: 0.0341, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0343, Val Loss: 0.0564\n",
            "Epoch 73/100, Train Loss: 0.0339, Val Loss: 0.0564\n",
            "Epoch 74/100, Train Loss: 0.0340, Val Loss: 0.0564\n",
            "Epoch 75/100, Train Loss: 0.0345, Val Loss: 0.0564\n",
            "Epoch 76/100, Train Loss: 0.0337, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0333, Val Loss: 0.0564\n",
            "Epoch 78/100, Train Loss: 0.0343, Val Loss: 0.0564\n",
            "Epoch 79/100, Train Loss: 0.0340, Val Loss: 0.0564\n",
            "Epoch 80/100, Train Loss: 0.0339, Val Loss: 0.0564\n",
            "Epoch 81/100, Train Loss: 0.0333, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0335, Val Loss: 0.0564\n",
            "Epoch 83/100, Train Loss: 0.0336, Val Loss: 0.0564\n",
            "Epoch 84/100, Train Loss: 0.0342, Val Loss: 0.0564\n",
            "Epoch 85/100, Train Loss: 0.0332, Val Loss: 0.0564\n",
            "Epoch 86/100, Train Loss: 0.0340, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0328, Val Loss: 0.0564\n",
            "Epoch 88/100, Train Loss: 0.0338, Val Loss: 0.0564\n",
            "Epoch 89/100, Train Loss: 0.0339, Val Loss: 0.0564\n",
            "Epoch 90/100, Train Loss: 0.0337, Val Loss: 0.0564\n",
            "Epoch 91/100, Train Loss: 0.0339, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0337, Val Loss: 0.0564\n",
            "Epoch 93/100, Train Loss: 0.0330, Val Loss: 0.0564\n",
            "Epoch 94/100, Train Loss: 0.0333, Val Loss: 0.0564\n",
            "Epoch 95/100, Train Loss: 0.0330, Val Loss: 0.0564\n",
            "Epoch 96/100, Train Loss: 0.0336, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0337, Val Loss: 0.0564\n",
            "Epoch 98/100, Train Loss: 0.0333, Val Loss: 0.0564\n",
            "Epoch 99/100, Train Loss: 0.0352, Val Loss: 0.0564\n",
            "Epoch 100/100, Train Loss: 0.0340, Val Loss: 0.0564\n",
            "\n",
            "Test iteration 2/3\n",
            "Current training set size: 181 samples\n",
            "Epoch 1/100, Train Loss: 0.1752, Val Loss: 0.3222\n",
            "Epoch 2/100, Train Loss: 0.1404, Val Loss: 0.0886\n",
            "Epoch 3/100, Train Loss: 0.1009, Val Loss: 0.0874\n",
            "Epoch 4/100, Train Loss: 0.0754, Val Loss: 0.0948\n",
            "Epoch 5/100, Train Loss: 0.0817, Val Loss: 0.0957\n",
            "Epoch 6/100, Train Loss: 0.0630, Val Loss: 0.0864\n",
            "Epoch 7/100, Train Loss: 0.0683, Val Loss: 0.0830\n",
            "Epoch 8/100, Train Loss: 0.0614, Val Loss: 0.0839\n",
            "Epoch 9/100, Train Loss: 0.0598, Val Loss: 0.0823\n",
            "Epoch 10/100, Train Loss: 0.0513, Val Loss: 0.0882\n",
            "Epoch 11/100, Train Loss: 0.0489, Val Loss: 0.0748\n",
            "Epoch 12/100, Train Loss: 0.0483, Val Loss: 0.0715\n",
            "Epoch 13/100, Train Loss: 0.0469, Val Loss: 0.0683\n",
            "Epoch 14/100, Train Loss: 0.0463, Val Loss: 0.0641\n",
            "Epoch 15/100, Train Loss: 0.0481, Val Loss: 0.0676\n",
            "Epoch 16/100, Train Loss: 0.0483, Val Loss: 0.0770\n",
            "Epoch 17/100, Train Loss: 0.0435, Val Loss: 0.0687\n",
            "Epoch 18/100, Train Loss: 0.0442, Val Loss: 0.0667\n",
            "Epoch 19/100, Train Loss: 0.0433, Val Loss: 0.0645\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0396, Val Loss: 0.0652\n",
            "Epoch 21/100, Train Loss: 0.0387, Val Loss: 0.0658\n",
            "Epoch 22/100, Train Loss: 0.0358, Val Loss: 0.0658\n",
            "Epoch 23/100, Train Loss: 0.0368, Val Loss: 0.0662\n",
            "Epoch 24/100, Train Loss: 0.0366, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0378, Val Loss: 0.0660\n",
            "Epoch 26/100, Train Loss: 0.0380, Val Loss: 0.0660\n",
            "Epoch 27/100, Train Loss: 0.0372, Val Loss: 0.0660\n",
            "Epoch 28/100, Train Loss: 0.0366, Val Loss: 0.0660\n",
            "Epoch 29/100, Train Loss: 0.0368, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0379, Val Loss: 0.0660\n",
            "Epoch 31/100, Train Loss: 0.0372, Val Loss: 0.0660\n",
            "Epoch 32/100, Train Loss: 0.0376, Val Loss: 0.0660\n",
            "Epoch 33/100, Train Loss: 0.0383, Val Loss: 0.0660\n",
            "Epoch 34/100, Train Loss: 0.0367, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0370, Val Loss: 0.0660\n",
            "Epoch 36/100, Train Loss: 0.0362, Val Loss: 0.0660\n",
            "Epoch 37/100, Train Loss: 0.0378, Val Loss: 0.0660\n",
            "Epoch 38/100, Train Loss: 0.0368, Val Loss: 0.0660\n",
            "Epoch 39/100, Train Loss: 0.0370, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0377, Val Loss: 0.0660\n",
            "Epoch 41/100, Train Loss: 0.0368, Val Loss: 0.0660\n",
            "Epoch 42/100, Train Loss: 0.0383, Val Loss: 0.0660\n",
            "Epoch 43/100, Train Loss: 0.0373, Val Loss: 0.0660\n",
            "Epoch 44/100, Train Loss: 0.0368, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0367, Val Loss: 0.0660\n",
            "Epoch 46/100, Train Loss: 0.0370, Val Loss: 0.0660\n",
            "Epoch 47/100, Train Loss: 0.0369, Val Loss: 0.0660\n",
            "Epoch 48/100, Train Loss: 0.0368, Val Loss: 0.0660\n",
            "Epoch 49/100, Train Loss: 0.0364, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0374, Val Loss: 0.0660\n",
            "Epoch 51/100, Train Loss: 0.0375, Val Loss: 0.0660\n",
            "Epoch 52/100, Train Loss: 0.0376, Val Loss: 0.0660\n",
            "Epoch 53/100, Train Loss: 0.0366, Val Loss: 0.0660\n",
            "Epoch 54/100, Train Loss: 0.0370, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0377, Val Loss: 0.0660\n",
            "Epoch 56/100, Train Loss: 0.0372, Val Loss: 0.0660\n",
            "Epoch 57/100, Train Loss: 0.0368, Val Loss: 0.0660\n",
            "Epoch 58/100, Train Loss: 0.0367, Val Loss: 0.0660\n",
            "Epoch 59/100, Train Loss: 0.0369, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0364, Val Loss: 0.0660\n",
            "Epoch 61/100, Train Loss: 0.0366, Val Loss: 0.0660\n",
            "Epoch 62/100, Train Loss: 0.0380, Val Loss: 0.0660\n",
            "Epoch 63/100, Train Loss: 0.0365, Val Loss: 0.0660\n",
            "Epoch 64/100, Train Loss: 0.0366, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0373, Val Loss: 0.0660\n",
            "Epoch 66/100, Train Loss: 0.0377, Val Loss: 0.0660\n",
            "Epoch 67/100, Train Loss: 0.0372, Val Loss: 0.0660\n",
            "Epoch 68/100, Train Loss: 0.0376, Val Loss: 0.0660\n",
            "Epoch 69/100, Train Loss: 0.0363, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0366, Val Loss: 0.0660\n",
            "Epoch 71/100, Train Loss: 0.0367, Val Loss: 0.0660\n",
            "Epoch 72/100, Train Loss: 0.0370, Val Loss: 0.0660\n",
            "Epoch 73/100, Train Loss: 0.0373, Val Loss: 0.0660\n",
            "Epoch 74/100, Train Loss: 0.0369, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0372, Val Loss: 0.0660\n",
            "Epoch 76/100, Train Loss: 0.0368, Val Loss: 0.0660\n",
            "Epoch 77/100, Train Loss: 0.0372, Val Loss: 0.0660\n",
            "Epoch 78/100, Train Loss: 0.0360, Val Loss: 0.0660\n",
            "Epoch 79/100, Train Loss: 0.0373, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0370, Val Loss: 0.0660\n",
            "Epoch 81/100, Train Loss: 0.0373, Val Loss: 0.0660\n",
            "Epoch 82/100, Train Loss: 0.0379, Val Loss: 0.0660\n",
            "Epoch 83/100, Train Loss: 0.0368, Val Loss: 0.0660\n",
            "Epoch 84/100, Train Loss: 0.0373, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0369, Val Loss: 0.0660\n",
            "Epoch 86/100, Train Loss: 0.0370, Val Loss: 0.0660\n",
            "Epoch 87/100, Train Loss: 0.0363, Val Loss: 0.0660\n",
            "Epoch 88/100, Train Loss: 0.0367, Val Loss: 0.0660\n",
            "Epoch 89/100, Train Loss: 0.0389, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0368, Val Loss: 0.0660\n",
            "Epoch 91/100, Train Loss: 0.0377, Val Loss: 0.0660\n",
            "Epoch 92/100, Train Loss: 0.0378, Val Loss: 0.0660\n",
            "Epoch 93/100, Train Loss: 0.0364, Val Loss: 0.0660\n",
            "Epoch 94/100, Train Loss: 0.0374, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0379, Val Loss: 0.0660\n",
            "Epoch 96/100, Train Loss: 0.0370, Val Loss: 0.0660\n",
            "Epoch 97/100, Train Loss: 0.0369, Val Loss: 0.0660\n",
            "Epoch 98/100, Train Loss: 0.0368, Val Loss: 0.0660\n",
            "Epoch 99/100, Train Loss: 0.0369, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0368, Val Loss: 0.0660\n",
            "\n",
            "Test iteration 3/3\n",
            "Current training set size: 182 samples\n",
            "Epoch 1/100, Train Loss: 0.1918, Val Loss: 0.3641\n",
            "Epoch 2/100, Train Loss: 0.1578, Val Loss: 0.2730\n",
            "Epoch 3/100, Train Loss: 0.1077, Val Loss: 0.0901\n",
            "Epoch 4/100, Train Loss: 0.0722, Val Loss: 0.0884\n",
            "Epoch 5/100, Train Loss: 0.0680, Val Loss: 0.0952\n",
            "Epoch 6/100, Train Loss: 0.0656, Val Loss: 0.0866\n",
            "Epoch 7/100, Train Loss: 0.0623, Val Loss: 0.0858\n",
            "Epoch 8/100, Train Loss: 0.0594, Val Loss: 0.0821\n",
            "Epoch 9/100, Train Loss: 0.0573, Val Loss: 0.0836\n",
            "Epoch 10/100, Train Loss: 0.0517, Val Loss: 0.0824\n",
            "Epoch 11/100, Train Loss: 0.0500, Val Loss: 0.0763\n",
            "Epoch 12/100, Train Loss: 0.0492, Val Loss: 0.0783\n",
            "Epoch 13/100, Train Loss: 0.0521, Val Loss: 0.0759\n",
            "Epoch 14/100, Train Loss: 0.0478, Val Loss: 0.0710\n",
            "Epoch 15/100, Train Loss: 0.0494, Val Loss: 0.0693\n",
            "Epoch 16/100, Train Loss: 0.0440, Val Loss: 0.0729\n",
            "Epoch 17/100, Train Loss: 0.0456, Val Loss: 0.0740\n",
            "Epoch 18/100, Train Loss: 0.0470, Val Loss: 0.0686\n",
            "Epoch 19/100, Train Loss: 0.0401, Val Loss: 0.0824\n",
            "Epoch 20/100, Train Loss: 0.0425, Val Loss: 0.0738\n",
            "Epoch 21/100, Train Loss: 0.0404, Val Loss: 0.0591\n",
            "Epoch 22/100, Train Loss: 0.0378, Val Loss: 0.0571\n",
            "Epoch 23/100, Train Loss: 0.0400, Val Loss: 0.0724\n",
            "Epoch 24/100, Train Loss: 0.0386, Val Loss: 0.0694\n",
            "Epoch 25/100, Train Loss: 0.0410, Val Loss: 0.0602\n",
            "Epoch 26/100, Train Loss: 0.0392, Val Loss: 0.0583\n",
            "Epoch 27/100, Train Loss: 0.0362, Val Loss: 0.0516\n",
            "Epoch 28/100, Train Loss: 0.0406, Val Loss: 0.0540\n",
            "Epoch 29/100, Train Loss: 0.0449, Val Loss: 0.1008\n",
            "Epoch 30/100, Train Loss: 0.0375, Val Loss: 0.0582\n",
            "Epoch 31/100, Train Loss: 0.0384, Val Loss: 0.0527\n",
            "Epoch 32/100, Train Loss: 0.0376, Val Loss: 0.0581\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0383, Val Loss: 0.0592\n",
            "Epoch 34/100, Train Loss: 0.0347, Val Loss: 0.0597\n",
            "Epoch 35/100, Train Loss: 0.0330, Val Loss: 0.0601\n",
            "Epoch 36/100, Train Loss: 0.0315, Val Loss: 0.0599\n",
            "Epoch 37/100, Train Loss: 0.0347, Val Loss: 0.0597\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0327, Val Loss: 0.0597\n",
            "Epoch 39/100, Train Loss: 0.0322, Val Loss: 0.0597\n",
            "Epoch 40/100, Train Loss: 0.0323, Val Loss: 0.0597\n",
            "Epoch 41/100, Train Loss: 0.0324, Val Loss: 0.0597\n",
            "Epoch 42/100, Train Loss: 0.0319, Val Loss: 0.0597\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0319, Val Loss: 0.0597\n",
            "Epoch 44/100, Train Loss: 0.0313, Val Loss: 0.0597\n",
            "Epoch 45/100, Train Loss: 0.0323, Val Loss: 0.0597\n",
            "Epoch 46/100, Train Loss: 0.0322, Val Loss: 0.0597\n",
            "Epoch 47/100, Train Loss: 0.0323, Val Loss: 0.0597\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0333, Val Loss: 0.0597\n",
            "Epoch 49/100, Train Loss: 0.0324, Val Loss: 0.0597\n",
            "Epoch 50/100, Train Loss: 0.0316, Val Loss: 0.0597\n",
            "Epoch 51/100, Train Loss: 0.0317, Val Loss: 0.0597\n",
            "Epoch 52/100, Train Loss: 0.0319, Val Loss: 0.0597\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0314, Val Loss: 0.0597\n",
            "Epoch 54/100, Train Loss: 0.0321, Val Loss: 0.0597\n",
            "Epoch 55/100, Train Loss: 0.0313, Val Loss: 0.0597\n",
            "Epoch 56/100, Train Loss: 0.0325, Val Loss: 0.0597\n",
            "Epoch 57/100, Train Loss: 0.0320, Val Loss: 0.0597\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0325, Val Loss: 0.0597\n",
            "Epoch 59/100, Train Loss: 0.0315, Val Loss: 0.0597\n",
            "Epoch 60/100, Train Loss: 0.0319, Val Loss: 0.0597\n",
            "Epoch 61/100, Train Loss: 0.0316, Val Loss: 0.0597\n",
            "Epoch 62/100, Train Loss: 0.0313, Val Loss: 0.0597\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0328, Val Loss: 0.0597\n",
            "Epoch 64/100, Train Loss: 0.0318, Val Loss: 0.0597\n",
            "Epoch 65/100, Train Loss: 0.0329, Val Loss: 0.0597\n",
            "Epoch 66/100, Train Loss: 0.0317, Val Loss: 0.0597\n",
            "Epoch 67/100, Train Loss: 0.0326, Val Loss: 0.0597\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0327, Val Loss: 0.0597\n",
            "Epoch 69/100, Train Loss: 0.0317, Val Loss: 0.0597\n",
            "Epoch 70/100, Train Loss: 0.0322, Val Loss: 0.0597\n",
            "Epoch 71/100, Train Loss: 0.0328, Val Loss: 0.0597\n",
            "Epoch 72/100, Train Loss: 0.0316, Val Loss: 0.0597\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0317, Val Loss: 0.0597\n",
            "Epoch 74/100, Train Loss: 0.0345, Val Loss: 0.0597\n",
            "Epoch 75/100, Train Loss: 0.0316, Val Loss: 0.0597\n",
            "Epoch 76/100, Train Loss: 0.0312, Val Loss: 0.0597\n",
            "Epoch 77/100, Train Loss: 0.0323, Val Loss: 0.0597\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0320, Val Loss: 0.0597\n",
            "Epoch 79/100, Train Loss: 0.0331, Val Loss: 0.0597\n",
            "Epoch 80/100, Train Loss: 0.0322, Val Loss: 0.0597\n",
            "Epoch 81/100, Train Loss: 0.0324, Val Loss: 0.0597\n",
            "Epoch 82/100, Train Loss: 0.0322, Val Loss: 0.0597\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0318, Val Loss: 0.0597\n",
            "Epoch 84/100, Train Loss: 0.0325, Val Loss: 0.0597\n",
            "Epoch 85/100, Train Loss: 0.0324, Val Loss: 0.0597\n",
            "Epoch 86/100, Train Loss: 0.0317, Val Loss: 0.0597\n",
            "Epoch 87/100, Train Loss: 0.0324, Val Loss: 0.0597\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0322, Val Loss: 0.0597\n",
            "Epoch 89/100, Train Loss: 0.0309, Val Loss: 0.0597\n",
            "Epoch 90/100, Train Loss: 0.0327, Val Loss: 0.0597\n",
            "Epoch 91/100, Train Loss: 0.0319, Val Loss: 0.0597\n",
            "Epoch 92/100, Train Loss: 0.0316, Val Loss: 0.0597\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0329, Val Loss: 0.0597\n",
            "Epoch 94/100, Train Loss: 0.0321, Val Loss: 0.0597\n",
            "Epoch 95/100, Train Loss: 0.0318, Val Loss: 0.0597\n",
            "Epoch 96/100, Train Loss: 0.0321, Val Loss: 0.0597\n",
            "Epoch 97/100, Train Loss: 0.0320, Val Loss: 0.0597\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0318, Val Loss: 0.0597\n",
            "Epoch 99/100, Train Loss: 0.0318, Val Loss: 0.0597\n",
            "Epoch 100/100, Train Loss: 0.0312, Val Loss: 0.0597\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: hybrid, Units: L256_G8, Dropout: 0.1, Dense Units: 64\n",
            "Batch Size: 4, Epochs: 100\n",
            "Device: cuda\n",
            "Total test samples: 3\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/3\n",
            "Current training set size: 180 samples\n",
            "Epoch 1/100, Train Loss: 0.1767, Val Loss: 0.3707\n",
            "Epoch 2/100, Train Loss: 0.1448, Val Loss: 0.2439\n",
            "Epoch 3/100, Train Loss: 0.0815, Val Loss: 0.0914\n",
            "Epoch 4/100, Train Loss: 0.0811, Val Loss: 0.0905\n",
            "Epoch 5/100, Train Loss: 0.0654, Val Loss: 0.0830\n",
            "Epoch 6/100, Train Loss: 0.0667, Val Loss: 0.0857\n",
            "Epoch 7/100, Train Loss: 0.0694, Val Loss: 0.1054\n",
            "Epoch 8/100, Train Loss: 0.0608, Val Loss: 0.0802\n",
            "Epoch 9/100, Train Loss: 0.0561, Val Loss: 0.0811\n",
            "Epoch 10/100, Train Loss: 0.0518, Val Loss: 0.0772\n",
            "Epoch 11/100, Train Loss: 0.0458, Val Loss: 0.0772\n",
            "Epoch 12/100, Train Loss: 0.0466, Val Loss: 0.0750\n",
            "Epoch 13/100, Train Loss: 0.0550, Val Loss: 0.0844\n",
            "Epoch 14/100, Train Loss: 0.0522, Val Loss: 0.0701\n",
            "Epoch 15/100, Train Loss: 0.0449, Val Loss: 0.0683\n",
            "Epoch 16/100, Train Loss: 0.0458, Val Loss: 0.0812\n",
            "Epoch 17/100, Train Loss: 0.0476, Val Loss: 0.0628\n",
            "Epoch 18/100, Train Loss: 0.0435, Val Loss: 0.0793\n",
            "Epoch 19/100, Train Loss: 0.0464, Val Loss: 0.0613\n",
            "Epoch 20/100, Train Loss: 0.0432, Val Loss: 0.0605\n",
            "Epoch 21/100, Train Loss: 0.0394, Val Loss: 0.0605\n",
            "Epoch 22/100, Train Loss: 0.0405, Val Loss: 0.0584\n",
            "Epoch 23/100, Train Loss: 0.0360, Val Loss: 0.0514\n",
            "Epoch 24/100, Train Loss: 0.0367, Val Loss: 0.0542\n",
            "Epoch 25/100, Train Loss: 0.0381, Val Loss: 0.0476\n",
            "Epoch 26/100, Train Loss: 0.0358, Val Loss: 0.0537\n",
            "Epoch 27/100, Train Loss: 0.0404, Val Loss: 0.0538\n",
            "Epoch 28/100, Train Loss: 0.0367, Val Loss: 0.0666\n",
            "Epoch 29/100, Train Loss: 0.0355, Val Loss: 0.0573\n",
            "Epoch 30/100, Train Loss: 0.0364, Val Loss: 0.0552\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0311, Val Loss: 0.0544\n",
            "Epoch 32/100, Train Loss: 0.0309, Val Loss: 0.0538\n",
            "Epoch 33/100, Train Loss: 0.0316, Val Loss: 0.0535\n",
            "Epoch 34/100, Train Loss: 0.0301, Val Loss: 0.0532\n",
            "Epoch 35/100, Train Loss: 0.0311, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0301, Val Loss: 0.0526\n",
            "Epoch 37/100, Train Loss: 0.0309, Val Loss: 0.0526\n",
            "Epoch 38/100, Train Loss: 0.0306, Val Loss: 0.0526\n",
            "Epoch 39/100, Train Loss: 0.0299, Val Loss: 0.0526\n",
            "Epoch 40/100, Train Loss: 0.0309, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0315, Val Loss: 0.0526\n",
            "Epoch 42/100, Train Loss: 0.0311, Val Loss: 0.0526\n",
            "Epoch 43/100, Train Loss: 0.0303, Val Loss: 0.0526\n",
            "Epoch 44/100, Train Loss: 0.0306, Val Loss: 0.0526\n",
            "Epoch 45/100, Train Loss: 0.0305, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0304, Val Loss: 0.0526\n",
            "Epoch 47/100, Train Loss: 0.0309, Val Loss: 0.0526\n",
            "Epoch 48/100, Train Loss: 0.0314, Val Loss: 0.0526\n",
            "Epoch 49/100, Train Loss: 0.0294, Val Loss: 0.0526\n",
            "Epoch 50/100, Train Loss: 0.0305, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0313, Val Loss: 0.0526\n",
            "Epoch 52/100, Train Loss: 0.0312, Val Loss: 0.0526\n",
            "Epoch 53/100, Train Loss: 0.0304, Val Loss: 0.0526\n",
            "Epoch 54/100, Train Loss: 0.0307, Val Loss: 0.0526\n",
            "Epoch 55/100, Train Loss: 0.0301, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0310, Val Loss: 0.0526\n",
            "Epoch 57/100, Train Loss: 0.0312, Val Loss: 0.0526\n",
            "Epoch 58/100, Train Loss: 0.0312, Val Loss: 0.0526\n",
            "Epoch 59/100, Train Loss: 0.0309, Val Loss: 0.0526\n",
            "Epoch 60/100, Train Loss: 0.0305, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0308, Val Loss: 0.0526\n",
            "Epoch 62/100, Train Loss: 0.0309, Val Loss: 0.0526\n",
            "Epoch 63/100, Train Loss: 0.0301, Val Loss: 0.0526\n",
            "Epoch 64/100, Train Loss: 0.0302, Val Loss: 0.0526\n",
            "Epoch 65/100, Train Loss: 0.0307, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0304, Val Loss: 0.0526\n",
            "Epoch 67/100, Train Loss: 0.0312, Val Loss: 0.0526\n",
            "Epoch 68/100, Train Loss: 0.0305, Val Loss: 0.0526\n",
            "Epoch 69/100, Train Loss: 0.0307, Val Loss: 0.0526\n",
            "Epoch 70/100, Train Loss: 0.0304, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0311, Val Loss: 0.0526\n",
            "Epoch 72/100, Train Loss: 0.0317, Val Loss: 0.0526\n",
            "Epoch 73/100, Train Loss: 0.0302, Val Loss: 0.0526\n",
            "Epoch 74/100, Train Loss: 0.0305, Val Loss: 0.0526\n",
            "Epoch 75/100, Train Loss: 0.0313, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0304, Val Loss: 0.0526\n",
            "Epoch 77/100, Train Loss: 0.0305, Val Loss: 0.0526\n",
            "Epoch 78/100, Train Loss: 0.0309, Val Loss: 0.0526\n",
            "Epoch 79/100, Train Loss: 0.0310, Val Loss: 0.0526\n",
            "Epoch 80/100, Train Loss: 0.0304, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0302, Val Loss: 0.0526\n",
            "Epoch 82/100, Train Loss: 0.0296, Val Loss: 0.0526\n",
            "Epoch 83/100, Train Loss: 0.0305, Val Loss: 0.0526\n",
            "Epoch 84/100, Train Loss: 0.0316, Val Loss: 0.0526\n",
            "Epoch 85/100, Train Loss: 0.0306, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0309, Val Loss: 0.0526\n",
            "Epoch 87/100, Train Loss: 0.0309, Val Loss: 0.0526\n",
            "Epoch 88/100, Train Loss: 0.0304, Val Loss: 0.0526\n",
            "Epoch 89/100, Train Loss: 0.0307, Val Loss: 0.0526\n",
            "Epoch 90/100, Train Loss: 0.0307, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0311, Val Loss: 0.0526\n",
            "Epoch 92/100, Train Loss: 0.0309, Val Loss: 0.0526\n",
            "Epoch 93/100, Train Loss: 0.0305, Val Loss: 0.0526\n",
            "Epoch 94/100, Train Loss: 0.0306, Val Loss: 0.0526\n",
            "Epoch 95/100, Train Loss: 0.0298, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0311, Val Loss: 0.0526\n",
            "Epoch 97/100, Train Loss: 0.0302, Val Loss: 0.0526\n",
            "Epoch 98/100, Train Loss: 0.0305, Val Loss: 0.0526\n",
            "Epoch 99/100, Train Loss: 0.0318, Val Loss: 0.0526\n",
            "Epoch 100/100, Train Loss: 0.0307, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 2/3\n",
            "Current training set size: 181 samples\n",
            "Epoch 1/100, Train Loss: 0.1741, Val Loss: 0.3232\n",
            "Epoch 2/100, Train Loss: 0.1347, Val Loss: 0.0871\n",
            "Epoch 3/100, Train Loss: 0.0849, Val Loss: 0.0896\n",
            "Epoch 4/100, Train Loss: 0.0642, Val Loss: 0.1023\n",
            "Epoch 5/100, Train Loss: 0.0712, Val Loss: 0.0829\n",
            "Epoch 6/100, Train Loss: 0.0595, Val Loss: 0.0873\n",
            "Epoch 7/100, Train Loss: 0.0613, Val Loss: 0.0844\n",
            "Epoch 8/100, Train Loss: 0.0645, Val Loss: 0.0820\n",
            "Epoch 9/100, Train Loss: 0.0601, Val Loss: 0.0787\n",
            "Epoch 10/100, Train Loss: 0.0498, Val Loss: 0.0983\n",
            "Epoch 11/100, Train Loss: 0.0468, Val Loss: 0.0738\n",
            "Epoch 12/100, Train Loss: 0.0482, Val Loss: 0.0680\n",
            "Epoch 13/100, Train Loss: 0.0465, Val Loss: 0.0657\n",
            "Epoch 14/100, Train Loss: 0.0444, Val Loss: 0.0666\n",
            "Epoch 15/100, Train Loss: 0.0503, Val Loss: 0.0631\n",
            "Epoch 16/100, Train Loss: 0.0448, Val Loss: 0.0673\n",
            "Epoch 17/100, Train Loss: 0.0461, Val Loss: 0.0664\n",
            "Epoch 18/100, Train Loss: 0.0408, Val Loss: 0.0652\n",
            "Epoch 19/100, Train Loss: 0.0407, Val Loss: 0.0702\n",
            "Epoch 20/100, Train Loss: 0.0430, Val Loss: 0.0819\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0414, Val Loss: 0.0826\n",
            "Epoch 22/100, Train Loss: 0.0371, Val Loss: 0.0813\n",
            "Epoch 23/100, Train Loss: 0.0368, Val Loss: 0.0798\n",
            "Epoch 24/100, Train Loss: 0.0372, Val Loss: 0.0790\n",
            "Epoch 25/100, Train Loss: 0.0369, Val Loss: 0.0774\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0368, Val Loss: 0.0774\n",
            "Epoch 27/100, Train Loss: 0.0364, Val Loss: 0.0774\n",
            "Epoch 28/100, Train Loss: 0.0357, Val Loss: 0.0774\n",
            "Epoch 29/100, Train Loss: 0.0359, Val Loss: 0.0773\n",
            "Epoch 30/100, Train Loss: 0.0371, Val Loss: 0.0773\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0362, Val Loss: 0.0773\n",
            "Epoch 32/100, Train Loss: 0.0370, Val Loss: 0.0773\n",
            "Epoch 33/100, Train Loss: 0.0371, Val Loss: 0.0773\n",
            "Epoch 34/100, Train Loss: 0.0356, Val Loss: 0.0773\n",
            "Epoch 35/100, Train Loss: 0.0361, Val Loss: 0.0773\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0356, Val Loss: 0.0773\n",
            "Epoch 37/100, Train Loss: 0.0369, Val Loss: 0.0773\n",
            "Epoch 38/100, Train Loss: 0.0359, Val Loss: 0.0773\n",
            "Epoch 39/100, Train Loss: 0.0356, Val Loss: 0.0773\n",
            "Epoch 40/100, Train Loss: 0.0369, Val Loss: 0.0773\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0356, Val Loss: 0.0773\n",
            "Epoch 42/100, Train Loss: 0.0368, Val Loss: 0.0773\n",
            "Epoch 43/100, Train Loss: 0.0367, Val Loss: 0.0773\n",
            "Epoch 44/100, Train Loss: 0.0359, Val Loss: 0.0773\n",
            "Epoch 45/100, Train Loss: 0.0358, Val Loss: 0.0773\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0361, Val Loss: 0.0773\n",
            "Epoch 47/100, Train Loss: 0.0363, Val Loss: 0.0773\n",
            "Epoch 48/100, Train Loss: 0.0352, Val Loss: 0.0773\n",
            "Epoch 49/100, Train Loss: 0.0351, Val Loss: 0.0773\n",
            "Epoch 50/100, Train Loss: 0.0361, Val Loss: 0.0773\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0358, Val Loss: 0.0773\n",
            "Epoch 52/100, Train Loss: 0.0365, Val Loss: 0.0773\n",
            "Epoch 53/100, Train Loss: 0.0361, Val Loss: 0.0773\n",
            "Epoch 54/100, Train Loss: 0.0356, Val Loss: 0.0773\n",
            "Epoch 55/100, Train Loss: 0.0360, Val Loss: 0.0773\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0364, Val Loss: 0.0773\n",
            "Epoch 57/100, Train Loss: 0.0361, Val Loss: 0.0773\n",
            "Epoch 58/100, Train Loss: 0.0351, Val Loss: 0.0773\n",
            "Epoch 59/100, Train Loss: 0.0354, Val Loss: 0.0773\n",
            "Epoch 60/100, Train Loss: 0.0350, Val Loss: 0.0773\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0351, Val Loss: 0.0773\n",
            "Epoch 62/100, Train Loss: 0.0367, Val Loss: 0.0773\n",
            "Epoch 63/100, Train Loss: 0.0362, Val Loss: 0.0773\n",
            "Epoch 64/100, Train Loss: 0.0357, Val Loss: 0.0773\n",
            "Epoch 65/100, Train Loss: 0.0361, Val Loss: 0.0773\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0367, Val Loss: 0.0773\n",
            "Epoch 67/100, Train Loss: 0.0366, Val Loss: 0.0773\n",
            "Epoch 68/100, Train Loss: 0.0362, Val Loss: 0.0773\n",
            "Epoch 69/100, Train Loss: 0.0355, Val Loss: 0.0773\n",
            "Epoch 70/100, Train Loss: 0.0351, Val Loss: 0.0773\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0353, Val Loss: 0.0773\n",
            "Epoch 72/100, Train Loss: 0.0358, Val Loss: 0.0773\n",
            "Epoch 73/100, Train Loss: 0.0365, Val Loss: 0.0773\n",
            "Epoch 74/100, Train Loss: 0.0364, Val Loss: 0.0773\n",
            "Epoch 75/100, Train Loss: 0.0363, Val Loss: 0.0773\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0356, Val Loss: 0.0773\n",
            "Epoch 77/100, Train Loss: 0.0361, Val Loss: 0.0773\n",
            "Epoch 78/100, Train Loss: 0.0353, Val Loss: 0.0773\n",
            "Epoch 79/100, Train Loss: 0.0369, Val Loss: 0.0773\n",
            "Epoch 80/100, Train Loss: 0.0359, Val Loss: 0.0773\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0361, Val Loss: 0.0773\n",
            "Epoch 82/100, Train Loss: 0.0373, Val Loss: 0.0773\n",
            "Epoch 83/100, Train Loss: 0.0354, Val Loss: 0.0773\n",
            "Epoch 84/100, Train Loss: 0.0361, Val Loss: 0.0773\n",
            "Epoch 85/100, Train Loss: 0.0361, Val Loss: 0.0773\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0356, Val Loss: 0.0773\n",
            "Epoch 87/100, Train Loss: 0.0355, Val Loss: 0.0773\n",
            "Epoch 88/100, Train Loss: 0.0354, Val Loss: 0.0773\n",
            "Epoch 89/100, Train Loss: 0.0370, Val Loss: 0.0773\n",
            "Epoch 90/100, Train Loss: 0.0357, Val Loss: 0.0773\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0364, Val Loss: 0.0773\n",
            "Epoch 92/100, Train Loss: 0.0368, Val Loss: 0.0773\n",
            "Epoch 93/100, Train Loss: 0.0359, Val Loss: 0.0773\n",
            "Epoch 94/100, Train Loss: 0.0368, Val Loss: 0.0773\n",
            "Epoch 95/100, Train Loss: 0.0368, Val Loss: 0.0773\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0357, Val Loss: 0.0773\n",
            "Epoch 97/100, Train Loss: 0.0361, Val Loss: 0.0773\n",
            "Epoch 98/100, Train Loss: 0.0353, Val Loss: 0.0773\n",
            "Epoch 99/100, Train Loss: 0.0358, Val Loss: 0.0773\n",
            "Epoch 100/100, Train Loss: 0.0355, Val Loss: 0.0773\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 3/3\n",
            "Current training set size: 182 samples\n",
            "Epoch 1/100, Train Loss: 0.1881, Val Loss: 0.3498\n",
            "Epoch 2/100, Train Loss: 0.1591, Val Loss: 0.2820\n",
            "Epoch 3/100, Train Loss: 0.1179, Val Loss: 0.1249\n",
            "Epoch 4/100, Train Loss: 0.0713, Val Loss: 0.0903\n",
            "Epoch 5/100, Train Loss: 0.0628, Val Loss: 0.0989\n",
            "Epoch 6/100, Train Loss: 0.0633, Val Loss: 0.0840\n",
            "Epoch 7/100, Train Loss: 0.0599, Val Loss: 0.0851\n",
            "Epoch 8/100, Train Loss: 0.0522, Val Loss: 0.0834\n",
            "Epoch 9/100, Train Loss: 0.0526, Val Loss: 0.0842\n",
            "Epoch 10/100, Train Loss: 0.0488, Val Loss: 0.0991\n",
            "Epoch 11/100, Train Loss: 0.0591, Val Loss: 0.0747\n",
            "Epoch 12/100, Train Loss: 0.0525, Val Loss: 0.0753\n",
            "Epoch 13/100, Train Loss: 0.0475, Val Loss: 0.0697\n",
            "Epoch 14/100, Train Loss: 0.0489, Val Loss: 0.0650\n",
            "Epoch 15/100, Train Loss: 0.0505, Val Loss: 0.0676\n",
            "Epoch 16/100, Train Loss: 0.0473, Val Loss: 0.0988\n",
            "Epoch 17/100, Train Loss: 0.0456, Val Loss: 0.0748\n",
            "Epoch 18/100, Train Loss: 0.0444, Val Loss: 0.0680\n",
            "Epoch 19/100, Train Loss: 0.0400, Val Loss: 0.1037\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0543, Val Loss: 0.0902\n",
            "Epoch 21/100, Train Loss: 0.0417, Val Loss: 0.0812\n",
            "Epoch 22/100, Train Loss: 0.0393, Val Loss: 0.0778\n",
            "Epoch 23/100, Train Loss: 0.0377, Val Loss: 0.0765\n",
            "Epoch 24/100, Train Loss: 0.0402, Val Loss: 0.0754\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0378, Val Loss: 0.0754\n",
            "Epoch 26/100, Train Loss: 0.0384, Val Loss: 0.0754\n",
            "Epoch 27/100, Train Loss: 0.0384, Val Loss: 0.0753\n",
            "Epoch 28/100, Train Loss: 0.0395, Val Loss: 0.0753\n",
            "Epoch 29/100, Train Loss: 0.0377, Val Loss: 0.0753\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0377, Val Loss: 0.0753\n",
            "Epoch 31/100, Train Loss: 0.0381, Val Loss: 0.0753\n",
            "Epoch 32/100, Train Loss: 0.0391, Val Loss: 0.0753\n",
            "Epoch 33/100, Train Loss: 0.0379, Val Loss: 0.0753\n",
            "Epoch 34/100, Train Loss: 0.0381, Val Loss: 0.0753\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0382, Val Loss: 0.0753\n",
            "Epoch 36/100, Train Loss: 0.0382, Val Loss: 0.0753\n",
            "Epoch 37/100, Train Loss: 0.0419, Val Loss: 0.0753\n",
            "Epoch 38/100, Train Loss: 0.0381, Val Loss: 0.0753\n",
            "Epoch 39/100, Train Loss: 0.0378, Val Loss: 0.0753\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0374, Val Loss: 0.0753\n",
            "Epoch 41/100, Train Loss: 0.0387, Val Loss: 0.0753\n",
            "Epoch 42/100, Train Loss: 0.0377, Val Loss: 0.0753\n",
            "Epoch 43/100, Train Loss: 0.0389, Val Loss: 0.0753\n",
            "Epoch 44/100, Train Loss: 0.0368, Val Loss: 0.0753\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0381, Val Loss: 0.0753\n",
            "Epoch 46/100, Train Loss: 0.0379, Val Loss: 0.0753\n",
            "Epoch 47/100, Train Loss: 0.0380, Val Loss: 0.0753\n",
            "Epoch 48/100, Train Loss: 0.0391, Val Loss: 0.0753\n",
            "Epoch 49/100, Train Loss: 0.0389, Val Loss: 0.0753\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0372, Val Loss: 0.0753\n",
            "Epoch 51/100, Train Loss: 0.0380, Val Loss: 0.0753\n",
            "Epoch 52/100, Train Loss: 0.0379, Val Loss: 0.0753\n",
            "Epoch 53/100, Train Loss: 0.0378, Val Loss: 0.0753\n",
            "Epoch 54/100, Train Loss: 0.0377, Val Loss: 0.0753\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0375, Val Loss: 0.0753\n",
            "Epoch 56/100, Train Loss: 0.0385, Val Loss: 0.0753\n",
            "Epoch 57/100, Train Loss: 0.0388, Val Loss: 0.0753\n",
            "Epoch 58/100, Train Loss: 0.0384, Val Loss: 0.0753\n",
            "Epoch 59/100, Train Loss: 0.0380, Val Loss: 0.0753\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0380, Val Loss: 0.0753\n",
            "Epoch 61/100, Train Loss: 0.0379, Val Loss: 0.0753\n",
            "Epoch 62/100, Train Loss: 0.0371, Val Loss: 0.0753\n",
            "Epoch 63/100, Train Loss: 0.0380, Val Loss: 0.0753\n",
            "Epoch 64/100, Train Loss: 0.0381, Val Loss: 0.0753\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0386, Val Loss: 0.0753\n",
            "Epoch 66/100, Train Loss: 0.0371, Val Loss: 0.0753\n",
            "Epoch 67/100, Train Loss: 0.0382, Val Loss: 0.0753\n",
            "Epoch 68/100, Train Loss: 0.0380, Val Loss: 0.0753\n",
            "Epoch 69/100, Train Loss: 0.0379, Val Loss: 0.0753\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0382, Val Loss: 0.0753\n",
            "Epoch 71/100, Train Loss: 0.0390, Val Loss: 0.0753\n",
            "Epoch 72/100, Train Loss: 0.0385, Val Loss: 0.0753\n",
            "Epoch 73/100, Train Loss: 0.0372, Val Loss: 0.0753\n",
            "Epoch 74/100, Train Loss: 0.0405, Val Loss: 0.0753\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0382, Val Loss: 0.0753\n",
            "Epoch 76/100, Train Loss: 0.0373, Val Loss: 0.0753\n",
            "Epoch 77/100, Train Loss: 0.0392, Val Loss: 0.0753\n",
            "Epoch 78/100, Train Loss: 0.0378, Val Loss: 0.0753\n",
            "Epoch 79/100, Train Loss: 0.0387, Val Loss: 0.0753\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0378, Val Loss: 0.0753\n",
            "Epoch 81/100, Train Loss: 0.0377, Val Loss: 0.0753\n",
            "Epoch 82/100, Train Loss: 0.0382, Val Loss: 0.0753\n",
            "Epoch 83/100, Train Loss: 0.0388, Val Loss: 0.0753\n",
            "Epoch 84/100, Train Loss: 0.0380, Val Loss: 0.0753\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0384, Val Loss: 0.0753\n",
            "Epoch 86/100, Train Loss: 0.0375, Val Loss: 0.0753\n",
            "Epoch 87/100, Train Loss: 0.0384, Val Loss: 0.0753\n",
            "Epoch 88/100, Train Loss: 0.0385, Val Loss: 0.0753\n",
            "Epoch 89/100, Train Loss: 0.0371, Val Loss: 0.0753\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0379, Val Loss: 0.0753\n",
            "Epoch 91/100, Train Loss: 0.0379, Val Loss: 0.0753\n",
            "Epoch 92/100, Train Loss: 0.0380, Val Loss: 0.0753\n",
            "Epoch 93/100, Train Loss: 0.0380, Val Loss: 0.0753\n",
            "Epoch 94/100, Train Loss: 0.0381, Val Loss: 0.0753\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0380, Val Loss: 0.0753\n",
            "Epoch 96/100, Train Loss: 0.0387, Val Loss: 0.0753\n",
            "Epoch 97/100, Train Loss: 0.0380, Val Loss: 0.0753\n",
            "Epoch 98/100, Train Loss: 0.0377, Val Loss: 0.0753\n",
            "Epoch 99/100, Train Loss: 0.0381, Val Loss: 0.0753\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0372, Val Loss: 0.0753\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: hybrid, Units: L256_G8, Dropout: 0.2, Dense Units: 32\n",
            "Batch Size: 4, Epochs: 100\n",
            "Device: cuda\n",
            "Total test samples: 3\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/3\n",
            "Current training set size: 180 samples\n",
            "Epoch 1/100, Train Loss: 0.1757, Val Loss: 0.3698\n",
            "Epoch 2/100, Train Loss: 0.1500, Val Loss: 0.2181\n",
            "Epoch 3/100, Train Loss: 0.0928, Val Loss: 0.0853\n",
            "Epoch 4/100, Train Loss: 0.0749, Val Loss: 0.0980\n",
            "Epoch 5/100, Train Loss: 0.0685, Val Loss: 0.0834\n",
            "Epoch 6/100, Train Loss: 0.0701, Val Loss: 0.0905\n",
            "Epoch 7/100, Train Loss: 0.0687, Val Loss: 0.0861\n",
            "Epoch 8/100, Train Loss: 0.0588, Val Loss: 0.0847\n",
            "Epoch 9/100, Train Loss: 0.0589, Val Loss: 0.0870\n",
            "Epoch 10/100, Train Loss: 0.0600, Val Loss: 0.0797\n",
            "Epoch 11/100, Train Loss: 0.0475, Val Loss: 0.0811\n",
            "Epoch 12/100, Train Loss: 0.0495, Val Loss: 0.0785\n",
            "Epoch 13/100, Train Loss: 0.0511, Val Loss: 0.0789\n",
            "Epoch 14/100, Train Loss: 0.0480, Val Loss: 0.0733\n",
            "Epoch 15/100, Train Loss: 0.0465, Val Loss: 0.0716\n",
            "Epoch 16/100, Train Loss: 0.0448, Val Loss: 0.0725\n",
            "Epoch 17/100, Train Loss: 0.0478, Val Loss: 0.0661\n",
            "Epoch 18/100, Train Loss: 0.0430, Val Loss: 0.0696\n",
            "Epoch 19/100, Train Loss: 0.0537, Val Loss: 0.0628\n",
            "Epoch 20/100, Train Loss: 0.0440, Val Loss: 0.0632\n",
            "Epoch 21/100, Train Loss: 0.0466, Val Loss: 0.0678\n",
            "Epoch 22/100, Train Loss: 0.0411, Val Loss: 0.0603\n",
            "Epoch 23/100, Train Loss: 0.0401, Val Loss: 0.0747\n",
            "Epoch 24/100, Train Loss: 0.0415, Val Loss: 0.0618\n",
            "Epoch 25/100, Train Loss: 0.0391, Val Loss: 0.0777\n",
            "Epoch 26/100, Train Loss: 0.0390, Val Loss: 0.0600\n",
            "Epoch 27/100, Train Loss: 0.0422, Val Loss: 0.0503\n",
            "Epoch 28/100, Train Loss: 0.0387, Val Loss: 0.0582\n",
            "Epoch 29/100, Train Loss: 0.0412, Val Loss: 0.0526\n",
            "Epoch 30/100, Train Loss: 0.0361, Val Loss: 0.0520\n",
            "Epoch 31/100, Train Loss: 0.0377, Val Loss: 0.0667\n",
            "Epoch 32/100, Train Loss: 0.0379, Val Loss: 0.0587\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0397, Val Loss: 0.0527\n",
            "Epoch 34/100, Train Loss: 0.0334, Val Loss: 0.0509\n",
            "Epoch 35/100, Train Loss: 0.0333, Val Loss: 0.0503\n",
            "Epoch 36/100, Train Loss: 0.0314, Val Loss: 0.0502\n",
            "Epoch 37/100, Train Loss: 0.0332, Val Loss: 0.0501\n",
            "Epoch 38/100, Train Loss: 0.0324, Val Loss: 0.0502\n",
            "Epoch 39/100, Train Loss: 0.0323, Val Loss: 0.0500\n",
            "Epoch 40/100, Train Loss: 0.0322, Val Loss: 0.0504\n",
            "Epoch 41/100, Train Loss: 0.0336, Val Loss: 0.0503\n",
            "Epoch 42/100, Train Loss: 0.0329, Val Loss: 0.0500\n",
            "Epoch 43/100, Train Loss: 0.0324, Val Loss: 0.0500\n",
            "Epoch 44/100, Train Loss: 0.0326, Val Loss: 0.0499\n",
            "Epoch 45/100, Train Loss: 0.0322, Val Loss: 0.0496\n",
            "Epoch 46/100, Train Loss: 0.0313, Val Loss: 0.0495\n",
            "Epoch 47/100, Train Loss: 0.0328, Val Loss: 0.0496\n",
            "Epoch 48/100, Train Loss: 0.0328, Val Loss: 0.0497\n",
            "Epoch 49/100, Train Loss: 0.0311, Val Loss: 0.0496\n",
            "Epoch 50/100, Train Loss: 0.0321, Val Loss: 0.0496\n",
            "Epoch 51/100, Train Loss: 0.0325, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0316, Val Loss: 0.0498\n",
            "Epoch 53/100, Train Loss: 0.0317, Val Loss: 0.0498\n",
            "Epoch 54/100, Train Loss: 0.0318, Val Loss: 0.0498\n",
            "Epoch 55/100, Train Loss: 0.0326, Val Loss: 0.0498\n",
            "Epoch 56/100, Train Loss: 0.0316, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0335, Val Loss: 0.0498\n",
            "Epoch 58/100, Train Loss: 0.0330, Val Loss: 0.0498\n",
            "Epoch 59/100, Train Loss: 0.0317, Val Loss: 0.0498\n",
            "Epoch 60/100, Train Loss: 0.0320, Val Loss: 0.0498\n",
            "Epoch 61/100, Train Loss: 0.0311, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0326, Val Loss: 0.0498\n",
            "Epoch 63/100, Train Loss: 0.0311, Val Loss: 0.0498\n",
            "Epoch 64/100, Train Loss: 0.0304, Val Loss: 0.0498\n",
            "Epoch 65/100, Train Loss: 0.0317, Val Loss: 0.0498\n",
            "Epoch 66/100, Train Loss: 0.0321, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0322, Val Loss: 0.0498\n",
            "Epoch 68/100, Train Loss: 0.0317, Val Loss: 0.0498\n",
            "Epoch 69/100, Train Loss: 0.0325, Val Loss: 0.0498\n",
            "Epoch 70/100, Train Loss: 0.0321, Val Loss: 0.0498\n",
            "Epoch 71/100, Train Loss: 0.0328, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0326, Val Loss: 0.0498\n",
            "Epoch 73/100, Train Loss: 0.0325, Val Loss: 0.0498\n",
            "Epoch 74/100, Train Loss: 0.0321, Val Loss: 0.0498\n",
            "Epoch 75/100, Train Loss: 0.0327, Val Loss: 0.0498\n",
            "Epoch 76/100, Train Loss: 0.0314, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0318, Val Loss: 0.0498\n",
            "Epoch 78/100, Train Loss: 0.0318, Val Loss: 0.0498\n",
            "Epoch 79/100, Train Loss: 0.0322, Val Loss: 0.0498\n",
            "Epoch 80/100, Train Loss: 0.0320, Val Loss: 0.0498\n",
            "Epoch 81/100, Train Loss: 0.0308, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0318, Val Loss: 0.0498\n",
            "Epoch 83/100, Train Loss: 0.0309, Val Loss: 0.0498\n",
            "Epoch 84/100, Train Loss: 0.0324, Val Loss: 0.0498\n",
            "Epoch 85/100, Train Loss: 0.0320, Val Loss: 0.0498\n",
            "Epoch 86/100, Train Loss: 0.0328, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0325, Val Loss: 0.0498\n",
            "Epoch 88/100, Train Loss: 0.0325, Val Loss: 0.0498\n",
            "Epoch 89/100, Train Loss: 0.0325, Val Loss: 0.0498\n",
            "Epoch 90/100, Train Loss: 0.0312, Val Loss: 0.0498\n",
            "Epoch 91/100, Train Loss: 0.0321, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0337, Val Loss: 0.0498\n",
            "Epoch 93/100, Train Loss: 0.0314, Val Loss: 0.0498\n",
            "Epoch 94/100, Train Loss: 0.0309, Val Loss: 0.0498\n",
            "Epoch 95/100, Train Loss: 0.0315, Val Loss: 0.0498\n",
            "Epoch 96/100, Train Loss: 0.0324, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0321, Val Loss: 0.0498\n",
            "Epoch 98/100, Train Loss: 0.0309, Val Loss: 0.0498\n",
            "Epoch 99/100, Train Loss: 0.0322, Val Loss: 0.0498\n",
            "Epoch 100/100, Train Loss: 0.0324, Val Loss: 0.0498\n",
            "\n",
            "Test iteration 2/3\n",
            "Current training set size: 181 samples\n",
            "Epoch 1/100, Train Loss: 0.1753, Val Loss: 0.3265\n",
            "Epoch 2/100, Train Loss: 0.1389, Val Loss: 0.0957\n",
            "Epoch 3/100, Train Loss: 0.1421, Val Loss: 0.2005\n",
            "Epoch 4/100, Train Loss: 0.0991, Val Loss: 0.0908\n",
            "Epoch 5/100, Train Loss: 0.0683, Val Loss: 0.0860\n",
            "Epoch 6/100, Train Loss: 0.0647, Val Loss: 0.0951\n",
            "Epoch 7/100, Train Loss: 0.0713, Val Loss: 0.0854\n",
            "Epoch 8/100, Train Loss: 0.0643, Val Loss: 0.0822\n",
            "Epoch 9/100, Train Loss: 0.0608, Val Loss: 0.0843\n",
            "Epoch 10/100, Train Loss: 0.0515, Val Loss: 0.1167\n",
            "Epoch 11/100, Train Loss: 0.0509, Val Loss: 0.0787\n",
            "Epoch 12/100, Train Loss: 0.0517, Val Loss: 0.0774\n",
            "Epoch 13/100, Train Loss: 0.0481, Val Loss: 0.0750\n",
            "Epoch 14/100, Train Loss: 0.0485, Val Loss: 0.0722\n",
            "Epoch 15/100, Train Loss: 0.0528, Val Loss: 0.0716\n",
            "Epoch 16/100, Train Loss: 0.0465, Val Loss: 0.0997\n",
            "Epoch 17/100, Train Loss: 0.0511, Val Loss: 0.0713\n",
            "Epoch 18/100, Train Loss: 0.0474, Val Loss: 0.0637\n",
            "Epoch 19/100, Train Loss: 0.0426, Val Loss: 0.0582\n",
            "Epoch 20/100, Train Loss: 0.0416, Val Loss: 0.0601\n",
            "Epoch 21/100, Train Loss: 0.0462, Val Loss: 0.0588\n",
            "Epoch 22/100, Train Loss: 0.0433, Val Loss: 0.0615\n",
            "Epoch 23/100, Train Loss: 0.0428, Val Loss: 0.0664\n",
            "Epoch 24/100, Train Loss: 0.0412, Val Loss: 0.0776\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0374, Val Loss: 0.0765\n",
            "Epoch 26/100, Train Loss: 0.0375, Val Loss: 0.0747\n",
            "Epoch 27/100, Train Loss: 0.0379, Val Loss: 0.0731\n",
            "Epoch 28/100, Train Loss: 0.0377, Val Loss: 0.0723\n",
            "Epoch 29/100, Train Loss: 0.0375, Val Loss: 0.0719\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0375, Val Loss: 0.0719\n",
            "Epoch 31/100, Train Loss: 0.0368, Val Loss: 0.0719\n",
            "Epoch 32/100, Train Loss: 0.0383, Val Loss: 0.0719\n",
            "Epoch 33/100, Train Loss: 0.0380, Val Loss: 0.0719\n",
            "Epoch 34/100, Train Loss: 0.0368, Val Loss: 0.0718\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0362, Val Loss: 0.0718\n",
            "Epoch 36/100, Train Loss: 0.0361, Val Loss: 0.0718\n",
            "Epoch 37/100, Train Loss: 0.0375, Val Loss: 0.0718\n",
            "Epoch 38/100, Train Loss: 0.0366, Val Loss: 0.0718\n",
            "Epoch 39/100, Train Loss: 0.0369, Val Loss: 0.0718\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0384, Val Loss: 0.0718\n",
            "Epoch 41/100, Train Loss: 0.0358, Val Loss: 0.0718\n",
            "Epoch 42/100, Train Loss: 0.0372, Val Loss: 0.0718\n",
            "Epoch 43/100, Train Loss: 0.0368, Val Loss: 0.0718\n",
            "Epoch 44/100, Train Loss: 0.0378, Val Loss: 0.0718\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0359, Val Loss: 0.0718\n",
            "Epoch 46/100, Train Loss: 0.0370, Val Loss: 0.0718\n",
            "Epoch 47/100, Train Loss: 0.0369, Val Loss: 0.0718\n",
            "Epoch 48/100, Train Loss: 0.0363, Val Loss: 0.0718\n",
            "Epoch 49/100, Train Loss: 0.0355, Val Loss: 0.0718\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0352, Val Loss: 0.0718\n",
            "Epoch 51/100, Train Loss: 0.0358, Val Loss: 0.0718\n",
            "Epoch 52/100, Train Loss: 0.0370, Val Loss: 0.0718\n",
            "Epoch 53/100, Train Loss: 0.0371, Val Loss: 0.0718\n",
            "Epoch 54/100, Train Loss: 0.0366, Val Loss: 0.0718\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0366, Val Loss: 0.0718\n",
            "Epoch 56/100, Train Loss: 0.0367, Val Loss: 0.0718\n",
            "Epoch 57/100, Train Loss: 0.0370, Val Loss: 0.0718\n",
            "Epoch 58/100, Train Loss: 0.0371, Val Loss: 0.0718\n",
            "Epoch 59/100, Train Loss: 0.0373, Val Loss: 0.0718\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0363, Val Loss: 0.0718\n",
            "Epoch 61/100, Train Loss: 0.0363, Val Loss: 0.0718\n",
            "Epoch 62/100, Train Loss: 0.0383, Val Loss: 0.0718\n",
            "Epoch 63/100, Train Loss: 0.0369, Val Loss: 0.0718\n",
            "Epoch 64/100, Train Loss: 0.0360, Val Loss: 0.0718\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0365, Val Loss: 0.0718\n",
            "Epoch 66/100, Train Loss: 0.0368, Val Loss: 0.0718\n",
            "Epoch 67/100, Train Loss: 0.0373, Val Loss: 0.0718\n",
            "Epoch 68/100, Train Loss: 0.0366, Val Loss: 0.0718\n",
            "Epoch 69/100, Train Loss: 0.0367, Val Loss: 0.0718\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0366, Val Loss: 0.0718\n",
            "Epoch 71/100, Train Loss: 0.0358, Val Loss: 0.0718\n",
            "Epoch 72/100, Train Loss: 0.0358, Val Loss: 0.0718\n",
            "Epoch 73/100, Train Loss: 0.0372, Val Loss: 0.0718\n",
            "Epoch 74/100, Train Loss: 0.0373, Val Loss: 0.0718\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0364, Val Loss: 0.0718\n",
            "Epoch 76/100, Train Loss: 0.0364, Val Loss: 0.0718\n",
            "Epoch 77/100, Train Loss: 0.0375, Val Loss: 0.0718\n",
            "Epoch 78/100, Train Loss: 0.0358, Val Loss: 0.0718\n",
            "Epoch 79/100, Train Loss: 0.0375, Val Loss: 0.0718\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0368, Val Loss: 0.0718\n",
            "Epoch 81/100, Train Loss: 0.0375, Val Loss: 0.0718\n",
            "Epoch 82/100, Train Loss: 0.0375, Val Loss: 0.0718\n",
            "Epoch 83/100, Train Loss: 0.0380, Val Loss: 0.0718\n",
            "Epoch 84/100, Train Loss: 0.0387, Val Loss: 0.0718\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0374, Val Loss: 0.0718\n",
            "Epoch 86/100, Train Loss: 0.0364, Val Loss: 0.0718\n",
            "Epoch 87/100, Train Loss: 0.0366, Val Loss: 0.0718\n",
            "Epoch 88/100, Train Loss: 0.0370, Val Loss: 0.0718\n",
            "Epoch 89/100, Train Loss: 0.0376, Val Loss: 0.0718\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0373, Val Loss: 0.0718\n",
            "Epoch 91/100, Train Loss: 0.0369, Val Loss: 0.0718\n",
            "Epoch 92/100, Train Loss: 0.0370, Val Loss: 0.0718\n",
            "Epoch 93/100, Train Loss: 0.0370, Val Loss: 0.0718\n",
            "Epoch 94/100, Train Loss: 0.0374, Val Loss: 0.0718\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0383, Val Loss: 0.0718\n",
            "Epoch 96/100, Train Loss: 0.0371, Val Loss: 0.0718\n",
            "Epoch 97/100, Train Loss: 0.0366, Val Loss: 0.0718\n",
            "Epoch 98/100, Train Loss: 0.0368, Val Loss: 0.0718\n",
            "Epoch 99/100, Train Loss: 0.0368, Val Loss: 0.0718\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0359, Val Loss: 0.0718\n",
            "\n",
            "Test iteration 3/3\n",
            "Current training set size: 182 samples\n",
            "Epoch 1/100, Train Loss: 0.1903, Val Loss: 0.3426\n",
            "Epoch 2/100, Train Loss: 0.1573, Val Loss: 0.2872\n",
            "Epoch 3/100, Train Loss: 0.1086, Val Loss: 0.0882\n",
            "Epoch 4/100, Train Loss: 0.0754, Val Loss: 0.1156\n",
            "Epoch 5/100, Train Loss: 0.0680, Val Loss: 0.0926\n",
            "Epoch 6/100, Train Loss: 0.0693, Val Loss: 0.0856\n",
            "Epoch 7/100, Train Loss: 0.0585, Val Loss: 0.0864\n",
            "Epoch 8/100, Train Loss: 0.0573, Val Loss: 0.0834\n",
            "Epoch 9/100, Train Loss: 0.0607, Val Loss: 0.0841\n",
            "Epoch 10/100, Train Loss: 0.0536, Val Loss: 0.0826\n",
            "Epoch 11/100, Train Loss: 0.0525, Val Loss: 0.0778\n",
            "Epoch 12/100, Train Loss: 0.0491, Val Loss: 0.0773\n",
            "Epoch 13/100, Train Loss: 0.0516, Val Loss: 0.0752\n",
            "Epoch 14/100, Train Loss: 0.0479, Val Loss: 0.0697\n",
            "Epoch 15/100, Train Loss: 0.0462, Val Loss: 0.0674\n",
            "Epoch 16/100, Train Loss: 0.0453, Val Loss: 0.0704\n",
            "Epoch 17/100, Train Loss: 0.0464, Val Loss: 0.0721\n",
            "Epoch 18/100, Train Loss: 0.0447, Val Loss: 0.0702\n",
            "Epoch 19/100, Train Loss: 0.0436, Val Loss: 0.0671\n",
            "Epoch 20/100, Train Loss: 0.0423, Val Loss: 0.0666\n",
            "Epoch 21/100, Train Loss: 0.0413, Val Loss: 0.0613\n",
            "Epoch 22/100, Train Loss: 0.0409, Val Loss: 0.0596\n",
            "Epoch 23/100, Train Loss: 0.0446, Val Loss: 0.0547\n",
            "Epoch 24/100, Train Loss: 0.0428, Val Loss: 0.0938\n",
            "Epoch 25/100, Train Loss: 0.0459, Val Loss: 0.0749\n",
            "Epoch 26/100, Train Loss: 0.0391, Val Loss: 0.0689\n",
            "Epoch 27/100, Train Loss: 0.0379, Val Loss: 0.0512\n",
            "Epoch 28/100, Train Loss: 0.0437, Val Loss: 0.0494\n",
            "Epoch 29/100, Train Loss: 0.0420, Val Loss: 0.0936\n",
            "Epoch 30/100, Train Loss: 0.0382, Val Loss: 0.0552\n",
            "Epoch 31/100, Train Loss: 0.0397, Val Loss: 0.0511\n",
            "Epoch 32/100, Train Loss: 0.0421, Val Loss: 0.0569\n",
            "Epoch 33/100, Train Loss: 0.0390, Val Loss: 0.0451\n",
            "Epoch 34/100, Train Loss: 0.0367, Val Loss: 0.0538\n",
            "Epoch 35/100, Train Loss: 0.0400, Val Loss: 0.0479\n",
            "Epoch 36/100, Train Loss: 0.0410, Val Loss: 0.0538\n",
            "Epoch 37/100, Train Loss: 0.0429, Val Loss: 0.0544\n",
            "Epoch 38/100, Train Loss: 0.0419, Val Loss: 0.0631\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0352, Val Loss: 0.0627\n",
            "Epoch 40/100, Train Loss: 0.0344, Val Loss: 0.0623\n",
            "Epoch 41/100, Train Loss: 0.0339, Val Loss: 0.0619\n",
            "Epoch 42/100, Train Loss: 0.0338, Val Loss: 0.0614\n",
            "Epoch 43/100, Train Loss: 0.0345, Val Loss: 0.0613\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0329, Val Loss: 0.0613\n",
            "Epoch 45/100, Train Loss: 0.0334, Val Loss: 0.0613\n",
            "Epoch 46/100, Train Loss: 0.0333, Val Loss: 0.0613\n",
            "Epoch 47/100, Train Loss: 0.0334, Val Loss: 0.0613\n",
            "Epoch 48/100, Train Loss: 0.0350, Val Loss: 0.0613\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0333, Val Loss: 0.0613\n",
            "Epoch 50/100, Train Loss: 0.0334, Val Loss: 0.0613\n",
            "Epoch 51/100, Train Loss: 0.0331, Val Loss: 0.0613\n",
            "Epoch 52/100, Train Loss: 0.0330, Val Loss: 0.0613\n",
            "Epoch 53/100, Train Loss: 0.0329, Val Loss: 0.0613\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0330, Val Loss: 0.0613\n",
            "Epoch 55/100, Train Loss: 0.0341, Val Loss: 0.0613\n",
            "Epoch 56/100, Train Loss: 0.0341, Val Loss: 0.0613\n",
            "Epoch 57/100, Train Loss: 0.0328, Val Loss: 0.0613\n",
            "Epoch 58/100, Train Loss: 0.0333, Val Loss: 0.0613\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0336, Val Loss: 0.0613\n",
            "Epoch 60/100, Train Loss: 0.0336, Val Loss: 0.0613\n",
            "Epoch 61/100, Train Loss: 0.0324, Val Loss: 0.0613\n",
            "Epoch 62/100, Train Loss: 0.0328, Val Loss: 0.0613\n",
            "Epoch 63/100, Train Loss: 0.0329, Val Loss: 0.0613\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0339, Val Loss: 0.0613\n",
            "Epoch 65/100, Train Loss: 0.0328, Val Loss: 0.0613\n",
            "Epoch 66/100, Train Loss: 0.0329, Val Loss: 0.0613\n",
            "Epoch 67/100, Train Loss: 0.0332, Val Loss: 0.0613\n",
            "Epoch 68/100, Train Loss: 0.0330, Val Loss: 0.0613\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0320, Val Loss: 0.0613\n",
            "Epoch 70/100, Train Loss: 0.0331, Val Loss: 0.0613\n",
            "Epoch 71/100, Train Loss: 0.0335, Val Loss: 0.0613\n",
            "Epoch 72/100, Train Loss: 0.0321, Val Loss: 0.0613\n",
            "Epoch 73/100, Train Loss: 0.0326, Val Loss: 0.0613\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0361, Val Loss: 0.0613\n",
            "Epoch 75/100, Train Loss: 0.0321, Val Loss: 0.0613\n",
            "Epoch 76/100, Train Loss: 0.0332, Val Loss: 0.0613\n",
            "Epoch 77/100, Train Loss: 0.0334, Val Loss: 0.0613\n",
            "Epoch 78/100, Train Loss: 0.0329, Val Loss: 0.0613\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0336, Val Loss: 0.0613\n",
            "Epoch 80/100, Train Loss: 0.0338, Val Loss: 0.0613\n",
            "Epoch 81/100, Train Loss: 0.0332, Val Loss: 0.0613\n",
            "Epoch 82/100, Train Loss: 0.0338, Val Loss: 0.0613\n",
            "Epoch 83/100, Train Loss: 0.0330, Val Loss: 0.0613\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0325, Val Loss: 0.0613\n",
            "Epoch 85/100, Train Loss: 0.0317, Val Loss: 0.0613\n",
            "Epoch 86/100, Train Loss: 0.0328, Val Loss: 0.0613\n",
            "Epoch 87/100, Train Loss: 0.0323, Val Loss: 0.0613\n",
            "Epoch 88/100, Train Loss: 0.0325, Val Loss: 0.0613\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0333, Val Loss: 0.0613\n",
            "Epoch 90/100, Train Loss: 0.0333, Val Loss: 0.0613\n",
            "Epoch 91/100, Train Loss: 0.0327, Val Loss: 0.0613\n",
            "Epoch 92/100, Train Loss: 0.0312, Val Loss: 0.0613\n",
            "Epoch 93/100, Train Loss: 0.0346, Val Loss: 0.0613\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0330, Val Loss: 0.0613\n",
            "Epoch 95/100, Train Loss: 0.0333, Val Loss: 0.0613\n",
            "Epoch 96/100, Train Loss: 0.0334, Val Loss: 0.0613\n",
            "Epoch 97/100, Train Loss: 0.0332, Val Loss: 0.0613\n",
            "Epoch 98/100, Train Loss: 0.0334, Val Loss: 0.0613\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0327, Val Loss: 0.0613\n",
            "Epoch 100/100, Train Loss: 0.0331, Val Loss: 0.0613\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: hybrid, Units: L256_G8, Dropout: 0.2, Dense Units: 64\n",
            "Batch Size: 4, Epochs: 100\n",
            "Device: cuda\n",
            "Total test samples: 3\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/3\n",
            "Current training set size: 180 samples\n",
            "Epoch 1/100, Train Loss: 0.1774, Val Loss: 0.3812\n",
            "Epoch 2/100, Train Loss: 0.1464, Val Loss: 0.1748\n",
            "Epoch 3/100, Train Loss: 0.0821, Val Loss: 0.1032\n",
            "Epoch 4/100, Train Loss: 0.0795, Val Loss: 0.0851\n",
            "Epoch 5/100, Train Loss: 0.0662, Val Loss: 0.0849\n",
            "Epoch 6/100, Train Loss: 0.0660, Val Loss: 0.0939\n",
            "Epoch 7/100, Train Loss: 0.0625, Val Loss: 0.0839\n",
            "Epoch 8/100, Train Loss: 0.0582, Val Loss: 0.0809\n",
            "Epoch 9/100, Train Loss: 0.0533, Val Loss: 0.0828\n",
            "Epoch 10/100, Train Loss: 0.0518, Val Loss: 0.0747\n",
            "Epoch 11/100, Train Loss: 0.0465, Val Loss: 0.0764\n",
            "Epoch 12/100, Train Loss: 0.0477, Val Loss: 0.0743\n",
            "Epoch 13/100, Train Loss: 0.0481, Val Loss: 0.0749\n",
            "Epoch 14/100, Train Loss: 0.0466, Val Loss: 0.0695\n",
            "Epoch 15/100, Train Loss: 0.0482, Val Loss: 0.0665\n",
            "Epoch 16/100, Train Loss: 0.0451, Val Loss: 0.0727\n",
            "Epoch 17/100, Train Loss: 0.0454, Val Loss: 0.0641\n",
            "Epoch 18/100, Train Loss: 0.0419, Val Loss: 0.0739\n",
            "Epoch 19/100, Train Loss: 0.0485, Val Loss: 0.0590\n",
            "Epoch 20/100, Train Loss: 0.0422, Val Loss: 0.0588\n",
            "Epoch 21/100, Train Loss: 0.0397, Val Loss: 0.0594\n",
            "Epoch 22/100, Train Loss: 0.0401, Val Loss: 0.0535\n",
            "Epoch 23/100, Train Loss: 0.0394, Val Loss: 0.0553\n",
            "Epoch 24/100, Train Loss: 0.0383, Val Loss: 0.0578\n",
            "Epoch 25/100, Train Loss: 0.0383, Val Loss: 0.0779\n",
            "Epoch 26/100, Train Loss: 0.0396, Val Loss: 0.0563\n",
            "Epoch 27/100, Train Loss: 0.0423, Val Loss: 0.0522\n",
            "Epoch 28/100, Train Loss: 0.0413, Val Loss: 0.0555\n",
            "Epoch 29/100, Train Loss: 0.0388, Val Loss: 0.0526\n",
            "Epoch 30/100, Train Loss: 0.0380, Val Loss: 0.0511\n",
            "Epoch 31/100, Train Loss: 0.0454, Val Loss: 0.0542\n",
            "Epoch 32/100, Train Loss: 0.0426, Val Loss: 0.0785\n",
            "Epoch 33/100, Train Loss: 0.0394, Val Loss: 0.0559\n",
            "Epoch 34/100, Train Loss: 0.0404, Val Loss: 0.0538\n",
            "Epoch 35/100, Train Loss: 0.0376, Val Loss: 0.0468\n",
            "Epoch 36/100, Train Loss: 0.0369, Val Loss: 0.0515\n",
            "Epoch 37/100, Train Loss: 0.0353, Val Loss: 0.0495\n",
            "Epoch 38/100, Train Loss: 0.0384, Val Loss: 0.0512\n",
            "Epoch 39/100, Train Loss: 0.0396, Val Loss: 0.0518\n",
            "Epoch 40/100, Train Loss: 0.0336, Val Loss: 0.0495\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0357, Val Loss: 0.0486\n",
            "Epoch 42/100, Train Loss: 0.0316, Val Loss: 0.0482\n",
            "Epoch 43/100, Train Loss: 0.0298, Val Loss: 0.0477\n",
            "Epoch 44/100, Train Loss: 0.0302, Val Loss: 0.0479\n",
            "Epoch 45/100, Train Loss: 0.0303, Val Loss: 0.0483\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0301, Val Loss: 0.0482\n",
            "Epoch 47/100, Train Loss: 0.0316, Val Loss: 0.0482\n",
            "Epoch 48/100, Train Loss: 0.0313, Val Loss: 0.0482\n",
            "Epoch 49/100, Train Loss: 0.0300, Val Loss: 0.0482\n",
            "Epoch 50/100, Train Loss: 0.0303, Val Loss: 0.0482\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0300, Val Loss: 0.0482\n",
            "Epoch 52/100, Train Loss: 0.0311, Val Loss: 0.0482\n",
            "Epoch 53/100, Train Loss: 0.0315, Val Loss: 0.0482\n",
            "Epoch 54/100, Train Loss: 0.0308, Val Loss: 0.0482\n",
            "Epoch 55/100, Train Loss: 0.0305, Val Loss: 0.0482\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0314, Val Loss: 0.0482\n",
            "Epoch 57/100, Train Loss: 0.0318, Val Loss: 0.0482\n",
            "Epoch 58/100, Train Loss: 0.0309, Val Loss: 0.0482\n",
            "Epoch 59/100, Train Loss: 0.0315, Val Loss: 0.0482\n",
            "Epoch 60/100, Train Loss: 0.0296, Val Loss: 0.0482\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0301, Val Loss: 0.0482\n",
            "Epoch 62/100, Train Loss: 0.0313, Val Loss: 0.0482\n",
            "Epoch 63/100, Train Loss: 0.0303, Val Loss: 0.0482\n",
            "Epoch 64/100, Train Loss: 0.0305, Val Loss: 0.0482\n",
            "Epoch 65/100, Train Loss: 0.0303, Val Loss: 0.0482\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0315, Val Loss: 0.0482\n",
            "Epoch 67/100, Train Loss: 0.0319, Val Loss: 0.0482\n",
            "Epoch 68/100, Train Loss: 0.0310, Val Loss: 0.0482\n",
            "Epoch 69/100, Train Loss: 0.0315, Val Loss: 0.0482\n",
            "Epoch 70/100, Train Loss: 0.0320, Val Loss: 0.0482\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0312, Val Loss: 0.0482\n",
            "Epoch 72/100, Train Loss: 0.0315, Val Loss: 0.0482\n",
            "Epoch 73/100, Train Loss: 0.0309, Val Loss: 0.0482\n",
            "Epoch 74/100, Train Loss: 0.0317, Val Loss: 0.0482\n",
            "Epoch 75/100, Train Loss: 0.0317, Val Loss: 0.0482\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0310, Val Loss: 0.0482\n",
            "Epoch 77/100, Train Loss: 0.0302, Val Loss: 0.0482\n",
            "Epoch 78/100, Train Loss: 0.0316, Val Loss: 0.0482\n",
            "Epoch 79/100, Train Loss: 0.0312, Val Loss: 0.0482\n",
            "Epoch 80/100, Train Loss: 0.0300, Val Loss: 0.0482\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0298, Val Loss: 0.0482\n",
            "Epoch 82/100, Train Loss: 0.0299, Val Loss: 0.0482\n",
            "Epoch 83/100, Train Loss: 0.0302, Val Loss: 0.0482\n",
            "Epoch 84/100, Train Loss: 0.0324, Val Loss: 0.0482\n",
            "Epoch 85/100, Train Loss: 0.0313, Val Loss: 0.0482\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0317, Val Loss: 0.0482\n",
            "Epoch 87/100, Train Loss: 0.0309, Val Loss: 0.0482\n",
            "Epoch 88/100, Train Loss: 0.0313, Val Loss: 0.0482\n",
            "Epoch 89/100, Train Loss: 0.0313, Val Loss: 0.0482\n",
            "Epoch 90/100, Train Loss: 0.0299, Val Loss: 0.0482\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0309, Val Loss: 0.0482\n",
            "Epoch 92/100, Train Loss: 0.0317, Val Loss: 0.0482\n",
            "Epoch 93/100, Train Loss: 0.0320, Val Loss: 0.0482\n",
            "Epoch 94/100, Train Loss: 0.0303, Val Loss: 0.0482\n",
            "Epoch 95/100, Train Loss: 0.0311, Val Loss: 0.0482\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0318, Val Loss: 0.0482\n",
            "Epoch 97/100, Train Loss: 0.0309, Val Loss: 0.0482\n",
            "Epoch 98/100, Train Loss: 0.0299, Val Loss: 0.0482\n",
            "Epoch 99/100, Train Loss: 0.0317, Val Loss: 0.0482\n",
            "Epoch 100/100, Train Loss: 0.0314, Val Loss: 0.0482\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 2/3\n",
            "Current training set size: 181 samples\n",
            "Epoch 1/100, Train Loss: 0.1768, Val Loss: 0.3136\n",
            "Epoch 2/100, Train Loss: 0.1430, Val Loss: 0.1321\n",
            "Epoch 3/100, Train Loss: 0.0947, Val Loss: 0.0874\n",
            "Epoch 4/100, Train Loss: 0.0636, Val Loss: 0.1392\n",
            "Epoch 5/100, Train Loss: 0.0799, Val Loss: 0.0860\n",
            "Epoch 6/100, Train Loss: 0.0587, Val Loss: 0.0855\n",
            "Epoch 7/100, Train Loss: 0.0659, Val Loss: 0.0814\n",
            "Epoch 8/100, Train Loss: 0.0621, Val Loss: 0.0868\n",
            "Epoch 9/100, Train Loss: 0.0556, Val Loss: 0.0787\n",
            "Epoch 10/100, Train Loss: 0.0488, Val Loss: 0.1269\n",
            "Epoch 11/100, Train Loss: 0.0483, Val Loss: 0.0752\n",
            "Epoch 12/100, Train Loss: 0.0455, Val Loss: 0.0706\n",
            "Epoch 13/100, Train Loss: 0.0446, Val Loss: 0.0700\n",
            "Epoch 14/100, Train Loss: 0.0428, Val Loss: 0.0608\n",
            "Epoch 15/100, Train Loss: 0.0492, Val Loss: 0.0558\n",
            "Epoch 16/100, Train Loss: 0.0475, Val Loss: 0.0610\n",
            "Epoch 17/100, Train Loss: 0.0470, Val Loss: 0.0681\n",
            "Epoch 18/100, Train Loss: 0.0461, Val Loss: 0.0569\n",
            "Epoch 19/100, Train Loss: 0.0428, Val Loss: 0.0682\n",
            "Epoch 20/100, Train Loss: 0.0436, Val Loss: 0.0646\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0464, Val Loss: 0.0661\n",
            "Epoch 22/100, Train Loss: 0.0404, Val Loss: 0.0671\n",
            "Epoch 23/100, Train Loss: 0.0389, Val Loss: 0.0686\n",
            "Epoch 24/100, Train Loss: 0.0396, Val Loss: 0.0695\n",
            "Epoch 25/100, Train Loss: 0.0384, Val Loss: 0.0707\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0381, Val Loss: 0.0707\n",
            "Epoch 27/100, Train Loss: 0.0382, Val Loss: 0.0707\n",
            "Epoch 28/100, Train Loss: 0.0380, Val Loss: 0.0707\n",
            "Epoch 29/100, Train Loss: 0.0375, Val Loss: 0.0707\n",
            "Epoch 30/100, Train Loss: 0.0387, Val Loss: 0.0707\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0379, Val Loss: 0.0707\n",
            "Epoch 32/100, Train Loss: 0.0387, Val Loss: 0.0707\n",
            "Epoch 33/100, Train Loss: 0.0388, Val Loss: 0.0707\n",
            "Epoch 34/100, Train Loss: 0.0365, Val Loss: 0.0707\n",
            "Epoch 35/100, Train Loss: 0.0385, Val Loss: 0.0707\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0370, Val Loss: 0.0707\n",
            "Epoch 37/100, Train Loss: 0.0386, Val Loss: 0.0707\n",
            "Epoch 38/100, Train Loss: 0.0375, Val Loss: 0.0707\n",
            "Epoch 39/100, Train Loss: 0.0383, Val Loss: 0.0707\n",
            "Epoch 40/100, Train Loss: 0.0403, Val Loss: 0.0707\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0364, Val Loss: 0.0707\n",
            "Epoch 42/100, Train Loss: 0.0379, Val Loss: 0.0707\n",
            "Epoch 43/100, Train Loss: 0.0389, Val Loss: 0.0707\n",
            "Epoch 44/100, Train Loss: 0.0386, Val Loss: 0.0707\n",
            "Epoch 45/100, Train Loss: 0.0373, Val Loss: 0.0707\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0378, Val Loss: 0.0707\n",
            "Epoch 47/100, Train Loss: 0.0383, Val Loss: 0.0707\n",
            "Epoch 48/100, Train Loss: 0.0370, Val Loss: 0.0707\n",
            "Epoch 49/100, Train Loss: 0.0367, Val Loss: 0.0707\n",
            "Epoch 50/100, Train Loss: 0.0365, Val Loss: 0.0707\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0370, Val Loss: 0.0707\n",
            "Epoch 52/100, Train Loss: 0.0376, Val Loss: 0.0707\n",
            "Epoch 53/100, Train Loss: 0.0379, Val Loss: 0.0707\n",
            "Epoch 54/100, Train Loss: 0.0378, Val Loss: 0.0707\n",
            "Epoch 55/100, Train Loss: 0.0387, Val Loss: 0.0707\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0388, Val Loss: 0.0707\n",
            "Epoch 57/100, Train Loss: 0.0380, Val Loss: 0.0707\n",
            "Epoch 58/100, Train Loss: 0.0374, Val Loss: 0.0707\n",
            "Epoch 59/100, Train Loss: 0.0375, Val Loss: 0.0707\n",
            "Epoch 60/100, Train Loss: 0.0371, Val Loss: 0.0707\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0371, Val Loss: 0.0707\n",
            "Epoch 62/100, Train Loss: 0.0385, Val Loss: 0.0707\n",
            "Epoch 63/100, Train Loss: 0.0385, Val Loss: 0.0707\n",
            "Epoch 64/100, Train Loss: 0.0372, Val Loss: 0.0707\n",
            "Epoch 65/100, Train Loss: 0.0377, Val Loss: 0.0707\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0367, Val Loss: 0.0707\n",
            "Epoch 67/100, Train Loss: 0.0390, Val Loss: 0.0707\n",
            "Epoch 68/100, Train Loss: 0.0378, Val Loss: 0.0707\n",
            "Epoch 69/100, Train Loss: 0.0379, Val Loss: 0.0707\n",
            "Epoch 70/100, Train Loss: 0.0373, Val Loss: 0.0707\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0366, Val Loss: 0.0707\n",
            "Epoch 72/100, Train Loss: 0.0372, Val Loss: 0.0707\n",
            "Epoch 73/100, Train Loss: 0.0374, Val Loss: 0.0707\n",
            "Epoch 74/100, Train Loss: 0.0383, Val Loss: 0.0707\n",
            "Epoch 75/100, Train Loss: 0.0380, Val Loss: 0.0707\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0377, Val Loss: 0.0707\n",
            "Epoch 77/100, Train Loss: 0.0379, Val Loss: 0.0707\n",
            "Epoch 78/100, Train Loss: 0.0368, Val Loss: 0.0707\n",
            "Epoch 79/100, Train Loss: 0.0387, Val Loss: 0.0707\n",
            "Epoch 80/100, Train Loss: 0.0369, Val Loss: 0.0707\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0389, Val Loss: 0.0707\n",
            "Epoch 82/100, Train Loss: 0.0391, Val Loss: 0.0707\n",
            "Epoch 83/100, Train Loss: 0.0386, Val Loss: 0.0707\n",
            "Epoch 84/100, Train Loss: 0.0390, Val Loss: 0.0707\n",
            "Epoch 85/100, Train Loss: 0.0379, Val Loss: 0.0707\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0379, Val Loss: 0.0707\n",
            "Epoch 87/100, Train Loss: 0.0370, Val Loss: 0.0707\n",
            "Epoch 88/100, Train Loss: 0.0372, Val Loss: 0.0707\n",
            "Epoch 89/100, Train Loss: 0.0387, Val Loss: 0.0707\n",
            "Epoch 90/100, Train Loss: 0.0374, Val Loss: 0.0707\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0380, Val Loss: 0.0707\n",
            "Epoch 92/100, Train Loss: 0.0389, Val Loss: 0.0707\n",
            "Epoch 93/100, Train Loss: 0.0384, Val Loss: 0.0707\n",
            "Epoch 94/100, Train Loss: 0.0372, Val Loss: 0.0707\n",
            "Epoch 95/100, Train Loss: 0.0404, Val Loss: 0.0707\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0380, Val Loss: 0.0707\n",
            "Epoch 97/100, Train Loss: 0.0380, Val Loss: 0.0707\n",
            "Epoch 98/100, Train Loss: 0.0374, Val Loss: 0.0707\n",
            "Epoch 99/100, Train Loss: 0.0377, Val Loss: 0.0707\n",
            "Epoch 100/100, Train Loss: 0.0371, Val Loss: 0.0707\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 3/3\n",
            "Current training set size: 182 samples\n",
            "Epoch 1/100, Train Loss: 0.1886, Val Loss: 0.3545\n",
            "Epoch 2/100, Train Loss: 0.1579, Val Loss: 0.2821\n",
            "Epoch 3/100, Train Loss: 0.1108, Val Loss: 0.0849\n",
            "Epoch 4/100, Train Loss: 0.0770, Val Loss: 0.1003\n",
            "Epoch 5/100, Train Loss: 0.0614, Val Loss: 0.0895\n",
            "Epoch 6/100, Train Loss: 0.0645, Val Loss: 0.0842\n",
            "Epoch 7/100, Train Loss: 0.0583, Val Loss: 0.0839\n",
            "Epoch 8/100, Train Loss: 0.0548, Val Loss: 0.0792\n",
            "Epoch 9/100, Train Loss: 0.0592, Val Loss: 0.0827\n",
            "Epoch 10/100, Train Loss: 0.0514, Val Loss: 0.0820\n",
            "Epoch 11/100, Train Loss: 0.0529, Val Loss: 0.0759\n",
            "Epoch 12/100, Train Loss: 0.0461, Val Loss: 0.0756\n",
            "Epoch 13/100, Train Loss: 0.0521, Val Loss: 0.0731\n",
            "Epoch 14/100, Train Loss: 0.0512, Val Loss: 0.0690\n",
            "Epoch 15/100, Train Loss: 0.0464, Val Loss: 0.0635\n",
            "Epoch 16/100, Train Loss: 0.0427, Val Loss: 0.0683\n",
            "Epoch 17/100, Train Loss: 0.0453, Val Loss: 0.0687\n",
            "Epoch 18/100, Train Loss: 0.0442, Val Loss: 0.0693\n",
            "Epoch 19/100, Train Loss: 0.0421, Val Loss: 0.0667\n",
            "Epoch 20/100, Train Loss: 0.0409, Val Loss: 0.0816\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0388, Val Loss: 0.0764\n",
            "Epoch 22/100, Train Loss: 0.0380, Val Loss: 0.0726\n",
            "Epoch 23/100, Train Loss: 0.0390, Val Loss: 0.0691\n",
            "Epoch 24/100, Train Loss: 0.0387, Val Loss: 0.0671\n",
            "Epoch 25/100, Train Loss: 0.0376, Val Loss: 0.0662\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0373, Val Loss: 0.0661\n",
            "Epoch 27/100, Train Loss: 0.0377, Val Loss: 0.0661\n",
            "Epoch 28/100, Train Loss: 0.0382, Val Loss: 0.0661\n",
            "Epoch 29/100, Train Loss: 0.0378, Val Loss: 0.0660\n",
            "Epoch 30/100, Train Loss: 0.0371, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0384, Val Loss: 0.0660\n",
            "Epoch 32/100, Train Loss: 0.0377, Val Loss: 0.0660\n",
            "Epoch 33/100, Train Loss: 0.0378, Val Loss: 0.0660\n",
            "Epoch 34/100, Train Loss: 0.0379, Val Loss: 0.0660\n",
            "Epoch 35/100, Train Loss: 0.0375, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0375, Val Loss: 0.0660\n",
            "Epoch 37/100, Train Loss: 0.0403, Val Loss: 0.0660\n",
            "Epoch 38/100, Train Loss: 0.0377, Val Loss: 0.0660\n",
            "Epoch 39/100, Train Loss: 0.0381, Val Loss: 0.0660\n",
            "Epoch 40/100, Train Loss: 0.0388, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0382, Val Loss: 0.0660\n",
            "Epoch 42/100, Train Loss: 0.0377, Val Loss: 0.0660\n",
            "Epoch 43/100, Train Loss: 0.0387, Val Loss: 0.0660\n",
            "Epoch 44/100, Train Loss: 0.0376, Val Loss: 0.0660\n",
            "Epoch 45/100, Train Loss: 0.0380, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0368, Val Loss: 0.0660\n",
            "Epoch 47/100, Train Loss: 0.0385, Val Loss: 0.0660\n",
            "Epoch 48/100, Train Loss: 0.0394, Val Loss: 0.0660\n",
            "Epoch 49/100, Train Loss: 0.0370, Val Loss: 0.0660\n",
            "Epoch 50/100, Train Loss: 0.0376, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0371, Val Loss: 0.0660\n",
            "Epoch 52/100, Train Loss: 0.0374, Val Loss: 0.0660\n",
            "Epoch 53/100, Train Loss: 0.0373, Val Loss: 0.0660\n",
            "Epoch 54/100, Train Loss: 0.0374, Val Loss: 0.0660\n",
            "Epoch 55/100, Train Loss: 0.0380, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0385, Val Loss: 0.0660\n",
            "Epoch 57/100, Train Loss: 0.0369, Val Loss: 0.0660\n",
            "Epoch 58/100, Train Loss: 0.0381, Val Loss: 0.0660\n",
            "Epoch 59/100, Train Loss: 0.0382, Val Loss: 0.0660\n",
            "Epoch 60/100, Train Loss: 0.0379, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0364, Val Loss: 0.0660\n",
            "Epoch 62/100, Train Loss: 0.0367, Val Loss: 0.0660\n",
            "Epoch 63/100, Train Loss: 0.0383, Val Loss: 0.0660\n",
            "Epoch 64/100, Train Loss: 0.0386, Val Loss: 0.0660\n",
            "Epoch 65/100, Train Loss: 0.0382, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0363, Val Loss: 0.0660\n",
            "Epoch 67/100, Train Loss: 0.0378, Val Loss: 0.0660\n",
            "Epoch 68/100, Train Loss: 0.0380, Val Loss: 0.0660\n",
            "Epoch 69/100, Train Loss: 0.0364, Val Loss: 0.0660\n",
            "Epoch 70/100, Train Loss: 0.0382, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0390, Val Loss: 0.0660\n",
            "Epoch 72/100, Train Loss: 0.0370, Val Loss: 0.0660\n",
            "Epoch 73/100, Train Loss: 0.0373, Val Loss: 0.0660\n",
            "Epoch 74/100, Train Loss: 0.0410, Val Loss: 0.0660\n",
            "Epoch 75/100, Train Loss: 0.0369, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0379, Val Loss: 0.0660\n",
            "Epoch 77/100, Train Loss: 0.0383, Val Loss: 0.0660\n",
            "Epoch 78/100, Train Loss: 0.0378, Val Loss: 0.0660\n",
            "Epoch 79/100, Train Loss: 0.0383, Val Loss: 0.0660\n",
            "Epoch 80/100, Train Loss: 0.0374, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0390, Val Loss: 0.0660\n",
            "Epoch 82/100, Train Loss: 0.0386, Val Loss: 0.0660\n",
            "Epoch 83/100, Train Loss: 0.0379, Val Loss: 0.0660\n",
            "Epoch 84/100, Train Loss: 0.0370, Val Loss: 0.0660\n",
            "Epoch 85/100, Train Loss: 0.0372, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0367, Val Loss: 0.0660\n",
            "Epoch 87/100, Train Loss: 0.0375, Val Loss: 0.0660\n",
            "Epoch 88/100, Train Loss: 0.0369, Val Loss: 0.0660\n",
            "Epoch 89/100, Train Loss: 0.0379, Val Loss: 0.0660\n",
            "Epoch 90/100, Train Loss: 0.0385, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0383, Val Loss: 0.0660\n",
            "Epoch 92/100, Train Loss: 0.0357, Val Loss: 0.0660\n",
            "Epoch 93/100, Train Loss: 0.0389, Val Loss: 0.0660\n",
            "Epoch 94/100, Train Loss: 0.0378, Val Loss: 0.0660\n",
            "Epoch 95/100, Train Loss: 0.0376, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0387, Val Loss: 0.0660\n",
            "Epoch 97/100, Train Loss: 0.0375, Val Loss: 0.0660\n",
            "Epoch 98/100, Train Loss: 0.0370, Val Loss: 0.0660\n",
            "Epoch 99/100, Train Loss: 0.0371, Val Loss: 0.0660\n",
            "Epoch 100/100, Train Loss: 0.0366, Val Loss: 0.0660\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: hybrid, Units: L256_G16, Dropout: 0.1, Dense Units: 32\n",
            "Batch Size: 4, Epochs: 100\n",
            "Device: cuda\n",
            "Total test samples: 3\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/3\n",
            "Current training set size: 180 samples\n",
            "Epoch 1/100, Train Loss: 0.1764, Val Loss: 0.3738\n",
            "Epoch 2/100, Train Loss: 0.1486, Val Loss: 0.2231\n",
            "Epoch 3/100, Train Loss: 0.0941, Val Loss: 0.0844\n",
            "Epoch 4/100, Train Loss: 0.0814, Val Loss: 0.0879\n",
            "Epoch 5/100, Train Loss: 0.0704, Val Loss: 0.0830\n",
            "Epoch 6/100, Train Loss: 0.0809, Val Loss: 0.0988\n",
            "Epoch 7/100, Train Loss: 0.0729, Val Loss: 0.0915\n",
            "Epoch 8/100, Train Loss: 0.0596, Val Loss: 0.0796\n",
            "Epoch 9/100, Train Loss: 0.0563, Val Loss: 0.0821\n",
            "Epoch 10/100, Train Loss: 0.0554, Val Loss: 0.0790\n",
            "Epoch 11/100, Train Loss: 0.0495, Val Loss: 0.0770\n",
            "Epoch 12/100, Train Loss: 0.0486, Val Loss: 0.0725\n",
            "Epoch 13/100, Train Loss: 0.0495, Val Loss: 0.0713\n",
            "Epoch 14/100, Train Loss: 0.0469, Val Loss: 0.0673\n",
            "Epoch 15/100, Train Loss: 0.0453, Val Loss: 0.0654\n",
            "Epoch 16/100, Train Loss: 0.0500, Val Loss: 0.0749\n",
            "Epoch 17/100, Train Loss: 0.0462, Val Loss: 0.0661\n",
            "Epoch 18/100, Train Loss: 0.0420, Val Loss: 0.0905\n",
            "Epoch 19/100, Train Loss: 0.0466, Val Loss: 0.0606\n",
            "Epoch 20/100, Train Loss: 0.0389, Val Loss: 0.0618\n",
            "Epoch 21/100, Train Loss: 0.0386, Val Loss: 0.0570\n",
            "Epoch 22/100, Train Loss: 0.0397, Val Loss: 0.0657\n",
            "Epoch 23/100, Train Loss: 0.0362, Val Loss: 0.0522\n",
            "Epoch 24/100, Train Loss: 0.0399, Val Loss: 0.0488\n",
            "Epoch 25/100, Train Loss: 0.0371, Val Loss: 0.0503\n",
            "Epoch 26/100, Train Loss: 0.0385, Val Loss: 0.0531\n",
            "Epoch 27/100, Train Loss: 0.0398, Val Loss: 0.0462\n",
            "Epoch 28/100, Train Loss: 0.0406, Val Loss: 0.0547\n",
            "Epoch 29/100, Train Loss: 0.0368, Val Loss: 0.0498\n",
            "Epoch 30/100, Train Loss: 0.0376, Val Loss: 0.0491\n",
            "Epoch 31/100, Train Loss: 0.0356, Val Loss: 0.0474\n",
            "Epoch 32/100, Train Loss: 0.0365, Val Loss: 0.0480\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0311, Val Loss: 0.0471\n",
            "Epoch 34/100, Train Loss: 0.0307, Val Loss: 0.0472\n",
            "Epoch 35/100, Train Loss: 0.0305, Val Loss: 0.0470\n",
            "Epoch 36/100, Train Loss: 0.0304, Val Loss: 0.0467\n",
            "Epoch 37/100, Train Loss: 0.0311, Val Loss: 0.0463\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0309, Val Loss: 0.0463\n",
            "Epoch 39/100, Train Loss: 0.0303, Val Loss: 0.0463\n",
            "Epoch 40/100, Train Loss: 0.0321, Val Loss: 0.0462\n",
            "Epoch 41/100, Train Loss: 0.0315, Val Loss: 0.0462\n",
            "Epoch 42/100, Train Loss: 0.0312, Val Loss: 0.0462\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0309, Val Loss: 0.0462\n",
            "Epoch 44/100, Train Loss: 0.0301, Val Loss: 0.0462\n",
            "Epoch 45/100, Train Loss: 0.0309, Val Loss: 0.0462\n",
            "Epoch 46/100, Train Loss: 0.0304, Val Loss: 0.0462\n",
            "Epoch 47/100, Train Loss: 0.0310, Val Loss: 0.0462\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0310, Val Loss: 0.0462\n",
            "Epoch 49/100, Train Loss: 0.0300, Val Loss: 0.0462\n",
            "Epoch 50/100, Train Loss: 0.0308, Val Loss: 0.0462\n",
            "Epoch 51/100, Train Loss: 0.0314, Val Loss: 0.0462\n",
            "Epoch 52/100, Train Loss: 0.0301, Val Loss: 0.0462\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0314, Val Loss: 0.0462\n",
            "Epoch 54/100, Train Loss: 0.0313, Val Loss: 0.0462\n",
            "Epoch 55/100, Train Loss: 0.0310, Val Loss: 0.0462\n",
            "Epoch 56/100, Train Loss: 0.0312, Val Loss: 0.0462\n",
            "Epoch 57/100, Train Loss: 0.0309, Val Loss: 0.0462\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0304, Val Loss: 0.0462\n",
            "Epoch 59/100, Train Loss: 0.0311, Val Loss: 0.0462\n",
            "Epoch 60/100, Train Loss: 0.0312, Val Loss: 0.0462\n",
            "Epoch 61/100, Train Loss: 0.0310, Val Loss: 0.0462\n",
            "Epoch 62/100, Train Loss: 0.0310, Val Loss: 0.0462\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0307, Val Loss: 0.0462\n",
            "Epoch 64/100, Train Loss: 0.0304, Val Loss: 0.0462\n",
            "Epoch 65/100, Train Loss: 0.0311, Val Loss: 0.0462\n",
            "Epoch 66/100, Train Loss: 0.0305, Val Loss: 0.0462\n",
            "Epoch 67/100, Train Loss: 0.0307, Val Loss: 0.0462\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0311, Val Loss: 0.0462\n",
            "Epoch 69/100, Train Loss: 0.0303, Val Loss: 0.0462\n",
            "Epoch 70/100, Train Loss: 0.0300, Val Loss: 0.0462\n",
            "Epoch 71/100, Train Loss: 0.0309, Val Loss: 0.0462\n",
            "Epoch 72/100, Train Loss: 0.0305, Val Loss: 0.0462\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0315, Val Loss: 0.0462\n",
            "Epoch 74/100, Train Loss: 0.0302, Val Loss: 0.0462\n",
            "Epoch 75/100, Train Loss: 0.0304, Val Loss: 0.0462\n",
            "Epoch 76/100, Train Loss: 0.0310, Val Loss: 0.0462\n",
            "Epoch 77/100, Train Loss: 0.0301, Val Loss: 0.0462\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0309, Val Loss: 0.0462\n",
            "Epoch 79/100, Train Loss: 0.0309, Val Loss: 0.0462\n",
            "Epoch 80/100, Train Loss: 0.0304, Val Loss: 0.0462\n",
            "Epoch 81/100, Train Loss: 0.0306, Val Loss: 0.0462\n",
            "Epoch 82/100, Train Loss: 0.0295, Val Loss: 0.0462\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0307, Val Loss: 0.0462\n",
            "Epoch 84/100, Train Loss: 0.0308, Val Loss: 0.0462\n",
            "Epoch 85/100, Train Loss: 0.0307, Val Loss: 0.0462\n",
            "Epoch 86/100, Train Loss: 0.0313, Val Loss: 0.0462\n",
            "Epoch 87/100, Train Loss: 0.0310, Val Loss: 0.0462\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0306, Val Loss: 0.0462\n",
            "Epoch 89/100, Train Loss: 0.0311, Val Loss: 0.0462\n",
            "Epoch 90/100, Train Loss: 0.0311, Val Loss: 0.0462\n",
            "Epoch 91/100, Train Loss: 0.0302, Val Loss: 0.0462\n",
            "Epoch 92/100, Train Loss: 0.0299, Val Loss: 0.0462\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0306, Val Loss: 0.0462\n",
            "Epoch 94/100, Train Loss: 0.0308, Val Loss: 0.0462\n",
            "Epoch 95/100, Train Loss: 0.0304, Val Loss: 0.0462\n",
            "Epoch 96/100, Train Loss: 0.0309, Val Loss: 0.0462\n",
            "Epoch 97/100, Train Loss: 0.0304, Val Loss: 0.0462\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0300, Val Loss: 0.0462\n",
            "Epoch 99/100, Train Loss: 0.0302, Val Loss: 0.0462\n",
            "Epoch 100/100, Train Loss: 0.0305, Val Loss: 0.0462\n",
            "\n",
            "Test iteration 2/3\n",
            "Current training set size: 181 samples\n",
            "Epoch 1/100, Train Loss: 0.1778, Val Loss: 0.3233\n",
            "Epoch 2/100, Train Loss: 0.1491, Val Loss: 0.1721\n",
            "Epoch 3/100, Train Loss: 0.1016, Val Loss: 0.0868\n",
            "Epoch 4/100, Train Loss: 0.0850, Val Loss: 0.0904\n",
            "Epoch 5/100, Train Loss: 0.0691, Val Loss: 0.0851\n",
            "Epoch 6/100, Train Loss: 0.0607, Val Loss: 0.0880\n",
            "Epoch 7/100, Train Loss: 0.0670, Val Loss: 0.0834\n",
            "Epoch 8/100, Train Loss: 0.0581, Val Loss: 0.0831\n",
            "Epoch 9/100, Train Loss: 0.0633, Val Loss: 0.0811\n",
            "Epoch 10/100, Train Loss: 0.0549, Val Loss: 0.1416\n",
            "Epoch 11/100, Train Loss: 0.0528, Val Loss: 0.0771\n",
            "Epoch 12/100, Train Loss: 0.0476, Val Loss: 0.0741\n",
            "Epoch 13/100, Train Loss: 0.0485, Val Loss: 0.0691\n",
            "Epoch 14/100, Train Loss: 0.0453, Val Loss: 0.0666\n",
            "Epoch 15/100, Train Loss: 0.0510, Val Loss: 0.0646\n",
            "Epoch 16/100, Train Loss: 0.0470, Val Loss: 0.0749\n",
            "Epoch 17/100, Train Loss: 0.0472, Val Loss: 0.0680\n",
            "Epoch 18/100, Train Loss: 0.0417, Val Loss: 0.0608\n",
            "Epoch 19/100, Train Loss: 0.0426, Val Loss: 0.0582\n",
            "Epoch 20/100, Train Loss: 0.0417, Val Loss: 0.0694\n",
            "Epoch 21/100, Train Loss: 0.0404, Val Loss: 0.0471\n",
            "Epoch 22/100, Train Loss: 0.0412, Val Loss: 0.0478\n",
            "Epoch 23/100, Train Loss: 0.0407, Val Loss: 0.0488\n",
            "Epoch 24/100, Train Loss: 0.0381, Val Loss: 0.0571\n",
            "Epoch 25/100, Train Loss: 0.0426, Val Loss: 0.0642\n",
            "Epoch 26/100, Train Loss: 0.0443, Val Loss: 0.0694\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0377, Val Loss: 0.0643\n",
            "Epoch 28/100, Train Loss: 0.0357, Val Loss: 0.0613\n",
            "Epoch 29/100, Train Loss: 0.0348, Val Loss: 0.0606\n",
            "Epoch 30/100, Train Loss: 0.0354, Val Loss: 0.0598\n",
            "Epoch 31/100, Train Loss: 0.0349, Val Loss: 0.0589\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0351, Val Loss: 0.0589\n",
            "Epoch 33/100, Train Loss: 0.0349, Val Loss: 0.0589\n",
            "Epoch 34/100, Train Loss: 0.0337, Val Loss: 0.0589\n",
            "Epoch 35/100, Train Loss: 0.0345, Val Loss: 0.0589\n",
            "Epoch 36/100, Train Loss: 0.0339, Val Loss: 0.0589\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0350, Val Loss: 0.0589\n",
            "Epoch 38/100, Train Loss: 0.0336, Val Loss: 0.0589\n",
            "Epoch 39/100, Train Loss: 0.0334, Val Loss: 0.0589\n",
            "Epoch 40/100, Train Loss: 0.0341, Val Loss: 0.0589\n",
            "Epoch 41/100, Train Loss: 0.0340, Val Loss: 0.0589\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0339, Val Loss: 0.0589\n",
            "Epoch 43/100, Train Loss: 0.0346, Val Loss: 0.0589\n",
            "Epoch 44/100, Train Loss: 0.0335, Val Loss: 0.0589\n",
            "Epoch 45/100, Train Loss: 0.0341, Val Loss: 0.0589\n",
            "Epoch 46/100, Train Loss: 0.0346, Val Loss: 0.0589\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0334, Val Loss: 0.0589\n",
            "Epoch 48/100, Train Loss: 0.0344, Val Loss: 0.0589\n",
            "Epoch 49/100, Train Loss: 0.0347, Val Loss: 0.0589\n",
            "Epoch 50/100, Train Loss: 0.0346, Val Loss: 0.0589\n",
            "Epoch 51/100, Train Loss: 0.0337, Val Loss: 0.0589\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0349, Val Loss: 0.0589\n",
            "Epoch 53/100, Train Loss: 0.0339, Val Loss: 0.0589\n",
            "Epoch 54/100, Train Loss: 0.0342, Val Loss: 0.0589\n",
            "Epoch 55/100, Train Loss: 0.0334, Val Loss: 0.0589\n",
            "Epoch 56/100, Train Loss: 0.0338, Val Loss: 0.0589\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0339, Val Loss: 0.0589\n",
            "Epoch 58/100, Train Loss: 0.0346, Val Loss: 0.0589\n",
            "Epoch 59/100, Train Loss: 0.0349, Val Loss: 0.0589\n",
            "Epoch 60/100, Train Loss: 0.0340, Val Loss: 0.0589\n",
            "Epoch 61/100, Train Loss: 0.0340, Val Loss: 0.0589\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0356, Val Loss: 0.0589\n",
            "Epoch 63/100, Train Loss: 0.0343, Val Loss: 0.0589\n",
            "Epoch 64/100, Train Loss: 0.0338, Val Loss: 0.0589\n",
            "Epoch 65/100, Train Loss: 0.0354, Val Loss: 0.0589\n",
            "Epoch 66/100, Train Loss: 0.0348, Val Loss: 0.0589\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0333, Val Loss: 0.0589\n",
            "Epoch 68/100, Train Loss: 0.0348, Val Loss: 0.0589\n",
            "Epoch 69/100, Train Loss: 0.0342, Val Loss: 0.0589\n",
            "Epoch 70/100, Train Loss: 0.0340, Val Loss: 0.0589\n",
            "Epoch 71/100, Train Loss: 0.0340, Val Loss: 0.0589\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0341, Val Loss: 0.0589\n",
            "Epoch 73/100, Train Loss: 0.0346, Val Loss: 0.0589\n",
            "Epoch 74/100, Train Loss: 0.0346, Val Loss: 0.0589\n",
            "Epoch 75/100, Train Loss: 0.0345, Val Loss: 0.0589\n",
            "Epoch 76/100, Train Loss: 0.0342, Val Loss: 0.0589\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0346, Val Loss: 0.0589\n",
            "Epoch 78/100, Train Loss: 0.0338, Val Loss: 0.0589\n",
            "Epoch 79/100, Train Loss: 0.0340, Val Loss: 0.0589\n",
            "Epoch 80/100, Train Loss: 0.0334, Val Loss: 0.0589\n",
            "Epoch 81/100, Train Loss: 0.0338, Val Loss: 0.0589\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0353, Val Loss: 0.0589\n",
            "Epoch 83/100, Train Loss: 0.0337, Val Loss: 0.0589\n",
            "Epoch 84/100, Train Loss: 0.0351, Val Loss: 0.0589\n",
            "Epoch 85/100, Train Loss: 0.0337, Val Loss: 0.0589\n",
            "Epoch 86/100, Train Loss: 0.0342, Val Loss: 0.0589\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0348, Val Loss: 0.0589\n",
            "Epoch 88/100, Train Loss: 0.0343, Val Loss: 0.0589\n",
            "Epoch 89/100, Train Loss: 0.0361, Val Loss: 0.0589\n",
            "Epoch 90/100, Train Loss: 0.0345, Val Loss: 0.0589\n",
            "Epoch 91/100, Train Loss: 0.0351, Val Loss: 0.0589\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0343, Val Loss: 0.0589\n",
            "Epoch 93/100, Train Loss: 0.0344, Val Loss: 0.0589\n",
            "Epoch 94/100, Train Loss: 0.0335, Val Loss: 0.0589\n",
            "Epoch 95/100, Train Loss: 0.0357, Val Loss: 0.0589\n",
            "Epoch 96/100, Train Loss: 0.0345, Val Loss: 0.0589\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0350, Val Loss: 0.0589\n",
            "Epoch 98/100, Train Loss: 0.0338, Val Loss: 0.0589\n",
            "Epoch 99/100, Train Loss: 0.0340, Val Loss: 0.0589\n",
            "Epoch 100/100, Train Loss: 0.0337, Val Loss: 0.0589\n",
            "\n",
            "Test iteration 3/3\n",
            "Current training set size: 182 samples\n",
            "Epoch 1/100, Train Loss: 0.1919, Val Loss: 0.3738\n",
            "Epoch 2/100, Train Loss: 0.1581, Val Loss: 0.2899\n",
            "Epoch 3/100, Train Loss: 0.1124, Val Loss: 0.0971\n",
            "Epoch 4/100, Train Loss: 0.0742, Val Loss: 0.1451\n",
            "Epoch 5/100, Train Loss: 0.0815, Val Loss: 0.0930\n",
            "Epoch 6/100, Train Loss: 0.0638, Val Loss: 0.0893\n",
            "Epoch 7/100, Train Loss: 0.0589, Val Loss: 0.0868\n",
            "Epoch 8/100, Train Loss: 0.0561, Val Loss: 0.0862\n",
            "Epoch 9/100, Train Loss: 0.0557, Val Loss: 0.0843\n",
            "Epoch 10/100, Train Loss: 0.0538, Val Loss: 0.0835\n",
            "Epoch 11/100, Train Loss: 0.0537, Val Loss: 0.0772\n",
            "Epoch 12/100, Train Loss: 0.0500, Val Loss: 0.0783\n",
            "Epoch 13/100, Train Loss: 0.0527, Val Loss: 0.0768\n",
            "Epoch 14/100, Train Loss: 0.0489, Val Loss: 0.0740\n",
            "Epoch 15/100, Train Loss: 0.0533, Val Loss: 0.0710\n",
            "Epoch 16/100, Train Loss: 0.0461, Val Loss: 0.0839\n",
            "Epoch 17/100, Train Loss: 0.0470, Val Loss: 0.0701\n",
            "Epoch 18/100, Train Loss: 0.0484, Val Loss: 0.0750\n",
            "Epoch 19/100, Train Loss: 0.0416, Val Loss: 0.0752\n",
            "Epoch 20/100, Train Loss: 0.0442, Val Loss: 0.0732\n",
            "Epoch 21/100, Train Loss: 0.0385, Val Loss: 0.0625\n",
            "Epoch 22/100, Train Loss: 0.0377, Val Loss: 0.0636\n",
            "Epoch 23/100, Train Loss: 0.0399, Val Loss: 0.0512\n",
            "Epoch 24/100, Train Loss: 0.0404, Val Loss: 0.0861\n",
            "Epoch 25/100, Train Loss: 0.0447, Val Loss: 0.0534\n",
            "Epoch 26/100, Train Loss: 0.0382, Val Loss: 0.0509\n",
            "Epoch 27/100, Train Loss: 0.0371, Val Loss: 0.0473\n",
            "Epoch 28/100, Train Loss: 0.0409, Val Loss: 0.0529\n",
            "Epoch 29/100, Train Loss: 0.0406, Val Loss: 0.0785\n",
            "Epoch 30/100, Train Loss: 0.0366, Val Loss: 0.0581\n",
            "Epoch 31/100, Train Loss: 0.0429, Val Loss: 0.0649\n",
            "Epoch 32/100, Train Loss: 0.0390, Val Loss: 0.0515\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0342, Val Loss: 0.0516\n",
            "Epoch 34/100, Train Loss: 0.0329, Val Loss: 0.0517\n",
            "Epoch 35/100, Train Loss: 0.0332, Val Loss: 0.0519\n",
            "Epoch 36/100, Train Loss: 0.0323, Val Loss: 0.0523\n",
            "Epoch 37/100, Train Loss: 0.0341, Val Loss: 0.0523\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0325, Val Loss: 0.0523\n",
            "Epoch 39/100, Train Loss: 0.0330, Val Loss: 0.0523\n",
            "Epoch 40/100, Train Loss: 0.0326, Val Loss: 0.0523\n",
            "Epoch 41/100, Train Loss: 0.0328, Val Loss: 0.0523\n",
            "Epoch 42/100, Train Loss: 0.0328, Val Loss: 0.0523\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0327, Val Loss: 0.0523\n",
            "Epoch 44/100, Train Loss: 0.0322, Val Loss: 0.0523\n",
            "Epoch 45/100, Train Loss: 0.0331, Val Loss: 0.0523\n",
            "Epoch 46/100, Train Loss: 0.0330, Val Loss: 0.0523\n",
            "Epoch 47/100, Train Loss: 0.0325, Val Loss: 0.0523\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0335, Val Loss: 0.0523\n",
            "Epoch 49/100, Train Loss: 0.0328, Val Loss: 0.0523\n",
            "Epoch 50/100, Train Loss: 0.0323, Val Loss: 0.0523\n",
            "Epoch 51/100, Train Loss: 0.0324, Val Loss: 0.0523\n",
            "Epoch 52/100, Train Loss: 0.0328, Val Loss: 0.0523\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0319, Val Loss: 0.0523\n",
            "Epoch 54/100, Train Loss: 0.0317, Val Loss: 0.0523\n",
            "Epoch 55/100, Train Loss: 0.0329, Val Loss: 0.0523\n",
            "Epoch 56/100, Train Loss: 0.0330, Val Loss: 0.0523\n",
            "Epoch 57/100, Train Loss: 0.0334, Val Loss: 0.0523\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0329, Val Loss: 0.0523\n",
            "Epoch 59/100, Train Loss: 0.0330, Val Loss: 0.0523\n",
            "Epoch 60/100, Train Loss: 0.0324, Val Loss: 0.0523\n",
            "Epoch 61/100, Train Loss: 0.0333, Val Loss: 0.0523\n",
            "Epoch 62/100, Train Loss: 0.0325, Val Loss: 0.0523\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0323, Val Loss: 0.0523\n",
            "Epoch 64/100, Train Loss: 0.0318, Val Loss: 0.0523\n",
            "Epoch 65/100, Train Loss: 0.0328, Val Loss: 0.0523\n",
            "Epoch 66/100, Train Loss: 0.0329, Val Loss: 0.0523\n",
            "Epoch 67/100, Train Loss: 0.0323, Val Loss: 0.0523\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0329, Val Loss: 0.0523\n",
            "Epoch 69/100, Train Loss: 0.0325, Val Loss: 0.0523\n",
            "Epoch 70/100, Train Loss: 0.0320, Val Loss: 0.0523\n",
            "Epoch 71/100, Train Loss: 0.0326, Val Loss: 0.0523\n",
            "Epoch 72/100, Train Loss: 0.0331, Val Loss: 0.0523\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0327, Val Loss: 0.0523\n",
            "Epoch 74/100, Train Loss: 0.0345, Val Loss: 0.0523\n",
            "Epoch 75/100, Train Loss: 0.0328, Val Loss: 0.0523\n",
            "Epoch 76/100, Train Loss: 0.0320, Val Loss: 0.0523\n",
            "Epoch 77/100, Train Loss: 0.0320, Val Loss: 0.0523\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0318, Val Loss: 0.0523\n",
            "Epoch 79/100, Train Loss: 0.0329, Val Loss: 0.0523\n",
            "Epoch 80/100, Train Loss: 0.0324, Val Loss: 0.0523\n",
            "Epoch 81/100, Train Loss: 0.0329, Val Loss: 0.0523\n",
            "Epoch 82/100, Train Loss: 0.0325, Val Loss: 0.0523\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0324, Val Loss: 0.0523\n",
            "Epoch 84/100, Train Loss: 0.0326, Val Loss: 0.0523\n",
            "Epoch 85/100, Train Loss: 0.0329, Val Loss: 0.0523\n",
            "Epoch 86/100, Train Loss: 0.0321, Val Loss: 0.0523\n",
            "Epoch 87/100, Train Loss: 0.0327, Val Loss: 0.0523\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0328, Val Loss: 0.0523\n",
            "Epoch 89/100, Train Loss: 0.0321, Val Loss: 0.0523\n",
            "Epoch 90/100, Train Loss: 0.0333, Val Loss: 0.0523\n",
            "Epoch 91/100, Train Loss: 0.0323, Val Loss: 0.0523\n",
            "Epoch 92/100, Train Loss: 0.0327, Val Loss: 0.0523\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0328, Val Loss: 0.0523\n",
            "Epoch 94/100, Train Loss: 0.0324, Val Loss: 0.0523\n",
            "Epoch 95/100, Train Loss: 0.0336, Val Loss: 0.0523\n",
            "Epoch 96/100, Train Loss: 0.0323, Val Loss: 0.0523\n",
            "Epoch 97/100, Train Loss: 0.0322, Val Loss: 0.0523\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0332, Val Loss: 0.0523\n",
            "Epoch 99/100, Train Loss: 0.0329, Val Loss: 0.0523\n",
            "Epoch 100/100, Train Loss: 0.0329, Val Loss: 0.0523\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: hybrid, Units: L256_G16, Dropout: 0.1, Dense Units: 64\n",
            "Batch Size: 4, Epochs: 100\n",
            "Device: cuda\n",
            "Total test samples: 3\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/3\n",
            "Current training set size: 180 samples\n",
            "Epoch 1/100, Train Loss: 0.1790, Val Loss: 0.3736\n",
            "Epoch 2/100, Train Loss: 0.1452, Val Loss: 0.1713\n",
            "Epoch 3/100, Train Loss: 0.0785, Val Loss: 0.0934\n",
            "Epoch 4/100, Train Loss: 0.0743, Val Loss: 0.0856\n",
            "Epoch 5/100, Train Loss: 0.0665, Val Loss: 0.0845\n",
            "Epoch 6/100, Train Loss: 0.0574, Val Loss: 0.0835\n",
            "Epoch 7/100, Train Loss: 0.0662, Val Loss: 0.0820\n",
            "Epoch 8/100, Train Loss: 0.0574, Val Loss: 0.0795\n",
            "Epoch 9/100, Train Loss: 0.0565, Val Loss: 0.0793\n",
            "Epoch 10/100, Train Loss: 0.0528, Val Loss: 0.0773\n",
            "Epoch 11/100, Train Loss: 0.0478, Val Loss: 0.0852\n",
            "Epoch 12/100, Train Loss: 0.0466, Val Loss: 0.0730\n",
            "Epoch 13/100, Train Loss: 0.0493, Val Loss: 0.0722\n",
            "Epoch 14/100, Train Loss: 0.0462, Val Loss: 0.0657\n",
            "Epoch 15/100, Train Loss: 0.0512, Val Loss: 0.0694\n",
            "Epoch 16/100, Train Loss: 0.0476, Val Loss: 0.0772\n",
            "Epoch 17/100, Train Loss: 0.0439, Val Loss: 0.0689\n",
            "Epoch 18/100, Train Loss: 0.0419, Val Loss: 0.0883\n",
            "Epoch 19/100, Train Loss: 0.0471, Val Loss: 0.0618\n",
            "Epoch 20/100, Train Loss: 0.0398, Val Loss: 0.0574\n",
            "Epoch 21/100, Train Loss: 0.0366, Val Loss: 0.0567\n",
            "Epoch 22/100, Train Loss: 0.0418, Val Loss: 0.0837\n",
            "Epoch 23/100, Train Loss: 0.0415, Val Loss: 0.0591\n",
            "Epoch 24/100, Train Loss: 0.0388, Val Loss: 0.0570\n",
            "Epoch 25/100, Train Loss: 0.0369, Val Loss: 0.0848\n",
            "Epoch 26/100, Train Loss: 0.0404, Val Loss: 0.0536\n",
            "Epoch 27/100, Train Loss: 0.0397, Val Loss: 0.0547\n",
            "Epoch 28/100, Train Loss: 0.0389, Val Loss: 0.0517\n",
            "Epoch 29/100, Train Loss: 0.0423, Val Loss: 0.0526\n",
            "Epoch 30/100, Train Loss: 0.0365, Val Loss: 0.0573\n",
            "Epoch 31/100, Train Loss: 0.0379, Val Loss: 0.0726\n",
            "Epoch 32/100, Train Loss: 0.0428, Val Loss: 0.0560\n",
            "Epoch 33/100, Train Loss: 0.0373, Val Loss: 0.0575\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0330, Val Loss: 0.0575\n",
            "Epoch 35/100, Train Loss: 0.0322, Val Loss: 0.0573\n",
            "Epoch 36/100, Train Loss: 0.0320, Val Loss: 0.0575\n",
            "Epoch 37/100, Train Loss: 0.0330, Val Loss: 0.0575\n",
            "Epoch 38/100, Train Loss: 0.0326, Val Loss: 0.0572\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0318, Val Loss: 0.0572\n",
            "Epoch 40/100, Train Loss: 0.0324, Val Loss: 0.0572\n",
            "Epoch 41/100, Train Loss: 0.0328, Val Loss: 0.0572\n",
            "Epoch 42/100, Train Loss: 0.0322, Val Loss: 0.0572\n",
            "Epoch 43/100, Train Loss: 0.0323, Val Loss: 0.0572\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0321, Val Loss: 0.0572\n",
            "Epoch 45/100, Train Loss: 0.0329, Val Loss: 0.0572\n",
            "Epoch 46/100, Train Loss: 0.0322, Val Loss: 0.0572\n",
            "Epoch 47/100, Train Loss: 0.0327, Val Loss: 0.0572\n",
            "Epoch 48/100, Train Loss: 0.0323, Val Loss: 0.0572\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0315, Val Loss: 0.0572\n",
            "Epoch 50/100, Train Loss: 0.0323, Val Loss: 0.0572\n",
            "Epoch 51/100, Train Loss: 0.0326, Val Loss: 0.0572\n",
            "Epoch 52/100, Train Loss: 0.0314, Val Loss: 0.0572\n",
            "Epoch 53/100, Train Loss: 0.0323, Val Loss: 0.0572\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0327, Val Loss: 0.0572\n",
            "Epoch 55/100, Train Loss: 0.0327, Val Loss: 0.0572\n",
            "Epoch 56/100, Train Loss: 0.0322, Val Loss: 0.0572\n",
            "Epoch 57/100, Train Loss: 0.0326, Val Loss: 0.0572\n",
            "Epoch 58/100, Train Loss: 0.0324, Val Loss: 0.0572\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0321, Val Loss: 0.0572\n",
            "Epoch 60/100, Train Loss: 0.0331, Val Loss: 0.0572\n",
            "Epoch 61/100, Train Loss: 0.0324, Val Loss: 0.0572\n",
            "Epoch 62/100, Train Loss: 0.0329, Val Loss: 0.0572\n",
            "Epoch 63/100, Train Loss: 0.0326, Val Loss: 0.0572\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0328, Val Loss: 0.0572\n",
            "Epoch 65/100, Train Loss: 0.0328, Val Loss: 0.0572\n",
            "Epoch 66/100, Train Loss: 0.0317, Val Loss: 0.0572\n",
            "Epoch 67/100, Train Loss: 0.0324, Val Loss: 0.0572\n",
            "Epoch 68/100, Train Loss: 0.0317, Val Loss: 0.0572\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0322, Val Loss: 0.0572\n",
            "Epoch 70/100, Train Loss: 0.0322, Val Loss: 0.0572\n",
            "Epoch 71/100, Train Loss: 0.0322, Val Loss: 0.0572\n",
            "Epoch 72/100, Train Loss: 0.0320, Val Loss: 0.0572\n",
            "Epoch 73/100, Train Loss: 0.0326, Val Loss: 0.0572\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0323, Val Loss: 0.0572\n",
            "Epoch 75/100, Train Loss: 0.0323, Val Loss: 0.0572\n",
            "Epoch 76/100, Train Loss: 0.0328, Val Loss: 0.0572\n",
            "Epoch 77/100, Train Loss: 0.0318, Val Loss: 0.0572\n",
            "Epoch 78/100, Train Loss: 0.0325, Val Loss: 0.0572\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0330, Val Loss: 0.0572\n",
            "Epoch 80/100, Train Loss: 0.0323, Val Loss: 0.0572\n",
            "Epoch 81/100, Train Loss: 0.0320, Val Loss: 0.0572\n",
            "Epoch 82/100, Train Loss: 0.0316, Val Loss: 0.0572\n",
            "Epoch 83/100, Train Loss: 0.0321, Val Loss: 0.0572\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0324, Val Loss: 0.0572\n",
            "Epoch 85/100, Train Loss: 0.0324, Val Loss: 0.0572\n",
            "Epoch 86/100, Train Loss: 0.0325, Val Loss: 0.0572\n",
            "Epoch 87/100, Train Loss: 0.0325, Val Loss: 0.0572\n",
            "Epoch 88/100, Train Loss: 0.0320, Val Loss: 0.0572\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0329, Val Loss: 0.0572\n",
            "Epoch 90/100, Train Loss: 0.0330, Val Loss: 0.0572\n",
            "Epoch 91/100, Train Loss: 0.0320, Val Loss: 0.0572\n",
            "Epoch 92/100, Train Loss: 0.0318, Val Loss: 0.0572\n",
            "Epoch 93/100, Train Loss: 0.0322, Val Loss: 0.0572\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0320, Val Loss: 0.0572\n",
            "Epoch 95/100, Train Loss: 0.0322, Val Loss: 0.0572\n",
            "Epoch 96/100, Train Loss: 0.0324, Val Loss: 0.0572\n",
            "Epoch 97/100, Train Loss: 0.0322, Val Loss: 0.0572\n",
            "Epoch 98/100, Train Loss: 0.0320, Val Loss: 0.0572\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0323, Val Loss: 0.0572\n",
            "Epoch 100/100, Train Loss: 0.0319, Val Loss: 0.0572\n",
            "\n",
            "Test iteration 2/3\n",
            "Current training set size: 181 samples\n",
            "Epoch 1/100, Train Loss: 0.1787, Val Loss: 0.3182\n",
            "Epoch 2/100, Train Loss: 0.1340, Val Loss: 0.0895\n",
            "Epoch 3/100, Train Loss: 0.0943, Val Loss: 0.1086\n",
            "Epoch 4/100, Train Loss: 0.0777, Val Loss: 0.0892\n",
            "Epoch 5/100, Train Loss: 0.0639, Val Loss: 0.0843\n",
            "Epoch 6/100, Train Loss: 0.0559, Val Loss: 0.0852\n",
            "Epoch 7/100, Train Loss: 0.0589, Val Loss: 0.0872\n",
            "Epoch 8/100, Train Loss: 0.0608, Val Loss: 0.0803\n",
            "Epoch 9/100, Train Loss: 0.0567, Val Loss: 0.0785\n",
            "Epoch 10/100, Train Loss: 0.0498, Val Loss: 0.1075\n",
            "Epoch 11/100, Train Loss: 0.0473, Val Loss: 0.0735\n",
            "Epoch 12/100, Train Loss: 0.0474, Val Loss: 0.0682\n",
            "Epoch 13/100, Train Loss: 0.0447, Val Loss: 0.0648\n",
            "Epoch 14/100, Train Loss: 0.0471, Val Loss: 0.0599\n",
            "Epoch 15/100, Train Loss: 0.0498, Val Loss: 0.0605\n",
            "Epoch 16/100, Train Loss: 0.0451, Val Loss: 0.0653\n",
            "Epoch 17/100, Train Loss: 0.0448, Val Loss: 0.0936\n",
            "Epoch 18/100, Train Loss: 0.0412, Val Loss: 0.0659\n",
            "Epoch 19/100, Train Loss: 0.0404, Val Loss: 0.0592\n",
            "Epoch 20/100, Train Loss: 0.0381, Val Loss: 0.0628\n",
            "Epoch 21/100, Train Loss: 0.0437, Val Loss: 0.0530\n",
            "Epoch 22/100, Train Loss: 0.0401, Val Loss: 0.0593\n",
            "Epoch 23/100, Train Loss: 0.0388, Val Loss: 0.0545\n",
            "Epoch 24/100, Train Loss: 0.0379, Val Loss: 0.0518\n",
            "Epoch 25/100, Train Loss: 0.0390, Val Loss: 0.0808\n",
            "Epoch 26/100, Train Loss: 0.0399, Val Loss: 0.0670\n",
            "Epoch 27/100, Train Loss: 0.0408, Val Loss: 0.0603\n",
            "Epoch 28/100, Train Loss: 0.0345, Val Loss: 0.0591\n",
            "Epoch 29/100, Train Loss: 0.0374, Val Loss: 0.0527\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0328, Val Loss: 0.0512\n",
            "Epoch 31/100, Train Loss: 0.0328, Val Loss: 0.0503\n",
            "Epoch 32/100, Train Loss: 0.0330, Val Loss: 0.0494\n",
            "Epoch 33/100, Train Loss: 0.0322, Val Loss: 0.0482\n",
            "Epoch 34/100, Train Loss: 0.0313, Val Loss: 0.0475\n",
            "Epoch 35/100, Train Loss: 0.0309, Val Loss: 0.0467\n",
            "Epoch 36/100, Train Loss: 0.0303, Val Loss: 0.0463\n",
            "Epoch 37/100, Train Loss: 0.0320, Val Loss: 0.0460\n",
            "Epoch 38/100, Train Loss: 0.0306, Val Loss: 0.0460\n",
            "Epoch 39/100, Train Loss: 0.0306, Val Loss: 0.0453\n",
            "Epoch 40/100, Train Loss: 0.0309, Val Loss: 0.0452\n",
            "Epoch 41/100, Train Loss: 0.0300, Val Loss: 0.0450\n",
            "Epoch 42/100, Train Loss: 0.0309, Val Loss: 0.0454\n",
            "Epoch 43/100, Train Loss: 0.0315, Val Loss: 0.0455\n",
            "Epoch 44/100, Train Loss: 0.0301, Val Loss: 0.0453\n",
            "Epoch 45/100, Train Loss: 0.0303, Val Loss: 0.0456\n",
            "Epoch 46/100, Train Loss: 0.0310, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0305, Val Loss: 0.0455\n",
            "Epoch 48/100, Train Loss: 0.0306, Val Loss: 0.0455\n",
            "Epoch 49/100, Train Loss: 0.0309, Val Loss: 0.0455\n",
            "Epoch 50/100, Train Loss: 0.0308, Val Loss: 0.0455\n",
            "Epoch 51/100, Train Loss: 0.0294, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0315, Val Loss: 0.0455\n",
            "Epoch 53/100, Train Loss: 0.0301, Val Loss: 0.0455\n",
            "Epoch 54/100, Train Loss: 0.0304, Val Loss: 0.0455\n",
            "Epoch 55/100, Train Loss: 0.0313, Val Loss: 0.0455\n",
            "Epoch 56/100, Train Loss: 0.0304, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0306, Val Loss: 0.0455\n",
            "Epoch 58/100, Train Loss: 0.0309, Val Loss: 0.0455\n",
            "Epoch 59/100, Train Loss: 0.0308, Val Loss: 0.0455\n",
            "Epoch 60/100, Train Loss: 0.0301, Val Loss: 0.0455\n",
            "Epoch 61/100, Train Loss: 0.0303, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0313, Val Loss: 0.0455\n",
            "Epoch 63/100, Train Loss: 0.0304, Val Loss: 0.0455\n",
            "Epoch 64/100, Train Loss: 0.0301, Val Loss: 0.0455\n",
            "Epoch 65/100, Train Loss: 0.0310, Val Loss: 0.0455\n",
            "Epoch 66/100, Train Loss: 0.0307, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0299, Val Loss: 0.0455\n",
            "Epoch 68/100, Train Loss: 0.0312, Val Loss: 0.0455\n",
            "Epoch 69/100, Train Loss: 0.0297, Val Loss: 0.0455\n",
            "Epoch 70/100, Train Loss: 0.0302, Val Loss: 0.0455\n",
            "Epoch 71/100, Train Loss: 0.0300, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0303, Val Loss: 0.0455\n",
            "Epoch 73/100, Train Loss: 0.0303, Val Loss: 0.0455\n",
            "Epoch 74/100, Train Loss: 0.0311, Val Loss: 0.0455\n",
            "Epoch 75/100, Train Loss: 0.0305, Val Loss: 0.0455\n",
            "Epoch 76/100, Train Loss: 0.0303, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0307, Val Loss: 0.0455\n",
            "Epoch 78/100, Train Loss: 0.0300, Val Loss: 0.0455\n",
            "Epoch 79/100, Train Loss: 0.0304, Val Loss: 0.0455\n",
            "Epoch 80/100, Train Loss: 0.0305, Val Loss: 0.0455\n",
            "Epoch 81/100, Train Loss: 0.0305, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0312, Val Loss: 0.0455\n",
            "Epoch 83/100, Train Loss: 0.0301, Val Loss: 0.0455\n",
            "Epoch 84/100, Train Loss: 0.0310, Val Loss: 0.0455\n",
            "Epoch 85/100, Train Loss: 0.0298, Val Loss: 0.0455\n",
            "Epoch 86/100, Train Loss: 0.0304, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0308, Val Loss: 0.0455\n",
            "Epoch 88/100, Train Loss: 0.0300, Val Loss: 0.0455\n",
            "Epoch 89/100, Train Loss: 0.0324, Val Loss: 0.0455\n",
            "Epoch 90/100, Train Loss: 0.0302, Val Loss: 0.0455\n",
            "Epoch 91/100, Train Loss: 0.0319, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0304, Val Loss: 0.0455\n",
            "Epoch 93/100, Train Loss: 0.0306, Val Loss: 0.0455\n",
            "Epoch 94/100, Train Loss: 0.0303, Val Loss: 0.0455\n",
            "Epoch 95/100, Train Loss: 0.0315, Val Loss: 0.0455\n",
            "Epoch 96/100, Train Loss: 0.0305, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0299, Val Loss: 0.0455\n",
            "Epoch 98/100, Train Loss: 0.0303, Val Loss: 0.0455\n",
            "Epoch 99/100, Train Loss: 0.0303, Val Loss: 0.0455\n",
            "Epoch 100/100, Train Loss: 0.0299, Val Loss: 0.0455\n",
            "\n",
            "Test iteration 3/3\n",
            "Current training set size: 182 samples\n",
            "Epoch 1/100, Train Loss: 0.1976, Val Loss: 0.3906\n",
            "Epoch 2/100, Train Loss: 0.1555, Val Loss: 0.2670\n",
            "Epoch 3/100, Train Loss: 0.0944, Val Loss: 0.1016\n",
            "Epoch 4/100, Train Loss: 0.0700, Val Loss: 0.1048\n",
            "Epoch 5/100, Train Loss: 0.0717, Val Loss: 0.0972\n",
            "Epoch 6/100, Train Loss: 0.0616, Val Loss: 0.0908\n",
            "Epoch 7/100, Train Loss: 0.0556, Val Loss: 0.0833\n",
            "Epoch 8/100, Train Loss: 0.0512, Val Loss: 0.0812\n",
            "Epoch 9/100, Train Loss: 0.0511, Val Loss: 0.0848\n",
            "Epoch 10/100, Train Loss: 0.0524, Val Loss: 0.0784\n",
            "Epoch 11/100, Train Loss: 0.0504, Val Loss: 0.0787\n",
            "Epoch 12/100, Train Loss: 0.0503, Val Loss: 0.0745\n",
            "Epoch 13/100, Train Loss: 0.0486, Val Loss: 0.0704\n",
            "Epoch 14/100, Train Loss: 0.0495, Val Loss: 0.0773\n",
            "Epoch 15/100, Train Loss: 0.0517, Val Loss: 0.0660\n",
            "Epoch 16/100, Train Loss: 0.0427, Val Loss: 0.0654\n",
            "Epoch 17/100, Train Loss: 0.0416, Val Loss: 0.0628\n",
            "Epoch 18/100, Train Loss: 0.0426, Val Loss: 0.0656\n",
            "Epoch 19/100, Train Loss: 0.0391, Val Loss: 0.0917\n",
            "Epoch 20/100, Train Loss: 0.0413, Val Loss: 0.0957\n",
            "Epoch 21/100, Train Loss: 0.0421, Val Loss: 0.0619\n",
            "Epoch 22/100, Train Loss: 0.0407, Val Loss: 0.0620\n",
            "Epoch 23/100, Train Loss: 0.0415, Val Loss: 0.0591\n",
            "Epoch 24/100, Train Loss: 0.0382, Val Loss: 0.0632\n",
            "Epoch 25/100, Train Loss: 0.0448, Val Loss: 0.0520\n",
            "Epoch 26/100, Train Loss: 0.0379, Val Loss: 0.0556\n",
            "Epoch 27/100, Train Loss: 0.0420, Val Loss: 0.0586\n",
            "Epoch 28/100, Train Loss: 0.0396, Val Loss: 0.0527\n",
            "Epoch 29/100, Train Loss: 0.0412, Val Loss: 0.0989\n",
            "Epoch 30/100, Train Loss: 0.0370, Val Loss: 0.0582\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0343, Val Loss: 0.0584\n",
            "Epoch 32/100, Train Loss: 0.0339, Val Loss: 0.0582\n",
            "Epoch 33/100, Train Loss: 0.0336, Val Loss: 0.0580\n",
            "Epoch 34/100, Train Loss: 0.0327, Val Loss: 0.0576\n",
            "Epoch 35/100, Train Loss: 0.0333, Val Loss: 0.0576\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0321, Val Loss: 0.0576\n",
            "Epoch 37/100, Train Loss: 0.0344, Val Loss: 0.0576\n",
            "Epoch 38/100, Train Loss: 0.0323, Val Loss: 0.0576\n",
            "Epoch 39/100, Train Loss: 0.0330, Val Loss: 0.0576\n",
            "Epoch 40/100, Train Loss: 0.0330, Val Loss: 0.0576\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0327, Val Loss: 0.0576\n",
            "Epoch 42/100, Train Loss: 0.0325, Val Loss: 0.0576\n",
            "Epoch 43/100, Train Loss: 0.0327, Val Loss: 0.0576\n",
            "Epoch 44/100, Train Loss: 0.0324, Val Loss: 0.0576\n",
            "Epoch 45/100, Train Loss: 0.0339, Val Loss: 0.0576\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0328, Val Loss: 0.0576\n",
            "Epoch 47/100, Train Loss: 0.0324, Val Loss: 0.0576\n",
            "Epoch 48/100, Train Loss: 0.0336, Val Loss: 0.0576\n",
            "Epoch 49/100, Train Loss: 0.0329, Val Loss: 0.0576\n",
            "Epoch 50/100, Train Loss: 0.0326, Val Loss: 0.0576\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0328, Val Loss: 0.0576\n",
            "Epoch 52/100, Train Loss: 0.0330, Val Loss: 0.0576\n",
            "Epoch 53/100, Train Loss: 0.0321, Val Loss: 0.0576\n",
            "Epoch 54/100, Train Loss: 0.0317, Val Loss: 0.0576\n",
            "Epoch 55/100, Train Loss: 0.0331, Val Loss: 0.0576\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0331, Val Loss: 0.0576\n",
            "Epoch 57/100, Train Loss: 0.0334, Val Loss: 0.0576\n",
            "Epoch 58/100, Train Loss: 0.0335, Val Loss: 0.0576\n",
            "Epoch 59/100, Train Loss: 0.0331, Val Loss: 0.0576\n",
            "Epoch 60/100, Train Loss: 0.0327, Val Loss: 0.0576\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0329, Val Loss: 0.0576\n",
            "Epoch 62/100, Train Loss: 0.0327, Val Loss: 0.0576\n",
            "Epoch 63/100, Train Loss: 0.0319, Val Loss: 0.0576\n",
            "Epoch 64/100, Train Loss: 0.0320, Val Loss: 0.0576\n",
            "Epoch 65/100, Train Loss: 0.0333, Val Loss: 0.0576\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0329, Val Loss: 0.0576\n",
            "Epoch 67/100, Train Loss: 0.0328, Val Loss: 0.0576\n",
            "Epoch 68/100, Train Loss: 0.0326, Val Loss: 0.0576\n",
            "Epoch 69/100, Train Loss: 0.0321, Val Loss: 0.0576\n",
            "Epoch 70/100, Train Loss: 0.0325, Val Loss: 0.0576\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0332, Val Loss: 0.0576\n",
            "Epoch 72/100, Train Loss: 0.0331, Val Loss: 0.0576\n",
            "Epoch 73/100, Train Loss: 0.0320, Val Loss: 0.0576\n",
            "Epoch 74/100, Train Loss: 0.0346, Val Loss: 0.0576\n",
            "Epoch 75/100, Train Loss: 0.0328, Val Loss: 0.0576\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0320, Val Loss: 0.0576\n",
            "Epoch 77/100, Train Loss: 0.0324, Val Loss: 0.0576\n",
            "Epoch 78/100, Train Loss: 0.0314, Val Loss: 0.0576\n",
            "Epoch 79/100, Train Loss: 0.0323, Val Loss: 0.0576\n",
            "Epoch 80/100, Train Loss: 0.0329, Val Loss: 0.0576\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0324, Val Loss: 0.0576\n",
            "Epoch 82/100, Train Loss: 0.0328, Val Loss: 0.0576\n",
            "Epoch 83/100, Train Loss: 0.0323, Val Loss: 0.0576\n",
            "Epoch 84/100, Train Loss: 0.0324, Val Loss: 0.0576\n",
            "Epoch 85/100, Train Loss: 0.0330, Val Loss: 0.0576\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0324, Val Loss: 0.0576\n",
            "Epoch 87/100, Train Loss: 0.0325, Val Loss: 0.0576\n",
            "Epoch 88/100, Train Loss: 0.0324, Val Loss: 0.0576\n",
            "Epoch 89/100, Train Loss: 0.0326, Val Loss: 0.0576\n",
            "Epoch 90/100, Train Loss: 0.0330, Val Loss: 0.0576\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0324, Val Loss: 0.0576\n",
            "Epoch 92/100, Train Loss: 0.0324, Val Loss: 0.0576\n",
            "Epoch 93/100, Train Loss: 0.0329, Val Loss: 0.0576\n",
            "Epoch 94/100, Train Loss: 0.0324, Val Loss: 0.0576\n",
            "Epoch 95/100, Train Loss: 0.0333, Val Loss: 0.0576\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0324, Val Loss: 0.0576\n",
            "Epoch 97/100, Train Loss: 0.0328, Val Loss: 0.0576\n",
            "Epoch 98/100, Train Loss: 0.0331, Val Loss: 0.0576\n",
            "Epoch 99/100, Train Loss: 0.0331, Val Loss: 0.0576\n",
            "Epoch 100/100, Train Loss: 0.0327, Val Loss: 0.0576\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: hybrid, Units: L256_G16, Dropout: 0.2, Dense Units: 32\n",
            "Batch Size: 4, Epochs: 100\n",
            "Device: cuda\n",
            "Total test samples: 3\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/3\n",
            "Current training set size: 180 samples\n",
            "Epoch 1/100, Train Loss: 0.1769, Val Loss: 0.3892\n",
            "Epoch 2/100, Train Loss: 0.1473, Val Loss: 0.1968\n",
            "Epoch 3/100, Train Loss: 0.0905, Val Loss: 0.0863\n",
            "Epoch 4/100, Train Loss: 0.0798, Val Loss: 0.0937\n",
            "Epoch 5/100, Train Loss: 0.0684, Val Loss: 0.0906\n",
            "Epoch 6/100, Train Loss: 0.0633, Val Loss: 0.0875\n",
            "Epoch 7/100, Train Loss: 0.0736, Val Loss: 0.0964\n",
            "Epoch 8/100, Train Loss: 0.0598, Val Loss: 0.0840\n",
            "Epoch 9/100, Train Loss: 0.0590, Val Loss: 0.0871\n",
            "Epoch 10/100, Train Loss: 0.0615, Val Loss: 0.0814\n",
            "Epoch 11/100, Train Loss: 0.0513, Val Loss: 0.0813\n",
            "Epoch 12/100, Train Loss: 0.0506, Val Loss: 0.0781\n",
            "Epoch 13/100, Train Loss: 0.0522, Val Loss: 0.0774\n",
            "Epoch 14/100, Train Loss: 0.0498, Val Loss: 0.0783\n",
            "Epoch 15/100, Train Loss: 0.0451, Val Loss: 0.0709\n",
            "Epoch 16/100, Train Loss: 0.0509, Val Loss: 0.0812\n",
            "Epoch 17/100, Train Loss: 0.0488, Val Loss: 0.0661\n",
            "Epoch 18/100, Train Loss: 0.0405, Val Loss: 0.0660\n",
            "Epoch 19/100, Train Loss: 0.0478, Val Loss: 0.0667\n",
            "Epoch 20/100, Train Loss: 0.0476, Val Loss: 0.0626\n",
            "Epoch 21/100, Train Loss: 0.0440, Val Loss: 0.0664\n",
            "Epoch 22/100, Train Loss: 0.0427, Val Loss: 0.0578\n",
            "Epoch 23/100, Train Loss: 0.0397, Val Loss: 0.0588\n",
            "Epoch 24/100, Train Loss: 0.0436, Val Loss: 0.0604\n",
            "Epoch 25/100, Train Loss: 0.0405, Val Loss: 0.0698\n",
            "Epoch 26/100, Train Loss: 0.0403, Val Loss: 0.0556\n",
            "Epoch 27/100, Train Loss: 0.0389, Val Loss: 0.0541\n",
            "Epoch 28/100, Train Loss: 0.0420, Val Loss: 0.0581\n",
            "Epoch 29/100, Train Loss: 0.0390, Val Loss: 0.0552\n",
            "Epoch 30/100, Train Loss: 0.0381, Val Loss: 0.0494\n",
            "Epoch 31/100, Train Loss: 0.0381, Val Loss: 0.0725\n",
            "Epoch 32/100, Train Loss: 0.0414, Val Loss: 0.0512\n",
            "Epoch 33/100, Train Loss: 0.0371, Val Loss: 0.0499\n",
            "Epoch 34/100, Train Loss: 0.0373, Val Loss: 0.0580\n",
            "Epoch 35/100, Train Loss: 0.0395, Val Loss: 0.0460\n",
            "Epoch 36/100, Train Loss: 0.0396, Val Loss: 0.0502\n",
            "Epoch 37/100, Train Loss: 0.0361, Val Loss: 0.0536\n",
            "Epoch 38/100, Train Loss: 0.0375, Val Loss: 0.0808\n",
            "Epoch 39/100, Train Loss: 0.0364, Val Loss: 0.0633\n",
            "Epoch 40/100, Train Loss: 0.0390, Val Loss: 0.0553\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0412, Val Loss: 0.0510\n",
            "Epoch 42/100, Train Loss: 0.0337, Val Loss: 0.0494\n",
            "Epoch 43/100, Train Loss: 0.0321, Val Loss: 0.0490\n",
            "Epoch 44/100, Train Loss: 0.0313, Val Loss: 0.0487\n",
            "Epoch 45/100, Train Loss: 0.0321, Val Loss: 0.0485\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0311, Val Loss: 0.0485\n",
            "Epoch 47/100, Train Loss: 0.0325, Val Loss: 0.0485\n",
            "Epoch 48/100, Train Loss: 0.0317, Val Loss: 0.0485\n",
            "Epoch 49/100, Train Loss: 0.0311, Val Loss: 0.0485\n",
            "Epoch 50/100, Train Loss: 0.0323, Val Loss: 0.0485\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0323, Val Loss: 0.0485\n",
            "Epoch 52/100, Train Loss: 0.0318, Val Loss: 0.0485\n",
            "Epoch 53/100, Train Loss: 0.0312, Val Loss: 0.0485\n",
            "Epoch 54/100, Train Loss: 0.0321, Val Loss: 0.0485\n",
            "Epoch 55/100, Train Loss: 0.0330, Val Loss: 0.0485\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0316, Val Loss: 0.0485\n",
            "Epoch 57/100, Train Loss: 0.0318, Val Loss: 0.0485\n",
            "Epoch 58/100, Train Loss: 0.0315, Val Loss: 0.0485\n",
            "Epoch 59/100, Train Loss: 0.0317, Val Loss: 0.0485\n",
            "Epoch 60/100, Train Loss: 0.0330, Val Loss: 0.0485\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0335, Val Loss: 0.0485\n",
            "Epoch 62/100, Train Loss: 0.0331, Val Loss: 0.0485\n",
            "Epoch 63/100, Train Loss: 0.0312, Val Loss: 0.0485\n",
            "Epoch 64/100, Train Loss: 0.0316, Val Loss: 0.0485\n",
            "Epoch 65/100, Train Loss: 0.0319, Val Loss: 0.0485\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0320, Val Loss: 0.0485\n",
            "Epoch 67/100, Train Loss: 0.0323, Val Loss: 0.0485\n",
            "Epoch 68/100, Train Loss: 0.0315, Val Loss: 0.0485\n",
            "Epoch 69/100, Train Loss: 0.0312, Val Loss: 0.0485\n",
            "Epoch 70/100, Train Loss: 0.0316, Val Loss: 0.0485\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0316, Val Loss: 0.0485\n",
            "Epoch 72/100, Train Loss: 0.0330, Val Loss: 0.0485\n",
            "Epoch 73/100, Train Loss: 0.0323, Val Loss: 0.0485\n",
            "Epoch 74/100, Train Loss: 0.0315, Val Loss: 0.0485\n",
            "Epoch 75/100, Train Loss: 0.0323, Val Loss: 0.0485\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0320, Val Loss: 0.0485\n",
            "Epoch 77/100, Train Loss: 0.0310, Val Loss: 0.0485\n",
            "Epoch 78/100, Train Loss: 0.0312, Val Loss: 0.0485\n",
            "Epoch 79/100, Train Loss: 0.0321, Val Loss: 0.0485\n",
            "Epoch 80/100, Train Loss: 0.0316, Val Loss: 0.0485\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0312, Val Loss: 0.0485\n",
            "Epoch 82/100, Train Loss: 0.0316, Val Loss: 0.0485\n",
            "Epoch 83/100, Train Loss: 0.0317, Val Loss: 0.0485\n",
            "Epoch 84/100, Train Loss: 0.0314, Val Loss: 0.0485\n",
            "Epoch 85/100, Train Loss: 0.0316, Val Loss: 0.0485\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0316, Val Loss: 0.0485\n",
            "Epoch 87/100, Train Loss: 0.0311, Val Loss: 0.0485\n",
            "Epoch 88/100, Train Loss: 0.0327, Val Loss: 0.0485\n",
            "Epoch 89/100, Train Loss: 0.0320, Val Loss: 0.0485\n",
            "Epoch 90/100, Train Loss: 0.0314, Val Loss: 0.0485\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0312, Val Loss: 0.0485\n",
            "Epoch 92/100, Train Loss: 0.0313, Val Loss: 0.0485\n",
            "Epoch 93/100, Train Loss: 0.0320, Val Loss: 0.0485\n",
            "Epoch 94/100, Train Loss: 0.0322, Val Loss: 0.0485\n",
            "Epoch 95/100, Train Loss: 0.0322, Val Loss: 0.0485\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0321, Val Loss: 0.0485\n",
            "Epoch 97/100, Train Loss: 0.0324, Val Loss: 0.0485\n",
            "Epoch 98/100, Train Loss: 0.0322, Val Loss: 0.0485\n",
            "Epoch 99/100, Train Loss: 0.0314, Val Loss: 0.0485\n",
            "Epoch 100/100, Train Loss: 0.0319, Val Loss: 0.0485\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 2/3\n",
            "Current training set size: 181 samples\n",
            "Epoch 1/100, Train Loss: 0.1761, Val Loss: 0.3687\n",
            "Epoch 2/100, Train Loss: 0.1398, Val Loss: 0.0970\n",
            "Epoch 3/100, Train Loss: 0.0949, Val Loss: 0.1203\n",
            "Epoch 4/100, Train Loss: 0.0704, Val Loss: 0.0871\n",
            "Epoch 5/100, Train Loss: 0.0861, Val Loss: 0.0945\n",
            "Epoch 6/100, Train Loss: 0.0677, Val Loss: 0.0849\n",
            "Epoch 7/100, Train Loss: 0.0638, Val Loss: 0.0856\n",
            "Epoch 8/100, Train Loss: 0.0609, Val Loss: 0.0845\n",
            "Epoch 9/100, Train Loss: 0.0673, Val Loss: 0.0838\n",
            "Epoch 10/100, Train Loss: 0.0537, Val Loss: 0.1414\n",
            "Epoch 11/100, Train Loss: 0.0523, Val Loss: 0.0773\n",
            "Epoch 12/100, Train Loss: 0.0504, Val Loss: 0.0748\n",
            "Epoch 13/100, Train Loss: 0.0468, Val Loss: 0.0741\n",
            "Epoch 14/100, Train Loss: 0.0464, Val Loss: 0.0718\n",
            "Epoch 15/100, Train Loss: 0.0530, Val Loss: 0.0660\n",
            "Epoch 16/100, Train Loss: 0.0509, Val Loss: 0.0691\n",
            "Epoch 17/100, Train Loss: 0.0486, Val Loss: 0.0831\n",
            "Epoch 18/100, Train Loss: 0.0431, Val Loss: 0.0684\n",
            "Epoch 19/100, Train Loss: 0.0432, Val Loss: 0.0735\n",
            "Epoch 20/100, Train Loss: 0.0427, Val Loss: 0.0638\n",
            "Epoch 21/100, Train Loss: 0.0429, Val Loss: 0.0606\n",
            "Epoch 22/100, Train Loss: 0.0422, Val Loss: 0.0593\n",
            "Epoch 23/100, Train Loss: 0.0441, Val Loss: 0.0668\n",
            "Epoch 24/100, Train Loss: 0.0420, Val Loss: 0.0557\n",
            "Epoch 25/100, Train Loss: 0.0429, Val Loss: 0.0655\n",
            "Epoch 26/100, Train Loss: 0.0435, Val Loss: 0.0706\n",
            "Epoch 27/100, Train Loss: 0.0397, Val Loss: 0.0574\n",
            "Epoch 28/100, Train Loss: 0.0387, Val Loss: 0.0468\n",
            "Epoch 29/100, Train Loss: 0.0367, Val Loss: 0.0497\n",
            "Epoch 30/100, Train Loss: 0.0395, Val Loss: 0.0505\n",
            "Epoch 31/100, Train Loss: 0.0443, Val Loss: 0.0539\n",
            "Epoch 32/100, Train Loss: 0.0387, Val Loss: 0.0532\n",
            "Epoch 33/100, Train Loss: 0.0361, Val Loss: 0.0448\n",
            "Epoch 34/100, Train Loss: 0.0373, Val Loss: 0.0497\n",
            "Epoch 35/100, Train Loss: 0.0375, Val Loss: 0.0751\n",
            "Epoch 36/100, Train Loss: 0.0351, Val Loss: 0.0558\n",
            "Epoch 37/100, Train Loss: 0.0372, Val Loss: 0.0540\n",
            "Epoch 38/100, Train Loss: 0.0355, Val Loss: 0.0544\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0327, Val Loss: 0.0542\n",
            "Epoch 40/100, Train Loss: 0.0328, Val Loss: 0.0544\n",
            "Epoch 41/100, Train Loss: 0.0298, Val Loss: 0.0536\n",
            "Epoch 42/100, Train Loss: 0.0304, Val Loss: 0.0529\n",
            "Epoch 43/100, Train Loss: 0.0307, Val Loss: 0.0521\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0292, Val Loss: 0.0521\n",
            "Epoch 45/100, Train Loss: 0.0292, Val Loss: 0.0521\n",
            "Epoch 46/100, Train Loss: 0.0311, Val Loss: 0.0521\n",
            "Epoch 47/100, Train Loss: 0.0303, Val Loss: 0.0521\n",
            "Epoch 48/100, Train Loss: 0.0307, Val Loss: 0.0521\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0308, Val Loss: 0.0521\n",
            "Epoch 50/100, Train Loss: 0.0312, Val Loss: 0.0521\n",
            "Epoch 51/100, Train Loss: 0.0299, Val Loss: 0.0521\n",
            "Epoch 52/100, Train Loss: 0.0306, Val Loss: 0.0521\n",
            "Epoch 53/100, Train Loss: 0.0306, Val Loss: 0.0521\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0310, Val Loss: 0.0521\n",
            "Epoch 55/100, Train Loss: 0.0303, Val Loss: 0.0521\n",
            "Epoch 56/100, Train Loss: 0.0305, Val Loss: 0.0521\n",
            "Epoch 57/100, Train Loss: 0.0305, Val Loss: 0.0521\n",
            "Epoch 58/100, Train Loss: 0.0310, Val Loss: 0.0521\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0307, Val Loss: 0.0521\n",
            "Epoch 60/100, Train Loss: 0.0310, Val Loss: 0.0521\n",
            "Epoch 61/100, Train Loss: 0.0303, Val Loss: 0.0521\n",
            "Epoch 62/100, Train Loss: 0.0298, Val Loss: 0.0521\n",
            "Epoch 63/100, Train Loss: 0.0321, Val Loss: 0.0521\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0305, Val Loss: 0.0521\n",
            "Epoch 65/100, Train Loss: 0.0312, Val Loss: 0.0521\n",
            "Epoch 66/100, Train Loss: 0.0307, Val Loss: 0.0521\n",
            "Epoch 67/100, Train Loss: 0.0303, Val Loss: 0.0521\n",
            "Epoch 68/100, Train Loss: 0.0301, Val Loss: 0.0521\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0300, Val Loss: 0.0521\n",
            "Epoch 70/100, Train Loss: 0.0304, Val Loss: 0.0521\n",
            "Epoch 71/100, Train Loss: 0.0300, Val Loss: 0.0521\n",
            "Epoch 72/100, Train Loss: 0.0296, Val Loss: 0.0521\n",
            "Epoch 73/100, Train Loss: 0.0298, Val Loss: 0.0521\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0307, Val Loss: 0.0521\n",
            "Epoch 75/100, Train Loss: 0.0311, Val Loss: 0.0521\n",
            "Epoch 76/100, Train Loss: 0.0302, Val Loss: 0.0521\n",
            "Epoch 77/100, Train Loss: 0.0305, Val Loss: 0.0521\n",
            "Epoch 78/100, Train Loss: 0.0302, Val Loss: 0.0521\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0307, Val Loss: 0.0521\n",
            "Epoch 80/100, Train Loss: 0.0306, Val Loss: 0.0521\n",
            "Epoch 81/100, Train Loss: 0.0299, Val Loss: 0.0521\n",
            "Epoch 82/100, Train Loss: 0.0317, Val Loss: 0.0521\n",
            "Epoch 83/100, Train Loss: 0.0308, Val Loss: 0.0521\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0299, Val Loss: 0.0521\n",
            "Epoch 85/100, Train Loss: 0.0306, Val Loss: 0.0521\n",
            "Epoch 86/100, Train Loss: 0.0314, Val Loss: 0.0521\n",
            "Epoch 87/100, Train Loss: 0.0322, Val Loss: 0.0521\n",
            "Epoch 88/100, Train Loss: 0.0303, Val Loss: 0.0521\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0331, Val Loss: 0.0521\n",
            "Epoch 90/100, Train Loss: 0.0304, Val Loss: 0.0521\n",
            "Epoch 91/100, Train Loss: 0.0316, Val Loss: 0.0521\n",
            "Epoch 92/100, Train Loss: 0.0296, Val Loss: 0.0521\n",
            "Epoch 93/100, Train Loss: 0.0310, Val Loss: 0.0521\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0303, Val Loss: 0.0521\n",
            "Epoch 95/100, Train Loss: 0.0324, Val Loss: 0.0521\n",
            "Epoch 96/100, Train Loss: 0.0307, Val Loss: 0.0521\n",
            "Epoch 97/100, Train Loss: 0.0306, Val Loss: 0.0521\n",
            "Epoch 98/100, Train Loss: 0.0303, Val Loss: 0.0521\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0299, Val Loss: 0.0521\n",
            "Epoch 100/100, Train Loss: 0.0303, Val Loss: 0.0521\n",
            "\n",
            "Test iteration 3/3\n",
            "Current training set size: 182 samples\n",
            "Epoch 1/100, Train Loss: 0.1916, Val Loss: 0.3785\n",
            "Epoch 2/100, Train Loss: 0.1575, Val Loss: 0.2904\n",
            "Epoch 3/100, Train Loss: 0.0952, Val Loss: 0.0885\n",
            "Epoch 4/100, Train Loss: 0.0746, Val Loss: 0.0958\n",
            "Epoch 5/100, Train Loss: 0.0680, Val Loss: 0.0900\n",
            "Epoch 6/100, Train Loss: 0.0681, Val Loss: 0.0845\n",
            "Epoch 7/100, Train Loss: 0.0632, Val Loss: 0.0846\n",
            "Epoch 8/100, Train Loss: 0.0545, Val Loss: 0.0837\n",
            "Epoch 9/100, Train Loss: 0.0573, Val Loss: 0.0832\n",
            "Epoch 10/100, Train Loss: 0.0521, Val Loss: 0.0818\n",
            "Epoch 11/100, Train Loss: 0.0521, Val Loss: 0.0771\n",
            "Epoch 12/100, Train Loss: 0.0492, Val Loss: 0.0756\n",
            "Epoch 13/100, Train Loss: 0.0515, Val Loss: 0.0737\n",
            "Epoch 14/100, Train Loss: 0.0484, Val Loss: 0.0696\n",
            "Epoch 15/100, Train Loss: 0.0521, Val Loss: 0.0705\n",
            "Epoch 16/100, Train Loss: 0.0522, Val Loss: 0.0855\n",
            "Epoch 17/100, Train Loss: 0.0455, Val Loss: 0.0675\n",
            "Epoch 18/100, Train Loss: 0.0452, Val Loss: 0.0666\n",
            "Epoch 19/100, Train Loss: 0.0410, Val Loss: 0.0729\n",
            "Epoch 20/100, Train Loss: 0.0446, Val Loss: 0.0735\n",
            "Epoch 21/100, Train Loss: 0.0410, Val Loss: 0.0542\n",
            "Epoch 22/100, Train Loss: 0.0401, Val Loss: 0.0553\n",
            "Epoch 23/100, Train Loss: 0.0418, Val Loss: 0.0514\n",
            "Epoch 24/100, Train Loss: 0.0410, Val Loss: 0.0658\n",
            "Epoch 25/100, Train Loss: 0.0448, Val Loss: 0.0541\n",
            "Epoch 26/100, Train Loss: 0.0385, Val Loss: 0.0519\n",
            "Epoch 27/100, Train Loss: 0.0378, Val Loss: 0.0536\n",
            "Epoch 28/100, Train Loss: 0.0399, Val Loss: 0.0520\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0346, Val Loss: 0.0524\n",
            "Epoch 30/100, Train Loss: 0.0333, Val Loss: 0.0527\n",
            "Epoch 31/100, Train Loss: 0.0334, Val Loss: 0.0527\n",
            "Epoch 32/100, Train Loss: 0.0352, Val Loss: 0.0525\n",
            "Epoch 33/100, Train Loss: 0.0329, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0339, Val Loss: 0.0526\n",
            "Epoch 35/100, Train Loss: 0.0341, Val Loss: 0.0526\n",
            "Epoch 36/100, Train Loss: 0.0336, Val Loss: 0.0526\n",
            "Epoch 37/100, Train Loss: 0.0368, Val Loss: 0.0526\n",
            "Epoch 38/100, Train Loss: 0.0340, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0345, Val Loss: 0.0526\n",
            "Epoch 40/100, Train Loss: 0.0328, Val Loss: 0.0526\n",
            "Epoch 41/100, Train Loss: 0.0326, Val Loss: 0.0526\n",
            "Epoch 42/100, Train Loss: 0.0324, Val Loss: 0.0526\n",
            "Epoch 43/100, Train Loss: 0.0345, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0331, Val Loss: 0.0526\n",
            "Epoch 45/100, Train Loss: 0.0342, Val Loss: 0.0526\n",
            "Epoch 46/100, Train Loss: 0.0339, Val Loss: 0.0526\n",
            "Epoch 47/100, Train Loss: 0.0334, Val Loss: 0.0526\n",
            "Epoch 48/100, Train Loss: 0.0339, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0331, Val Loss: 0.0526\n",
            "Epoch 50/100, Train Loss: 0.0335, Val Loss: 0.0526\n",
            "Epoch 51/100, Train Loss: 0.0323, Val Loss: 0.0526\n",
            "Epoch 52/100, Train Loss: 0.0340, Val Loss: 0.0526\n",
            "Epoch 53/100, Train Loss: 0.0332, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0338, Val Loss: 0.0526\n",
            "Epoch 55/100, Train Loss: 0.0340, Val Loss: 0.0526\n",
            "Epoch 56/100, Train Loss: 0.0347, Val Loss: 0.0526\n",
            "Epoch 57/100, Train Loss: 0.0331, Val Loss: 0.0526\n",
            "Epoch 58/100, Train Loss: 0.0343, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0320, Val Loss: 0.0526\n",
            "Epoch 60/100, Train Loss: 0.0336, Val Loss: 0.0526\n",
            "Epoch 61/100, Train Loss: 0.0336, Val Loss: 0.0526\n",
            "Epoch 62/100, Train Loss: 0.0335, Val Loss: 0.0526\n",
            "Epoch 63/100, Train Loss: 0.0326, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0327, Val Loss: 0.0526\n",
            "Epoch 65/100, Train Loss: 0.0333, Val Loss: 0.0526\n",
            "Epoch 66/100, Train Loss: 0.0341, Val Loss: 0.0526\n",
            "Epoch 67/100, Train Loss: 0.0327, Val Loss: 0.0526\n",
            "Epoch 68/100, Train Loss: 0.0328, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0343, Val Loss: 0.0526\n",
            "Epoch 70/100, Train Loss: 0.0331, Val Loss: 0.0526\n",
            "Epoch 71/100, Train Loss: 0.0335, Val Loss: 0.0526\n",
            "Epoch 72/100, Train Loss: 0.0342, Val Loss: 0.0526\n",
            "Epoch 73/100, Train Loss: 0.0353, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0350, Val Loss: 0.0526\n",
            "Epoch 75/100, Train Loss: 0.0345, Val Loss: 0.0526\n",
            "Epoch 76/100, Train Loss: 0.0327, Val Loss: 0.0526\n",
            "Epoch 77/100, Train Loss: 0.0327, Val Loss: 0.0526\n",
            "Epoch 78/100, Train Loss: 0.0326, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0341, Val Loss: 0.0526\n",
            "Epoch 80/100, Train Loss: 0.0335, Val Loss: 0.0526\n",
            "Epoch 81/100, Train Loss: 0.0341, Val Loss: 0.0526\n",
            "Epoch 82/100, Train Loss: 0.0336, Val Loss: 0.0526\n",
            "Epoch 83/100, Train Loss: 0.0336, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0337, Val Loss: 0.0526\n",
            "Epoch 85/100, Train Loss: 0.0344, Val Loss: 0.0526\n",
            "Epoch 86/100, Train Loss: 0.0331, Val Loss: 0.0526\n",
            "Epoch 87/100, Train Loss: 0.0336, Val Loss: 0.0526\n",
            "Epoch 88/100, Train Loss: 0.0334, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0329, Val Loss: 0.0526\n",
            "Epoch 90/100, Train Loss: 0.0351, Val Loss: 0.0526\n",
            "Epoch 91/100, Train Loss: 0.0325, Val Loss: 0.0526\n",
            "Epoch 92/100, Train Loss: 0.0330, Val Loss: 0.0526\n",
            "Epoch 93/100, Train Loss: 0.0349, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0336, Val Loss: 0.0526\n",
            "Epoch 95/100, Train Loss: 0.0342, Val Loss: 0.0526\n",
            "Epoch 96/100, Train Loss: 0.0329, Val Loss: 0.0526\n",
            "Epoch 97/100, Train Loss: 0.0328, Val Loss: 0.0526\n",
            "Epoch 98/100, Train Loss: 0.0333, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0334, Val Loss: 0.0526\n",
            "Epoch 100/100, Train Loss: 0.0330, Val Loss: 0.0526\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: hybrid, Units: L256_G16, Dropout: 0.2, Dense Units: 64\n",
            "Batch Size: 4, Epochs: 100\n",
            "Device: cuda\n",
            "Total test samples: 3\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/3\n",
            "Current training set size: 180 samples\n",
            "Epoch 1/100, Train Loss: 0.1784, Val Loss: 0.3681\n",
            "Epoch 2/100, Train Loss: 0.1512, Val Loss: 0.2329\n",
            "Epoch 3/100, Train Loss: 0.0886, Val Loss: 0.0872\n",
            "Epoch 4/100, Train Loss: 0.0747, Val Loss: 0.0857\n",
            "Epoch 5/100, Train Loss: 0.0648, Val Loss: 0.0879\n",
            "Epoch 6/100, Train Loss: 0.0657, Val Loss: 0.0926\n",
            "Epoch 7/100, Train Loss: 0.0643, Val Loss: 0.0835\n",
            "Epoch 8/100, Train Loss: 0.0621, Val Loss: 0.0827\n",
            "Epoch 9/100, Train Loss: 0.0577, Val Loss: 0.0824\n",
            "Epoch 10/100, Train Loss: 0.0557, Val Loss: 0.0792\n",
            "Epoch 11/100, Train Loss: 0.0465, Val Loss: 0.0781\n",
            "Epoch 12/100, Train Loss: 0.0450, Val Loss: 0.0779\n",
            "Epoch 13/100, Train Loss: 0.0512, Val Loss: 0.0915\n",
            "Epoch 14/100, Train Loss: 0.0456, Val Loss: 0.0706\n",
            "Epoch 15/100, Train Loss: 0.0464, Val Loss: 0.0697\n",
            "Epoch 16/100, Train Loss: 0.0491, Val Loss: 0.0863\n",
            "Epoch 17/100, Train Loss: 0.0466, Val Loss: 0.0696\n",
            "Epoch 18/100, Train Loss: 0.0423, Val Loss: 0.1086\n",
            "Epoch 19/100, Train Loss: 0.0475, Val Loss: 0.0642\n",
            "Epoch 20/100, Train Loss: 0.0398, Val Loss: 0.0628\n",
            "Epoch 21/100, Train Loss: 0.0451, Val Loss: 0.0642\n",
            "Epoch 22/100, Train Loss: 0.0447, Val Loss: 0.0912\n",
            "Epoch 23/100, Train Loss: 0.0399, Val Loss: 0.0615\n",
            "Epoch 24/100, Train Loss: 0.0377, Val Loss: 0.0613\n",
            "Epoch 25/100, Train Loss: 0.0413, Val Loss: 0.0636\n",
            "Epoch 26/100, Train Loss: 0.0396, Val Loss: 0.0593\n",
            "Epoch 27/100, Train Loss: 0.0422, Val Loss: 0.0508\n",
            "Epoch 28/100, Train Loss: 0.0391, Val Loss: 0.0570\n",
            "Epoch 29/100, Train Loss: 0.0383, Val Loss: 0.0531\n",
            "Epoch 30/100, Train Loss: 0.0391, Val Loss: 0.0557\n",
            "Epoch 31/100, Train Loss: 0.0402, Val Loss: 0.0462\n",
            "Epoch 32/100, Train Loss: 0.0378, Val Loss: 0.0484\n",
            "Epoch 33/100, Train Loss: 0.0372, Val Loss: 0.0501\n",
            "Epoch 34/100, Train Loss: 0.0368, Val Loss: 0.0563\n",
            "Epoch 35/100, Train Loss: 0.0374, Val Loss: 0.0438\n",
            "Epoch 36/100, Train Loss: 0.0409, Val Loss: 0.0505\n",
            "Epoch 37/100, Train Loss: 0.0357, Val Loss: 0.0516\n",
            "Epoch 38/100, Train Loss: 0.0363, Val Loss: 0.0796\n",
            "Epoch 39/100, Train Loss: 0.0358, Val Loss: 0.0499\n",
            "Epoch 40/100, Train Loss: 0.0366, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0354, Val Loss: 0.0486\n",
            "Epoch 42/100, Train Loss: 0.0311, Val Loss: 0.0479\n",
            "Epoch 43/100, Train Loss: 0.0317, Val Loss: 0.0476\n",
            "Epoch 44/100, Train Loss: 0.0301, Val Loss: 0.0474\n",
            "Epoch 45/100, Train Loss: 0.0313, Val Loss: 0.0473\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0302, Val Loss: 0.0473\n",
            "Epoch 47/100, Train Loss: 0.0312, Val Loss: 0.0473\n",
            "Epoch 48/100, Train Loss: 0.0306, Val Loss: 0.0473\n",
            "Epoch 49/100, Train Loss: 0.0297, Val Loss: 0.0473\n",
            "Epoch 50/100, Train Loss: 0.0313, Val Loss: 0.0473\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0314, Val Loss: 0.0473\n",
            "Epoch 52/100, Train Loss: 0.0305, Val Loss: 0.0473\n",
            "Epoch 53/100, Train Loss: 0.0304, Val Loss: 0.0473\n",
            "Epoch 54/100, Train Loss: 0.0307, Val Loss: 0.0473\n",
            "Epoch 55/100, Train Loss: 0.0311, Val Loss: 0.0473\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0303, Val Loss: 0.0473\n",
            "Epoch 57/100, Train Loss: 0.0305, Val Loss: 0.0473\n",
            "Epoch 58/100, Train Loss: 0.0307, Val Loss: 0.0473\n",
            "Epoch 59/100, Train Loss: 0.0301, Val Loss: 0.0473\n",
            "Epoch 60/100, Train Loss: 0.0311, Val Loss: 0.0473\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0318, Val Loss: 0.0473\n",
            "Epoch 62/100, Train Loss: 0.0322, Val Loss: 0.0473\n",
            "Epoch 63/100, Train Loss: 0.0298, Val Loss: 0.0473\n",
            "Epoch 64/100, Train Loss: 0.0312, Val Loss: 0.0473\n",
            "Epoch 65/100, Train Loss: 0.0306, Val Loss: 0.0473\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0303, Val Loss: 0.0473\n",
            "Epoch 67/100, Train Loss: 0.0317, Val Loss: 0.0473\n",
            "Epoch 68/100, Train Loss: 0.0307, Val Loss: 0.0473\n",
            "Epoch 69/100, Train Loss: 0.0298, Val Loss: 0.0473\n",
            "Epoch 70/100, Train Loss: 0.0304, Val Loss: 0.0473\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0301, Val Loss: 0.0473\n",
            "Epoch 72/100, Train Loss: 0.0315, Val Loss: 0.0473\n",
            "Epoch 73/100, Train Loss: 0.0310, Val Loss: 0.0473\n",
            "Epoch 74/100, Train Loss: 0.0304, Val Loss: 0.0473\n",
            "Epoch 75/100, Train Loss: 0.0298, Val Loss: 0.0473\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0308, Val Loss: 0.0473\n",
            "Epoch 77/100, Train Loss: 0.0291, Val Loss: 0.0473\n",
            "Epoch 78/100, Train Loss: 0.0293, Val Loss: 0.0473\n",
            "Epoch 79/100, Train Loss: 0.0316, Val Loss: 0.0473\n",
            "Epoch 80/100, Train Loss: 0.0302, Val Loss: 0.0473\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0305, Val Loss: 0.0473\n",
            "Epoch 82/100, Train Loss: 0.0294, Val Loss: 0.0473\n",
            "Epoch 83/100, Train Loss: 0.0301, Val Loss: 0.0473\n",
            "Epoch 84/100, Train Loss: 0.0308, Val Loss: 0.0473\n",
            "Epoch 85/100, Train Loss: 0.0308, Val Loss: 0.0473\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0312, Val Loss: 0.0473\n",
            "Epoch 87/100, Train Loss: 0.0306, Val Loss: 0.0473\n",
            "Epoch 88/100, Train Loss: 0.0312, Val Loss: 0.0473\n",
            "Epoch 89/100, Train Loss: 0.0319, Val Loss: 0.0473\n",
            "Epoch 90/100, Train Loss: 0.0303, Val Loss: 0.0473\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0302, Val Loss: 0.0473\n",
            "Epoch 92/100, Train Loss: 0.0303, Val Loss: 0.0473\n",
            "Epoch 93/100, Train Loss: 0.0300, Val Loss: 0.0473\n",
            "Epoch 94/100, Train Loss: 0.0307, Val Loss: 0.0473\n",
            "Epoch 95/100, Train Loss: 0.0305, Val Loss: 0.0473\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0305, Val Loss: 0.0473\n",
            "Epoch 97/100, Train Loss: 0.0306, Val Loss: 0.0473\n",
            "Epoch 98/100, Train Loss: 0.0309, Val Loss: 0.0473\n",
            "Epoch 99/100, Train Loss: 0.0294, Val Loss: 0.0473\n",
            "Epoch 100/100, Train Loss: 0.0309, Val Loss: 0.0473\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 2/3\n",
            "Current training set size: 181 samples\n",
            "Epoch 1/100, Train Loss: 0.1786, Val Loss: 0.3453\n",
            "Epoch 2/100, Train Loss: 0.1393, Val Loss: 0.1761\n",
            "Epoch 3/100, Train Loss: 0.0956, Val Loss: 0.0898\n",
            "Epoch 4/100, Train Loss: 0.0704, Val Loss: 0.1364\n",
            "Epoch 5/100, Train Loss: 0.0968, Val Loss: 0.1110\n",
            "Epoch 6/100, Train Loss: 0.0660, Val Loss: 0.0861\n",
            "Epoch 7/100, Train Loss: 0.0685, Val Loss: 0.0827\n",
            "Epoch 8/100, Train Loss: 0.0571, Val Loss: 0.0841\n",
            "Epoch 9/100, Train Loss: 0.0596, Val Loss: 0.0828\n",
            "Epoch 10/100, Train Loss: 0.0503, Val Loss: 0.1224\n",
            "Epoch 11/100, Train Loss: 0.0490, Val Loss: 0.0774\n",
            "Epoch 12/100, Train Loss: 0.0485, Val Loss: 0.0751\n",
            "Epoch 13/100, Train Loss: 0.0479, Val Loss: 0.0719\n",
            "Epoch 14/100, Train Loss: 0.0444, Val Loss: 0.0689\n",
            "Epoch 15/100, Train Loss: 0.0515, Val Loss: 0.0676\n",
            "Epoch 16/100, Train Loss: 0.0468, Val Loss: 0.0655\n",
            "Epoch 17/100, Train Loss: 0.0479, Val Loss: 0.0863\n",
            "Epoch 18/100, Train Loss: 0.0442, Val Loss: 0.0695\n",
            "Epoch 19/100, Train Loss: 0.0415, Val Loss: 0.0721\n",
            "Epoch 20/100, Train Loss: 0.0395, Val Loss: 0.0834\n",
            "Epoch 21/100, Train Loss: 0.0443, Val Loss: 0.0610\n",
            "Epoch 22/100, Train Loss: 0.0403, Val Loss: 0.0746\n",
            "Epoch 23/100, Train Loss: 0.0466, Val Loss: 0.0654\n",
            "Epoch 24/100, Train Loss: 0.0374, Val Loss: 0.0732\n",
            "Epoch 25/100, Train Loss: 0.0433, Val Loss: 0.0680\n",
            "Epoch 26/100, Train Loss: 0.0414, Val Loss: 0.0659\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0414, Val Loss: 0.0657\n",
            "Epoch 28/100, Train Loss: 0.0386, Val Loss: 0.0651\n",
            "Epoch 29/100, Train Loss: 0.0351, Val Loss: 0.0647\n",
            "Epoch 30/100, Train Loss: 0.0359, Val Loss: 0.0641\n",
            "Epoch 31/100, Train Loss: 0.0351, Val Loss: 0.0630\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0341, Val Loss: 0.0630\n",
            "Epoch 33/100, Train Loss: 0.0346, Val Loss: 0.0630\n",
            "Epoch 34/100, Train Loss: 0.0337, Val Loss: 0.0630\n",
            "Epoch 35/100, Train Loss: 0.0342, Val Loss: 0.0630\n",
            "Epoch 36/100, Train Loss: 0.0336, Val Loss: 0.0629\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0352, Val Loss: 0.0629\n",
            "Epoch 38/100, Train Loss: 0.0336, Val Loss: 0.0629\n",
            "Epoch 39/100, Train Loss: 0.0339, Val Loss: 0.0629\n",
            "Epoch 40/100, Train Loss: 0.0338, Val Loss: 0.0629\n",
            "Epoch 41/100, Train Loss: 0.0322, Val Loss: 0.0629\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0332, Val Loss: 0.0629\n",
            "Epoch 43/100, Train Loss: 0.0345, Val Loss: 0.0629\n",
            "Epoch 44/100, Train Loss: 0.0335, Val Loss: 0.0629\n",
            "Epoch 45/100, Train Loss: 0.0335, Val Loss: 0.0629\n",
            "Epoch 46/100, Train Loss: 0.0342, Val Loss: 0.0629\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0330, Val Loss: 0.0629\n",
            "Epoch 48/100, Train Loss: 0.0345, Val Loss: 0.0629\n",
            "Epoch 49/100, Train Loss: 0.0338, Val Loss: 0.0629\n",
            "Epoch 50/100, Train Loss: 0.0342, Val Loss: 0.0629\n",
            "Epoch 51/100, Train Loss: 0.0333, Val Loss: 0.0629\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0349, Val Loss: 0.0629\n",
            "Epoch 53/100, Train Loss: 0.0330, Val Loss: 0.0629\n",
            "Epoch 54/100, Train Loss: 0.0332, Val Loss: 0.0629\n",
            "Epoch 55/100, Train Loss: 0.0339, Val Loss: 0.0629\n",
            "Epoch 56/100, Train Loss: 0.0333, Val Loss: 0.0629\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0340, Val Loss: 0.0629\n",
            "Epoch 58/100, Train Loss: 0.0344, Val Loss: 0.0629\n",
            "Epoch 59/100, Train Loss: 0.0339, Val Loss: 0.0629\n",
            "Epoch 60/100, Train Loss: 0.0331, Val Loss: 0.0629\n",
            "Epoch 61/100, Train Loss: 0.0339, Val Loss: 0.0629\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0345, Val Loss: 0.0629\n",
            "Epoch 63/100, Train Loss: 0.0345, Val Loss: 0.0629\n",
            "Epoch 64/100, Train Loss: 0.0333, Val Loss: 0.0629\n",
            "Epoch 65/100, Train Loss: 0.0348, Val Loss: 0.0629\n",
            "Epoch 66/100, Train Loss: 0.0343, Val Loss: 0.0629\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0338, Val Loss: 0.0629\n",
            "Epoch 68/100, Train Loss: 0.0336, Val Loss: 0.0629\n",
            "Epoch 69/100, Train Loss: 0.0328, Val Loss: 0.0629\n",
            "Epoch 70/100, Train Loss: 0.0336, Val Loss: 0.0629\n",
            "Epoch 71/100, Train Loss: 0.0332, Val Loss: 0.0629\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0343, Val Loss: 0.0629\n",
            "Epoch 73/100, Train Loss: 0.0343, Val Loss: 0.0629\n",
            "Epoch 74/100, Train Loss: 0.0337, Val Loss: 0.0629\n",
            "Epoch 75/100, Train Loss: 0.0340, Val Loss: 0.0629\n",
            "Epoch 76/100, Train Loss: 0.0337, Val Loss: 0.0629\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0339, Val Loss: 0.0629\n",
            "Epoch 78/100, Train Loss: 0.0331, Val Loss: 0.0629\n",
            "Epoch 79/100, Train Loss: 0.0337, Val Loss: 0.0629\n",
            "Epoch 80/100, Train Loss: 0.0339, Val Loss: 0.0629\n",
            "Epoch 81/100, Train Loss: 0.0333, Val Loss: 0.0629\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0349, Val Loss: 0.0629\n",
            "Epoch 83/100, Train Loss: 0.0336, Val Loss: 0.0629\n",
            "Epoch 84/100, Train Loss: 0.0341, Val Loss: 0.0629\n",
            "Epoch 85/100, Train Loss: 0.0340, Val Loss: 0.0629\n",
            "Epoch 86/100, Train Loss: 0.0343, Val Loss: 0.0629\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0345, Val Loss: 0.0629\n",
            "Epoch 88/100, Train Loss: 0.0343, Val Loss: 0.0629\n",
            "Epoch 89/100, Train Loss: 0.0354, Val Loss: 0.0629\n",
            "Epoch 90/100, Train Loss: 0.0338, Val Loss: 0.0629\n",
            "Epoch 91/100, Train Loss: 0.0350, Val Loss: 0.0629\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0337, Val Loss: 0.0629\n",
            "Epoch 93/100, Train Loss: 0.0341, Val Loss: 0.0629\n",
            "Epoch 94/100, Train Loss: 0.0337, Val Loss: 0.0629\n",
            "Epoch 95/100, Train Loss: 0.0363, Val Loss: 0.0629\n",
            "Epoch 96/100, Train Loss: 0.0341, Val Loss: 0.0629\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0346, Val Loss: 0.0629\n",
            "Epoch 98/100, Train Loss: 0.0341, Val Loss: 0.0629\n",
            "Epoch 99/100, Train Loss: 0.0337, Val Loss: 0.0629\n",
            "Epoch 100/100, Train Loss: 0.0329, Val Loss: 0.0629\n",
            "\n",
            "Test iteration 3/3\n",
            "Current training set size: 182 samples\n",
            "Epoch 1/100, Train Loss: 0.1976, Val Loss: 0.3943\n",
            "Epoch 2/100, Train Loss: 0.1586, Val Loss: 0.2707\n",
            "Epoch 3/100, Train Loss: 0.1029, Val Loss: 0.0897\n",
            "Epoch 4/100, Train Loss: 0.0762, Val Loss: 0.0941\n",
            "Epoch 5/100, Train Loss: 0.0656, Val Loss: 0.0921\n",
            "Epoch 6/100, Train Loss: 0.0641, Val Loss: 0.0838\n",
            "Epoch 7/100, Train Loss: 0.0595, Val Loss: 0.0839\n",
            "Epoch 8/100, Train Loss: 0.0509, Val Loss: 0.0805\n",
            "Epoch 9/100, Train Loss: 0.0623, Val Loss: 0.0817\n",
            "Epoch 10/100, Train Loss: 0.0502, Val Loss: 0.0783\n",
            "Epoch 11/100, Train Loss: 0.0484, Val Loss: 0.0731\n",
            "Epoch 12/100, Train Loss: 0.0472, Val Loss: 0.0729\n",
            "Epoch 13/100, Train Loss: 0.0487, Val Loss: 0.0699\n",
            "Epoch 14/100, Train Loss: 0.0464, Val Loss: 0.0682\n",
            "Epoch 15/100, Train Loss: 0.0462, Val Loss: 0.0630\n",
            "Epoch 16/100, Train Loss: 0.0456, Val Loss: 0.0618\n",
            "Epoch 17/100, Train Loss: 0.0451, Val Loss: 0.0693\n",
            "Epoch 18/100, Train Loss: 0.0431, Val Loss: 0.0633\n",
            "Epoch 19/100, Train Loss: 0.0397, Val Loss: 0.0848\n",
            "Epoch 20/100, Train Loss: 0.0423, Val Loss: 0.0801\n",
            "Epoch 21/100, Train Loss: 0.0410, Val Loss: 0.0629\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0516, Val Loss: 0.0587\n",
            "Epoch 23/100, Train Loss: 0.0412, Val Loss: 0.0602\n",
            "Epoch 24/100, Train Loss: 0.0366, Val Loss: 0.0606\n",
            "Epoch 25/100, Train Loss: 0.0348, Val Loss: 0.0605\n",
            "Epoch 26/100, Train Loss: 0.0349, Val Loss: 0.0604\n",
            "Epoch 27/100, Train Loss: 0.0352, Val Loss: 0.0602\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0343, Val Loss: 0.0602\n",
            "Epoch 29/100, Train Loss: 0.0359, Val Loss: 0.0602\n",
            "Epoch 30/100, Train Loss: 0.0347, Val Loss: 0.0602\n",
            "Epoch 31/100, Train Loss: 0.0346, Val Loss: 0.0602\n",
            "Epoch 32/100, Train Loss: 0.0356, Val Loss: 0.0602\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0343, Val Loss: 0.0602\n",
            "Epoch 34/100, Train Loss: 0.0351, Val Loss: 0.0602\n",
            "Epoch 35/100, Train Loss: 0.0346, Val Loss: 0.0602\n",
            "Epoch 36/100, Train Loss: 0.0352, Val Loss: 0.0602\n",
            "Epoch 37/100, Train Loss: 0.0389, Val Loss: 0.0602\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0341, Val Loss: 0.0602\n",
            "Epoch 39/100, Train Loss: 0.0357, Val Loss: 0.0602\n",
            "Epoch 40/100, Train Loss: 0.0349, Val Loss: 0.0602\n",
            "Epoch 41/100, Train Loss: 0.0346, Val Loss: 0.0602\n",
            "Epoch 42/100, Train Loss: 0.0350, Val Loss: 0.0602\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0355, Val Loss: 0.0602\n",
            "Epoch 44/100, Train Loss: 0.0347, Val Loss: 0.0602\n",
            "Epoch 45/100, Train Loss: 0.0365, Val Loss: 0.0602\n",
            "Epoch 46/100, Train Loss: 0.0360, Val Loss: 0.0602\n",
            "Epoch 47/100, Train Loss: 0.0341, Val Loss: 0.0602\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0363, Val Loss: 0.0602\n",
            "Epoch 49/100, Train Loss: 0.0352, Val Loss: 0.0602\n",
            "Epoch 50/100, Train Loss: 0.0357, Val Loss: 0.0602\n",
            "Epoch 51/100, Train Loss: 0.0354, Val Loss: 0.0602\n",
            "Epoch 52/100, Train Loss: 0.0361, Val Loss: 0.0602\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0348, Val Loss: 0.0602\n",
            "Epoch 54/100, Train Loss: 0.0342, Val Loss: 0.0602\n",
            "Epoch 55/100, Train Loss: 0.0365, Val Loss: 0.0602\n",
            "Epoch 56/100, Train Loss: 0.0359, Val Loss: 0.0602\n",
            "Epoch 57/100, Train Loss: 0.0358, Val Loss: 0.0602\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0345, Val Loss: 0.0602\n",
            "Epoch 59/100, Train Loss: 0.0342, Val Loss: 0.0602\n",
            "Epoch 60/100, Train Loss: 0.0355, Val Loss: 0.0602\n",
            "Epoch 61/100, Train Loss: 0.0358, Val Loss: 0.0602\n",
            "Epoch 62/100, Train Loss: 0.0345, Val Loss: 0.0602\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0347, Val Loss: 0.0602\n",
            "Epoch 64/100, Train Loss: 0.0348, Val Loss: 0.0602\n",
            "Epoch 65/100, Train Loss: 0.0353, Val Loss: 0.0602\n",
            "Epoch 66/100, Train Loss: 0.0357, Val Loss: 0.0602\n",
            "Epoch 67/100, Train Loss: 0.0354, Val Loss: 0.0602\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0349, Val Loss: 0.0602\n",
            "Epoch 69/100, Train Loss: 0.0357, Val Loss: 0.0602\n",
            "Epoch 70/100, Train Loss: 0.0345, Val Loss: 0.0602\n",
            "Epoch 71/100, Train Loss: 0.0351, Val Loss: 0.0602\n",
            "Epoch 72/100, Train Loss: 0.0359, Val Loss: 0.0602\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0358, Val Loss: 0.0602\n",
            "Epoch 74/100, Train Loss: 0.0383, Val Loss: 0.0602\n",
            "Epoch 75/100, Train Loss: 0.0357, Val Loss: 0.0602\n",
            "Epoch 76/100, Train Loss: 0.0343, Val Loss: 0.0602\n",
            "Epoch 77/100, Train Loss: 0.0347, Val Loss: 0.0602\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0339, Val Loss: 0.0602\n",
            "Epoch 79/100, Train Loss: 0.0359, Val Loss: 0.0602\n",
            "Epoch 80/100, Train Loss: 0.0351, Val Loss: 0.0602\n",
            "Epoch 81/100, Train Loss: 0.0361, Val Loss: 0.0602\n",
            "Epoch 82/100, Train Loss: 0.0352, Val Loss: 0.0602\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0345, Val Loss: 0.0602\n",
            "Epoch 84/100, Train Loss: 0.0360, Val Loss: 0.0602\n",
            "Epoch 85/100, Train Loss: 0.0353, Val Loss: 0.0602\n",
            "Epoch 86/100, Train Loss: 0.0351, Val Loss: 0.0602\n",
            "Epoch 87/100, Train Loss: 0.0358, Val Loss: 0.0602\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0345, Val Loss: 0.0602\n",
            "Epoch 89/100, Train Loss: 0.0347, Val Loss: 0.0602\n",
            "Epoch 90/100, Train Loss: 0.0368, Val Loss: 0.0602\n",
            "Epoch 91/100, Train Loss: 0.0350, Val Loss: 0.0602\n",
            "Epoch 92/100, Train Loss: 0.0347, Val Loss: 0.0602\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0359, Val Loss: 0.0602\n",
            "Epoch 94/100, Train Loss: 0.0353, Val Loss: 0.0602\n",
            "Epoch 95/100, Train Loss: 0.0363, Val Loss: 0.0602\n",
            "Epoch 96/100, Train Loss: 0.0345, Val Loss: 0.0602\n",
            "Epoch 97/100, Train Loss: 0.0363, Val Loss: 0.0602\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0353, Val Loss: 0.0602\n",
            "Epoch 99/100, Train Loss: 0.0352, Val Loss: 0.0602\n",
            "Epoch 100/100, Train Loss: 0.0349, Val Loss: 0.0602\n",
            "Results:\n",
            "             RMSE            MAE       MAPE model_type     units  drop_rate  \\\n",
            "0   268846.258877  264327.500000  13.097408       lstm       128        0.1   \n",
            "1   208947.081990  182543.796875   8.915150       lstm       128        0.1   \n",
            "2   296711.634069  277411.781250  13.665507       lstm       128        0.2   \n",
            "3   287577.446459  274013.000000  13.514574       lstm       128        0.2   \n",
            "4   221825.744944  198950.875000   9.732743       lstm       256        0.1   \n",
            "5   202390.313760  174370.671875   8.497536       lstm       256        0.1   \n",
            "6   261022.530828  226467.296875  11.063681       lstm       256        0.2   \n",
            "7   222435.966013  200513.375000   9.818383       lstm       256        0.2   \n",
            "8   189052.706810  167609.671875   8.190430        gru         8        0.1   \n",
            "9   195718.393617  164602.171875   8.006814        gru         8        0.1   \n",
            "10  196044.390075  172808.796875   8.446384        gru         8        0.2   \n",
            "11  205732.168569  180741.296875   8.828740        gru         8        0.2   \n",
            "12  195201.468847  171252.578125   8.360096        gru        16        0.1   \n",
            "13  193628.644492  172049.125000   8.411149        gru        16        0.1   \n",
            "14  206692.654538  181873.453125   8.891973        gru        16        0.2   \n",
            "15  171585.754560  154279.171875   7.551521        gru        16        0.2   \n",
            "16  229629.351155  188041.078125   9.128286     hybrid   L128_G8        0.1   \n",
            "17  180579.178423  155846.375000   7.594780     hybrid   L128_G8        0.1   \n",
            "18  238616.676835  204522.328125   9.973700     hybrid   L128_G8        0.2   \n",
            "19  175030.113066  151546.000000   7.387715     hybrid   L128_G8        0.2   \n",
            "20  279958.239629  233335.578125  11.334047     hybrid  L128_G16        0.1   \n",
            "21  185069.277061  151797.796875   7.362802     hybrid  L128_G16        0.1   \n",
            "22  243116.920546  204073.453125   9.941549     hybrid  L128_G16        0.2   \n",
            "23  201447.774910  175067.546875   8.537970     hybrid  L128_G16        0.2   \n",
            "24  189040.270165  167451.546875   8.181506     hybrid   L256_G8        0.1   \n",
            "25  189553.198612  155682.375000   7.554221     hybrid   L256_G8        0.1   \n",
            "26  202054.355301  172195.046875   8.381744     hybrid   L256_G8        0.2   \n",
            "27  189156.458076  160778.953125   7.827387     hybrid   L256_G8        0.2   \n",
            "28  229155.384977  199825.953125   9.748106     hybrid  L256_G16        0.1   \n",
            "29  203528.613654  177988.578125   8.686036     hybrid  L256_G16        0.1   \n",
            "30  212119.289005  191918.046875   9.398703     hybrid  L256_G16        0.2   \n",
            "31  213517.131491  187172.625000   9.135922     hybrid  L256_G16        0.2   \n",
            "\n",
            "    dense_unit  batch_size  epochs  \n",
            "0           32           4     100  \n",
            "1           64           4     100  \n",
            "2           32           4     100  \n",
            "3           64           4     100  \n",
            "4           32           4     100  \n",
            "5           64           4     100  \n",
            "6           32           4     100  \n",
            "7           64           4     100  \n",
            "8           32           4     100  \n",
            "9           64           4     100  \n",
            "10          32           4     100  \n",
            "11          64           4     100  \n",
            "12          32           4     100  \n",
            "13          64           4     100  \n",
            "14          32           4     100  \n",
            "15          64           4     100  \n",
            "16          32           4     100  \n",
            "17          64           4     100  \n",
            "18          32           4     100  \n",
            "19          64           4     100  \n",
            "20          32           4     100  \n",
            "21          64           4     100  \n",
            "22          32           4     100  \n",
            "23          64           4     100  \n",
            "24          32           4     100  \n",
            "25          64           4     100  \n",
            "26          32           4     100  \n",
            "27          64           4     100  \n",
            "28          32           4     100  \n",
            "29          64           4     100  \n",
            "30          32           4     100  \n",
            "31          64           4     100  \n"
          ]
        }
      ],
      "source": [
        "# # Giảm số lượng/GRU unit, dense unit, epochs và sử dụng batch size nhỏ hơn để huấn luyện nhanh hơn.\n",
        "# model_types = ['multi-scale']\n",
        "# lstm_unit = [128, 256, 512]\n",
        "# gru_unit = [8, 16, 32]\n",
        "# drop_rate = [0.1, 0.2]\n",
        "# dense_unit = [16, 32, 64]\n",
        "# batch_size_num = [2, 4]\n",
        "# epochs = [100]\n",
        "\n",
        "model_types = ['lstm','gru','hybrid']\n",
        "lstm_unit = [128,256]\n",
        "gru_unit = [8,16]\n",
        "drop_rate = [0.1,0.2]\n",
        "dense_unit = [32,64]\n",
        "batch_size_num = [4]\n",
        "epochs = [100]\n",
        "\n",
        "# # Replace the current parameter definitions\n",
        "# model_types = ['hybrid', 'sequential', 'stacked', 'bidirectional', 'cnn-rnn', 'multi-scale', 'transformer-rnn', 'ensemble', 'lstm', 'gru']\n",
        "# lstm_unit = [128]\n",
        "# gru_unit = [8]\n",
        "# drop_rate = [0.1]\n",
        "# dense_unit = [64]\n",
        "# batch_size_num = [2]\n",
        "# epochs = [100]\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import concatenate\n",
        "import itertools\n",
        "from math import sqrt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "class AttentionGRU(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, gru_units, dropout_rate, dense_units):\n",
        "        super(AttentionGRU, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention1 = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # GRU layer\n",
        "        self.gru = nn.GRU(input_dim, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear(gru_units * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, time_steps, input_dim)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.attention1(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)  # (batch_size, time_steps, input_dim)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # GRU layer - output: (batch, seq, hidden_size)\n",
        "        gru_out, _ = self.gru(attention_mul)\n",
        "\n",
        "        # Flatten and Dense layers\n",
        "        flattened = self.flatten(gru_out)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class AttentionLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, dropout_rate, dense_units):\n",
        "        super(AttentionLSTM, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention1 = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear(lstm_units * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, time_steps, input_dim)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.attention1(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)  # (batch_size, time_steps, input_dim)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # LSTM layer - output: (batch, seq, hidden_size)\n",
        "        lstm_out, _ = self.lstm(attention_mul)\n",
        "\n",
        "        # Flatten and Dense layers\n",
        "        flattened = self.flatten(lstm_out)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class HybridLSTM_GRU(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(HybridLSTM_GRU, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention1 = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # LSTM and GRU layers\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "        self.gru = nn.GRU(input_dim, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        # Combined size from both LSTM and GRU\n",
        "        self.dense1 = nn.Linear((lstm_units + gru_units) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, time_steps, input_dim)\n",
        "\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.attention1(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)  # (batch_size, time_steps, input_dim)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # LSTM and GRU layers\n",
        "        lstm_out, _ = self.lstm(attention_mul)\n",
        "        gru_out, _ = self.gru(attention_mul)\n",
        "\n",
        "        # Concatenate LSTM and GRU outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Flatten and Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "def build_model(train_X, train_Y, val_X, val_Y, model_type='gru', lstm_units=128, gru_units=128, drop_rate=0.3, dense_unit=64, batch_size=32, epochs=100):\n",
        "    # Print training parameters\n",
        "    train_X_tensor = torch.FloatTensor(train_X)\n",
        "    train_Y_tensor = torch.FloatTensor(train_Y.reshape(-1, 1))\n",
        "    val_X_tensor = torch.FloatTensor(val_X)\n",
        "    val_Y_tensor = torch.FloatTensor(val_Y.reshape(-1, 1))\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
        "    val_dataset = TensorDataset(val_X_tensor, val_Y_tensor)\n",
        "\n",
        "    # Create reproducible DataLoaders with fixed seeds\n",
        "    train_generator = torch.Generator()\n",
        "    train_generator.manual_seed(SEED)\n",
        "    val_generator = torch.Generator()\n",
        "    val_generator.manual_seed(SEED)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=train_generator)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, generator=val_generator)\n",
        "\n",
        "    # Initialize model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    time_steps = train_X.shape[1]\n",
        "    input_dim = train_X.shape[2]\n",
        "\n",
        "    # Initialize model with fixed initial weights\n",
        "    torch.manual_seed(SEED)\n",
        "\n",
        "    if model_type == 'gru':\n",
        "        model = AttentionGRU(input_dim, time_steps, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'lstm':\n",
        "        model = AttentionLSTM(input_dim, time_steps, lstm_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'hybrid':\n",
        "        model = HybridLSTM_GRU(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "    # Initialize optimizer and loss function\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    criterion = nn.L1Loss()  # MAE loss\n",
        "\n",
        "    # Training loop\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    patience = 5\n",
        "    lr_factor = 0.01\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        # Print progress\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
        "\n",
        "        # Learning rate schedule based on validation loss\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] *= lr_factor\n",
        "                patience_counter = 0\n",
        "                print(f'Reducing learning rate by factor of {lr_factor}')\n",
        "\n",
        "    return model\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    mask = y_true != 0\n",
        "    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "    return mape\n",
        "\n",
        "def walk_forward(train_X, train_Y, val_X, val_Y, test_X, test_Y, grid_search, scaler):\n",
        "    r, f, c = test_X.shape\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    all_predictions = {}\n",
        "    all_adjusted_predictions = {}\n",
        "    all_ground_truths = {}\n",
        "\n",
        "    # Create lists to store all evaluation results\n",
        "    original_valuelists = []\n",
        "    adjusted_valuelists = []\n",
        "\n",
        "    for x in grid_search:\n",
        "        history_x = np.array([x for x in train_X])\n",
        "        history_y = np.array([y for y in train_Y])\n",
        "        predictions = list()\n",
        "        adjusted_predictions = list()\n",
        "        groundtrue = list()\n",
        "\n",
        "        # Extract model type first to determine how to unpack the rest\n",
        "        model_type = x[0]\n",
        "\n",
        "        # Create the appropriate config_key and extract parameters based on model type\n",
        "        if model_type in ['hybrid']:\n",
        "            # Hybrid model has 7 parameters\n",
        "            model_type, lstm_unit_val, gru_unit_val, drop, dense, batch, epoch = x\n",
        "            units = f\"L{lstm_unit_val}_G{gru_unit_val}\"  # For logging\n",
        "            config_key = f\"{model_type}_lstmUnit{lstm_unit_val}_gruUnit{gru_unit_val}_drop{drop}_dense{dense}_batch{batch}_epochs{epoch}\"\n",
        "        else:\n",
        "            # LSTM and GRU models have 6 parameters\n",
        "            model_type, units, drop, dense, batch, epoch = x\n",
        "            config_key = f\"{model_type}_unit{units}_drop{drop}_dense{dense}_batch{batch}_epochs{epoch}\"\n",
        "\n",
        "        print(\"\\n\" + \"*\"*50)\n",
        "        print(f\"Starting walk-forward validation with parameters:\")\n",
        "        print(f\"Model Type: {model_type}, Units: {units}, Dropout: {drop}, Dense Units: {dense}\")\n",
        "        print(f\"Batch Size: {batch}, Epochs: {epoch}\")\n",
        "        print(f\"Device: {device}\")\n",
        "        print(f\"Total test samples: {len(test_X)}\")\n",
        "        print(\"*\"*50 + \"\\n\")\n",
        "\n",
        "        for i in range(len(test_X)):\n",
        "            print(f\"\\nTest iteration {i+1}/{len(test_X)}\")\n",
        "            print(f\"Current training set size: {history_x.shape[0]} samples\")\n",
        "\n",
        "            if model_type in ['hybrid']:\n",
        "                model = build_model(history_x, history_y, val_X, val_Y, model_type=model_type,\n",
        "                                lstm_units=lstm_unit_val, gru_units=gru_unit_val, drop_rate=drop,\n",
        "                                dense_unit=dense, batch_size=batch, epochs=epoch)\n",
        "            else:\n",
        "                model = build_model(history_x, history_y, val_X, val_Y, model_type=model_type,\n",
        "                                lstm_units=units if model_type == 'lstm' else 128,\n",
        "                                gru_units=units if model_type == 'gru' else 128,\n",
        "                                drop_rate=drop, dense_unit=dense, batch_size=batch, epochs=epoch)\n",
        "\n",
        "            # Rest of the function remains the same\n",
        "            model.eval()\n",
        "\n",
        "            # Convert test data to tensor\n",
        "            test_tensor = torch.FloatTensor(test_X[i].reshape(1, f, c)).to(device)\n",
        "\n",
        "            # Predict\n",
        "            with torch.no_grad():\n",
        "                yhat = model(test_tensor).cpu().numpy()\n",
        "\n",
        "            inv_yhat, inv_y = inverscale(yhat, test_X[i], test_Y[i], scaler)\n",
        "            predictions.append(inv_yhat)\n",
        "            groundtrue.append(inv_y)\n",
        "\n",
        "            # Observation\n",
        "            obs_x = test_X[i]\n",
        "            obs_y = test_Y[i]\n",
        "\n",
        "            history_x = np.append(history_x, [obs_x], axis=0)\n",
        "            history_y = np.append(history_y, obs_y)\n",
        "\n",
        "        # Store predictions and ground truth for this configuration\n",
        "        all_predictions[config_key] = np.array(predictions).flatten()\n",
        "        all_ground_truths[config_key] = np.array(groundtrue).flatten()\n",
        "\n",
        "        original_valuelist = evalue(np.array(predictions).flatten(), np.array(groundtrue).flatten())\n",
        "        original_valuelist['model_type'] = model_type\n",
        "        original_valuelist['units'] = units\n",
        "        original_valuelist['drop_rate'] = drop\n",
        "        original_valuelist['dense_unit'] = dense\n",
        "        original_valuelist['batch_size'] = batch\n",
        "        original_valuelist['epochs'] = epoch\n",
        "\n",
        "        # Append to the lists of results\n",
        "        original_valuelists.append(original_valuelist)\n",
        "\n",
        "    # Combine all results\n",
        "    all_original_valuelist = pd.concat(original_valuelists, ignore_index=True)\n",
        "\n",
        "    return all_original_valuelist, all_predictions, all_ground_truths\n",
        "\n",
        "def evalue(yhat, inv_y):\n",
        "    valuelist = {}\n",
        "    DLM_rmse = sqrt(mean_squared_error(inv_y, yhat))\n",
        "    valuelist.update({'RMSE': {'DLM': DLM_rmse}})\n",
        "    DLM_mae = mean_absolute_error(inv_y, yhat)\n",
        "    valuelist.update({'MAE': {'DLM': DLM_mae}})\n",
        "    DLM_mape = mean_absolute_percentage_error(inv_y, yhat)\n",
        "    valuelist.update({'MAPE': {'DLM': DLM_mape}})\n",
        "    return pd.DataFrame(valuelist)\n",
        "\n",
        "def inverscale(yhat, test_X, test_Y, scaler):\n",
        "    feature = len(scaler.scale_)\n",
        "    test_Y = np.array(test_Y)\n",
        "    test_X = test_X[1, 0:feature]\n",
        "    test_X = test_X.reshape(1, test_X.shape[0])\n",
        "\n",
        "    if len(yhat.shape) == 1:\n",
        "        yhat = yhat.reshape(len(yhat), 1)\n",
        "\n",
        "    inv_yhat = concatenate((yhat, test_X[:, :-1]), axis=1)\n",
        "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "    inv_yhat = inv_yhat[:, 0]\n",
        "\n",
        "    test_Y = test_Y.reshape(1, 1)\n",
        "    inv_y = concatenate((test_Y, test_X[:, :-1]), axis=1)\n",
        "    inv_y = scaler.inverse_transform(inv_y)\n",
        "    inv_y = inv_y[:, 0]\n",
        "    return inv_yhat, inv_y\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    values = reframed.values\n",
        "    reframed_with_dates_values = reframed_with_dates.values\n",
        "\n",
        "    # Convert date strings to datetime objects\n",
        "    dates = pd.to_datetime(reframed_with_dates['date'])\n",
        "\n",
        "    # Create masks for each split according to the specified date ranges\n",
        "    # train_mask = ((dates >= '2008-01-01') & (dates <= '2017-12-31')) | ((dates >= '2020-01-01') & (dates <= '2020-05-31'))\n",
        "    # val_mask = (dates >= '2018-01-01') & (dates <= '2018-12-31')\n",
        "    # test_mask = ((dates >= '2019-01-01') & (dates <= '2019-12-31')) | ((dates >= '2020-06-01') & (dates <= '2020-07-31'))\n",
        "    train_mask = ((dates >= '2008-01-01') & (dates <= '2023-12-31'))\n",
        "    val_mask = (dates >= '2024-01-01') & (dates <= '2024-12-31')\n",
        "    test_mask = ((dates >= '2025-01-01') & (dates <= '2025-12-31'))\n",
        "\n",
        "    # Extract values for train, validation, and test sets (excluding the date column)\n",
        "    train_data = reframed.loc[train_mask].values\n",
        "    val_data = reframed.loc[val_mask].values\n",
        "    test_data = reframed.loc[test_mask].values\n",
        "\n",
        "    # Split into X and Y\n",
        "    train_X, train_Y = train_data[:, :-1], train_data[:, -1]\n",
        "    val_X, val_Y = val_data[:, :-1], val_data[:, -1]\n",
        "    test_X, test_Y = test_data[:, :-1], test_data[:, -1]\n",
        "\n",
        "    # Reshape input to be 3D [samples, timesteps, features]\n",
        "    train_X = train_X.reshape(train_X.shape[0], 12, int(train_X.shape[1]/12))\n",
        "    val_X = val_X.reshape(val_X.shape[0], 12, int(val_X.shape[1]/12))\n",
        "    test_X = test_X.reshape(test_X.shape[0], 12, int(test_X.shape[1]/12))\n",
        "\n",
        "    # Modified grid search creation for all model types\n",
        "    grid_search = []\n",
        "    for model_type in model_types:\n",
        "        if model_type == 'lstm':\n",
        "            grid_search.extend(\n",
        "                list(itertools.product([model_type], lstm_unit, drop_rate, dense_unit, batch_size_num, epochs))\n",
        "            )\n",
        "        elif model_type == 'gru':\n",
        "            grid_search.extend(\n",
        "                list(itertools.product([model_type], gru_unit, drop_rate, dense_unit, batch_size_num, epochs))\n",
        "            )\n",
        "        else:\n",
        "            # All other models (hybrid, sequential, stacked, etc.) need both LSTM and GRU units\n",
        "            grid_search.extend(\n",
        "                list(itertools.product([model_type], lstm_unit, gru_unit, drop_rate, dense_unit, batch_size_num, epochs))\n",
        "            )\n",
        "\n",
        "    original_valuelist, all_predictions, all_ground_truths = walk_forward(train_X, train_Y, val_X, val_Y, test_X, test_Y, grid_search, scaler)\n",
        "\n",
        "    # Group results by model type\n",
        "    # gru_results = adjusted_valuelist[adjusted_valuelist['model_type'] == 'gru']\n",
        "    # lstm_results = adjusted_valuelist[adjusted_valuelist['model_type'] == 'lstm']\n",
        "\n",
        "    print(\"Results:\")\n",
        "    print(original_valuelist)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result"
      ],
      "metadata": {
        "id": "SIIDmvgFHw1o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EIWQqXKlue4o",
        "outputId": "f8a8f544-4c11-45f9-a2a9-ae5e47306aa7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             RMSE            MAE       MAPE model_type     units  drop_rate  \\\n",
              "0   268846.258877  264327.500000  13.097408       lstm       128        0.1   \n",
              "1   208947.081990  182543.796875   8.915150       lstm       128        0.1   \n",
              "2   296711.634069  277411.781250  13.665507       lstm       128        0.2   \n",
              "3   287577.446459  274013.000000  13.514574       lstm       128        0.2   \n",
              "4   221825.744944  198950.875000   9.732743       lstm       256        0.1   \n",
              "5   202390.313760  174370.671875   8.497536       lstm       256        0.1   \n",
              "6   261022.530828  226467.296875  11.063681       lstm       256        0.2   \n",
              "7   222435.966013  200513.375000   9.818383       lstm       256        0.2   \n",
              "8   189052.706810  167609.671875   8.190430        gru         8        0.1   \n",
              "9   195718.393617  164602.171875   8.006814        gru         8        0.1   \n",
              "10  196044.390075  172808.796875   8.446384        gru         8        0.2   \n",
              "11  205732.168569  180741.296875   8.828740        gru         8        0.2   \n",
              "12  195201.468847  171252.578125   8.360096        gru        16        0.1   \n",
              "13  193628.644492  172049.125000   8.411149        gru        16        0.1   \n",
              "14  206692.654538  181873.453125   8.891973        gru        16        0.2   \n",
              "15  171585.754560  154279.171875   7.551521        gru        16        0.2   \n",
              "16  229629.351155  188041.078125   9.128286     hybrid   L128_G8        0.1   \n",
              "17  180579.178423  155846.375000   7.594780     hybrid   L128_G8        0.1   \n",
              "18  238616.676835  204522.328125   9.973700     hybrid   L128_G8        0.2   \n",
              "19  175030.113066  151546.000000   7.387715     hybrid   L128_G8        0.2   \n",
              "20  279958.239629  233335.578125  11.334047     hybrid  L128_G16        0.1   \n",
              "21  185069.277061  151797.796875   7.362802     hybrid  L128_G16        0.1   \n",
              "22  243116.920546  204073.453125   9.941549     hybrid  L128_G16        0.2   \n",
              "23  201447.774910  175067.546875   8.537970     hybrid  L128_G16        0.2   \n",
              "24  189040.270165  167451.546875   8.181506     hybrid   L256_G8        0.1   \n",
              "25  189553.198612  155682.375000   7.554221     hybrid   L256_G8        0.1   \n",
              "26  202054.355301  172195.046875   8.381744     hybrid   L256_G8        0.2   \n",
              "27  189156.458076  160778.953125   7.827387     hybrid   L256_G8        0.2   \n",
              "28  229155.384977  199825.953125   9.748106     hybrid  L256_G16        0.1   \n",
              "29  203528.613654  177988.578125   8.686036     hybrid  L256_G16        0.1   \n",
              "30  212119.289005  191918.046875   9.398703     hybrid  L256_G16        0.2   \n",
              "31  213517.131491  187172.625000   9.135922     hybrid  L256_G16        0.2   \n",
              "\n",
              "    dense_unit  batch_size  epochs  \n",
              "0           32           4     100  \n",
              "1           64           4     100  \n",
              "2           32           4     100  \n",
              "3           64           4     100  \n",
              "4           32           4     100  \n",
              "5           64           4     100  \n",
              "6           32           4     100  \n",
              "7           64           4     100  \n",
              "8           32           4     100  \n",
              "9           64           4     100  \n",
              "10          32           4     100  \n",
              "11          64           4     100  \n",
              "12          32           4     100  \n",
              "13          64           4     100  \n",
              "14          32           4     100  \n",
              "15          64           4     100  \n",
              "16          32           4     100  \n",
              "17          64           4     100  \n",
              "18          32           4     100  \n",
              "19          64           4     100  \n",
              "20          32           4     100  \n",
              "21          64           4     100  \n",
              "22          32           4     100  \n",
              "23          64           4     100  \n",
              "24          32           4     100  \n",
              "25          64           4     100  \n",
              "26          32           4     100  \n",
              "27          64           4     100  \n",
              "28          32           4     100  \n",
              "29          64           4     100  \n",
              "30          32           4     100  \n",
              "31          64           4     100  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2aa81bf7-7780-4a70-bee4-d0f7a238a789\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAE</th>\n",
              "      <th>MAPE</th>\n",
              "      <th>model_type</th>\n",
              "      <th>units</th>\n",
              "      <th>drop_rate</th>\n",
              "      <th>dense_unit</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>epochs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>268846.258877</td>\n",
              "      <td>264327.500000</td>\n",
              "      <td>13.097408</td>\n",
              "      <td>lstm</td>\n",
              "      <td>128</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>208947.081990</td>\n",
              "      <td>182543.796875</td>\n",
              "      <td>8.915150</td>\n",
              "      <td>lstm</td>\n",
              "      <td>128</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>296711.634069</td>\n",
              "      <td>277411.781250</td>\n",
              "      <td>13.665507</td>\n",
              "      <td>lstm</td>\n",
              "      <td>128</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>287577.446459</td>\n",
              "      <td>274013.000000</td>\n",
              "      <td>13.514574</td>\n",
              "      <td>lstm</td>\n",
              "      <td>128</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>221825.744944</td>\n",
              "      <td>198950.875000</td>\n",
              "      <td>9.732743</td>\n",
              "      <td>lstm</td>\n",
              "      <td>256</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>202390.313760</td>\n",
              "      <td>174370.671875</td>\n",
              "      <td>8.497536</td>\n",
              "      <td>lstm</td>\n",
              "      <td>256</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>261022.530828</td>\n",
              "      <td>226467.296875</td>\n",
              "      <td>11.063681</td>\n",
              "      <td>lstm</td>\n",
              "      <td>256</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>222435.966013</td>\n",
              "      <td>200513.375000</td>\n",
              "      <td>9.818383</td>\n",
              "      <td>lstm</td>\n",
              "      <td>256</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>189052.706810</td>\n",
              "      <td>167609.671875</td>\n",
              "      <td>8.190430</td>\n",
              "      <td>gru</td>\n",
              "      <td>8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>195718.393617</td>\n",
              "      <td>164602.171875</td>\n",
              "      <td>8.006814</td>\n",
              "      <td>gru</td>\n",
              "      <td>8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>196044.390075</td>\n",
              "      <td>172808.796875</td>\n",
              "      <td>8.446384</td>\n",
              "      <td>gru</td>\n",
              "      <td>8</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>205732.168569</td>\n",
              "      <td>180741.296875</td>\n",
              "      <td>8.828740</td>\n",
              "      <td>gru</td>\n",
              "      <td>8</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>195201.468847</td>\n",
              "      <td>171252.578125</td>\n",
              "      <td>8.360096</td>\n",
              "      <td>gru</td>\n",
              "      <td>16</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>193628.644492</td>\n",
              "      <td>172049.125000</td>\n",
              "      <td>8.411149</td>\n",
              "      <td>gru</td>\n",
              "      <td>16</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>206692.654538</td>\n",
              "      <td>181873.453125</td>\n",
              "      <td>8.891973</td>\n",
              "      <td>gru</td>\n",
              "      <td>16</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>171585.754560</td>\n",
              "      <td>154279.171875</td>\n",
              "      <td>7.551521</td>\n",
              "      <td>gru</td>\n",
              "      <td>16</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>229629.351155</td>\n",
              "      <td>188041.078125</td>\n",
              "      <td>9.128286</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L128_G8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>180579.178423</td>\n",
              "      <td>155846.375000</td>\n",
              "      <td>7.594780</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L128_G8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>238616.676835</td>\n",
              "      <td>204522.328125</td>\n",
              "      <td>9.973700</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L128_G8</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>175030.113066</td>\n",
              "      <td>151546.000000</td>\n",
              "      <td>7.387715</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L128_G8</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>279958.239629</td>\n",
              "      <td>233335.578125</td>\n",
              "      <td>11.334047</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L128_G16</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>185069.277061</td>\n",
              "      <td>151797.796875</td>\n",
              "      <td>7.362802</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L128_G16</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>243116.920546</td>\n",
              "      <td>204073.453125</td>\n",
              "      <td>9.941549</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L128_G16</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>201447.774910</td>\n",
              "      <td>175067.546875</td>\n",
              "      <td>8.537970</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L128_G16</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>189040.270165</td>\n",
              "      <td>167451.546875</td>\n",
              "      <td>8.181506</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L256_G8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>189553.198612</td>\n",
              "      <td>155682.375000</td>\n",
              "      <td>7.554221</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L256_G8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>202054.355301</td>\n",
              "      <td>172195.046875</td>\n",
              "      <td>8.381744</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L256_G8</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>189156.458076</td>\n",
              "      <td>160778.953125</td>\n",
              "      <td>7.827387</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L256_G8</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>229155.384977</td>\n",
              "      <td>199825.953125</td>\n",
              "      <td>9.748106</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L256_G16</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>203528.613654</td>\n",
              "      <td>177988.578125</td>\n",
              "      <td>8.686036</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L256_G16</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>212119.289005</td>\n",
              "      <td>191918.046875</td>\n",
              "      <td>9.398703</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L256_G16</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>213517.131491</td>\n",
              "      <td>187172.625000</td>\n",
              "      <td>9.135922</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L256_G16</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2aa81bf7-7780-4a70-bee4-d0f7a238a789')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2aa81bf7-7780-4a70-bee4-d0f7a238a789 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2aa81bf7-7780-4a70-bee4-d0f7a238a789');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6f6e0aba-a3fd-4eb4-8817-f81c4586af6b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6f6e0aba-a3fd-4eb4-8817-f81c4586af6b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6f6e0aba-a3fd-4eb4-8817-f81c4586af6b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_d1b2a29c-5500-4dec-803e-e1d081936c2e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('original_valuelist')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d1b2a29c-5500-4dec-803e-e1d081936c2e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('original_valuelist');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "original_valuelist",
              "summary": "{\n  \"name\": \"original_valuelist\",\n  \"rows\": 32,\n  \"fields\": [\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 32968.450718274005,\n        \"min\": 171585.75456021982,\n        \"max\": 296711.634069175,\n        \"num_unique_values\": 32,\n        \"samples\": [\n          203528.6136541985,\n          171585.75456021982,\n          189040.2701648514\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 33729.369366020444,\n        \"min\": 151546.0,\n        \"max\": 277411.78125,\n        \"num_unique_values\": 32,\n        \"samples\": [\n          177988.578125,\n          154279.171875,\n          167451.546875\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 32,\n        \"samples\": [\n          8.686036109924316,\n          7.551521301269531,\n          8.181506156921387\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"lstm\",\n          \"gru\",\n          \"hybrid\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"units\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          256,\n          \"L128_G16\",\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"drop_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.050800050800076206,\n        \"min\": 0.1,\n        \"max\": 0.2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.2,\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dense_unit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16,\n        \"min\": 32,\n        \"max\": 64,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          64,\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 4,\n        \"max\": 4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"epochs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 100,\n        \"max\": 100,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          100\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "original_valuelist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43gHjYqtufuP",
        "outputId": "f370d746-31f9-481c-ca1a-75429c71521a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lstm_unit128_drop0.1_dense32_batch4_epochs100': array([1760144.2, 1697628.4, 1767952.9], dtype=float32),\n",
              " 'lstm_unit128_drop0.1_dense64_batch4_epochs100': array([1844453.9, 1851815.6, 1774807.1], dtype=float32),\n",
              " 'lstm_unit128_drop0.2_dense32_batch4_epochs100': array([1658017.9, 1738303.9, 1790150.9], dtype=float32),\n",
              " 'lstm_unit128_drop0.2_dense64_batch4_epochs100': array([1694429.1, 1731097. , 1771142.9], dtype=float32),\n",
              " 'lstm_unit256_drop0.1_dense32_batch4_epochs100': array([1791929.9, 1833199.6, 1796725.9], dtype=float32),\n",
              " 'lstm_unit256_drop0.1_dense64_batch4_epochs100': array([1800604.5, 1862156. , 1832835.5], dtype=float32),\n",
              " 'lstm_unit256_drop0.2_dense32_batch4_epochs100': array([1809096.6, 1840975.6, 1689233.9], dtype=float32),\n",
              " 'lstm_unit256_drop0.2_dense64_batch4_epochs100': array([1817046. , 1828541.6, 1771580.2], dtype=float32),\n",
              " 'gru_unit8_drop0.1_dense32_batch4_epochs100': array([1824266.6, 1848320.2, 1843292.1], dtype=float32),\n",
              " 'gru_unit8_drop0.1_dense64_batch4_epochs100': array([1797260. , 1872916.8, 1854724.8], dtype=float32),\n",
              " 'gru_unit8_drop0.2_dense32_batch4_epochs100': array([1796474.8, 1843669.4, 1860137.5], dtype=float32),\n",
              " 'gru_unit8_drop0.2_dense64_batch4_epochs100': array([1786781.1, 1845530.2, 1844172.8], dtype=float32),\n",
              " 'gru_unit16_drop0.1_dense32_batch4_epochs100': array([1812699.6, 1852832.5, 1839418.1], dtype=float32),\n",
              " 'gru_unit16_drop0.1_dense64_batch4_epochs100': array([1811036.5, 1843742. , 1847782.1], dtype=float32),\n",
              " 'gru_unit16_drop0.2_dense32_batch4_epochs100': array([1775764.5, 1838603.6, 1858719.5], dtype=float32),\n",
              " 'gru_unit16_drop0.2_dense64_batch4_epochs100': array([1842472.1, 1842723.1, 1870675.2], dtype=float32),\n",
              " 'hybrid_lstmUnit128_gruUnit8_drop0.1_dense32_batch4_epochs100': array([1741421. , 1881957.6, 1831206.1], dtype=float32),\n",
              " 'hybrid_lstmUnit128_gruUnit8_drop0.1_dense64_batch4_epochs100': array([1843357. , 1866843.4, 1840968.5], dtype=float32),\n",
              " 'hybrid_lstmUnit128_gruUnit8_drop0.2_dense32_batch4_epochs100': array([1726798.4, 1849214.9, 1829127.8], dtype=float32),\n",
              " 'hybrid_lstmUnit128_gruUnit8_drop0.2_dense64_batch4_epochs100': array([1854231. , 1866194.8, 1843644.2], dtype=float32),\n",
              " 'hybrid_lstmUnit128_gruUnit16_drop0.1_dense32_batch4_epochs100': array([1714122.2, 1878780.9, 1725798.1], dtype=float32),\n",
              " 'hybrid_lstmUnit128_gruUnit16_drop0.1_dense64_batch4_epochs100': array([1832002.9, 1891198.4, 1840113.4], dtype=float32),\n",
              " 'hybrid_lstmUnit128_gruUnit16_drop0.2_dense32_batch4_epochs100': array([1708067.9, 1854985.6, 1843434.1], dtype=float32),\n",
              " 'hybrid_lstmUnit128_gruUnit16_drop0.2_dense64_batch4_epochs100': array([1805870.6, 1857902.5, 1829732.2], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit8_drop0.1_dense32_batch4_epochs100': array([1838341.8, 1850509.6, 1827502. ], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit8_drop0.1_dense64_batch4_epochs100': array([1811770.9, 1900224.4, 1852248.4], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit8_drop0.2_dense32_batch4_epochs100': array([1820650.5, 1871198.8, 1810273.6], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit8_drop0.2_dense64_batch4_epochs100': array([1809869.1, 1869218.2, 1857283.8], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit16_drop0.1_dense32_batch4_epochs100': array([1788476. , 1852702.8, 1778051.4], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit16_drop0.1_dense64_batch4_epochs100': array([1802136.4, 1853282.2, 1829323.6], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit16_drop0.2_dense32_batch4_epochs100': array([1794600.2, 1827401.5, 1820952.1], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit16_drop0.2_dense64_batch4_epochs100': array([1791936.1, 1850287.6, 1814966.4], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "all_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbN0QZjLujrW",
        "outputId": "cf83ef70-8a02-4f18-b650-51eae97d0f1c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lstm_unit128_drop0.1_dense32_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'lstm_unit128_drop0.1_dense64_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'lstm_unit128_drop0.2_dense32_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'lstm_unit128_drop0.2_dense64_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'lstm_unit256_drop0.1_dense32_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'lstm_unit256_drop0.1_dense64_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'lstm_unit256_drop0.2_dense32_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'lstm_unit256_drop0.2_dense64_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'gru_unit8_drop0.1_dense32_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'gru_unit8_drop0.1_dense64_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'gru_unit8_drop0.2_dense32_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'gru_unit8_drop0.2_dense64_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'gru_unit16_drop0.1_dense32_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'gru_unit16_drop0.1_dense64_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'gru_unit16_drop0.2_dense32_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'gru_unit16_drop0.2_dense64_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'hybrid_lstmUnit128_gruUnit8_drop0.1_dense32_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'hybrid_lstmUnit128_gruUnit8_drop0.1_dense64_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'hybrid_lstmUnit128_gruUnit8_drop0.2_dense32_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'hybrid_lstmUnit128_gruUnit8_drop0.2_dense64_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'hybrid_lstmUnit128_gruUnit16_drop0.1_dense32_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'hybrid_lstmUnit128_gruUnit16_drop0.1_dense64_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'hybrid_lstmUnit128_gruUnit16_drop0.2_dense32_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'hybrid_lstmUnit128_gruUnit16_drop0.2_dense64_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit8_drop0.1_dense32_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit8_drop0.1_dense64_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit8_drop0.2_dense32_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit8_drop0.2_dense64_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit16_drop0.1_dense32_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit16_drop0.1_dense64_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit16_drop0.2_dense32_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit16_drop0.2_dense64_batch4_epochs100': array([2070466., 1893933., 2054309.], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "all_ground_truths"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1V77OOSCfpgtp79CZ19W1ohXAfNfQ8V4B",
      "authorship_tag": "ABX9TyO3AkeucAksW7Dx5tFuJm/4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}