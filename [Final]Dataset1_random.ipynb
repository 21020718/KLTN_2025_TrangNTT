{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/21020718/KLTN_2025_TrangNTT/blob/main/%5BFinal%5DDataset1_random.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WewFwlm9iO4Z",
        "outputId": "ebdf3f43-bb05-4f1b-ede3-b0e95964d132"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "IPython.notebook.set_autosave_interval(60000)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autosaving every 60 seconds\n"
          ]
        }
      ],
      "source": [
        "%autosave 60\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bộ dữ liệu thứ nhất**"
      ],
      "metadata": {
        "id": "beWgi1rZP_xU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "VIUPZ-SrJwXj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VQqKmMrFiic6"
      },
      "outputs": [],
      "source": [
        "# Set random seeds for reproducibility\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Define a fixed seed value\n",
        "SEED = 42\n",
        "\n",
        "# Set random seeds for all libraries\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)  # For GPU if available\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "import sklearn.preprocessing\n",
        "import sklearn.preprocessing._data as data\n",
        "import sys\n",
        "sys.modules[\"sklearn.preprocessing.data\"] = data\n",
        "\n",
        "import pandas as pd\n",
        "from numpy import concatenate\n",
        "from math import sqrt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.metrics import normalized_mutual_info_score\n",
        "\n",
        "def Scaler(data):\n",
        "\n",
        "    \"\"\"\n",
        "        Scaler all feature to range(0,1)\n",
        "        Arguments:\n",
        "          data:  Pandas DataFrame of data\n",
        "        Return:\n",
        "          scaler: scaler\n",
        "          scaledDf:Pandas DataFrame of scaled data\n",
        "    \"\"\"\n",
        "\n",
        "    values = data.values\n",
        "    values = values.astype('float32')\n",
        "    scaler = MinMaxScaler(feature_range=(0,1))\n",
        "    scaled = scaler.fit_transform(values)\n",
        "    scaledDf = pd.DataFrame(scaled,columns=data.columns)\n",
        "    return scaler,scaledDf\n",
        "\n",
        "\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    \"\"\"\n",
        "    Frame a time series as a supervised learning dataset.\n",
        "    Arguments:\n",
        "        data: Sequence of observations as a list or NumPy array.\n",
        "        n_in: Number of lag observations as input (X).\n",
        "        n_out: Number of observations as output (y).\n",
        "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
        "    Returns:\n",
        "        Pandas DataFrame of series framed for supervised learning.\n",
        "    \"\"\"\n",
        "\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = pd.DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # put it all together\n",
        "    agg = pd.concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "\n",
        "    agg.drop(agg.columns[-(df.shape[1]-1):],axis = 1,inplace=True)\n",
        "    return agg\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # Load data with date column preserved\n",
        "    # data = pd.read_csv(\"/content/drive/MyDrive/KLTN/data/VN2008-2020/VN2008-2020.csv\", encoding='utf-8-sig')\n",
        "    data = pd.read_csv(\"/content/drive/MyDrive/VN2008-2020.csv\", encoding='utf-8-sig')\n",
        "\n",
        "    # Store dates separately before dropping the column\n",
        "    dates = data[\"date\"].copy()\n",
        "    data.drop(columns=[\"date\"], inplace=True)\n",
        "    data.fillna(0, inplace=True)\n",
        "\n",
        "    scaler, scaledDf = Scaler(data)\n",
        "\n",
        "    # Add dates back to scaled data for splitting\n",
        "    scaledDf_with_dates = scaledDf.copy()\n",
        "    scaledDf_with_dates['date'] = dates\n",
        "\n",
        "    # Create time series features\n",
        "    reframed = series_to_supervised(scaledDf, n_in=12)\n",
        "\n",
        "    # Add dates back to reframed data (dates correspond to the target time)\n",
        "    # Since we're using 12 months of history, the date for each row should be the last month in the sequence\n",
        "    reframed_dates = dates.reset_index(drop=True)\n",
        "    reframed_with_dates = reframed.copy()\n",
        "    reframed_with_dates['date'] = reframed_dates"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build model & Result"
      ],
      "metadata": {
        "id": "n42DwlrTxgme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hybrid"
      ],
      "metadata": {
        "id": "PA5vNRcdd4RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Giảm số lượng/GRU unit, dense unit, epochs và sử dụng batch size nhỏ hơn để huấn luyện nhanh hơn.\n",
        "# model_types = ['multi-scale']\n",
        "# lstm_unit = [128, 256, 512]\n",
        "# gru_unit = [8, 16, 32]\n",
        "# drop_rate = [0.1, 0.2]\n",
        "# dense_unit = [16, 32, 64]\n",
        "# batch_size_num = [2, 4]\n",
        "# epochs = [100]\n",
        "\n",
        "model_types = ['hybrid']\n",
        "lstm_unit = [256,512]\n",
        "gru_unit = [8,16]\n",
        "drop_rate = [0.1,0.2]\n",
        "dense_unit = [32,64]\n",
        "batch_size_num = [4]\n",
        "epochs = [100]\n",
        "\n",
        "# # Replace the current parameter definitions\n",
        "# model_types = ['hybrid', 'sequential', 'stacked', 'bidirectional', 'cnn-rnn', 'multi-scale', 'transformer-rnn', 'ensemble', 'lstm', 'gru']\n",
        "# lstm_unit = [128]\n",
        "# gru_unit = [8]\n",
        "# drop_rate = [0.1]\n",
        "# dense_unit = [64]\n",
        "# batch_size_num = [2]\n",
        "# epochs = [100]\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import concatenate\n",
        "import itertools\n",
        "from math import sqrt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "class AttentionGRU(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, gru_units, dropout_rate, dense_units):\n",
        "        super(AttentionGRU, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention1 = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # GRU layer\n",
        "        self.gru = nn.GRU(input_dim, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear(gru_units * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, time_steps, input_dim)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.attention1(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)  # (batch_size, time_steps, input_dim)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # GRU layer - output: (batch, seq, hidden_size)\n",
        "        gru_out, _ = self.gru(attention_mul)\n",
        "\n",
        "        # Flatten and Dense layers\n",
        "        flattened = self.flatten(gru_out)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class AttentionLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, dropout_rate, dense_units):\n",
        "        super(AttentionLSTM, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention1 = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear(lstm_units * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, time_steps, input_dim)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.attention1(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)  # (batch_size, time_steps, input_dim)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # LSTM layer - output: (batch, seq, hidden_size)\n",
        "        lstm_out, _ = self.lstm(attention_mul)\n",
        "\n",
        "        # Flatten and Dense layers\n",
        "        flattened = self.flatten(lstm_out)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class HybridLSTM_GRU(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(HybridLSTM_GRU, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention1 = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # LSTM and GRU layers\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "        self.gru = nn.GRU(input_dim, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        # Combined size from both LSTM and GRU\n",
        "        self.dense1 = nn.Linear((lstm_units + gru_units) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, time_steps, input_dim)\n",
        "\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.attention1(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)  # (batch_size, time_steps, input_dim)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # LSTM and GRU layers\n",
        "        lstm_out, _ = self.lstm(attention_mul)\n",
        "        gru_out, _ = self.gru(attention_mul)\n",
        "\n",
        "        # Concatenate LSTM and GRU outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Flatten and Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class SequentialHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(SequentialHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # LSTM followed by GRU\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "        self.gru = nn.GRU(lstm_units, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear(gru_units * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)\n",
        "        a = self.attention(a)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # Sequential processing: LSTM then GRU\n",
        "        lstm_out, _ = self.lstm(attention_mul)\n",
        "        gru_out, _ = self.gru(lstm_out)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(gru_out)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class StackedHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(StackedHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention\n",
        "        self.attention = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # Two stacked LSTM layers\n",
        "        self.lstm1 = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(lstm_units, lstm_units//2, batch_first=True)\n",
        "\n",
        "        # Two stacked GRU layers\n",
        "        self.gru1 = nn.GRU(input_dim, gru_units, batch_first=True)\n",
        "        self.gru2 = nn.GRU(gru_units, gru_units//2, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units//2 + gru_units//2) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)\n",
        "        a = self.attention(a)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # Stacked LSTM path\n",
        "        lstm_out1, _ = self.lstm1(attention_mul)\n",
        "        lstm_out2, _ = self.lstm2(lstm_out1)\n",
        "\n",
        "        # Stacked GRU path\n",
        "        gru_out1, _ = self.gru1(attention_mul)\n",
        "        gru_out2, _ = self.gru2(gru_out1)\n",
        "\n",
        "        # Concatenate final outputs\n",
        "        combined = torch.cat((lstm_out2, gru_out2), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class BidirectionalHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(BidirectionalHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # Bidirectional LSTM and GRU\n",
        "        self.bilstm = nn.LSTM(input_dim, lstm_units, batch_first=True, bidirectional=True)\n",
        "        self.bigru = nn.GRU(input_dim, gru_units, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units*2 + gru_units*2) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)\n",
        "        a = self.attention(a)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # Bidirectional networks\n",
        "        lstm_out, _ = self.bilstm(attention_mul)\n",
        "        gru_out, _ = self.bigru(attention_mul)\n",
        "\n",
        "        # Concatenate outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class CNNRNNHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(CNNRNNHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # 1D CNN for feature extraction\n",
        "        self.conv1 = nn.Conv1d(input_dim, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # RNN layers\n",
        "        self.lstm = nn.LSTM(64, lstm_units, batch_first=True)\n",
        "        self.gru = nn.GRU(64, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units + gru_units) * (time_steps-1), dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # CNN feature extraction\n",
        "        x = x.permute(0, 2, 1)  # (batch, features, seq_len)\n",
        "        cnn_out = self.relu(self.conv1(x))\n",
        "        cnn_out = self.maxpool(cnn_out)\n",
        "        cnn_out = self.relu(self.conv2(cnn_out))\n",
        "        cnn_out = cnn_out.permute(0, 2, 1)  # (batch, seq_len, features)\n",
        "\n",
        "        # RNN processing\n",
        "        lstm_out, _ = self.lstm(cnn_out)\n",
        "        gru_out, _ = self.gru(cnn_out)\n",
        "\n",
        "        # Combine outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class MultiScaleHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(MultiScaleHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # LSTM for long-term dependencies\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "\n",
        "        # GRU for shorter-term dependencies (operating on windows)\n",
        "        self.gru = nn.GRU(input_dim, gru_units, batch_first=True)\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units + gru_units) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Full sequence for LSTM (long-term)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Attention mechanism for GRU input\n",
        "        a = x.permute(0, 2, 1)\n",
        "        a = self.attention(a)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)\n",
        "\n",
        "        # Apply attention weights for GRU (short-term focus)\n",
        "        gru_input = torch.mul(x, a)\n",
        "        gru_out, _ = self.gru(gru_input)\n",
        "\n",
        "        # Combine outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class TransformerRNNHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units, nhead=4):\n",
        "        super(TransformerRNNHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Input projection for transformer\n",
        "        self.input_proj = nn.Linear(input_dim, 64)\n",
        "\n",
        "        # Transformer encoder layer\n",
        "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=64, nhead=nhead, dropout=dropout_rate, batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layer, num_layers=2)\n",
        "\n",
        "        # RNN layers\n",
        "        self.lstm = nn.LSTM(64, lstm_units, batch_first=True)\n",
        "        self.gru = nn.GRU(64, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units + gru_units) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Project input to transformer dimension\n",
        "        x_proj = self.input_proj(x)\n",
        "\n",
        "        # Apply transformer encoder\n",
        "        transformer_out = self.transformer_encoder(x_proj)\n",
        "\n",
        "        # Process with RNNs\n",
        "        lstm_out, _ = self.lstm(transformer_out)\n",
        "        gru_out, _ = self.gru(transformer_out)\n",
        "\n",
        "        # Combine outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class EnsembleHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(EnsembleHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Individual models\n",
        "        self.lstm_model = AttentionLSTM(input_dim, time_steps, lstm_units, dropout_rate, dense_units)\n",
        "        self.gru_model = AttentionGRU(input_dim, time_steps, gru_units, dropout_rate, dense_units)\n",
        "\n",
        "        # Combination layer\n",
        "        self.combine = nn.Linear(2, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get predictions from each model\n",
        "        lstm_pred = self.lstm_model(x)\n",
        "        gru_pred = self.gru_model(x)\n",
        "\n",
        "        # Combine predictions (learnable weights)\n",
        "        combined = torch.cat((lstm_pred, gru_pred), dim=1)\n",
        "        output = self.final_activation(self.combine(combined))\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "def build_model(train_X, train_Y, val_X, val_Y, model_type='gru', lstm_units=128, gru_units=128, drop_rate=0.3, dense_unit=64, batch_size=32, epochs=100):\n",
        "    # Print training parameters\n",
        "    train_X_tensor = torch.FloatTensor(train_X)\n",
        "    train_Y_tensor = torch.FloatTensor(train_Y.reshape(-1, 1))\n",
        "    val_X_tensor = torch.FloatTensor(val_X)\n",
        "    val_Y_tensor = torch.FloatTensor(val_Y.reshape(-1, 1))\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
        "    val_dataset = TensorDataset(val_X_tensor, val_Y_tensor)\n",
        "\n",
        "    # Create reproducible DataLoaders with fixed seeds\n",
        "    train_generator = torch.Generator()\n",
        "    train_generator.manual_seed(SEED)\n",
        "    val_generator = torch.Generator()\n",
        "    val_generator.manual_seed(SEED)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=train_generator)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, generator=val_generator)\n",
        "\n",
        "    # Initialize model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    time_steps = train_X.shape[1]\n",
        "    input_dim = train_X.shape[2]\n",
        "\n",
        "    # Initialize model with fixed initial weights\n",
        "    torch.manual_seed(SEED)\n",
        "\n",
        "    if model_type == 'gru':\n",
        "        model = AttentionGRU(input_dim, time_steps, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'lstm':\n",
        "        model = AttentionLSTM(input_dim, time_steps, lstm_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'hybrid':\n",
        "        model = HybridLSTM_GRU(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'sequential':\n",
        "        model = SequentialHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'stacked':\n",
        "        model = StackedHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'bidirectional':\n",
        "        model = BidirectionalHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'cnn-rnn':\n",
        "        model = CNNRNNHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'multi-scale':\n",
        "        model = MultiScaleHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'transformer-rnn':\n",
        "        model = TransformerRNNHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'ensemble':\n",
        "        model = EnsembleHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "    # Initialize optimizer and loss function\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    criterion = nn.L1Loss()  # MAE loss\n",
        "\n",
        "    # Training loop\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    patience = 5\n",
        "    lr_factor = 0.01\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        # Print progress\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
        "\n",
        "        # Learning rate schedule based on validation loss\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] *= lr_factor\n",
        "                patience_counter = 0\n",
        "                print(f'Reducing learning rate by factor of {lr_factor}')\n",
        "\n",
        "    return model\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    mask = y_true != 0\n",
        "    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "    return mape\n",
        "\n",
        "def walk_forward(train_X, train_Y, val_X, val_Y, test_X, test_Y, grid_search, scaler):\n",
        "    r, f, c = test_X.shape\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    all_predictions = {}\n",
        "    all_adjusted_predictions = {}\n",
        "    all_ground_truths = {}\n",
        "\n",
        "    # Create lists to store all evaluation results\n",
        "    original_valuelists = []\n",
        "    adjusted_valuelists = []\n",
        "\n",
        "    for x in grid_search:\n",
        "        history_x = np.array([x for x in train_X])\n",
        "        history_y = np.array([y for y in train_Y])\n",
        "        predictions = list()\n",
        "        adjusted_predictions = list()\n",
        "        groundtrue = list()\n",
        "\n",
        "        # Extract model type first to determine how to unpack the rest\n",
        "        model_type = x[0]\n",
        "\n",
        "        # Create the appropriate config_key and extract parameters based on model type\n",
        "        if model_type in ['hybrid', 'sequential', 'stacked', 'bidirectional', 'cnn-rnn', 'multi-scale', 'transformer-rnn', 'ensemble']:\n",
        "            # Hybrid model has 7 parameters\n",
        "            model_type, lstm_unit_val, gru_unit_val, drop, dense, batch, epoch = x\n",
        "            units = f\"L{lstm_unit_val}_G{gru_unit_val}\"  # For logging\n",
        "            config_key = f\"{model_type}_lstmUnit{lstm_unit_val}_gruUnit{gru_unit_val}_drop{drop}_dense{dense}_batch{batch}_epochs{epoch}\"\n",
        "        else:\n",
        "            # LSTM and GRU models have 6 parameters\n",
        "            model_type, units, drop, dense, batch, epoch = x\n",
        "            config_key = f\"{model_type}_unit{units}_drop{drop}_dense{dense}_batch{batch}_epochs{epoch}\"\n",
        "\n",
        "        print(\"\\n\" + \"*\"*50)\n",
        "        print(f\"Starting walk-forward validation with parameters:\")\n",
        "        print(f\"Model Type: {model_type}, Units: {units}, Dropout: {drop}, Dense Units: {dense}\")\n",
        "        print(f\"Batch Size: {batch}, Epochs: {epoch}\")\n",
        "        print(f\"Device: {device}\")\n",
        "        print(f\"Total test samples: {len(test_X)}\")\n",
        "        print(\"*\"*50 + \"\\n\")\n",
        "\n",
        "        for i in range(len(test_X)):\n",
        "            print(f\"\\nTest iteration {i+1}/{len(test_X)}\")\n",
        "            print(f\"Current training set size: {history_x.shape[0]} samples\")\n",
        "\n",
        "            if model_type in ['hybrid', 'sequential', 'stacked', 'bidirectional', 'cnn-rnn', 'multi-scale', 'transformer-rnn', 'ensemble']:\n",
        "                model = build_model(history_x, history_y, val_X, val_Y, model_type=model_type,\n",
        "                                lstm_units=lstm_unit_val, gru_units=gru_unit_val, drop_rate=drop,\n",
        "                                dense_unit=dense, batch_size=batch, epochs=epoch)\n",
        "            else:\n",
        "                model = build_model(history_x, history_y, val_X, val_Y, model_type=model_type,\n",
        "                                lstm_units=units if model_type == 'lstm' else 128,\n",
        "                                gru_units=units if model_type == 'gru' else 128,\n",
        "                                drop_rate=drop, dense_unit=dense, batch_size=batch, epochs=epoch)\n",
        "\n",
        "            # Rest of the function remains the same\n",
        "            model.eval()\n",
        "\n",
        "            # Convert test data to tensor\n",
        "            test_tensor = torch.FloatTensor(test_X[i].reshape(1, f, c)).to(device)\n",
        "\n",
        "            # Predict\n",
        "            with torch.no_grad():\n",
        "                yhat = model(test_tensor).cpu().numpy()\n",
        "\n",
        "            inv_yhat, inv_y = inverscale(yhat, test_X[i], test_Y[i], scaler)\n",
        "            prev_month_lockdown = test_X[i][11][5]\n",
        "            adjusted_inv_yhat = inv_yhat * (1 - prev_month_lockdown)\n",
        "            predictions.append(inv_yhat)\n",
        "            adjusted_predictions.append(adjusted_inv_yhat)\n",
        "            groundtrue.append(inv_y)\n",
        "\n",
        "            # Observation\n",
        "            obs_x = test_X[i]\n",
        "            obs_y = test_Y[i]\n",
        "\n",
        "            history_x = np.append(history_x, [obs_x], axis=0)\n",
        "            history_y = np.append(history_y, obs_y)\n",
        "\n",
        "        # Store predictions and ground truth for this configuration\n",
        "        all_predictions[config_key] = np.array(predictions).flatten()\n",
        "        all_adjusted_predictions[config_key] = np.array(adjusted_predictions).flatten()\n",
        "        all_ground_truths[config_key] = np.array(groundtrue).flatten()\n",
        "\n",
        "        original_valuelist = evalue(predictions, groundtrue)\n",
        "        original_valuelist['model_type'] = model_type\n",
        "        original_valuelist['units'] = units\n",
        "        original_valuelist['drop_rate'] = drop\n",
        "        original_valuelist['dense_unit'] = dense\n",
        "        original_valuelist['batch_size'] = batch\n",
        "        original_valuelist['epochs'] = epoch\n",
        "\n",
        "        # Evaluate with adjusted predictions\n",
        "        adjusted_valuelist = evalue(adjusted_predictions, groundtrue)\n",
        "        adjusted_valuelist['model_type'] = model_type\n",
        "        adjusted_valuelist['units'] = units\n",
        "        adjusted_valuelist['drop_rate'] = drop\n",
        "        adjusted_valuelist['dense_unit'] = dense\n",
        "        adjusted_valuelist['batch_size'] = batch\n",
        "        adjusted_valuelist['epochs'] = epoch\n",
        "\n",
        "        # Append to the lists of results\n",
        "        original_valuelists.append(original_valuelist)\n",
        "        adjusted_valuelists.append(adjusted_valuelist)\n",
        "\n",
        "    # Combine all results\n",
        "    all_original_valuelist = pd.concat(original_valuelists, ignore_index=True)\n",
        "    all_adjusted_valuelist = pd.concat(adjusted_valuelists, ignore_index=True)\n",
        "\n",
        "    return all_original_valuelist, all_adjusted_valuelist, all_predictions, all_ground_truths, all_adjusted_predictions\n",
        "\n",
        "def evalue(yhat, inv_y):\n",
        "    valuelist = {}\n",
        "    DLM_rmse = sqrt(mean_squared_error(inv_y, yhat))\n",
        "    valuelist.update({'RMSE': {'DLM': DLM_rmse}})\n",
        "    DLM_mae = mean_absolute_error(inv_y, yhat)\n",
        "    valuelist.update({'MAE': {'DLM': DLM_mae}})\n",
        "    DLM_mape = mean_absolute_percentage_error(inv_y, yhat)\n",
        "    valuelist.update({'MAPE': {'DLM': DLM_mape}})\n",
        "    return pd.DataFrame(valuelist)\n",
        "\n",
        "def inverscale(yhat, test_X, test_Y, scaler):\n",
        "    feature = len(scaler.scale_)\n",
        "    test_Y = np.array(test_Y)\n",
        "    test_X = test_X[1, 0:feature]\n",
        "    test_X = test_X.reshape(1, test_X.shape[0])\n",
        "\n",
        "    if len(yhat.shape) == 1:\n",
        "        yhat = yhat.reshape(len(yhat), 1)\n",
        "\n",
        "    inv_yhat = concatenate((yhat, test_X[:, :-1]), axis=1)\n",
        "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "    inv_yhat = inv_yhat[:, 0]\n",
        "\n",
        "    test_Y = test_Y.reshape(1, 1)\n",
        "    inv_y = concatenate((test_Y, test_X[:, :-1]), axis=1)\n",
        "    inv_y = scaler.inverse_transform(inv_y)\n",
        "    inv_y = inv_y[:, 0]\n",
        "    return inv_yhat, inv_y\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    values = reframed.values\n",
        "    reframed_with_dates_values = reframed_with_dates.values\n",
        "\n",
        "    # Import train_test_split for random splitting\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    # Drop the date column for the splitting but keep track of indices\n",
        "    reframed_without_dates = reframed.copy()\n",
        "\n",
        "    # First split: 80% train+val, 20% test\n",
        "    train_val_indices, test_indices = train_test_split(\n",
        "        np.arange(len(reframed_without_dates)),\n",
        "        test_size=0.2,\n",
        "        random_state=SEED\n",
        "    )\n",
        "\n",
        "    # Second split: From the 80%, use 7/8 for train (70% of total) and 1/8 for val (10% of total)\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        train_val_indices,\n",
        "        test_size=0.125,  # 0.125 * 0.8 = 0.1 (10% of total)\n",
        "        random_state=SEED\n",
        "    )\n",
        "\n",
        "    # Extract values for train, validation, and test sets\n",
        "    train_data = reframed.iloc[train_indices].values\n",
        "    val_data = reframed.iloc[val_indices].values\n",
        "    test_data = reframed.iloc[test_indices].values\n",
        "\n",
        "    # Store the corresponding dates for reference\n",
        "    train_dates = reframed_with_dates.iloc[train_indices]['date']\n",
        "    val_dates = reframed_with_dates.iloc[val_indices]['date']\n",
        "    test_dates = reframed_with_dates.iloc[test_indices]['date']\n",
        "\n",
        "    # Split into X and Y\n",
        "    train_X, train_Y = train_data[:, :-1], train_data[:, -1]\n",
        "    val_X, val_Y = val_data[:, :-1], val_data[:, -1]\n",
        "    test_X, test_Y = test_data[:, :-1], test_data[:, -1]\n",
        "\n",
        "    # Reshape input to be 3D [samples, timesteps, features]\n",
        "    train_X = train_X.reshape(train_X.shape[0], 12, int(train_X.shape[1]/12))\n",
        "    val_X = val_X.reshape(val_X.shape[0], 12, int(val_X.shape[1]/12))\n",
        "    test_X = test_X.reshape(test_X.shape[0], 12, int(test_X.shape[1]/12))\n",
        "\n",
        "    # Modified grid search creation for all model types\n",
        "    grid_search = []\n",
        "    for model_type in model_types:\n",
        "        if model_type == 'lstm':\n",
        "            grid_search.extend(\n",
        "                list(itertools.product([model_type], lstm_unit, drop_rate, dense_unit, batch_size_num, epochs))\n",
        "            )\n",
        "        elif model_type == 'gru':\n",
        "            grid_search.extend(\n",
        "                list(itertools.product([model_type], gru_unit, drop_rate, dense_unit, batch_size_num, epochs))\n",
        "            )\n",
        "        else:\n",
        "            # All other models (hybrid, sequential, stacked, etc.) need both LSTM and GRU units\n",
        "            grid_search.extend(\n",
        "                list(itertools.product([model_type], lstm_unit, gru_unit, drop_rate, dense_unit, batch_size_num, epochs))\n",
        "            )\n",
        "\n",
        "    original_valuelist, adjusted_valuelist, all_predictions, all_ground_truths, all_adjusted_predictions = walk_forward(train_X, train_Y, val_X, val_Y, test_X, test_Y, grid_search, scaler)\n",
        "\n",
        "    print(\"Results:\")\n",
        "    print(adjusted_valuelist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfZwq8_JxikN",
        "outputId": "1abeba27-9d49-4da9-925a-a17de5d86249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 82/100, Train Loss: 0.0380, Val Loss: 0.0398\n",
            "Epoch 83/100, Train Loss: 0.0376, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0369, Val Loss: 0.0398\n",
            "Epoch 85/100, Train Loss: 0.0375, Val Loss: 0.0398\n",
            "Epoch 86/100, Train Loss: 0.0371, Val Loss: 0.0398\n",
            "Epoch 87/100, Train Loss: 0.0382, Val Loss: 0.0398\n",
            "Epoch 88/100, Train Loss: 0.0365, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0370, Val Loss: 0.0398\n",
            "Epoch 90/100, Train Loss: 0.0383, Val Loss: 0.0398\n",
            "Epoch 91/100, Train Loss: 0.0370, Val Loss: 0.0398\n",
            "Epoch 92/100, Train Loss: 0.0368, Val Loss: 0.0398\n",
            "Epoch 93/100, Train Loss: 0.0378, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0376, Val Loss: 0.0398\n",
            "Epoch 95/100, Train Loss: 0.0369, Val Loss: 0.0398\n",
            "Epoch 96/100, Train Loss: 0.0376, Val Loss: 0.0398\n",
            "Epoch 97/100, Train Loss: 0.0389, Val Loss: 0.0398\n",
            "Epoch 98/100, Train Loss: 0.0385, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0381, Val Loss: 0.0398\n",
            "Epoch 100/100, Train Loss: 0.0373, Val Loss: 0.0398\n",
            "\n",
            "Test iteration 16/28\n",
            "Current training set size: 112 samples\n",
            "Epoch 1/100, Train Loss: 0.1553, Val Loss: 0.1837\n",
            "Epoch 2/100, Train Loss: 0.1426, Val Loss: 0.1894\n",
            "Epoch 3/100, Train Loss: 0.1400, Val Loss: 0.1562\n",
            "Epoch 4/100, Train Loss: 0.1278, Val Loss: 0.1037\n",
            "Epoch 5/100, Train Loss: 0.1051, Val Loss: 0.0696\n",
            "Epoch 6/100, Train Loss: 0.0826, Val Loss: 0.0403\n",
            "Epoch 7/100, Train Loss: 0.0639, Val Loss: 0.0393\n",
            "Epoch 8/100, Train Loss: 0.0628, Val Loss: 0.0396\n",
            "Epoch 9/100, Train Loss: 0.0641, Val Loss: 0.0477\n",
            "Epoch 10/100, Train Loss: 0.0552, Val Loss: 0.0425\n",
            "Epoch 11/100, Train Loss: 0.0521, Val Loss: 0.0360\n",
            "Epoch 12/100, Train Loss: 0.0492, Val Loss: 0.0320\n",
            "Epoch 13/100, Train Loss: 0.0506, Val Loss: 0.0669\n",
            "Epoch 14/100, Train Loss: 0.0479, Val Loss: 0.0559\n",
            "Epoch 15/100, Train Loss: 0.0421, Val Loss: 0.0344\n",
            "Epoch 16/100, Train Loss: 0.0460, Val Loss: 0.0510\n",
            "Epoch 17/100, Train Loss: 0.0409, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0405, Val Loss: 0.0363\n",
            "Epoch 19/100, Train Loss: 0.0353, Val Loss: 0.0366\n",
            "Epoch 20/100, Train Loss: 0.0350, Val Loss: 0.0376\n",
            "Epoch 21/100, Train Loss: 0.0349, Val Loss: 0.0378\n",
            "Epoch 22/100, Train Loss: 0.0336, Val Loss: 0.0378\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0347, Val Loss: 0.0378\n",
            "Epoch 24/100, Train Loss: 0.0337, Val Loss: 0.0378\n",
            "Epoch 25/100, Train Loss: 0.0341, Val Loss: 0.0378\n",
            "Epoch 26/100, Train Loss: 0.0343, Val Loss: 0.0378\n",
            "Epoch 27/100, Train Loss: 0.0338, Val Loss: 0.0378\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0331, Val Loss: 0.0378\n",
            "Epoch 29/100, Train Loss: 0.0328, Val Loss: 0.0378\n",
            "Epoch 30/100, Train Loss: 0.0343, Val Loss: 0.0378\n",
            "Epoch 31/100, Train Loss: 0.0333, Val Loss: 0.0378\n",
            "Epoch 32/100, Train Loss: 0.0336, Val Loss: 0.0378\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0330, Val Loss: 0.0378\n",
            "Epoch 34/100, Train Loss: 0.0339, Val Loss: 0.0378\n",
            "Epoch 35/100, Train Loss: 0.0328, Val Loss: 0.0378\n",
            "Epoch 36/100, Train Loss: 0.0335, Val Loss: 0.0378\n",
            "Epoch 37/100, Train Loss: 0.0336, Val Loss: 0.0378\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0329, Val Loss: 0.0378\n",
            "Epoch 39/100, Train Loss: 0.0328, Val Loss: 0.0378\n",
            "Epoch 40/100, Train Loss: 0.0336, Val Loss: 0.0378\n",
            "Epoch 41/100, Train Loss: 0.0342, Val Loss: 0.0378\n",
            "Epoch 42/100, Train Loss: 0.0338, Val Loss: 0.0378\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0329, Val Loss: 0.0378\n",
            "Epoch 44/100, Train Loss: 0.0348, Val Loss: 0.0378\n",
            "Epoch 45/100, Train Loss: 0.0349, Val Loss: 0.0378\n",
            "Epoch 46/100, Train Loss: 0.0335, Val Loss: 0.0378\n",
            "Epoch 47/100, Train Loss: 0.0339, Val Loss: 0.0378\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0331, Val Loss: 0.0378\n",
            "Epoch 49/100, Train Loss: 0.0344, Val Loss: 0.0378\n",
            "Epoch 50/100, Train Loss: 0.0338, Val Loss: 0.0378\n",
            "Epoch 51/100, Train Loss: 0.0336, Val Loss: 0.0378\n",
            "Epoch 52/100, Train Loss: 0.0343, Val Loss: 0.0378\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0330, Val Loss: 0.0378\n",
            "Epoch 54/100, Train Loss: 0.0346, Val Loss: 0.0378\n",
            "Epoch 55/100, Train Loss: 0.0324, Val Loss: 0.0378\n",
            "Epoch 56/100, Train Loss: 0.0338, Val Loss: 0.0378\n",
            "Epoch 57/100, Train Loss: 0.0341, Val Loss: 0.0378\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0336, Val Loss: 0.0378\n",
            "Epoch 59/100, Train Loss: 0.0335, Val Loss: 0.0378\n",
            "Epoch 60/100, Train Loss: 0.0338, Val Loss: 0.0378\n",
            "Epoch 61/100, Train Loss: 0.0340, Val Loss: 0.0378\n",
            "Epoch 62/100, Train Loss: 0.0347, Val Loss: 0.0378\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0339, Val Loss: 0.0378\n",
            "Epoch 64/100, Train Loss: 0.0332, Val Loss: 0.0378\n",
            "Epoch 65/100, Train Loss: 0.0347, Val Loss: 0.0378\n",
            "Epoch 66/100, Train Loss: 0.0334, Val Loss: 0.0378\n",
            "Epoch 67/100, Train Loss: 0.0334, Val Loss: 0.0378\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0329, Val Loss: 0.0378\n",
            "Epoch 69/100, Train Loss: 0.0341, Val Loss: 0.0378\n",
            "Epoch 70/100, Train Loss: 0.0331, Val Loss: 0.0378\n",
            "Epoch 71/100, Train Loss: 0.0342, Val Loss: 0.0378\n",
            "Epoch 72/100, Train Loss: 0.0333, Val Loss: 0.0378\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0338, Val Loss: 0.0378\n",
            "Epoch 74/100, Train Loss: 0.0341, Val Loss: 0.0378\n",
            "Epoch 75/100, Train Loss: 0.0335, Val Loss: 0.0378\n",
            "Epoch 76/100, Train Loss: 0.0343, Val Loss: 0.0378\n",
            "Epoch 77/100, Train Loss: 0.0335, Val Loss: 0.0378\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0334, Val Loss: 0.0378\n",
            "Epoch 79/100, Train Loss: 0.0348, Val Loss: 0.0378\n",
            "Epoch 80/100, Train Loss: 0.0339, Val Loss: 0.0378\n",
            "Epoch 81/100, Train Loss: 0.0337, Val Loss: 0.0378\n",
            "Epoch 82/100, Train Loss: 0.0350, Val Loss: 0.0378\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0330, Val Loss: 0.0378\n",
            "Epoch 84/100, Train Loss: 0.0341, Val Loss: 0.0378\n",
            "Epoch 85/100, Train Loss: 0.0332, Val Loss: 0.0378\n",
            "Epoch 86/100, Train Loss: 0.0336, Val Loss: 0.0378\n",
            "Epoch 87/100, Train Loss: 0.0341, Val Loss: 0.0378\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0336, Val Loss: 0.0378\n",
            "Epoch 89/100, Train Loss: 0.0332, Val Loss: 0.0378\n",
            "Epoch 90/100, Train Loss: 0.0337, Val Loss: 0.0378\n",
            "Epoch 91/100, Train Loss: 0.0347, Val Loss: 0.0378\n",
            "Epoch 92/100, Train Loss: 0.0341, Val Loss: 0.0378\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0335, Val Loss: 0.0378\n",
            "Epoch 94/100, Train Loss: 0.0334, Val Loss: 0.0378\n",
            "Epoch 95/100, Train Loss: 0.0331, Val Loss: 0.0378\n",
            "Epoch 96/100, Train Loss: 0.0335, Val Loss: 0.0378\n",
            "Epoch 97/100, Train Loss: 0.0351, Val Loss: 0.0378\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0333, Val Loss: 0.0378\n",
            "Epoch 99/100, Train Loss: 0.0321, Val Loss: 0.0378\n",
            "Epoch 100/100, Train Loss: 0.0334, Val Loss: 0.0378\n",
            "\n",
            "Test iteration 17/28\n",
            "Current training set size: 113 samples\n",
            "Epoch 1/100, Train Loss: 0.1574, Val Loss: 0.1914\n",
            "Epoch 2/100, Train Loss: 0.1387, Val Loss: 0.1952\n",
            "Epoch 3/100, Train Loss: 0.1324, Val Loss: 0.1804\n",
            "Epoch 4/100, Train Loss: 0.1311, Val Loss: 0.1126\n",
            "Epoch 5/100, Train Loss: 0.1007, Val Loss: 0.0775\n",
            "Epoch 6/100, Train Loss: 0.0870, Val Loss: 0.0493\n",
            "Epoch 7/100, Train Loss: 0.0714, Val Loss: 0.0611\n",
            "Epoch 8/100, Train Loss: 0.0659, Val Loss: 0.0431\n",
            "Epoch 9/100, Train Loss: 0.0611, Val Loss: 0.0535\n",
            "Epoch 10/100, Train Loss: 0.0637, Val Loss: 0.0494\n",
            "Epoch 11/100, Train Loss: 0.0499, Val Loss: 0.0578\n",
            "Epoch 12/100, Train Loss: 0.0449, Val Loss: 0.0380\n",
            "Epoch 13/100, Train Loss: 0.0442, Val Loss: 0.0429\n",
            "Epoch 14/100, Train Loss: 0.0451, Val Loss: 0.0521\n",
            "Epoch 15/100, Train Loss: 0.0492, Val Loss: 0.0416\n",
            "Epoch 16/100, Train Loss: 0.0486, Val Loss: 0.0401\n",
            "Epoch 17/100, Train Loss: 0.0463, Val Loss: 0.0472\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0389, Val Loss: 0.0457\n",
            "Epoch 19/100, Train Loss: 0.0370, Val Loss: 0.0448\n",
            "Epoch 20/100, Train Loss: 0.0363, Val Loss: 0.0443\n",
            "Epoch 21/100, Train Loss: 0.0374, Val Loss: 0.0434\n",
            "Epoch 22/100, Train Loss: 0.0360, Val Loss: 0.0428\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0361, Val Loss: 0.0428\n",
            "Epoch 24/100, Train Loss: 0.0387, Val Loss: 0.0428\n",
            "Epoch 25/100, Train Loss: 0.0359, Val Loss: 0.0428\n",
            "Epoch 26/100, Train Loss: 0.0395, Val Loss: 0.0428\n",
            "Epoch 27/100, Train Loss: 0.0375, Val Loss: 0.0428\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0368, Val Loss: 0.0428\n",
            "Epoch 29/100, Train Loss: 0.0375, Val Loss: 0.0428\n",
            "Epoch 30/100, Train Loss: 0.0361, Val Loss: 0.0428\n",
            "Epoch 31/100, Train Loss: 0.0359, Val Loss: 0.0428\n",
            "Epoch 32/100, Train Loss: 0.0350, Val Loss: 0.0428\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0383, Val Loss: 0.0428\n",
            "Epoch 34/100, Train Loss: 0.0364, Val Loss: 0.0428\n",
            "Epoch 35/100, Train Loss: 0.0370, Val Loss: 0.0428\n",
            "Epoch 36/100, Train Loss: 0.0371, Val Loss: 0.0428\n",
            "Epoch 37/100, Train Loss: 0.0364, Val Loss: 0.0428\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0362, Val Loss: 0.0428\n",
            "Epoch 39/100, Train Loss: 0.0365, Val Loss: 0.0428\n",
            "Epoch 40/100, Train Loss: 0.0358, Val Loss: 0.0428\n",
            "Epoch 41/100, Train Loss: 0.0365, Val Loss: 0.0428\n",
            "Epoch 42/100, Train Loss: 0.0352, Val Loss: 0.0428\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0371, Val Loss: 0.0428\n",
            "Epoch 44/100, Train Loss: 0.0380, Val Loss: 0.0428\n",
            "Epoch 45/100, Train Loss: 0.0379, Val Loss: 0.0428\n",
            "Epoch 46/100, Train Loss: 0.0370, Val Loss: 0.0428\n",
            "Epoch 47/100, Train Loss: 0.0356, Val Loss: 0.0428\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0372, Val Loss: 0.0428\n",
            "Epoch 49/100, Train Loss: 0.0353, Val Loss: 0.0428\n",
            "Epoch 50/100, Train Loss: 0.0349, Val Loss: 0.0428\n",
            "Epoch 51/100, Train Loss: 0.0362, Val Loss: 0.0428\n",
            "Epoch 52/100, Train Loss: 0.0375, Val Loss: 0.0428\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0368, Val Loss: 0.0428\n",
            "Epoch 54/100, Train Loss: 0.0384, Val Loss: 0.0428\n",
            "Epoch 55/100, Train Loss: 0.0358, Val Loss: 0.0428\n",
            "Epoch 56/100, Train Loss: 0.0356, Val Loss: 0.0428\n",
            "Epoch 57/100, Train Loss: 0.0365, Val Loss: 0.0428\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0377, Val Loss: 0.0428\n",
            "Epoch 59/100, Train Loss: 0.0362, Val Loss: 0.0428\n",
            "Epoch 60/100, Train Loss: 0.0367, Val Loss: 0.0428\n",
            "Epoch 61/100, Train Loss: 0.0363, Val Loss: 0.0428\n",
            "Epoch 62/100, Train Loss: 0.0361, Val Loss: 0.0428\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0359, Val Loss: 0.0428\n",
            "Epoch 64/100, Train Loss: 0.0366, Val Loss: 0.0428\n",
            "Epoch 65/100, Train Loss: 0.0384, Val Loss: 0.0428\n",
            "Epoch 66/100, Train Loss: 0.0370, Val Loss: 0.0428\n",
            "Epoch 67/100, Train Loss: 0.0369, Val Loss: 0.0428\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0368, Val Loss: 0.0428\n",
            "Epoch 69/100, Train Loss: 0.0368, Val Loss: 0.0428\n",
            "Epoch 70/100, Train Loss: 0.0377, Val Loss: 0.0428\n",
            "Epoch 71/100, Train Loss: 0.0366, Val Loss: 0.0428\n",
            "Epoch 72/100, Train Loss: 0.0373, Val Loss: 0.0428\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0380, Val Loss: 0.0428\n",
            "Epoch 74/100, Train Loss: 0.0371, Val Loss: 0.0428\n",
            "Epoch 75/100, Train Loss: 0.0353, Val Loss: 0.0428\n",
            "Epoch 76/100, Train Loss: 0.0365, Val Loss: 0.0428\n",
            "Epoch 77/100, Train Loss: 0.0372, Val Loss: 0.0428\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0371, Val Loss: 0.0428\n",
            "Epoch 79/100, Train Loss: 0.0380, Val Loss: 0.0428\n",
            "Epoch 80/100, Train Loss: 0.0381, Val Loss: 0.0428\n",
            "Epoch 81/100, Train Loss: 0.0360, Val Loss: 0.0428\n",
            "Epoch 82/100, Train Loss: 0.0363, Val Loss: 0.0428\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0359, Val Loss: 0.0428\n",
            "Epoch 84/100, Train Loss: 0.0352, Val Loss: 0.0428\n",
            "Epoch 85/100, Train Loss: 0.0351, Val Loss: 0.0428\n",
            "Epoch 86/100, Train Loss: 0.0364, Val Loss: 0.0428\n",
            "Epoch 87/100, Train Loss: 0.0371, Val Loss: 0.0428\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0358, Val Loss: 0.0428\n",
            "Epoch 89/100, Train Loss: 0.0363, Val Loss: 0.0428\n",
            "Epoch 90/100, Train Loss: 0.0368, Val Loss: 0.0428\n",
            "Epoch 91/100, Train Loss: 0.0357, Val Loss: 0.0428\n",
            "Epoch 92/100, Train Loss: 0.0371, Val Loss: 0.0428\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0407, Val Loss: 0.0428\n",
            "Epoch 94/100, Train Loss: 0.0377, Val Loss: 0.0428\n",
            "Epoch 95/100, Train Loss: 0.0372, Val Loss: 0.0428\n",
            "Epoch 96/100, Train Loss: 0.0365, Val Loss: 0.0428\n",
            "Epoch 97/100, Train Loss: 0.0360, Val Loss: 0.0428\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0392, Val Loss: 0.0428\n",
            "Epoch 99/100, Train Loss: 0.0351, Val Loss: 0.0428\n",
            "Epoch 100/100, Train Loss: 0.0368, Val Loss: 0.0428\n",
            "\n",
            "Test iteration 18/28\n",
            "Current training set size: 114 samples\n",
            "Epoch 1/100, Train Loss: 0.1608, Val Loss: 0.2180\n",
            "Epoch 2/100, Train Loss: 0.1451, Val Loss: 0.1866\n",
            "Epoch 3/100, Train Loss: 0.1412, Val Loss: 0.1659\n",
            "Epoch 4/100, Train Loss: 0.1350, Val Loss: 0.1519\n",
            "Epoch 5/100, Train Loss: 0.1081, Val Loss: 0.1300\n",
            "Epoch 6/100, Train Loss: 0.0994, Val Loss: 0.0364\n",
            "Epoch 7/100, Train Loss: 0.0716, Val Loss: 0.0793\n",
            "Epoch 8/100, Train Loss: 0.0762, Val Loss: 0.0686\n",
            "Epoch 9/100, Train Loss: 0.0770, Val Loss: 0.0591\n",
            "Epoch 10/100, Train Loss: 0.0556, Val Loss: 0.0390\n",
            "Epoch 11/100, Train Loss: 0.0497, Val Loss: 0.0418\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 12/100, Train Loss: 0.0482, Val Loss: 0.0407\n",
            "Epoch 13/100, Train Loss: 0.0444, Val Loss: 0.0385\n",
            "Epoch 14/100, Train Loss: 0.0498, Val Loss: 0.0379\n",
            "Epoch 15/100, Train Loss: 0.0453, Val Loss: 0.0363\n",
            "Epoch 16/100, Train Loss: 0.0434, Val Loss: 0.0356\n",
            "Epoch 17/100, Train Loss: 0.0437, Val Loss: 0.0347\n",
            "Epoch 18/100, Train Loss: 0.0431, Val Loss: 0.0336\n",
            "Epoch 19/100, Train Loss: 0.0425, Val Loss: 0.0339\n",
            "Epoch 20/100, Train Loss: 0.0432, Val Loss: 0.0339\n",
            "Epoch 21/100, Train Loss: 0.0416, Val Loss: 0.0346\n",
            "Epoch 22/100, Train Loss: 0.0420, Val Loss: 0.0345\n",
            "Epoch 23/100, Train Loss: 0.0410, Val Loss: 0.0345\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0415, Val Loss: 0.0345\n",
            "Epoch 25/100, Train Loss: 0.0412, Val Loss: 0.0345\n",
            "Epoch 26/100, Train Loss: 0.0424, Val Loss: 0.0345\n",
            "Epoch 27/100, Train Loss: 0.0414, Val Loss: 0.0345\n",
            "Epoch 28/100, Train Loss: 0.0418, Val Loss: 0.0345\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0432, Val Loss: 0.0345\n",
            "Epoch 30/100, Train Loss: 0.0412, Val Loss: 0.0345\n",
            "Epoch 31/100, Train Loss: 0.0428, Val Loss: 0.0345\n",
            "Epoch 32/100, Train Loss: 0.0416, Val Loss: 0.0345\n",
            "Epoch 33/100, Train Loss: 0.0424, Val Loss: 0.0345\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0428, Val Loss: 0.0345\n",
            "Epoch 35/100, Train Loss: 0.0416, Val Loss: 0.0345\n",
            "Epoch 36/100, Train Loss: 0.0411, Val Loss: 0.0345\n",
            "Epoch 37/100, Train Loss: 0.0418, Val Loss: 0.0345\n",
            "Epoch 38/100, Train Loss: 0.0423, Val Loss: 0.0345\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0430, Val Loss: 0.0345\n",
            "Epoch 40/100, Train Loss: 0.0410, Val Loss: 0.0345\n",
            "Epoch 41/100, Train Loss: 0.0421, Val Loss: 0.0345\n",
            "Epoch 42/100, Train Loss: 0.0438, Val Loss: 0.0345\n",
            "Epoch 43/100, Train Loss: 0.0424, Val Loss: 0.0345\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0417, Val Loss: 0.0345\n",
            "Epoch 45/100, Train Loss: 0.0409, Val Loss: 0.0345\n",
            "Epoch 46/100, Train Loss: 0.0416, Val Loss: 0.0345\n",
            "Epoch 47/100, Train Loss: 0.0416, Val Loss: 0.0345\n",
            "Epoch 48/100, Train Loss: 0.0415, Val Loss: 0.0345\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0422, Val Loss: 0.0345\n",
            "Epoch 50/100, Train Loss: 0.0411, Val Loss: 0.0345\n",
            "Epoch 51/100, Train Loss: 0.0407, Val Loss: 0.0345\n",
            "Epoch 52/100, Train Loss: 0.0414, Val Loss: 0.0345\n",
            "Epoch 53/100, Train Loss: 0.0478, Val Loss: 0.0345\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0426, Val Loss: 0.0345\n",
            "Epoch 55/100, Train Loss: 0.0410, Val Loss: 0.0345\n",
            "Epoch 56/100, Train Loss: 0.0418, Val Loss: 0.0345\n",
            "Epoch 57/100, Train Loss: 0.0410, Val Loss: 0.0345\n",
            "Epoch 58/100, Train Loss: 0.0415, Val Loss: 0.0345\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0421, Val Loss: 0.0345\n",
            "Epoch 60/100, Train Loss: 0.0411, Val Loss: 0.0345\n",
            "Epoch 61/100, Train Loss: 0.0425, Val Loss: 0.0345\n",
            "Epoch 62/100, Train Loss: 0.0424, Val Loss: 0.0345\n",
            "Epoch 63/100, Train Loss: 0.0420, Val Loss: 0.0345\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0412, Val Loss: 0.0345\n",
            "Epoch 65/100, Train Loss: 0.0424, Val Loss: 0.0345\n",
            "Epoch 66/100, Train Loss: 0.0406, Val Loss: 0.0345\n",
            "Epoch 67/100, Train Loss: 0.0427, Val Loss: 0.0345\n",
            "Epoch 68/100, Train Loss: 0.0424, Val Loss: 0.0345\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0418, Val Loss: 0.0345\n",
            "Epoch 70/100, Train Loss: 0.0409, Val Loss: 0.0345\n",
            "Epoch 71/100, Train Loss: 0.0413, Val Loss: 0.0345\n",
            "Epoch 72/100, Train Loss: 0.0429, Val Loss: 0.0345\n",
            "Epoch 73/100, Train Loss: 0.0420, Val Loss: 0.0345\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0421, Val Loss: 0.0345\n",
            "Epoch 75/100, Train Loss: 0.0409, Val Loss: 0.0345\n",
            "Epoch 76/100, Train Loss: 0.0440, Val Loss: 0.0345\n",
            "Epoch 77/100, Train Loss: 0.0412, Val Loss: 0.0345\n",
            "Epoch 78/100, Train Loss: 0.0421, Val Loss: 0.0345\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0415, Val Loss: 0.0345\n",
            "Epoch 80/100, Train Loss: 0.0412, Val Loss: 0.0345\n",
            "Epoch 81/100, Train Loss: 0.0409, Val Loss: 0.0345\n",
            "Epoch 82/100, Train Loss: 0.0427, Val Loss: 0.0345\n",
            "Epoch 83/100, Train Loss: 0.0419, Val Loss: 0.0345\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0412, Val Loss: 0.0345\n",
            "Epoch 85/100, Train Loss: 0.0415, Val Loss: 0.0345\n",
            "Epoch 86/100, Train Loss: 0.0414, Val Loss: 0.0345\n",
            "Epoch 87/100, Train Loss: 0.0402, Val Loss: 0.0345\n",
            "Epoch 88/100, Train Loss: 0.0416, Val Loss: 0.0345\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0453, Val Loss: 0.0345\n",
            "Epoch 90/100, Train Loss: 0.0414, Val Loss: 0.0345\n",
            "Epoch 91/100, Train Loss: 0.0423, Val Loss: 0.0345\n",
            "Epoch 92/100, Train Loss: 0.0419, Val Loss: 0.0345\n",
            "Epoch 93/100, Train Loss: 0.0420, Val Loss: 0.0345\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0420, Val Loss: 0.0345\n",
            "Epoch 95/100, Train Loss: 0.0413, Val Loss: 0.0345\n",
            "Epoch 96/100, Train Loss: 0.0421, Val Loss: 0.0345\n",
            "Epoch 97/100, Train Loss: 0.0412, Val Loss: 0.0345\n",
            "Epoch 98/100, Train Loss: 0.0428, Val Loss: 0.0345\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0425, Val Loss: 0.0345\n",
            "Epoch 100/100, Train Loss: 0.0427, Val Loss: 0.0345\n",
            "\n",
            "Test iteration 19/28\n",
            "Current training set size: 115 samples\n",
            "Epoch 1/100, Train Loss: 0.1574, Val Loss: 0.1906\n",
            "Epoch 2/100, Train Loss: 0.1493, Val Loss: 0.1997\n",
            "Epoch 3/100, Train Loss: 0.1348, Val Loss: 0.1719\n",
            "Epoch 4/100, Train Loss: 0.1105, Val Loss: 0.0866\n",
            "Epoch 5/100, Train Loss: 0.0900, Val Loss: 0.1483\n",
            "Epoch 6/100, Train Loss: 0.0905, Val Loss: 0.0400\n",
            "Epoch 7/100, Train Loss: 0.0804, Val Loss: 0.0480\n",
            "Epoch 8/100, Train Loss: 0.0718, Val Loss: 0.0529\n",
            "Epoch 9/100, Train Loss: 0.0661, Val Loss: 0.0477\n",
            "Epoch 10/100, Train Loss: 0.0687, Val Loss: 0.0438\n",
            "Epoch 11/100, Train Loss: 0.0611, Val Loss: 0.0410\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 12/100, Train Loss: 0.0575, Val Loss: 0.0385\n",
            "Epoch 13/100, Train Loss: 0.0536, Val Loss: 0.0367\n",
            "Epoch 14/100, Train Loss: 0.0529, Val Loss: 0.0364\n",
            "Epoch 15/100, Train Loss: 0.0508, Val Loss: 0.0367\n",
            "Epoch 16/100, Train Loss: 0.0507, Val Loss: 0.0360\n",
            "Epoch 17/100, Train Loss: 0.0508, Val Loss: 0.0362\n",
            "Epoch 18/100, Train Loss: 0.0499, Val Loss: 0.0365\n",
            "Epoch 19/100, Train Loss: 0.0511, Val Loss: 0.0366\n",
            "Epoch 20/100, Train Loss: 0.0512, Val Loss: 0.0368\n",
            "Epoch 21/100, Train Loss: 0.0509, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0502, Val Loss: 0.0370\n",
            "Epoch 23/100, Train Loss: 0.0497, Val Loss: 0.0370\n",
            "Epoch 24/100, Train Loss: 0.0501, Val Loss: 0.0369\n",
            "Epoch 25/100, Train Loss: 0.0517, Val Loss: 0.0370\n",
            "Epoch 26/100, Train Loss: 0.0513, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0511, Val Loss: 0.0369\n",
            "Epoch 28/100, Train Loss: 0.0494, Val Loss: 0.0369\n",
            "Epoch 29/100, Train Loss: 0.0498, Val Loss: 0.0369\n",
            "Epoch 30/100, Train Loss: 0.0503, Val Loss: 0.0369\n",
            "Epoch 31/100, Train Loss: 0.0507, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0501, Val Loss: 0.0369\n",
            "Epoch 33/100, Train Loss: 0.0512, Val Loss: 0.0369\n",
            "Epoch 34/100, Train Loss: 0.0501, Val Loss: 0.0369\n",
            "Epoch 35/100, Train Loss: 0.0513, Val Loss: 0.0369\n",
            "Epoch 36/100, Train Loss: 0.0494, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0503, Val Loss: 0.0369\n",
            "Epoch 38/100, Train Loss: 0.0500, Val Loss: 0.0369\n",
            "Epoch 39/100, Train Loss: 0.0504, Val Loss: 0.0369\n",
            "Epoch 40/100, Train Loss: 0.0507, Val Loss: 0.0369\n",
            "Epoch 41/100, Train Loss: 0.0494, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0505, Val Loss: 0.0369\n",
            "Epoch 43/100, Train Loss: 0.0489, Val Loss: 0.0369\n",
            "Epoch 44/100, Train Loss: 0.0504, Val Loss: 0.0369\n",
            "Epoch 45/100, Train Loss: 0.0498, Val Loss: 0.0369\n",
            "Epoch 46/100, Train Loss: 0.0503, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0510, Val Loss: 0.0369\n",
            "Epoch 48/100, Train Loss: 0.0505, Val Loss: 0.0369\n",
            "Epoch 49/100, Train Loss: 0.0499, Val Loss: 0.0369\n",
            "Epoch 50/100, Train Loss: 0.0503, Val Loss: 0.0369\n",
            "Epoch 51/100, Train Loss: 0.0500, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0507, Val Loss: 0.0369\n",
            "Epoch 53/100, Train Loss: 0.0503, Val Loss: 0.0369\n",
            "Epoch 54/100, Train Loss: 0.0507, Val Loss: 0.0369\n",
            "Epoch 55/100, Train Loss: 0.0507, Val Loss: 0.0369\n",
            "Epoch 56/100, Train Loss: 0.0508, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0503, Val Loss: 0.0369\n",
            "Epoch 58/100, Train Loss: 0.0494, Val Loss: 0.0369\n",
            "Epoch 59/100, Train Loss: 0.0514, Val Loss: 0.0369\n",
            "Epoch 60/100, Train Loss: 0.0497, Val Loss: 0.0369\n",
            "Epoch 61/100, Train Loss: 0.0499, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0508, Val Loss: 0.0369\n",
            "Epoch 63/100, Train Loss: 0.0493, Val Loss: 0.0369\n",
            "Epoch 64/100, Train Loss: 0.0505, Val Loss: 0.0369\n",
            "Epoch 65/100, Train Loss: 0.0503, Val Loss: 0.0369\n",
            "Epoch 66/100, Train Loss: 0.0505, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0496, Val Loss: 0.0369\n",
            "Epoch 68/100, Train Loss: 0.0498, Val Loss: 0.0369\n",
            "Epoch 69/100, Train Loss: 0.0504, Val Loss: 0.0369\n",
            "Epoch 70/100, Train Loss: 0.0499, Val Loss: 0.0369\n",
            "Epoch 71/100, Train Loss: 0.0504, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0503, Val Loss: 0.0369\n",
            "Epoch 73/100, Train Loss: 0.0512, Val Loss: 0.0369\n",
            "Epoch 74/100, Train Loss: 0.0501, Val Loss: 0.0369\n",
            "Epoch 75/100, Train Loss: 0.0495, Val Loss: 0.0369\n",
            "Epoch 76/100, Train Loss: 0.0503, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0488, Val Loss: 0.0369\n",
            "Epoch 78/100, Train Loss: 0.0512, Val Loss: 0.0369\n",
            "Epoch 79/100, Train Loss: 0.0500, Val Loss: 0.0369\n",
            "Epoch 80/100, Train Loss: 0.0505, Val Loss: 0.0369\n",
            "Epoch 81/100, Train Loss: 0.0500, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0506, Val Loss: 0.0369\n",
            "Epoch 83/100, Train Loss: 0.0517, Val Loss: 0.0369\n",
            "Epoch 84/100, Train Loss: 0.0509, Val Loss: 0.0369\n",
            "Epoch 85/100, Train Loss: 0.0520, Val Loss: 0.0369\n",
            "Epoch 86/100, Train Loss: 0.0517, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0494, Val Loss: 0.0369\n",
            "Epoch 88/100, Train Loss: 0.0510, Val Loss: 0.0369\n",
            "Epoch 89/100, Train Loss: 0.0504, Val Loss: 0.0369\n",
            "Epoch 90/100, Train Loss: 0.0498, Val Loss: 0.0369\n",
            "Epoch 91/100, Train Loss: 0.0497, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0496, Val Loss: 0.0369\n",
            "Epoch 93/100, Train Loss: 0.0512, Val Loss: 0.0369\n",
            "Epoch 94/100, Train Loss: 0.0507, Val Loss: 0.0369\n",
            "Epoch 95/100, Train Loss: 0.0497, Val Loss: 0.0369\n",
            "Epoch 96/100, Train Loss: 0.0525, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0501, Val Loss: 0.0369\n",
            "Epoch 98/100, Train Loss: 0.0511, Val Loss: 0.0369\n",
            "Epoch 99/100, Train Loss: 0.0494, Val Loss: 0.0369\n",
            "Epoch 100/100, Train Loss: 0.0500, Val Loss: 0.0369\n",
            "\n",
            "Test iteration 20/28\n",
            "Current training set size: 116 samples\n",
            "Epoch 1/100, Train Loss: 0.1546, Val Loss: 0.1936\n",
            "Epoch 2/100, Train Loss: 0.1414, Val Loss: 0.1736\n",
            "Epoch 3/100, Train Loss: 0.1333, Val Loss: 0.1610\n",
            "Epoch 4/100, Train Loss: 0.1285, Val Loss: 0.1405\n",
            "Epoch 5/100, Train Loss: 0.1159, Val Loss: 0.0418\n",
            "Epoch 6/100, Train Loss: 0.0889, Val Loss: 0.0333\n",
            "Epoch 7/100, Train Loss: 0.0846, Val Loss: 0.0574\n",
            "Epoch 8/100, Train Loss: 0.0735, Val Loss: 0.0472\n",
            "Epoch 9/100, Train Loss: 0.0740, Val Loss: 0.0630\n",
            "Epoch 10/100, Train Loss: 0.0728, Val Loss: 0.0486\n",
            "Epoch 11/100, Train Loss: 0.0738, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 12/100, Train Loss: 0.0536, Val Loss: 0.0373\n",
            "Epoch 13/100, Train Loss: 0.0534, Val Loss: 0.0373\n",
            "Epoch 14/100, Train Loss: 0.0523, Val Loss: 0.0372\n",
            "Epoch 15/100, Train Loss: 0.0533, Val Loss: 0.0373\n",
            "Epoch 16/100, Train Loss: 0.0523, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0535, Val Loss: 0.0374\n",
            "Epoch 18/100, Train Loss: 0.0521, Val Loss: 0.0374\n",
            "Epoch 19/100, Train Loss: 0.0521, Val Loss: 0.0374\n",
            "Epoch 20/100, Train Loss: 0.0532, Val Loss: 0.0374\n",
            "Epoch 21/100, Train Loss: 0.0530, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0532, Val Loss: 0.0374\n",
            "Epoch 23/100, Train Loss: 0.0536, Val Loss: 0.0374\n",
            "Epoch 24/100, Train Loss: 0.0536, Val Loss: 0.0374\n",
            "Epoch 25/100, Train Loss: 0.0528, Val Loss: 0.0374\n",
            "Epoch 26/100, Train Loss: 0.0533, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0528, Val Loss: 0.0374\n",
            "Epoch 28/100, Train Loss: 0.0525, Val Loss: 0.0374\n",
            "Epoch 29/100, Train Loss: 0.0526, Val Loss: 0.0374\n",
            "Epoch 30/100, Train Loss: 0.0521, Val Loss: 0.0374\n",
            "Epoch 31/100, Train Loss: 0.0529, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0526, Val Loss: 0.0374\n",
            "Epoch 33/100, Train Loss: 0.0536, Val Loss: 0.0374\n",
            "Epoch 34/100, Train Loss: 0.0529, Val Loss: 0.0374\n",
            "Epoch 35/100, Train Loss: 0.0514, Val Loss: 0.0374\n",
            "Epoch 36/100, Train Loss: 0.0527, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0522, Val Loss: 0.0374\n",
            "Epoch 38/100, Train Loss: 0.0529, Val Loss: 0.0374\n",
            "Epoch 39/100, Train Loss: 0.0524, Val Loss: 0.0374\n",
            "Epoch 40/100, Train Loss: 0.0533, Val Loss: 0.0374\n",
            "Epoch 41/100, Train Loss: 0.0531, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0532, Val Loss: 0.0374\n",
            "Epoch 43/100, Train Loss: 0.0527, Val Loss: 0.0374\n",
            "Epoch 44/100, Train Loss: 0.0521, Val Loss: 0.0374\n",
            "Epoch 45/100, Train Loss: 0.0529, Val Loss: 0.0374\n",
            "Epoch 46/100, Train Loss: 0.0526, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0540, Val Loss: 0.0374\n",
            "Epoch 48/100, Train Loss: 0.0522, Val Loss: 0.0374\n",
            "Epoch 49/100, Train Loss: 0.0532, Val Loss: 0.0374\n",
            "Epoch 50/100, Train Loss: 0.0520, Val Loss: 0.0374\n",
            "Epoch 51/100, Train Loss: 0.0530, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0529, Val Loss: 0.0374\n",
            "Epoch 53/100, Train Loss: 0.0532, Val Loss: 0.0374\n",
            "Epoch 54/100, Train Loss: 0.0527, Val Loss: 0.0374\n",
            "Epoch 55/100, Train Loss: 0.0511, Val Loss: 0.0374\n",
            "Epoch 56/100, Train Loss: 0.0523, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0530, Val Loss: 0.0374\n",
            "Epoch 58/100, Train Loss: 0.0544, Val Loss: 0.0374\n",
            "Epoch 59/100, Train Loss: 0.0526, Val Loss: 0.0374\n",
            "Epoch 60/100, Train Loss: 0.0524, Val Loss: 0.0374\n",
            "Epoch 61/100, Train Loss: 0.0531, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0526, Val Loss: 0.0374\n",
            "Epoch 63/100, Train Loss: 0.0512, Val Loss: 0.0374\n",
            "Epoch 64/100, Train Loss: 0.0521, Val Loss: 0.0374\n",
            "Epoch 65/100, Train Loss: 0.0531, Val Loss: 0.0374\n",
            "Epoch 66/100, Train Loss: 0.0532, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0537, Val Loss: 0.0374\n",
            "Epoch 68/100, Train Loss: 0.0539, Val Loss: 0.0374\n",
            "Epoch 69/100, Train Loss: 0.0523, Val Loss: 0.0374\n",
            "Epoch 70/100, Train Loss: 0.0532, Val Loss: 0.0374\n",
            "Epoch 71/100, Train Loss: 0.0530, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0527, Val Loss: 0.0374\n",
            "Epoch 73/100, Train Loss: 0.0519, Val Loss: 0.0374\n",
            "Epoch 74/100, Train Loss: 0.0531, Val Loss: 0.0374\n",
            "Epoch 75/100, Train Loss: 0.0524, Val Loss: 0.0374\n",
            "Epoch 76/100, Train Loss: 0.0520, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0543, Val Loss: 0.0374\n",
            "Epoch 78/100, Train Loss: 0.0521, Val Loss: 0.0374\n",
            "Epoch 79/100, Train Loss: 0.0525, Val Loss: 0.0374\n",
            "Epoch 80/100, Train Loss: 0.0533, Val Loss: 0.0374\n",
            "Epoch 81/100, Train Loss: 0.0535, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0521, Val Loss: 0.0374\n",
            "Epoch 83/100, Train Loss: 0.0532, Val Loss: 0.0374\n",
            "Epoch 84/100, Train Loss: 0.0543, Val Loss: 0.0374\n",
            "Epoch 85/100, Train Loss: 0.0523, Val Loss: 0.0374\n",
            "Epoch 86/100, Train Loss: 0.0525, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0522, Val Loss: 0.0374\n",
            "Epoch 88/100, Train Loss: 0.0530, Val Loss: 0.0374\n",
            "Epoch 89/100, Train Loss: 0.0520, Val Loss: 0.0374\n",
            "Epoch 90/100, Train Loss: 0.0529, Val Loss: 0.0374\n",
            "Epoch 91/100, Train Loss: 0.0538, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0525, Val Loss: 0.0374\n",
            "Epoch 93/100, Train Loss: 0.0521, Val Loss: 0.0374\n",
            "Epoch 94/100, Train Loss: 0.0535, Val Loss: 0.0374\n",
            "Epoch 95/100, Train Loss: 0.0517, Val Loss: 0.0374\n",
            "Epoch 96/100, Train Loss: 0.0518, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0529, Val Loss: 0.0374\n",
            "Epoch 98/100, Train Loss: 0.0529, Val Loss: 0.0374\n",
            "Epoch 99/100, Train Loss: 0.0528, Val Loss: 0.0374\n",
            "Epoch 100/100, Train Loss: 0.0521, Val Loss: 0.0374\n",
            "\n",
            "Test iteration 21/28\n",
            "Current training set size: 117 samples\n",
            "Epoch 1/100, Train Loss: 0.1580, Val Loss: 0.1894\n",
            "Epoch 2/100, Train Loss: 0.1462, Val Loss: 0.1769\n",
            "Epoch 3/100, Train Loss: 0.1513, Val Loss: 0.1768\n",
            "Epoch 4/100, Train Loss: 0.1325, Val Loss: 0.1858\n",
            "Epoch 5/100, Train Loss: 0.1304, Val Loss: 0.1279\n",
            "Epoch 6/100, Train Loss: 0.0916, Val Loss: 0.0742\n",
            "Epoch 7/100, Train Loss: 0.0751, Val Loss: 0.0685\n",
            "Epoch 8/100, Train Loss: 0.0721, Val Loss: 0.0538\n",
            "Epoch 9/100, Train Loss: 0.0617, Val Loss: 0.0411\n",
            "Epoch 10/100, Train Loss: 0.0572, Val Loss: 0.0400\n",
            "Epoch 11/100, Train Loss: 0.0567, Val Loss: 0.0720\n",
            "Epoch 12/100, Train Loss: 0.0558, Val Loss: 0.0436\n",
            "Epoch 13/100, Train Loss: 0.0551, Val Loss: 0.0490\n",
            "Epoch 14/100, Train Loss: 0.0483, Val Loss: 0.0456\n",
            "Epoch 15/100, Train Loss: 0.0561, Val Loss: 0.0410\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0428, Val Loss: 0.0409\n",
            "Epoch 17/100, Train Loss: 0.0436, Val Loss: 0.0406\n",
            "Epoch 18/100, Train Loss: 0.0414, Val Loss: 0.0405\n",
            "Epoch 19/100, Train Loss: 0.0413, Val Loss: 0.0402\n",
            "Epoch 20/100, Train Loss: 0.0421, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0414, Val Loss: 0.0400\n",
            "Epoch 22/100, Train Loss: 0.0416, Val Loss: 0.0400\n",
            "Epoch 23/100, Train Loss: 0.0421, Val Loss: 0.0400\n",
            "Epoch 24/100, Train Loss: 0.0424, Val Loss: 0.0400\n",
            "Epoch 25/100, Train Loss: 0.0409, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0423, Val Loss: 0.0400\n",
            "Epoch 27/100, Train Loss: 0.0420, Val Loss: 0.0400\n",
            "Epoch 28/100, Train Loss: 0.0413, Val Loss: 0.0400\n",
            "Epoch 29/100, Train Loss: 0.0406, Val Loss: 0.0400\n",
            "Epoch 30/100, Train Loss: 0.0420, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0423, Val Loss: 0.0400\n",
            "Epoch 32/100, Train Loss: 0.0428, Val Loss: 0.0400\n",
            "Epoch 33/100, Train Loss: 0.0418, Val Loss: 0.0400\n",
            "Epoch 34/100, Train Loss: 0.0416, Val Loss: 0.0400\n",
            "Epoch 35/100, Train Loss: 0.0401, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0414, Val Loss: 0.0400\n",
            "Epoch 37/100, Train Loss: 0.0420, Val Loss: 0.0400\n",
            "Epoch 38/100, Train Loss: 0.0425, Val Loss: 0.0400\n",
            "Epoch 39/100, Train Loss: 0.0422, Val Loss: 0.0400\n",
            "Epoch 40/100, Train Loss: 0.0412, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0418, Val Loss: 0.0400\n",
            "Epoch 42/100, Train Loss: 0.0416, Val Loss: 0.0400\n",
            "Epoch 43/100, Train Loss: 0.0413, Val Loss: 0.0400\n",
            "Epoch 44/100, Train Loss: 0.0411, Val Loss: 0.0400\n",
            "Epoch 45/100, Train Loss: 0.0417, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0465, Val Loss: 0.0400\n",
            "Epoch 47/100, Train Loss: 0.0443, Val Loss: 0.0400\n",
            "Epoch 48/100, Train Loss: 0.0424, Val Loss: 0.0400\n",
            "Epoch 49/100, Train Loss: 0.0417, Val Loss: 0.0400\n",
            "Epoch 50/100, Train Loss: 0.0440, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0404, Val Loss: 0.0400\n",
            "Epoch 52/100, Train Loss: 0.0404, Val Loss: 0.0400\n",
            "Epoch 53/100, Train Loss: 0.0483, Val Loss: 0.0400\n",
            "Epoch 54/100, Train Loss: 0.0417, Val Loss: 0.0400\n",
            "Epoch 55/100, Train Loss: 0.0415, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0478, Val Loss: 0.0400\n",
            "Epoch 57/100, Train Loss: 0.0409, Val Loss: 0.0400\n",
            "Epoch 58/100, Train Loss: 0.0416, Val Loss: 0.0400\n",
            "Epoch 59/100, Train Loss: 0.0408, Val Loss: 0.0400\n",
            "Epoch 60/100, Train Loss: 0.0413, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0419, Val Loss: 0.0400\n",
            "Epoch 62/100, Train Loss: 0.0413, Val Loss: 0.0400\n",
            "Epoch 63/100, Train Loss: 0.0408, Val Loss: 0.0400\n",
            "Epoch 64/100, Train Loss: 0.0409, Val Loss: 0.0400\n",
            "Epoch 65/100, Train Loss: 0.0431, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0413, Val Loss: 0.0400\n",
            "Epoch 67/100, Train Loss: 0.0419, Val Loss: 0.0400\n",
            "Epoch 68/100, Train Loss: 0.0435, Val Loss: 0.0400\n",
            "Epoch 69/100, Train Loss: 0.0431, Val Loss: 0.0400\n",
            "Epoch 70/100, Train Loss: 0.0411, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0431, Val Loss: 0.0400\n",
            "Epoch 72/100, Train Loss: 0.0419, Val Loss: 0.0400\n",
            "Epoch 73/100, Train Loss: 0.0422, Val Loss: 0.0400\n",
            "Epoch 74/100, Train Loss: 0.0423, Val Loss: 0.0400\n",
            "Epoch 75/100, Train Loss: 0.0458, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0424, Val Loss: 0.0400\n",
            "Epoch 77/100, Train Loss: 0.0418, Val Loss: 0.0400\n",
            "Epoch 78/100, Train Loss: 0.0451, Val Loss: 0.0400\n",
            "Epoch 79/100, Train Loss: 0.0426, Val Loss: 0.0400\n",
            "Epoch 80/100, Train Loss: 0.0404, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0417, Val Loss: 0.0400\n",
            "Epoch 82/100, Train Loss: 0.0424, Val Loss: 0.0400\n",
            "Epoch 83/100, Train Loss: 0.0417, Val Loss: 0.0400\n",
            "Epoch 84/100, Train Loss: 0.0426, Val Loss: 0.0400\n",
            "Epoch 85/100, Train Loss: 0.0419, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0423, Val Loss: 0.0400\n",
            "Epoch 87/100, Train Loss: 0.0434, Val Loss: 0.0400\n",
            "Epoch 88/100, Train Loss: 0.0420, Val Loss: 0.0400\n",
            "Epoch 89/100, Train Loss: 0.0415, Val Loss: 0.0400\n",
            "Epoch 90/100, Train Loss: 0.0427, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0424, Val Loss: 0.0400\n",
            "Epoch 92/100, Train Loss: 0.0429, Val Loss: 0.0400\n",
            "Epoch 93/100, Train Loss: 0.0415, Val Loss: 0.0400\n",
            "Epoch 94/100, Train Loss: 0.0415, Val Loss: 0.0400\n",
            "Epoch 95/100, Train Loss: 0.0424, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0404, Val Loss: 0.0400\n",
            "Epoch 97/100, Train Loss: 0.0416, Val Loss: 0.0400\n",
            "Epoch 98/100, Train Loss: 0.0416, Val Loss: 0.0400\n",
            "Epoch 99/100, Train Loss: 0.0411, Val Loss: 0.0400\n",
            "Epoch 100/100, Train Loss: 0.0432, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 22/28\n",
            "Current training set size: 118 samples\n",
            "Epoch 1/100, Train Loss: 0.1505, Val Loss: 0.1879\n",
            "Epoch 2/100, Train Loss: 0.1516, Val Loss: 0.1857\n",
            "Epoch 3/100, Train Loss: 0.1394, Val Loss: 0.1922\n",
            "Epoch 4/100, Train Loss: 0.1462, Val Loss: 0.1716\n",
            "Epoch 5/100, Train Loss: 0.1308, Val Loss: 0.1311\n",
            "Epoch 6/100, Train Loss: 0.0928, Val Loss: 0.0533\n",
            "Epoch 7/100, Train Loss: 0.0768, Val Loss: 0.0662\n",
            "Epoch 8/100, Train Loss: 0.0739, Val Loss: 0.0743\n",
            "Epoch 9/100, Train Loss: 0.0651, Val Loss: 0.0378\n",
            "Epoch 10/100, Train Loss: 0.0564, Val Loss: 0.0361\n",
            "Epoch 11/100, Train Loss: 0.0612, Val Loss: 0.0940\n",
            "Epoch 12/100, Train Loss: 0.0692, Val Loss: 0.0363\n",
            "Epoch 13/100, Train Loss: 0.0528, Val Loss: 0.0360\n",
            "Epoch 14/100, Train Loss: 0.0505, Val Loss: 0.0438\n",
            "Epoch 15/100, Train Loss: 0.0544, Val Loss: 0.0516\n",
            "Epoch 16/100, Train Loss: 0.0462, Val Loss: 0.0422\n",
            "Epoch 17/100, Train Loss: 0.0518, Val Loss: 0.0429\n",
            "Epoch 18/100, Train Loss: 0.0446, Val Loss: 0.0430\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0394, Val Loss: 0.0430\n",
            "Epoch 20/100, Train Loss: 0.0396, Val Loss: 0.0440\n",
            "Epoch 21/100, Train Loss: 0.0398, Val Loss: 0.0440\n",
            "Epoch 22/100, Train Loss: 0.0387, Val Loss: 0.0440\n",
            "Epoch 23/100, Train Loss: 0.0390, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0372, Val Loss: 0.0435\n",
            "Epoch 25/100, Train Loss: 0.0392, Val Loss: 0.0435\n",
            "Epoch 26/100, Train Loss: 0.0385, Val Loss: 0.0435\n",
            "Epoch 27/100, Train Loss: 0.0427, Val Loss: 0.0435\n",
            "Epoch 28/100, Train Loss: 0.0375, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0386, Val Loss: 0.0435\n",
            "Epoch 30/100, Train Loss: 0.0391, Val Loss: 0.0435\n",
            "Epoch 31/100, Train Loss: 0.0391, Val Loss: 0.0435\n",
            "Epoch 32/100, Train Loss: 0.0376, Val Loss: 0.0435\n",
            "Epoch 33/100, Train Loss: 0.0374, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0396, Val Loss: 0.0435\n",
            "Epoch 35/100, Train Loss: 0.0374, Val Loss: 0.0435\n",
            "Epoch 36/100, Train Loss: 0.0385, Val Loss: 0.0435\n",
            "Epoch 37/100, Train Loss: 0.0383, Val Loss: 0.0435\n",
            "Epoch 38/100, Train Loss: 0.0397, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0394, Val Loss: 0.0435\n",
            "Epoch 40/100, Train Loss: 0.0379, Val Loss: 0.0435\n",
            "Epoch 41/100, Train Loss: 0.0387, Val Loss: 0.0435\n",
            "Epoch 42/100, Train Loss: 0.0383, Val Loss: 0.0435\n",
            "Epoch 43/100, Train Loss: 0.0383, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0385, Val Loss: 0.0435\n",
            "Epoch 45/100, Train Loss: 0.0391, Val Loss: 0.0435\n",
            "Epoch 46/100, Train Loss: 0.0377, Val Loss: 0.0435\n",
            "Epoch 47/100, Train Loss: 0.0381, Val Loss: 0.0435\n",
            "Epoch 48/100, Train Loss: 0.0389, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0376, Val Loss: 0.0435\n",
            "Epoch 50/100, Train Loss: 0.0385, Val Loss: 0.0435\n",
            "Epoch 51/100, Train Loss: 0.0380, Val Loss: 0.0435\n",
            "Epoch 52/100, Train Loss: 0.0391, Val Loss: 0.0435\n",
            "Epoch 53/100, Train Loss: 0.0390, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0373, Val Loss: 0.0435\n",
            "Epoch 55/100, Train Loss: 0.0384, Val Loss: 0.0435\n",
            "Epoch 56/100, Train Loss: 0.0419, Val Loss: 0.0435\n",
            "Epoch 57/100, Train Loss: 0.0387, Val Loss: 0.0435\n",
            "Epoch 58/100, Train Loss: 0.0379, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0391, Val Loss: 0.0435\n",
            "Epoch 60/100, Train Loss: 0.0394, Val Loss: 0.0435\n",
            "Epoch 61/100, Train Loss: 0.0369, Val Loss: 0.0435\n",
            "Epoch 62/100, Train Loss: 0.0384, Val Loss: 0.0435\n",
            "Epoch 63/100, Train Loss: 0.0386, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0385, Val Loss: 0.0435\n",
            "Epoch 65/100, Train Loss: 0.0382, Val Loss: 0.0435\n",
            "Epoch 66/100, Train Loss: 0.0376, Val Loss: 0.0435\n",
            "Epoch 67/100, Train Loss: 0.0374, Val Loss: 0.0435\n",
            "Epoch 68/100, Train Loss: 0.0380, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0392, Val Loss: 0.0435\n",
            "Epoch 70/100, Train Loss: 0.0387, Val Loss: 0.0435\n",
            "Epoch 71/100, Train Loss: 0.0377, Val Loss: 0.0435\n",
            "Epoch 72/100, Train Loss: 0.0380, Val Loss: 0.0435\n",
            "Epoch 73/100, Train Loss: 0.0381, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0378, Val Loss: 0.0435\n",
            "Epoch 75/100, Train Loss: 0.0378, Val Loss: 0.0435\n",
            "Epoch 76/100, Train Loss: 0.0379, Val Loss: 0.0435\n",
            "Epoch 77/100, Train Loss: 0.0430, Val Loss: 0.0435\n",
            "Epoch 78/100, Train Loss: 0.0384, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0386, Val Loss: 0.0435\n",
            "Epoch 80/100, Train Loss: 0.0371, Val Loss: 0.0435\n",
            "Epoch 81/100, Train Loss: 0.0406, Val Loss: 0.0435\n",
            "Epoch 82/100, Train Loss: 0.0380, Val Loss: 0.0435\n",
            "Epoch 83/100, Train Loss: 0.0391, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0379, Val Loss: 0.0435\n",
            "Epoch 85/100, Train Loss: 0.0381, Val Loss: 0.0435\n",
            "Epoch 86/100, Train Loss: 0.0395, Val Loss: 0.0435\n",
            "Epoch 87/100, Train Loss: 0.0378, Val Loss: 0.0435\n",
            "Epoch 88/100, Train Loss: 0.0388, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0384, Val Loss: 0.0435\n",
            "Epoch 90/100, Train Loss: 0.0377, Val Loss: 0.0435\n",
            "Epoch 91/100, Train Loss: 0.0380, Val Loss: 0.0435\n",
            "Epoch 92/100, Train Loss: 0.0395, Val Loss: 0.0435\n",
            "Epoch 93/100, Train Loss: 0.0383, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0380, Val Loss: 0.0435\n",
            "Epoch 95/100, Train Loss: 0.0383, Val Loss: 0.0435\n",
            "Epoch 96/100, Train Loss: 0.0385, Val Loss: 0.0435\n",
            "Epoch 97/100, Train Loss: 0.0392, Val Loss: 0.0435\n",
            "Epoch 98/100, Train Loss: 0.0379, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0380, Val Loss: 0.0435\n",
            "Epoch 100/100, Train Loss: 0.0378, Val Loss: 0.0435\n",
            "\n",
            "Test iteration 23/28\n",
            "Current training set size: 119 samples\n",
            "Epoch 1/100, Train Loss: 0.1526, Val Loss: 0.2062\n",
            "Epoch 2/100, Train Loss: 0.1503, Val Loss: 0.1877\n",
            "Epoch 3/100, Train Loss: 0.1458, Val Loss: 0.1762\n",
            "Epoch 4/100, Train Loss: 0.1293, Val Loss: 0.0808\n",
            "Epoch 5/100, Train Loss: 0.0858, Val Loss: 0.0487\n",
            "Epoch 6/100, Train Loss: 0.0770, Val Loss: 0.0641\n",
            "Epoch 7/100, Train Loss: 0.0837, Val Loss: 0.0375\n",
            "Epoch 8/100, Train Loss: 0.0758, Val Loss: 0.0345\n",
            "Epoch 9/100, Train Loss: 0.0731, Val Loss: 0.0382\n",
            "Epoch 10/100, Train Loss: 0.0611, Val Loss: 0.0428\n",
            "Epoch 11/100, Train Loss: 0.0560, Val Loss: 0.0372\n",
            "Epoch 12/100, Train Loss: 0.0512, Val Loss: 0.0333\n",
            "Epoch 13/100, Train Loss: 0.0571, Val Loss: 0.0477\n",
            "Epoch 14/100, Train Loss: 0.0517, Val Loss: 0.0345\n",
            "Epoch 15/100, Train Loss: 0.0454, Val Loss: 0.0457\n",
            "Epoch 16/100, Train Loss: 0.0491, Val Loss: 0.0382\n",
            "Epoch 17/100, Train Loss: 0.0477, Val Loss: 0.0599\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0497, Val Loss: 0.0520\n",
            "Epoch 19/100, Train Loss: 0.0447, Val Loss: 0.0465\n",
            "Epoch 20/100, Train Loss: 0.0433, Val Loss: 0.0433\n",
            "Epoch 21/100, Train Loss: 0.0407, Val Loss: 0.0409\n",
            "Epoch 22/100, Train Loss: 0.0414, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0410, Val Loss: 0.0403\n",
            "Epoch 24/100, Train Loss: 0.0407, Val Loss: 0.0403\n",
            "Epoch 25/100, Train Loss: 0.0406, Val Loss: 0.0403\n",
            "Epoch 26/100, Train Loss: 0.0429, Val Loss: 0.0403\n",
            "Epoch 27/100, Train Loss: 0.0400, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0409, Val Loss: 0.0403\n",
            "Epoch 29/100, Train Loss: 0.0398, Val Loss: 0.0403\n",
            "Epoch 30/100, Train Loss: 0.0396, Val Loss: 0.0403\n",
            "Epoch 31/100, Train Loss: 0.0408, Val Loss: 0.0403\n",
            "Epoch 32/100, Train Loss: 0.0418, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0424, Val Loss: 0.0403\n",
            "Epoch 34/100, Train Loss: 0.0415, Val Loss: 0.0403\n",
            "Epoch 35/100, Train Loss: 0.0409, Val Loss: 0.0403\n",
            "Epoch 36/100, Train Loss: 0.0408, Val Loss: 0.0403\n",
            "Epoch 37/100, Train Loss: 0.0407, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0397, Val Loss: 0.0403\n",
            "Epoch 39/100, Train Loss: 0.0421, Val Loss: 0.0403\n",
            "Epoch 40/100, Train Loss: 0.0403, Val Loss: 0.0403\n",
            "Epoch 41/100, Train Loss: 0.0404, Val Loss: 0.0403\n",
            "Epoch 42/100, Train Loss: 0.0403, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0419, Val Loss: 0.0403\n",
            "Epoch 44/100, Train Loss: 0.0411, Val Loss: 0.0403\n",
            "Epoch 45/100, Train Loss: 0.0408, Val Loss: 0.0403\n",
            "Epoch 46/100, Train Loss: 0.0408, Val Loss: 0.0403\n",
            "Epoch 47/100, Train Loss: 0.0407, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0417, Val Loss: 0.0403\n",
            "Epoch 49/100, Train Loss: 0.0409, Val Loss: 0.0403\n",
            "Epoch 50/100, Train Loss: 0.0418, Val Loss: 0.0403\n",
            "Epoch 51/100, Train Loss: 0.0412, Val Loss: 0.0403\n",
            "Epoch 52/100, Train Loss: 0.0419, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0408, Val Loss: 0.0403\n",
            "Epoch 54/100, Train Loss: 0.0411, Val Loss: 0.0403\n",
            "Epoch 55/100, Train Loss: 0.0402, Val Loss: 0.0403\n",
            "Epoch 56/100, Train Loss: 0.0415, Val Loss: 0.0403\n",
            "Epoch 57/100, Train Loss: 0.0404, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0416, Val Loss: 0.0403\n",
            "Epoch 59/100, Train Loss: 0.0420, Val Loss: 0.0403\n",
            "Epoch 60/100, Train Loss: 0.0411, Val Loss: 0.0403\n",
            "Epoch 61/100, Train Loss: 0.0407, Val Loss: 0.0403\n",
            "Epoch 62/100, Train Loss: 0.0412, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0403, Val Loss: 0.0403\n",
            "Epoch 64/100, Train Loss: 0.0413, Val Loss: 0.0403\n",
            "Epoch 65/100, Train Loss: 0.0403, Val Loss: 0.0403\n",
            "Epoch 66/100, Train Loss: 0.0432, Val Loss: 0.0403\n",
            "Epoch 67/100, Train Loss: 0.0409, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0393, Val Loss: 0.0403\n",
            "Epoch 69/100, Train Loss: 0.0405, Val Loss: 0.0403\n",
            "Epoch 70/100, Train Loss: 0.0415, Val Loss: 0.0403\n",
            "Epoch 71/100, Train Loss: 0.0411, Val Loss: 0.0403\n",
            "Epoch 72/100, Train Loss: 0.0408, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0429, Val Loss: 0.0403\n",
            "Epoch 74/100, Train Loss: 0.0405, Val Loss: 0.0403\n",
            "Epoch 75/100, Train Loss: 0.0406, Val Loss: 0.0403\n",
            "Epoch 76/100, Train Loss: 0.0408, Val Loss: 0.0403\n",
            "Epoch 77/100, Train Loss: 0.0412, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0408, Val Loss: 0.0403\n",
            "Epoch 79/100, Train Loss: 0.0403, Val Loss: 0.0403\n",
            "Epoch 80/100, Train Loss: 0.0413, Val Loss: 0.0403\n",
            "Epoch 81/100, Train Loss: 0.0407, Val Loss: 0.0403\n",
            "Epoch 82/100, Train Loss: 0.0418, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0408, Val Loss: 0.0403\n",
            "Epoch 84/100, Train Loss: 0.0401, Val Loss: 0.0403\n",
            "Epoch 85/100, Train Loss: 0.0403, Val Loss: 0.0403\n",
            "Epoch 86/100, Train Loss: 0.0417, Val Loss: 0.0403\n",
            "Epoch 87/100, Train Loss: 0.0411, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0414, Val Loss: 0.0403\n",
            "Epoch 89/100, Train Loss: 0.0400, Val Loss: 0.0403\n",
            "Epoch 90/100, Train Loss: 0.0399, Val Loss: 0.0403\n",
            "Epoch 91/100, Train Loss: 0.0417, Val Loss: 0.0403\n",
            "Epoch 92/100, Train Loss: 0.0417, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0406, Val Loss: 0.0403\n",
            "Epoch 94/100, Train Loss: 0.0415, Val Loss: 0.0403\n",
            "Epoch 95/100, Train Loss: 0.0408, Val Loss: 0.0403\n",
            "Epoch 96/100, Train Loss: 0.0436, Val Loss: 0.0403\n",
            "Epoch 97/100, Train Loss: 0.0409, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0410, Val Loss: 0.0403\n",
            "Epoch 99/100, Train Loss: 0.0410, Val Loss: 0.0403\n",
            "Epoch 100/100, Train Loss: 0.0417, Val Loss: 0.0403\n",
            "\n",
            "Test iteration 24/28\n",
            "Current training set size: 120 samples\n",
            "Epoch 1/100, Train Loss: 0.1577, Val Loss: 0.1894\n",
            "Epoch 2/100, Train Loss: 0.1446, Val Loss: 0.1842\n",
            "Epoch 3/100, Train Loss: 0.1437, Val Loss: 0.1730\n",
            "Epoch 4/100, Train Loss: 0.1311, Val Loss: 0.1053\n",
            "Epoch 5/100, Train Loss: 0.1569, Val Loss: 0.1790\n",
            "Epoch 6/100, Train Loss: 0.1240, Val Loss: 0.1486\n",
            "Epoch 7/100, Train Loss: 0.1135, Val Loss: 0.1140\n",
            "Epoch 8/100, Train Loss: 0.0882, Val Loss: 0.0890\n",
            "Epoch 9/100, Train Loss: 0.0820, Val Loss: 0.0404\n",
            "Epoch 10/100, Train Loss: 0.0713, Val Loss: 0.0419\n",
            "Epoch 11/100, Train Loss: 0.0768, Val Loss: 0.0462\n",
            "Epoch 12/100, Train Loss: 0.0596, Val Loss: 0.0413\n",
            "Epoch 13/100, Train Loss: 0.0537, Val Loss: 0.0317\n",
            "Epoch 14/100, Train Loss: 0.0500, Val Loss: 0.0395\n",
            "Epoch 15/100, Train Loss: 0.0506, Val Loss: 0.0416\n",
            "Epoch 16/100, Train Loss: 0.0479, Val Loss: 0.0395\n",
            "Epoch 17/100, Train Loss: 0.0469, Val Loss: 0.0483\n",
            "Epoch 18/100, Train Loss: 0.0570, Val Loss: 0.0465\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0482, Val Loss: 0.0471\n",
            "Epoch 20/100, Train Loss: 0.0449, Val Loss: 0.0474\n",
            "Epoch 21/100, Train Loss: 0.0437, Val Loss: 0.0477\n",
            "Epoch 22/100, Train Loss: 0.0431, Val Loss: 0.0480\n",
            "Epoch 23/100, Train Loss: 0.0428, Val Loss: 0.0479\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0438, Val Loss: 0.0479\n",
            "Epoch 25/100, Train Loss: 0.0435, Val Loss: 0.0479\n",
            "Epoch 26/100, Train Loss: 0.0423, Val Loss: 0.0479\n",
            "Epoch 27/100, Train Loss: 0.0435, Val Loss: 0.0479\n",
            "Epoch 28/100, Train Loss: 0.0421, Val Loss: 0.0479\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0453, Val Loss: 0.0479\n",
            "Epoch 30/100, Train Loss: 0.0429, Val Loss: 0.0479\n",
            "Epoch 31/100, Train Loss: 0.0424, Val Loss: 0.0479\n",
            "Epoch 32/100, Train Loss: 0.0438, Val Loss: 0.0479\n",
            "Epoch 33/100, Train Loss: 0.0446, Val Loss: 0.0479\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0438, Val Loss: 0.0479\n",
            "Epoch 35/100, Train Loss: 0.0445, Val Loss: 0.0479\n",
            "Epoch 36/100, Train Loss: 0.0434, Val Loss: 0.0479\n",
            "Epoch 37/100, Train Loss: 0.0429, Val Loss: 0.0479\n",
            "Epoch 38/100, Train Loss: 0.0426, Val Loss: 0.0479\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0432, Val Loss: 0.0479\n",
            "Epoch 40/100, Train Loss: 0.0423, Val Loss: 0.0479\n",
            "Epoch 41/100, Train Loss: 0.0428, Val Loss: 0.0479\n",
            "Epoch 42/100, Train Loss: 0.0433, Val Loss: 0.0479\n",
            "Epoch 43/100, Train Loss: 0.0446, Val Loss: 0.0479\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0432, Val Loss: 0.0479\n",
            "Epoch 45/100, Train Loss: 0.0438, Val Loss: 0.0479\n",
            "Epoch 46/100, Train Loss: 0.0432, Val Loss: 0.0479\n",
            "Epoch 47/100, Train Loss: 0.0446, Val Loss: 0.0479\n",
            "Epoch 48/100, Train Loss: 0.0427, Val Loss: 0.0479\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0429, Val Loss: 0.0479\n",
            "Epoch 50/100, Train Loss: 0.0430, Val Loss: 0.0479\n",
            "Epoch 51/100, Train Loss: 0.0438, Val Loss: 0.0479\n",
            "Epoch 52/100, Train Loss: 0.0427, Val Loss: 0.0479\n",
            "Epoch 53/100, Train Loss: 0.0432, Val Loss: 0.0479\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0433, Val Loss: 0.0479\n",
            "Epoch 55/100, Train Loss: 0.0432, Val Loss: 0.0479\n",
            "Epoch 56/100, Train Loss: 0.0428, Val Loss: 0.0479\n",
            "Epoch 57/100, Train Loss: 0.0424, Val Loss: 0.0479\n",
            "Epoch 58/100, Train Loss: 0.0425, Val Loss: 0.0479\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0432, Val Loss: 0.0479\n",
            "Epoch 60/100, Train Loss: 0.0436, Val Loss: 0.0479\n",
            "Epoch 61/100, Train Loss: 0.0422, Val Loss: 0.0479\n",
            "Epoch 62/100, Train Loss: 0.0416, Val Loss: 0.0479\n",
            "Epoch 63/100, Train Loss: 0.0431, Val Loss: 0.0479\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0435, Val Loss: 0.0479\n",
            "Epoch 65/100, Train Loss: 0.0428, Val Loss: 0.0479\n",
            "Epoch 66/100, Train Loss: 0.0428, Val Loss: 0.0479\n",
            "Epoch 67/100, Train Loss: 0.0424, Val Loss: 0.0479\n",
            "Epoch 68/100, Train Loss: 0.0434, Val Loss: 0.0479\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0443, Val Loss: 0.0479\n",
            "Epoch 70/100, Train Loss: 0.0419, Val Loss: 0.0479\n",
            "Epoch 71/100, Train Loss: 0.0433, Val Loss: 0.0479\n",
            "Epoch 72/100, Train Loss: 0.0431, Val Loss: 0.0479\n",
            "Epoch 73/100, Train Loss: 0.0438, Val Loss: 0.0479\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0425, Val Loss: 0.0479\n",
            "Epoch 75/100, Train Loss: 0.0422, Val Loss: 0.0479\n",
            "Epoch 76/100, Train Loss: 0.0424, Val Loss: 0.0479\n",
            "Epoch 77/100, Train Loss: 0.0430, Val Loss: 0.0479\n",
            "Epoch 78/100, Train Loss: 0.0444, Val Loss: 0.0479\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0428, Val Loss: 0.0479\n",
            "Epoch 80/100, Train Loss: 0.0422, Val Loss: 0.0479\n",
            "Epoch 81/100, Train Loss: 0.0432, Val Loss: 0.0479\n",
            "Epoch 82/100, Train Loss: 0.0429, Val Loss: 0.0479\n",
            "Epoch 83/100, Train Loss: 0.0433, Val Loss: 0.0479\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0436, Val Loss: 0.0479\n",
            "Epoch 85/100, Train Loss: 0.0420, Val Loss: 0.0479\n",
            "Epoch 86/100, Train Loss: 0.0440, Val Loss: 0.0479\n",
            "Epoch 87/100, Train Loss: 0.0437, Val Loss: 0.0479\n",
            "Epoch 88/100, Train Loss: 0.0426, Val Loss: 0.0479\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0438, Val Loss: 0.0479\n",
            "Epoch 90/100, Train Loss: 0.0427, Val Loss: 0.0479\n",
            "Epoch 91/100, Train Loss: 0.0425, Val Loss: 0.0479\n",
            "Epoch 92/100, Train Loss: 0.0418, Val Loss: 0.0479\n",
            "Epoch 93/100, Train Loss: 0.0445, Val Loss: 0.0479\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0440, Val Loss: 0.0479\n",
            "Epoch 95/100, Train Loss: 0.0431, Val Loss: 0.0479\n",
            "Epoch 96/100, Train Loss: 0.0437, Val Loss: 0.0479\n",
            "Epoch 97/100, Train Loss: 0.0426, Val Loss: 0.0479\n",
            "Epoch 98/100, Train Loss: 0.0422, Val Loss: 0.0479\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0431, Val Loss: 0.0479\n",
            "Epoch 100/100, Train Loss: 0.0438, Val Loss: 0.0479\n",
            "\n",
            "Test iteration 25/28\n",
            "Current training set size: 121 samples\n",
            "Epoch 1/100, Train Loss: 0.1807, Val Loss: 0.1908\n",
            "Epoch 2/100, Train Loss: 0.1524, Val Loss: 0.1868\n",
            "Epoch 3/100, Train Loss: 0.1380, Val Loss: 0.1919\n",
            "Epoch 4/100, Train Loss: 0.1341, Val Loss: 0.1469\n",
            "Epoch 5/100, Train Loss: 0.1126, Val Loss: 0.0664\n",
            "Epoch 6/100, Train Loss: 0.0821, Val Loss: 0.0600\n",
            "Epoch 7/100, Train Loss: 0.0809, Val Loss: 0.0417\n",
            "Epoch 8/100, Train Loss: 0.0713, Val Loss: 0.0604\n",
            "Epoch 9/100, Train Loss: 0.0680, Val Loss: 0.0394\n",
            "Epoch 10/100, Train Loss: 0.0599, Val Loss: 0.0666\n",
            "Epoch 11/100, Train Loss: 0.0633, Val Loss: 0.0371\n",
            "Epoch 12/100, Train Loss: 0.0480, Val Loss: 0.0440\n",
            "Epoch 13/100, Train Loss: 0.0535, Val Loss: 0.0463\n",
            "Epoch 14/100, Train Loss: 0.0496, Val Loss: 0.0452\n",
            "Epoch 15/100, Train Loss: 0.0515, Val Loss: 0.0583\n",
            "Epoch 16/100, Train Loss: 0.0487, Val Loss: 0.0696\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0547, Val Loss: 0.0595\n",
            "Epoch 18/100, Train Loss: 0.0482, Val Loss: 0.0490\n",
            "Epoch 19/100, Train Loss: 0.0446, Val Loss: 0.0426\n",
            "Epoch 20/100, Train Loss: 0.0431, Val Loss: 0.0408\n",
            "Epoch 21/100, Train Loss: 0.0424, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0420, Val Loss: 0.0391\n",
            "Epoch 23/100, Train Loss: 0.0406, Val Loss: 0.0391\n",
            "Epoch 24/100, Train Loss: 0.0411, Val Loss: 0.0391\n",
            "Epoch 25/100, Train Loss: 0.0419, Val Loss: 0.0391\n",
            "Epoch 26/100, Train Loss: 0.0410, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0403, Val Loss: 0.0391\n",
            "Epoch 28/100, Train Loss: 0.0418, Val Loss: 0.0391\n",
            "Epoch 29/100, Train Loss: 0.0407, Val Loss: 0.0391\n",
            "Epoch 30/100, Train Loss: 0.0427, Val Loss: 0.0391\n",
            "Epoch 31/100, Train Loss: 0.0415, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0426, Val Loss: 0.0391\n",
            "Epoch 33/100, Train Loss: 0.0437, Val Loss: 0.0391\n",
            "Epoch 34/100, Train Loss: 0.0418, Val Loss: 0.0391\n",
            "Epoch 35/100, Train Loss: 0.0402, Val Loss: 0.0391\n",
            "Epoch 36/100, Train Loss: 0.0413, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0430, Val Loss: 0.0391\n",
            "Epoch 38/100, Train Loss: 0.0422, Val Loss: 0.0391\n",
            "Epoch 39/100, Train Loss: 0.0424, Val Loss: 0.0391\n",
            "Epoch 40/100, Train Loss: 0.0418, Val Loss: 0.0391\n",
            "Epoch 41/100, Train Loss: 0.0428, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0542, Val Loss: 0.0391\n",
            "Epoch 43/100, Train Loss: 0.0410, Val Loss: 0.0391\n",
            "Epoch 44/100, Train Loss: 0.0426, Val Loss: 0.0391\n",
            "Epoch 45/100, Train Loss: 0.0439, Val Loss: 0.0391\n",
            "Epoch 46/100, Train Loss: 0.0403, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0415, Val Loss: 0.0391\n",
            "Epoch 48/100, Train Loss: 0.0411, Val Loss: 0.0391\n",
            "Epoch 49/100, Train Loss: 0.0411, Val Loss: 0.0391\n",
            "Epoch 50/100, Train Loss: 0.0538, Val Loss: 0.0391\n",
            "Epoch 51/100, Train Loss: 0.0411, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0407, Val Loss: 0.0391\n",
            "Epoch 53/100, Train Loss: 0.0403, Val Loss: 0.0391\n",
            "Epoch 54/100, Train Loss: 0.0416, Val Loss: 0.0391\n",
            "Epoch 55/100, Train Loss: 0.0401, Val Loss: 0.0391\n",
            "Epoch 56/100, Train Loss: 0.0403, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0543, Val Loss: 0.0391\n",
            "Epoch 58/100, Train Loss: 0.0414, Val Loss: 0.0391\n",
            "Epoch 59/100, Train Loss: 0.0408, Val Loss: 0.0391\n",
            "Epoch 60/100, Train Loss: 0.0400, Val Loss: 0.0391\n",
            "Epoch 61/100, Train Loss: 0.0409, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0415, Val Loss: 0.0391\n",
            "Epoch 63/100, Train Loss: 0.0406, Val Loss: 0.0391\n",
            "Epoch 64/100, Train Loss: 0.0396, Val Loss: 0.0391\n",
            "Epoch 65/100, Train Loss: 0.0421, Val Loss: 0.0391\n",
            "Epoch 66/100, Train Loss: 0.0409, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0423, Val Loss: 0.0391\n",
            "Epoch 68/100, Train Loss: 0.0436, Val Loss: 0.0391\n",
            "Epoch 69/100, Train Loss: 0.0429, Val Loss: 0.0391\n",
            "Epoch 70/100, Train Loss: 0.0409, Val Loss: 0.0391\n",
            "Epoch 71/100, Train Loss: 0.0413, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0425, Val Loss: 0.0391\n",
            "Epoch 73/100, Train Loss: 0.0412, Val Loss: 0.0391\n",
            "Epoch 74/100, Train Loss: 0.0422, Val Loss: 0.0391\n",
            "Epoch 75/100, Train Loss: 0.0423, Val Loss: 0.0391\n",
            "Epoch 76/100, Train Loss: 0.0433, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0424, Val Loss: 0.0391\n",
            "Epoch 78/100, Train Loss: 0.0546, Val Loss: 0.0391\n",
            "Epoch 79/100, Train Loss: 0.0391, Val Loss: 0.0391\n",
            "Epoch 80/100, Train Loss: 0.0429, Val Loss: 0.0391\n",
            "Epoch 81/100, Train Loss: 0.0409, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0413, Val Loss: 0.0391\n",
            "Epoch 83/100, Train Loss: 0.0413, Val Loss: 0.0391\n",
            "Epoch 84/100, Train Loss: 0.0416, Val Loss: 0.0391\n",
            "Epoch 85/100, Train Loss: 0.0415, Val Loss: 0.0391\n",
            "Epoch 86/100, Train Loss: 0.0416, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0401, Val Loss: 0.0391\n",
            "Epoch 88/100, Train Loss: 0.0412, Val Loss: 0.0391\n",
            "Epoch 89/100, Train Loss: 0.0412, Val Loss: 0.0391\n",
            "Epoch 90/100, Train Loss: 0.0419, Val Loss: 0.0391\n",
            "Epoch 91/100, Train Loss: 0.0417, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0417, Val Loss: 0.0391\n",
            "Epoch 93/100, Train Loss: 0.0405, Val Loss: 0.0391\n",
            "Epoch 94/100, Train Loss: 0.0409, Val Loss: 0.0391\n",
            "Epoch 95/100, Train Loss: 0.0422, Val Loss: 0.0391\n",
            "Epoch 96/100, Train Loss: 0.0412, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0396, Val Loss: 0.0391\n",
            "Epoch 98/100, Train Loss: 0.0413, Val Loss: 0.0391\n",
            "Epoch 99/100, Train Loss: 0.0409, Val Loss: 0.0391\n",
            "Epoch 100/100, Train Loss: 0.0419, Val Loss: 0.0391\n",
            "\n",
            "Test iteration 26/28\n",
            "Current training set size: 122 samples\n",
            "Epoch 1/100, Train Loss: 0.1551, Val Loss: 0.2063\n",
            "Epoch 2/100, Train Loss: 0.1498, Val Loss: 0.1850\n",
            "Epoch 3/100, Train Loss: 0.1496, Val Loss: 0.1756\n",
            "Epoch 4/100, Train Loss: 0.1274, Val Loss: 0.1051\n",
            "Epoch 5/100, Train Loss: 0.1015, Val Loss: 0.0729\n",
            "Epoch 6/100, Train Loss: 0.0738, Val Loss: 0.0544\n",
            "Epoch 7/100, Train Loss: 0.0880, Val Loss: 0.0645\n",
            "Epoch 8/100, Train Loss: 0.0699, Val Loss: 0.0777\n",
            "Epoch 9/100, Train Loss: 0.0672, Val Loss: 0.0393\n",
            "Epoch 10/100, Train Loss: 0.0598, Val Loss: 0.0383\n",
            "Epoch 11/100, Train Loss: 0.0484, Val Loss: 0.0496\n",
            "Epoch 12/100, Train Loss: 0.0569, Val Loss: 0.0411\n",
            "Epoch 13/100, Train Loss: 0.0480, Val Loss: 0.0383\n",
            "Epoch 14/100, Train Loss: 0.0494, Val Loss: 0.0390\n",
            "Epoch 15/100, Train Loss: 0.0497, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0405, Val Loss: 0.0403\n",
            "Epoch 17/100, Train Loss: 0.0403, Val Loss: 0.0402\n",
            "Epoch 18/100, Train Loss: 0.0399, Val Loss: 0.0400\n",
            "Epoch 19/100, Train Loss: 0.0401, Val Loss: 0.0400\n",
            "Epoch 20/100, Train Loss: 0.0405, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0404, Val Loss: 0.0400\n",
            "Epoch 22/100, Train Loss: 0.0397, Val Loss: 0.0400\n",
            "Epoch 23/100, Train Loss: 0.0402, Val Loss: 0.0400\n",
            "Epoch 24/100, Train Loss: 0.0400, Val Loss: 0.0400\n",
            "Epoch 25/100, Train Loss: 0.0398, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0402, Val Loss: 0.0400\n",
            "Epoch 27/100, Train Loss: 0.0401, Val Loss: 0.0400\n",
            "Epoch 28/100, Train Loss: 0.0389, Val Loss: 0.0400\n",
            "Epoch 29/100, Train Loss: 0.0392, Val Loss: 0.0400\n",
            "Epoch 30/100, Train Loss: 0.0400, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0391, Val Loss: 0.0400\n",
            "Epoch 32/100, Train Loss: 0.0393, Val Loss: 0.0400\n",
            "Epoch 33/100, Train Loss: 0.0421, Val Loss: 0.0400\n",
            "Epoch 34/100, Train Loss: 0.0391, Val Loss: 0.0400\n",
            "Epoch 35/100, Train Loss: 0.0387, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0476, Val Loss: 0.0400\n",
            "Epoch 37/100, Train Loss: 0.0403, Val Loss: 0.0400\n",
            "Epoch 38/100, Train Loss: 0.0402, Val Loss: 0.0400\n",
            "Epoch 39/100, Train Loss: 0.0395, Val Loss: 0.0400\n",
            "Epoch 40/100, Train Loss: 0.0397, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0389, Val Loss: 0.0400\n",
            "Epoch 42/100, Train Loss: 0.0412, Val Loss: 0.0400\n",
            "Epoch 43/100, Train Loss: 0.0398, Val Loss: 0.0400\n",
            "Epoch 44/100, Train Loss: 0.0408, Val Loss: 0.0400\n",
            "Epoch 45/100, Train Loss: 0.0404, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0390, Val Loss: 0.0400\n",
            "Epoch 47/100, Train Loss: 0.0390, Val Loss: 0.0400\n",
            "Epoch 48/100, Train Loss: 0.0402, Val Loss: 0.0400\n",
            "Epoch 49/100, Train Loss: 0.0400, Val Loss: 0.0400\n",
            "Epoch 50/100, Train Loss: 0.0394, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0422, Val Loss: 0.0400\n",
            "Epoch 52/100, Train Loss: 0.0377, Val Loss: 0.0400\n",
            "Epoch 53/100, Train Loss: 0.0417, Val Loss: 0.0400\n",
            "Epoch 54/100, Train Loss: 0.0388, Val Loss: 0.0400\n",
            "Epoch 55/100, Train Loss: 0.0387, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0402, Val Loss: 0.0400\n",
            "Epoch 57/100, Train Loss: 0.0401, Val Loss: 0.0400\n",
            "Epoch 58/100, Train Loss: 0.0400, Val Loss: 0.0400\n",
            "Epoch 59/100, Train Loss: 0.0392, Val Loss: 0.0400\n",
            "Epoch 60/100, Train Loss: 0.0394, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0405, Val Loss: 0.0400\n",
            "Epoch 62/100, Train Loss: 0.0384, Val Loss: 0.0400\n",
            "Epoch 63/100, Train Loss: 0.0397, Val Loss: 0.0400\n",
            "Epoch 64/100, Train Loss: 0.0382, Val Loss: 0.0400\n",
            "Epoch 65/100, Train Loss: 0.0399, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0394, Val Loss: 0.0400\n",
            "Epoch 67/100, Train Loss: 0.0396, Val Loss: 0.0400\n",
            "Epoch 68/100, Train Loss: 0.0409, Val Loss: 0.0400\n",
            "Epoch 69/100, Train Loss: 0.0386, Val Loss: 0.0400\n",
            "Epoch 70/100, Train Loss: 0.0397, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0390, Val Loss: 0.0400\n",
            "Epoch 72/100, Train Loss: 0.0388, Val Loss: 0.0400\n",
            "Epoch 73/100, Train Loss: 0.0401, Val Loss: 0.0400\n",
            "Epoch 74/100, Train Loss: 0.0394, Val Loss: 0.0400\n",
            "Epoch 75/100, Train Loss: 0.0393, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0391, Val Loss: 0.0400\n",
            "Epoch 77/100, Train Loss: 0.0394, Val Loss: 0.0400\n",
            "Epoch 78/100, Train Loss: 0.0390, Val Loss: 0.0400\n",
            "Epoch 79/100, Train Loss: 0.0408, Val Loss: 0.0400\n",
            "Epoch 80/100, Train Loss: 0.0401, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0400, Val Loss: 0.0400\n",
            "Epoch 82/100, Train Loss: 0.0389, Val Loss: 0.0400\n",
            "Epoch 83/100, Train Loss: 0.0399, Val Loss: 0.0400\n",
            "Epoch 84/100, Train Loss: 0.0400, Val Loss: 0.0400\n",
            "Epoch 85/100, Train Loss: 0.0408, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0406, Val Loss: 0.0400\n",
            "Epoch 87/100, Train Loss: 0.0447, Val Loss: 0.0400\n",
            "Epoch 88/100, Train Loss: 0.0388, Val Loss: 0.0400\n",
            "Epoch 89/100, Train Loss: 0.0393, Val Loss: 0.0400\n",
            "Epoch 90/100, Train Loss: 0.0387, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0401, Val Loss: 0.0400\n",
            "Epoch 92/100, Train Loss: 0.0397, Val Loss: 0.0400\n",
            "Epoch 93/100, Train Loss: 0.0411, Val Loss: 0.0400\n",
            "Epoch 94/100, Train Loss: 0.0407, Val Loss: 0.0400\n",
            "Epoch 95/100, Train Loss: 0.0397, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0405, Val Loss: 0.0400\n",
            "Epoch 97/100, Train Loss: 0.0399, Val Loss: 0.0400\n",
            "Epoch 98/100, Train Loss: 0.0403, Val Loss: 0.0400\n",
            "Epoch 99/100, Train Loss: 0.0397, Val Loss: 0.0400\n",
            "Epoch 100/100, Train Loss: 0.0413, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 27/28\n",
            "Current training set size: 123 samples\n",
            "Epoch 1/100, Train Loss: 0.1492, Val Loss: 0.1864\n",
            "Epoch 2/100, Train Loss: 0.1470, Val Loss: 0.1903\n",
            "Epoch 3/100, Train Loss: 0.1421, Val Loss: 0.1697\n",
            "Epoch 4/100, Train Loss: 0.1264, Val Loss: 0.1386\n",
            "Epoch 5/100, Train Loss: 0.0920, Val Loss: 0.0768\n",
            "Epoch 6/100, Train Loss: 0.0808, Val Loss: 0.0662\n",
            "Epoch 7/100, Train Loss: 0.0726, Val Loss: 0.0505\n",
            "Epoch 8/100, Train Loss: 0.0735, Val Loss: 0.0354\n",
            "Epoch 9/100, Train Loss: 0.0607, Val Loss: 0.0505\n",
            "Epoch 10/100, Train Loss: 0.0597, Val Loss: 0.0736\n",
            "Epoch 11/100, Train Loss: 0.0621, Val Loss: 0.0430\n",
            "Epoch 12/100, Train Loss: 0.0513, Val Loss: 0.0368\n",
            "Epoch 13/100, Train Loss: 0.0455, Val Loss: 0.0396\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 14/100, Train Loss: 0.0429, Val Loss: 0.0384\n",
            "Epoch 15/100, Train Loss: 0.0399, Val Loss: 0.0376\n",
            "Epoch 16/100, Train Loss: 0.0409, Val Loss: 0.0373\n",
            "Epoch 17/100, Train Loss: 0.0396, Val Loss: 0.0372\n",
            "Epoch 18/100, Train Loss: 0.0412, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0401, Val Loss: 0.0374\n",
            "Epoch 20/100, Train Loss: 0.0403, Val Loss: 0.0374\n",
            "Epoch 21/100, Train Loss: 0.0397, Val Loss: 0.0374\n",
            "Epoch 22/100, Train Loss: 0.0403, Val Loss: 0.0373\n",
            "Epoch 23/100, Train Loss: 0.0394, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0402, Val Loss: 0.0374\n",
            "Epoch 25/100, Train Loss: 0.0412, Val Loss: 0.0374\n",
            "Epoch 26/100, Train Loss: 0.0418, Val Loss: 0.0374\n",
            "Epoch 27/100, Train Loss: 0.0407, Val Loss: 0.0374\n",
            "Epoch 28/100, Train Loss: 0.0404, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0403, Val Loss: 0.0374\n",
            "Epoch 30/100, Train Loss: 0.0406, Val Loss: 0.0374\n",
            "Epoch 31/100, Train Loss: 0.0412, Val Loss: 0.0374\n",
            "Epoch 32/100, Train Loss: 0.0406, Val Loss: 0.0374\n",
            "Epoch 33/100, Train Loss: 0.0398, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0413, Val Loss: 0.0374\n",
            "Epoch 35/100, Train Loss: 0.0401, Val Loss: 0.0374\n",
            "Epoch 36/100, Train Loss: 0.0414, Val Loss: 0.0374\n",
            "Epoch 37/100, Train Loss: 0.0401, Val Loss: 0.0374\n",
            "Epoch 38/100, Train Loss: 0.0404, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0418, Val Loss: 0.0374\n",
            "Epoch 40/100, Train Loss: 0.0405, Val Loss: 0.0374\n",
            "Epoch 41/100, Train Loss: 0.0420, Val Loss: 0.0374\n",
            "Epoch 42/100, Train Loss: 0.0396, Val Loss: 0.0374\n",
            "Epoch 43/100, Train Loss: 0.0407, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0397, Val Loss: 0.0374\n",
            "Epoch 45/100, Train Loss: 0.0417, Val Loss: 0.0374\n",
            "Epoch 46/100, Train Loss: 0.0415, Val Loss: 0.0374\n",
            "Epoch 47/100, Train Loss: 0.0404, Val Loss: 0.0374\n",
            "Epoch 48/100, Train Loss: 0.0417, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0407, Val Loss: 0.0374\n",
            "Epoch 50/100, Train Loss: 0.0414, Val Loss: 0.0374\n",
            "Epoch 51/100, Train Loss: 0.0403, Val Loss: 0.0374\n",
            "Epoch 52/100, Train Loss: 0.0400, Val Loss: 0.0374\n",
            "Epoch 53/100, Train Loss: 0.0411, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0410, Val Loss: 0.0374\n",
            "Epoch 55/100, Train Loss: 0.0385, Val Loss: 0.0374\n",
            "Epoch 56/100, Train Loss: 0.0407, Val Loss: 0.0374\n",
            "Epoch 57/100, Train Loss: 0.0402, Val Loss: 0.0374\n",
            "Epoch 58/100, Train Loss: 0.0398, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0400, Val Loss: 0.0374\n",
            "Epoch 60/100, Train Loss: 0.0401, Val Loss: 0.0374\n",
            "Epoch 61/100, Train Loss: 0.0407, Val Loss: 0.0374\n",
            "Epoch 62/100, Train Loss: 0.0419, Val Loss: 0.0374\n",
            "Epoch 63/100, Train Loss: 0.0401, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0394, Val Loss: 0.0374\n",
            "Epoch 65/100, Train Loss: 0.0403, Val Loss: 0.0374\n",
            "Epoch 66/100, Train Loss: 0.0406, Val Loss: 0.0374\n",
            "Epoch 67/100, Train Loss: 0.0414, Val Loss: 0.0374\n",
            "Epoch 68/100, Train Loss: 0.0405, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0405, Val Loss: 0.0374\n",
            "Epoch 70/100, Train Loss: 0.0403, Val Loss: 0.0374\n",
            "Epoch 71/100, Train Loss: 0.0397, Val Loss: 0.0374\n",
            "Epoch 72/100, Train Loss: 0.0397, Val Loss: 0.0374\n",
            "Epoch 73/100, Train Loss: 0.0411, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0407, Val Loss: 0.0374\n",
            "Epoch 75/100, Train Loss: 0.0403, Val Loss: 0.0374\n",
            "Epoch 76/100, Train Loss: 0.0397, Val Loss: 0.0374\n",
            "Epoch 77/100, Train Loss: 0.0407, Val Loss: 0.0374\n",
            "Epoch 78/100, Train Loss: 0.0413, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0410, Val Loss: 0.0374\n",
            "Epoch 80/100, Train Loss: 0.0402, Val Loss: 0.0374\n",
            "Epoch 81/100, Train Loss: 0.0405, Val Loss: 0.0374\n",
            "Epoch 82/100, Train Loss: 0.0404, Val Loss: 0.0374\n",
            "Epoch 83/100, Train Loss: 0.0406, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0399, Val Loss: 0.0374\n",
            "Epoch 85/100, Train Loss: 0.0400, Val Loss: 0.0374\n",
            "Epoch 86/100, Train Loss: 0.0415, Val Loss: 0.0374\n",
            "Epoch 87/100, Train Loss: 0.0408, Val Loss: 0.0374\n",
            "Epoch 88/100, Train Loss: 0.0407, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0415, Val Loss: 0.0374\n",
            "Epoch 90/100, Train Loss: 0.0408, Val Loss: 0.0374\n",
            "Epoch 91/100, Train Loss: 0.0398, Val Loss: 0.0374\n",
            "Epoch 92/100, Train Loss: 0.0403, Val Loss: 0.0374\n",
            "Epoch 93/100, Train Loss: 0.0408, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0400, Val Loss: 0.0374\n",
            "Epoch 95/100, Train Loss: 0.0416, Val Loss: 0.0374\n",
            "Epoch 96/100, Train Loss: 0.0404, Val Loss: 0.0374\n",
            "Epoch 97/100, Train Loss: 0.0407, Val Loss: 0.0374\n",
            "Epoch 98/100, Train Loss: 0.0410, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0412, Val Loss: 0.0374\n",
            "Epoch 100/100, Train Loss: 0.0403, Val Loss: 0.0374\n",
            "\n",
            "Test iteration 28/28\n",
            "Current training set size: 124 samples\n",
            "Epoch 1/100, Train Loss: 0.1661, Val Loss: 0.1955\n",
            "Epoch 2/100, Train Loss: 0.1457, Val Loss: 0.1867\n",
            "Epoch 3/100, Train Loss: 0.1384, Val Loss: 0.1357\n",
            "Epoch 4/100, Train Loss: 0.1129, Val Loss: 0.1374\n",
            "Epoch 5/100, Train Loss: 0.1223, Val Loss: 0.1037\n",
            "Epoch 6/100, Train Loss: 0.0952, Val Loss: 0.0903\n",
            "Epoch 7/100, Train Loss: 0.0803, Val Loss: 0.0744\n",
            "Epoch 8/100, Train Loss: 0.0740, Val Loss: 0.0593\n",
            "Epoch 9/100, Train Loss: 0.0746, Val Loss: 0.0429\n",
            "Epoch 10/100, Train Loss: 0.0733, Val Loss: 0.0407\n",
            "Epoch 11/100, Train Loss: 0.0726, Val Loss: 0.0735\n",
            "Epoch 12/100, Train Loss: 0.0692, Val Loss: 0.0410\n",
            "Epoch 13/100, Train Loss: 0.0589, Val Loss: 0.0400\n",
            "Epoch 14/100, Train Loss: 0.0521, Val Loss: 0.0417\n",
            "Epoch 15/100, Train Loss: 0.0508, Val Loss: 0.0379\n",
            "Epoch 16/100, Train Loss: 0.0544, Val Loss: 0.0733\n",
            "Epoch 17/100, Train Loss: 0.0533, Val Loss: 0.0424\n",
            "Epoch 18/100, Train Loss: 0.0448, Val Loss: 0.0510\n",
            "Epoch 19/100, Train Loss: 0.0431, Val Loss: 0.0572\n",
            "Epoch 20/100, Train Loss: 0.0438, Val Loss: 0.0431\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0402, Val Loss: 0.0424\n",
            "Epoch 22/100, Train Loss: 0.0401, Val Loss: 0.0424\n",
            "Epoch 23/100, Train Loss: 0.0400, Val Loss: 0.0423\n",
            "Epoch 24/100, Train Loss: 0.0381, Val Loss: 0.0420\n",
            "Epoch 25/100, Train Loss: 0.0384, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0369, Val Loss: 0.0420\n",
            "Epoch 27/100, Train Loss: 0.0389, Val Loss: 0.0420\n",
            "Epoch 28/100, Train Loss: 0.0400, Val Loss: 0.0420\n",
            "Epoch 29/100, Train Loss: 0.0385, Val Loss: 0.0420\n",
            "Epoch 30/100, Train Loss: 0.0385, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0381, Val Loss: 0.0420\n",
            "Epoch 32/100, Train Loss: 0.0397, Val Loss: 0.0420\n",
            "Epoch 33/100, Train Loss: 0.0387, Val Loss: 0.0420\n",
            "Epoch 34/100, Train Loss: 0.0388, Val Loss: 0.0420\n",
            "Epoch 35/100, Train Loss: 0.0380, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0389, Val Loss: 0.0420\n",
            "Epoch 37/100, Train Loss: 0.0396, Val Loss: 0.0420\n",
            "Epoch 38/100, Train Loss: 0.0372, Val Loss: 0.0420\n",
            "Epoch 39/100, Train Loss: 0.0389, Val Loss: 0.0420\n",
            "Epoch 40/100, Train Loss: 0.0389, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0391, Val Loss: 0.0420\n",
            "Epoch 42/100, Train Loss: 0.0385, Val Loss: 0.0420\n",
            "Epoch 43/100, Train Loss: 0.0386, Val Loss: 0.0420\n",
            "Epoch 44/100, Train Loss: 0.0386, Val Loss: 0.0420\n",
            "Epoch 45/100, Train Loss: 0.0391, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0381, Val Loss: 0.0420\n",
            "Epoch 47/100, Train Loss: 0.0380, Val Loss: 0.0420\n",
            "Epoch 48/100, Train Loss: 0.0393, Val Loss: 0.0420\n",
            "Epoch 49/100, Train Loss: 0.0383, Val Loss: 0.0420\n",
            "Epoch 50/100, Train Loss: 0.0388, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0392, Val Loss: 0.0420\n",
            "Epoch 52/100, Train Loss: 0.0405, Val Loss: 0.0420\n",
            "Epoch 53/100, Train Loss: 0.0386, Val Loss: 0.0420\n",
            "Epoch 54/100, Train Loss: 0.0381, Val Loss: 0.0420\n",
            "Epoch 55/100, Train Loss: 0.0388, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0385, Val Loss: 0.0420\n",
            "Epoch 57/100, Train Loss: 0.0387, Val Loss: 0.0420\n",
            "Epoch 58/100, Train Loss: 0.0390, Val Loss: 0.0420\n",
            "Epoch 59/100, Train Loss: 0.0388, Val Loss: 0.0420\n",
            "Epoch 60/100, Train Loss: 0.0393, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0375, Val Loss: 0.0420\n",
            "Epoch 62/100, Train Loss: 0.0380, Val Loss: 0.0420\n",
            "Epoch 63/100, Train Loss: 0.0383, Val Loss: 0.0420\n",
            "Epoch 64/100, Train Loss: 0.0387, Val Loss: 0.0420\n",
            "Epoch 65/100, Train Loss: 0.0392, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0389, Val Loss: 0.0420\n",
            "Epoch 67/100, Train Loss: 0.0382, Val Loss: 0.0420\n",
            "Epoch 68/100, Train Loss: 0.0392, Val Loss: 0.0420\n",
            "Epoch 69/100, Train Loss: 0.0378, Val Loss: 0.0420\n",
            "Epoch 70/100, Train Loss: 0.0389, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0394, Val Loss: 0.0420\n",
            "Epoch 72/100, Train Loss: 0.0389, Val Loss: 0.0420\n",
            "Epoch 73/100, Train Loss: 0.0383, Val Loss: 0.0420\n",
            "Epoch 74/100, Train Loss: 0.0381, Val Loss: 0.0420\n",
            "Epoch 75/100, Train Loss: 0.0391, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0396, Val Loss: 0.0420\n",
            "Epoch 77/100, Train Loss: 0.0390, Val Loss: 0.0420\n",
            "Epoch 78/100, Train Loss: 0.0379, Val Loss: 0.0420\n",
            "Epoch 79/100, Train Loss: 0.0398, Val Loss: 0.0420\n",
            "Epoch 80/100, Train Loss: 0.0389, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0391, Val Loss: 0.0420\n",
            "Epoch 82/100, Train Loss: 0.0392, Val Loss: 0.0420\n",
            "Epoch 83/100, Train Loss: 0.0396, Val Loss: 0.0420\n",
            "Epoch 84/100, Train Loss: 0.0378, Val Loss: 0.0420\n",
            "Epoch 85/100, Train Loss: 0.0393, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0395, Val Loss: 0.0420\n",
            "Epoch 87/100, Train Loss: 0.0392, Val Loss: 0.0420\n",
            "Epoch 88/100, Train Loss: 0.0393, Val Loss: 0.0420\n",
            "Epoch 89/100, Train Loss: 0.0382, Val Loss: 0.0420\n",
            "Epoch 90/100, Train Loss: 0.0393, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0392, Val Loss: 0.0420\n",
            "Epoch 92/100, Train Loss: 0.0384, Val Loss: 0.0420\n",
            "Epoch 93/100, Train Loss: 0.0387, Val Loss: 0.0420\n",
            "Epoch 94/100, Train Loss: 0.0383, Val Loss: 0.0420\n",
            "Epoch 95/100, Train Loss: 0.0379, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0387, Val Loss: 0.0420\n",
            "Epoch 97/100, Train Loss: 0.0388, Val Loss: 0.0420\n",
            "Epoch 98/100, Train Loss: 0.0383, Val Loss: 0.0420\n",
            "Epoch 99/100, Train Loss: 0.0390, Val Loss: 0.0420\n",
            "Epoch 100/100, Train Loss: 0.0388, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: hybrid, Units: L512_G16, Dropout: 0.2, Dense Units: 64\n",
            "Batch Size: 4, Epochs: 100\n",
            "Device: cuda\n",
            "Total test samples: 28\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/28\n",
            "Current training set size: 97 samples\n",
            "Epoch 1/100, Train Loss: 0.1427, Val Loss: 0.1850\n",
            "Epoch 2/100, Train Loss: 0.1393, Val Loss: 0.1883\n",
            "Epoch 3/100, Train Loss: 0.1407, Val Loss: 0.1060\n",
            "Epoch 4/100, Train Loss: 0.1111, Val Loss: 0.1473\n",
            "Epoch 5/100, Train Loss: 0.1042, Val Loss: 0.0954\n",
            "Epoch 6/100, Train Loss: 0.1121, Val Loss: 0.0793\n",
            "Epoch 7/100, Train Loss: 0.0661, Val Loss: 0.0936\n",
            "Epoch 8/100, Train Loss: 0.0810, Val Loss: 0.0584\n",
            "Epoch 9/100, Train Loss: 0.0577, Val Loss: 0.0438\n",
            "Epoch 10/100, Train Loss: 0.0587, Val Loss: 0.0333\n",
            "Epoch 11/100, Train Loss: 0.0545, Val Loss: 0.0543\n",
            "Epoch 12/100, Train Loss: 0.0505, Val Loss: 0.0590\n",
            "Epoch 13/100, Train Loss: 0.0431, Val Loss: 0.0418\n",
            "Epoch 14/100, Train Loss: 0.0430, Val Loss: 0.0773\n",
            "Epoch 15/100, Train Loss: 0.0512, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0383, Val Loss: 0.0369\n",
            "Epoch 17/100, Train Loss: 0.0373, Val Loss: 0.0367\n",
            "Epoch 18/100, Train Loss: 0.0373, Val Loss: 0.0371\n",
            "Epoch 19/100, Train Loss: 0.0346, Val Loss: 0.0371\n",
            "Epoch 20/100, Train Loss: 0.0358, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0343, Val Loss: 0.0374\n",
            "Epoch 22/100, Train Loss: 0.0364, Val Loss: 0.0374\n",
            "Epoch 23/100, Train Loss: 0.0354, Val Loss: 0.0374\n",
            "Epoch 24/100, Train Loss: 0.0345, Val Loss: 0.0374\n",
            "Epoch 25/100, Train Loss: 0.0370, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0346, Val Loss: 0.0374\n",
            "Epoch 27/100, Train Loss: 0.0342, Val Loss: 0.0374\n",
            "Epoch 28/100, Train Loss: 0.0381, Val Loss: 0.0374\n",
            "Epoch 29/100, Train Loss: 0.0342, Val Loss: 0.0374\n",
            "Epoch 30/100, Train Loss: 0.0354, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0359, Val Loss: 0.0374\n",
            "Epoch 32/100, Train Loss: 0.0341, Val Loss: 0.0374\n",
            "Epoch 33/100, Train Loss: 0.0349, Val Loss: 0.0374\n",
            "Epoch 34/100, Train Loss: 0.0348, Val Loss: 0.0374\n",
            "Epoch 35/100, Train Loss: 0.0377, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0358, Val Loss: 0.0374\n",
            "Epoch 37/100, Train Loss: 0.0363, Val Loss: 0.0374\n",
            "Epoch 38/100, Train Loss: 0.0343, Val Loss: 0.0374\n",
            "Epoch 39/100, Train Loss: 0.0359, Val Loss: 0.0374\n",
            "Epoch 40/100, Train Loss: 0.0353, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0364, Val Loss: 0.0374\n",
            "Epoch 42/100, Train Loss: 0.0332, Val Loss: 0.0374\n",
            "Epoch 43/100, Train Loss: 0.0362, Val Loss: 0.0374\n",
            "Epoch 44/100, Train Loss: 0.0352, Val Loss: 0.0374\n",
            "Epoch 45/100, Train Loss: 0.0345, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0347, Val Loss: 0.0374\n",
            "Epoch 47/100, Train Loss: 0.0351, Val Loss: 0.0374\n",
            "Epoch 48/100, Train Loss: 0.0348, Val Loss: 0.0374\n",
            "Epoch 49/100, Train Loss: 0.0427, Val Loss: 0.0374\n",
            "Epoch 50/100, Train Loss: 0.0354, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0358, Val Loss: 0.0374\n",
            "Epoch 52/100, Train Loss: 0.0371, Val Loss: 0.0374\n",
            "Epoch 53/100, Train Loss: 0.0346, Val Loss: 0.0374\n",
            "Epoch 54/100, Train Loss: 0.0356, Val Loss: 0.0374\n",
            "Epoch 55/100, Train Loss: 0.0360, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0362, Val Loss: 0.0374\n",
            "Epoch 57/100, Train Loss: 0.0338, Val Loss: 0.0374\n",
            "Epoch 58/100, Train Loss: 0.0329, Val Loss: 0.0374\n",
            "Epoch 59/100, Train Loss: 0.0345, Val Loss: 0.0374\n",
            "Epoch 60/100, Train Loss: 0.0334, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0355, Val Loss: 0.0374\n",
            "Epoch 62/100, Train Loss: 0.0331, Val Loss: 0.0374\n",
            "Epoch 63/100, Train Loss: 0.0370, Val Loss: 0.0374\n",
            "Epoch 64/100, Train Loss: 0.0362, Val Loss: 0.0374\n",
            "Epoch 65/100, Train Loss: 0.0333, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0363, Val Loss: 0.0374\n",
            "Epoch 67/100, Train Loss: 0.0369, Val Loss: 0.0374\n",
            "Epoch 68/100, Train Loss: 0.0356, Val Loss: 0.0374\n",
            "Epoch 69/100, Train Loss: 0.0367, Val Loss: 0.0374\n",
            "Epoch 70/100, Train Loss: 0.0324, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0352, Val Loss: 0.0374\n",
            "Epoch 72/100, Train Loss: 0.0354, Val Loss: 0.0374\n",
            "Epoch 73/100, Train Loss: 0.0347, Val Loss: 0.0374\n",
            "Epoch 74/100, Train Loss: 0.0356, Val Loss: 0.0374\n",
            "Epoch 75/100, Train Loss: 0.0354, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0375, Val Loss: 0.0374\n",
            "Epoch 77/100, Train Loss: 0.0340, Val Loss: 0.0374\n",
            "Epoch 78/100, Train Loss: 0.0359, Val Loss: 0.0374\n",
            "Epoch 79/100, Train Loss: 0.0357, Val Loss: 0.0374\n",
            "Epoch 80/100, Train Loss: 0.0351, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0362, Val Loss: 0.0374\n",
            "Epoch 82/100, Train Loss: 0.0358, Val Loss: 0.0374\n",
            "Epoch 83/100, Train Loss: 0.0376, Val Loss: 0.0374\n",
            "Epoch 84/100, Train Loss: 0.0360, Val Loss: 0.0374\n",
            "Epoch 85/100, Train Loss: 0.0375, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0341, Val Loss: 0.0374\n",
            "Epoch 87/100, Train Loss: 0.0345, Val Loss: 0.0374\n",
            "Epoch 88/100, Train Loss: 0.0354, Val Loss: 0.0374\n",
            "Epoch 89/100, Train Loss: 0.0359, Val Loss: 0.0374\n",
            "Epoch 90/100, Train Loss: 0.0355, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0344, Val Loss: 0.0374\n",
            "Epoch 92/100, Train Loss: 0.0360, Val Loss: 0.0374\n",
            "Epoch 93/100, Train Loss: 0.0337, Val Loss: 0.0374\n",
            "Epoch 94/100, Train Loss: 0.0361, Val Loss: 0.0374\n",
            "Epoch 95/100, Train Loss: 0.0358, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0342, Val Loss: 0.0374\n",
            "Epoch 97/100, Train Loss: 0.0349, Val Loss: 0.0374\n",
            "Epoch 98/100, Train Loss: 0.0361, Val Loss: 0.0374\n",
            "Epoch 99/100, Train Loss: 0.0362, Val Loss: 0.0374\n",
            "Epoch 100/100, Train Loss: 0.0382, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 2/28\n",
            "Current training set size: 98 samples\n",
            "Epoch 1/100, Train Loss: 0.1801, Val Loss: 0.2091\n",
            "Epoch 2/100, Train Loss: 0.1510, Val Loss: 0.2038\n",
            "Epoch 3/100, Train Loss: 0.1457, Val Loss: 0.1777\n",
            "Epoch 4/100, Train Loss: 0.1408, Val Loss: 0.1708\n",
            "Epoch 5/100, Train Loss: 0.1157, Val Loss: 0.1385\n",
            "Epoch 6/100, Train Loss: 0.1158, Val Loss: 0.1451\n",
            "Epoch 7/100, Train Loss: 0.0944, Val Loss: 0.0727\n",
            "Epoch 8/100, Train Loss: 0.0801, Val Loss: 0.0711\n",
            "Epoch 9/100, Train Loss: 0.0755, Val Loss: 0.0514\n",
            "Epoch 10/100, Train Loss: 0.0690, Val Loss: 0.1072\n",
            "Epoch 11/100, Train Loss: 0.0724, Val Loss: 0.0457\n",
            "Epoch 12/100, Train Loss: 0.0686, Val Loss: 0.0432\n",
            "Epoch 13/100, Train Loss: 0.0585, Val Loss: 0.0378\n",
            "Epoch 14/100, Train Loss: 0.0573, Val Loss: 0.0531\n",
            "Epoch 15/100, Train Loss: 0.0504, Val Loss: 0.0424\n",
            "Epoch 16/100, Train Loss: 0.0457, Val Loss: 0.0491\n",
            "Epoch 17/100, Train Loss: 0.0469, Val Loss: 0.0381\n",
            "Epoch 18/100, Train Loss: 0.0504, Val Loss: 0.0511\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0462, Val Loss: 0.0496\n",
            "Epoch 20/100, Train Loss: 0.0425, Val Loss: 0.0476\n",
            "Epoch 21/100, Train Loss: 0.0434, Val Loss: 0.0462\n",
            "Epoch 22/100, Train Loss: 0.0420, Val Loss: 0.0449\n",
            "Epoch 23/100, Train Loss: 0.0419, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0419, Val Loss: 0.0435\n",
            "Epoch 25/100, Train Loss: 0.0420, Val Loss: 0.0435\n",
            "Epoch 26/100, Train Loss: 0.0386, Val Loss: 0.0435\n",
            "Epoch 27/100, Train Loss: 0.0392, Val Loss: 0.0434\n",
            "Epoch 28/100, Train Loss: 0.0404, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0414, Val Loss: 0.0434\n",
            "Epoch 30/100, Train Loss: 0.0410, Val Loss: 0.0434\n",
            "Epoch 31/100, Train Loss: 0.0391, Val Loss: 0.0434\n",
            "Epoch 32/100, Train Loss: 0.0411, Val Loss: 0.0434\n",
            "Epoch 33/100, Train Loss: 0.0404, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0413, Val Loss: 0.0434\n",
            "Epoch 35/100, Train Loss: 0.0411, Val Loss: 0.0434\n",
            "Epoch 36/100, Train Loss: 0.0395, Val Loss: 0.0434\n",
            "Epoch 37/100, Train Loss: 0.0406, Val Loss: 0.0434\n",
            "Epoch 38/100, Train Loss: 0.0396, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0415, Val Loss: 0.0434\n",
            "Epoch 40/100, Train Loss: 0.0419, Val Loss: 0.0434\n",
            "Epoch 41/100, Train Loss: 0.0423, Val Loss: 0.0434\n",
            "Epoch 42/100, Train Loss: 0.0412, Val Loss: 0.0434\n",
            "Epoch 43/100, Train Loss: 0.0413, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0402, Val Loss: 0.0434\n",
            "Epoch 45/100, Train Loss: 0.0405, Val Loss: 0.0434\n",
            "Epoch 46/100, Train Loss: 0.0456, Val Loss: 0.0434\n",
            "Epoch 47/100, Train Loss: 0.0407, Val Loss: 0.0434\n",
            "Epoch 48/100, Train Loss: 0.0403, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0412, Val Loss: 0.0434\n",
            "Epoch 50/100, Train Loss: 0.0402, Val Loss: 0.0434\n",
            "Epoch 51/100, Train Loss: 0.0416, Val Loss: 0.0434\n",
            "Epoch 52/100, Train Loss: 0.0410, Val Loss: 0.0434\n",
            "Epoch 53/100, Train Loss: 0.0398, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0399, Val Loss: 0.0434\n",
            "Epoch 55/100, Train Loss: 0.0408, Val Loss: 0.0434\n",
            "Epoch 56/100, Train Loss: 0.0415, Val Loss: 0.0434\n",
            "Epoch 57/100, Train Loss: 0.0394, Val Loss: 0.0434\n",
            "Epoch 58/100, Train Loss: 0.0395, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0402, Val Loss: 0.0434\n",
            "Epoch 60/100, Train Loss: 0.0419, Val Loss: 0.0434\n",
            "Epoch 61/100, Train Loss: 0.0411, Val Loss: 0.0434\n",
            "Epoch 62/100, Train Loss: 0.0402, Val Loss: 0.0434\n",
            "Epoch 63/100, Train Loss: 0.0417, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0407, Val Loss: 0.0434\n",
            "Epoch 65/100, Train Loss: 0.0374, Val Loss: 0.0434\n",
            "Epoch 66/100, Train Loss: 0.0418, Val Loss: 0.0434\n",
            "Epoch 67/100, Train Loss: 0.0414, Val Loss: 0.0434\n",
            "Epoch 68/100, Train Loss: 0.0415, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0407, Val Loss: 0.0434\n",
            "Epoch 70/100, Train Loss: 0.0412, Val Loss: 0.0434\n",
            "Epoch 71/100, Train Loss: 0.0463, Val Loss: 0.0434\n",
            "Epoch 72/100, Train Loss: 0.0440, Val Loss: 0.0434\n",
            "Epoch 73/100, Train Loss: 0.0403, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0414, Val Loss: 0.0434\n",
            "Epoch 75/100, Train Loss: 0.0429, Val Loss: 0.0434\n",
            "Epoch 76/100, Train Loss: 0.0419, Val Loss: 0.0434\n",
            "Epoch 77/100, Train Loss: 0.0401, Val Loss: 0.0434\n",
            "Epoch 78/100, Train Loss: 0.0407, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0399, Val Loss: 0.0434\n",
            "Epoch 80/100, Train Loss: 0.0407, Val Loss: 0.0434\n",
            "Epoch 81/100, Train Loss: 0.0403, Val Loss: 0.0434\n",
            "Epoch 82/100, Train Loss: 0.0408, Val Loss: 0.0434\n",
            "Epoch 83/100, Train Loss: 0.0394, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0413, Val Loss: 0.0434\n",
            "Epoch 85/100, Train Loss: 0.0403, Val Loss: 0.0434\n",
            "Epoch 86/100, Train Loss: 0.0382, Val Loss: 0.0434\n",
            "Epoch 87/100, Train Loss: 0.0395, Val Loss: 0.0434\n",
            "Epoch 88/100, Train Loss: 0.0402, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0400, Val Loss: 0.0434\n",
            "Epoch 90/100, Train Loss: 0.0391, Val Loss: 0.0434\n",
            "Epoch 91/100, Train Loss: 0.0434, Val Loss: 0.0434\n",
            "Epoch 92/100, Train Loss: 0.0395, Val Loss: 0.0434\n",
            "Epoch 93/100, Train Loss: 0.0413, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0402, Val Loss: 0.0434\n",
            "Epoch 95/100, Train Loss: 0.0391, Val Loss: 0.0434\n",
            "Epoch 96/100, Train Loss: 0.0414, Val Loss: 0.0434\n",
            "Epoch 97/100, Train Loss: 0.0414, Val Loss: 0.0434\n",
            "Epoch 98/100, Train Loss: 0.0417, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0391, Val Loss: 0.0434\n",
            "Epoch 100/100, Train Loss: 0.0415, Val Loss: 0.0434\n",
            "\n",
            "Test iteration 3/28\n",
            "Current training set size: 99 samples\n",
            "Epoch 1/100, Train Loss: 0.1628, Val Loss: 0.1906\n",
            "Epoch 2/100, Train Loss: 0.1477, Val Loss: 0.1844\n",
            "Epoch 3/100, Train Loss: 0.1360, Val Loss: 0.1765\n",
            "Epoch 4/100, Train Loss: 0.1347, Val Loss: 0.1563\n",
            "Epoch 5/100, Train Loss: 0.1020, Val Loss: 0.0437\n",
            "Epoch 6/100, Train Loss: 0.0819, Val Loss: 0.0525\n",
            "Epoch 7/100, Train Loss: 0.0839, Val Loss: 0.0639\n",
            "Epoch 8/100, Train Loss: 0.0718, Val Loss: 0.0645\n",
            "Epoch 9/100, Train Loss: 0.0740, Val Loss: 0.0560\n",
            "Epoch 10/100, Train Loss: 0.0644, Val Loss: 0.0716\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 11/100, Train Loss: 0.0806, Val Loss: 0.0546\n",
            "Epoch 12/100, Train Loss: 0.0627, Val Loss: 0.0411\n",
            "Epoch 13/100, Train Loss: 0.0584, Val Loss: 0.0345\n",
            "Epoch 14/100, Train Loss: 0.0569, Val Loss: 0.0316\n",
            "Epoch 15/100, Train Loss: 0.0538, Val Loss: 0.0312\n",
            "Epoch 16/100, Train Loss: 0.0559, Val Loss: 0.0313\n",
            "Epoch 17/100, Train Loss: 0.0557, Val Loss: 0.0313\n",
            "Epoch 18/100, Train Loss: 0.0540, Val Loss: 0.0314\n",
            "Epoch 19/100, Train Loss: 0.0533, Val Loss: 0.0316\n",
            "Epoch 20/100, Train Loss: 0.0539, Val Loss: 0.0319\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0566, Val Loss: 0.0319\n",
            "Epoch 22/100, Train Loss: 0.0537, Val Loss: 0.0319\n",
            "Epoch 23/100, Train Loss: 0.0537, Val Loss: 0.0319\n",
            "Epoch 24/100, Train Loss: 0.0519, Val Loss: 0.0319\n",
            "Epoch 25/100, Train Loss: 0.0539, Val Loss: 0.0319\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0537, Val Loss: 0.0319\n",
            "Epoch 27/100, Train Loss: 0.0550, Val Loss: 0.0319\n",
            "Epoch 28/100, Train Loss: 0.0539, Val Loss: 0.0319\n",
            "Epoch 29/100, Train Loss: 0.0550, Val Loss: 0.0319\n",
            "Epoch 30/100, Train Loss: 0.0548, Val Loss: 0.0319\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0543, Val Loss: 0.0319\n",
            "Epoch 32/100, Train Loss: 0.0552, Val Loss: 0.0319\n",
            "Epoch 33/100, Train Loss: 0.0543, Val Loss: 0.0319\n",
            "Epoch 34/100, Train Loss: 0.0537, Val Loss: 0.0319\n",
            "Epoch 35/100, Train Loss: 0.0538, Val Loss: 0.0319\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0542, Val Loss: 0.0319\n",
            "Epoch 37/100, Train Loss: 0.0529, Val Loss: 0.0319\n",
            "Epoch 38/100, Train Loss: 0.0540, Val Loss: 0.0319\n",
            "Epoch 39/100, Train Loss: 0.0548, Val Loss: 0.0319\n",
            "Epoch 40/100, Train Loss: 0.0557, Val Loss: 0.0319\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0537, Val Loss: 0.0319\n",
            "Epoch 42/100, Train Loss: 0.0532, Val Loss: 0.0319\n",
            "Epoch 43/100, Train Loss: 0.0546, Val Loss: 0.0319\n",
            "Epoch 44/100, Train Loss: 0.0536, Val Loss: 0.0319\n",
            "Epoch 45/100, Train Loss: 0.0546, Val Loss: 0.0319\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0535, Val Loss: 0.0319\n",
            "Epoch 47/100, Train Loss: 0.0548, Val Loss: 0.0319\n",
            "Epoch 48/100, Train Loss: 0.0544, Val Loss: 0.0319\n",
            "Epoch 49/100, Train Loss: 0.0543, Val Loss: 0.0319\n",
            "Epoch 50/100, Train Loss: 0.0543, Val Loss: 0.0319\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0549, Val Loss: 0.0319\n",
            "Epoch 52/100, Train Loss: 0.0525, Val Loss: 0.0319\n",
            "Epoch 53/100, Train Loss: 0.0545, Val Loss: 0.0319\n",
            "Epoch 54/100, Train Loss: 0.0541, Val Loss: 0.0319\n",
            "Epoch 55/100, Train Loss: 0.0553, Val Loss: 0.0319\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0538, Val Loss: 0.0319\n",
            "Epoch 57/100, Train Loss: 0.0536, Val Loss: 0.0319\n",
            "Epoch 58/100, Train Loss: 0.0532, Val Loss: 0.0319\n",
            "Epoch 59/100, Train Loss: 0.0548, Val Loss: 0.0319\n",
            "Epoch 60/100, Train Loss: 0.0564, Val Loss: 0.0319\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0510, Val Loss: 0.0319\n",
            "Epoch 62/100, Train Loss: 0.0537, Val Loss: 0.0319\n",
            "Epoch 63/100, Train Loss: 0.0530, Val Loss: 0.0319\n",
            "Epoch 64/100, Train Loss: 0.0544, Val Loss: 0.0319\n",
            "Epoch 65/100, Train Loss: 0.0547, Val Loss: 0.0319\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0546, Val Loss: 0.0319\n",
            "Epoch 67/100, Train Loss: 0.0540, Val Loss: 0.0319\n",
            "Epoch 68/100, Train Loss: 0.0539, Val Loss: 0.0319\n",
            "Epoch 69/100, Train Loss: 0.0536, Val Loss: 0.0319\n",
            "Epoch 70/100, Train Loss: 0.0553, Val Loss: 0.0319\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0536, Val Loss: 0.0319\n",
            "Epoch 72/100, Train Loss: 0.0539, Val Loss: 0.0319\n",
            "Epoch 73/100, Train Loss: 0.0531, Val Loss: 0.0319\n",
            "Epoch 74/100, Train Loss: 0.0535, Val Loss: 0.0319\n",
            "Epoch 75/100, Train Loss: 0.0537, Val Loss: 0.0319\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0525, Val Loss: 0.0319\n",
            "Epoch 77/100, Train Loss: 0.0528, Val Loss: 0.0319\n",
            "Epoch 78/100, Train Loss: 0.0540, Val Loss: 0.0319\n",
            "Epoch 79/100, Train Loss: 0.0544, Val Loss: 0.0319\n",
            "Epoch 80/100, Train Loss: 0.0547, Val Loss: 0.0319\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0538, Val Loss: 0.0319\n",
            "Epoch 82/100, Train Loss: 0.0527, Val Loss: 0.0319\n",
            "Epoch 83/100, Train Loss: 0.0533, Val Loss: 0.0319\n",
            "Epoch 84/100, Train Loss: 0.0547, Val Loss: 0.0319\n",
            "Epoch 85/100, Train Loss: 0.0538, Val Loss: 0.0319\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0543, Val Loss: 0.0319\n",
            "Epoch 87/100, Train Loss: 0.0550, Val Loss: 0.0319\n",
            "Epoch 88/100, Train Loss: 0.0521, Val Loss: 0.0319\n",
            "Epoch 89/100, Train Loss: 0.0556, Val Loss: 0.0319\n",
            "Epoch 90/100, Train Loss: 0.0523, Val Loss: 0.0319\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0532, Val Loss: 0.0319\n",
            "Epoch 92/100, Train Loss: 0.0541, Val Loss: 0.0319\n",
            "Epoch 93/100, Train Loss: 0.0559, Val Loss: 0.0319\n",
            "Epoch 94/100, Train Loss: 0.0536, Val Loss: 0.0319\n",
            "Epoch 95/100, Train Loss: 0.0524, Val Loss: 0.0319\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0539, Val Loss: 0.0319\n",
            "Epoch 97/100, Train Loss: 0.0536, Val Loss: 0.0319\n",
            "Epoch 98/100, Train Loss: 0.0546, Val Loss: 0.0319\n",
            "Epoch 99/100, Train Loss: 0.0532, Val Loss: 0.0319\n",
            "Epoch 100/100, Train Loss: 0.0550, Val Loss: 0.0319\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 4/28\n",
            "Current training set size: 100 samples\n",
            "Epoch 1/100, Train Loss: 0.1503, Val Loss: 0.1861\n",
            "Epoch 2/100, Train Loss: 0.1424, Val Loss: 0.1792\n",
            "Epoch 3/100, Train Loss: 0.1396, Val Loss: 0.1859\n",
            "Epoch 4/100, Train Loss: 0.1331, Val Loss: 0.1725\n",
            "Epoch 5/100, Train Loss: 0.1133, Val Loss: 0.1374\n",
            "Epoch 6/100, Train Loss: 0.0948, Val Loss: 0.0419\n",
            "Epoch 7/100, Train Loss: 0.0799, Val Loss: 0.0531\n",
            "Epoch 8/100, Train Loss: 0.0730, Val Loss: 0.0716\n",
            "Epoch 9/100, Train Loss: 0.0677, Val Loss: 0.0658\n",
            "Epoch 10/100, Train Loss: 0.0620, Val Loss: 0.0356\n",
            "Epoch 11/100, Train Loss: 0.0560, Val Loss: 0.0612\n",
            "Epoch 12/100, Train Loss: 0.0551, Val Loss: 0.0373\n",
            "Epoch 13/100, Train Loss: 0.0485, Val Loss: 0.0345\n",
            "Epoch 14/100, Train Loss: 0.0477, Val Loss: 0.0529\n",
            "Epoch 15/100, Train Loss: 0.0428, Val Loss: 0.0341\n",
            "Epoch 16/100, Train Loss: 0.0436, Val Loss: 0.0359\n",
            "Epoch 17/100, Train Loss: 0.0461, Val Loss: 0.0507\n",
            "Epoch 18/100, Train Loss: 0.0419, Val Loss: 0.0529\n",
            "Epoch 19/100, Train Loss: 0.0424, Val Loss: 0.0749\n",
            "Epoch 20/100, Train Loss: 0.0448, Val Loss: 0.0408\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0367, Val Loss: 0.0406\n",
            "Epoch 22/100, Train Loss: 0.0346, Val Loss: 0.0403\n",
            "Epoch 23/100, Train Loss: 0.0336, Val Loss: 0.0402\n",
            "Epoch 24/100, Train Loss: 0.0349, Val Loss: 0.0402\n",
            "Epoch 25/100, Train Loss: 0.0328, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0335, Val Loss: 0.0401\n",
            "Epoch 27/100, Train Loss: 0.0345, Val Loss: 0.0401\n",
            "Epoch 28/100, Train Loss: 0.0337, Val Loss: 0.0401\n",
            "Epoch 29/100, Train Loss: 0.0330, Val Loss: 0.0401\n",
            "Epoch 30/100, Train Loss: 0.0335, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0341, Val Loss: 0.0401\n",
            "Epoch 32/100, Train Loss: 0.0338, Val Loss: 0.0401\n",
            "Epoch 33/100, Train Loss: 0.0319, Val Loss: 0.0401\n",
            "Epoch 34/100, Train Loss: 0.0322, Val Loss: 0.0401\n",
            "Epoch 35/100, Train Loss: 0.0333, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0328, Val Loss: 0.0401\n",
            "Epoch 37/100, Train Loss: 0.0333, Val Loss: 0.0401\n",
            "Epoch 38/100, Train Loss: 0.0336, Val Loss: 0.0401\n",
            "Epoch 39/100, Train Loss: 0.0321, Val Loss: 0.0401\n",
            "Epoch 40/100, Train Loss: 0.0328, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0350, Val Loss: 0.0401\n",
            "Epoch 42/100, Train Loss: 0.0333, Val Loss: 0.0401\n",
            "Epoch 43/100, Train Loss: 0.0337, Val Loss: 0.0401\n",
            "Epoch 44/100, Train Loss: 0.0336, Val Loss: 0.0401\n",
            "Epoch 45/100, Train Loss: 0.0325, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0334, Val Loss: 0.0401\n",
            "Epoch 47/100, Train Loss: 0.0331, Val Loss: 0.0401\n",
            "Epoch 48/100, Train Loss: 0.0325, Val Loss: 0.0401\n",
            "Epoch 49/100, Train Loss: 0.0342, Val Loss: 0.0401\n",
            "Epoch 50/100, Train Loss: 0.0352, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0340, Val Loss: 0.0401\n",
            "Epoch 52/100, Train Loss: 0.0311, Val Loss: 0.0401\n",
            "Epoch 53/100, Train Loss: 0.0316, Val Loss: 0.0401\n",
            "Epoch 54/100, Train Loss: 0.0337, Val Loss: 0.0401\n",
            "Epoch 55/100, Train Loss: 0.0334, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0328, Val Loss: 0.0401\n",
            "Epoch 57/100, Train Loss: 0.0321, Val Loss: 0.0401\n",
            "Epoch 58/100, Train Loss: 0.0343, Val Loss: 0.0401\n",
            "Epoch 59/100, Train Loss: 0.0332, Val Loss: 0.0401\n",
            "Epoch 60/100, Train Loss: 0.0324, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0336, Val Loss: 0.0401\n",
            "Epoch 62/100, Train Loss: 0.0343, Val Loss: 0.0401\n",
            "Epoch 63/100, Train Loss: 0.0346, Val Loss: 0.0401\n",
            "Epoch 64/100, Train Loss: 0.0324, Val Loss: 0.0401\n",
            "Epoch 65/100, Train Loss: 0.0326, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0328, Val Loss: 0.0401\n",
            "Epoch 67/100, Train Loss: 0.0337, Val Loss: 0.0401\n",
            "Epoch 68/100, Train Loss: 0.0347, Val Loss: 0.0401\n",
            "Epoch 69/100, Train Loss: 0.0331, Val Loss: 0.0401\n",
            "Epoch 70/100, Train Loss: 0.0330, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0336, Val Loss: 0.0401\n",
            "Epoch 72/100, Train Loss: 0.0339, Val Loss: 0.0401\n",
            "Epoch 73/100, Train Loss: 0.0332, Val Loss: 0.0401\n",
            "Epoch 74/100, Train Loss: 0.0339, Val Loss: 0.0401\n",
            "Epoch 75/100, Train Loss: 0.0329, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0343, Val Loss: 0.0401\n",
            "Epoch 77/100, Train Loss: 0.0340, Val Loss: 0.0401\n",
            "Epoch 78/100, Train Loss: 0.0328, Val Loss: 0.0401\n",
            "Epoch 79/100, Train Loss: 0.0338, Val Loss: 0.0401\n",
            "Epoch 80/100, Train Loss: 0.0341, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0329, Val Loss: 0.0401\n",
            "Epoch 82/100, Train Loss: 0.0316, Val Loss: 0.0401\n",
            "Epoch 83/100, Train Loss: 0.0334, Val Loss: 0.0401\n",
            "Epoch 84/100, Train Loss: 0.0334, Val Loss: 0.0401\n",
            "Epoch 85/100, Train Loss: 0.0338, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0347, Val Loss: 0.0401\n",
            "Epoch 87/100, Train Loss: 0.0325, Val Loss: 0.0401\n",
            "Epoch 88/100, Train Loss: 0.0353, Val Loss: 0.0401\n",
            "Epoch 89/100, Train Loss: 0.0325, Val Loss: 0.0401\n",
            "Epoch 90/100, Train Loss: 0.0318, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0338, Val Loss: 0.0401\n",
            "Epoch 92/100, Train Loss: 0.0329, Val Loss: 0.0401\n",
            "Epoch 93/100, Train Loss: 0.0335, Val Loss: 0.0401\n",
            "Epoch 94/100, Train Loss: 0.0352, Val Loss: 0.0401\n",
            "Epoch 95/100, Train Loss: 0.0321, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0326, Val Loss: 0.0401\n",
            "Epoch 97/100, Train Loss: 0.0326, Val Loss: 0.0401\n",
            "Epoch 98/100, Train Loss: 0.0338, Val Loss: 0.0401\n",
            "Epoch 99/100, Train Loss: 0.0349, Val Loss: 0.0401\n",
            "Epoch 100/100, Train Loss: 0.0342, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 5/28\n",
            "Current training set size: 101 samples\n",
            "Epoch 1/100, Train Loss: 0.1594, Val Loss: 0.2330\n",
            "Epoch 2/100, Train Loss: 0.1563, Val Loss: 0.2138\n",
            "Epoch 3/100, Train Loss: 0.1469, Val Loss: 0.1828\n",
            "Epoch 4/100, Train Loss: 0.1352, Val Loss: 0.1636\n",
            "Epoch 5/100, Train Loss: 0.1409, Val Loss: 0.1669\n",
            "Epoch 6/100, Train Loss: 0.1068, Val Loss: 0.0876\n",
            "Epoch 7/100, Train Loss: 0.0921, Val Loss: 0.0557\n",
            "Epoch 8/100, Train Loss: 0.0758, Val Loss: 0.0447\n",
            "Epoch 9/100, Train Loss: 0.0773, Val Loss: 0.0579\n",
            "Epoch 10/100, Train Loss: 0.0627, Val Loss: 0.0358\n",
            "Epoch 11/100, Train Loss: 0.0606, Val Loss: 0.0400\n",
            "Epoch 12/100, Train Loss: 0.0539, Val Loss: 0.0466\n",
            "Epoch 13/100, Train Loss: 0.0593, Val Loss: 0.0353\n",
            "Epoch 14/100, Train Loss: 0.0502, Val Loss: 0.0458\n",
            "Epoch 15/100, Train Loss: 0.0505, Val Loss: 0.0420\n",
            "Epoch 16/100, Train Loss: 0.0462, Val Loss: 0.0607\n",
            "Epoch 17/100, Train Loss: 0.0463, Val Loss: 0.0372\n",
            "Epoch 18/100, Train Loss: 0.0444, Val Loss: 0.0626\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0584, Val Loss: 0.0572\n",
            "Epoch 20/100, Train Loss: 0.0508, Val Loss: 0.0540\n",
            "Epoch 21/100, Train Loss: 0.0460, Val Loss: 0.0509\n",
            "Epoch 22/100, Train Loss: 0.0405, Val Loss: 0.0480\n",
            "Epoch 23/100, Train Loss: 0.0378, Val Loss: 0.0460\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0380, Val Loss: 0.0460\n",
            "Epoch 25/100, Train Loss: 0.0385, Val Loss: 0.0460\n",
            "Epoch 26/100, Train Loss: 0.0380, Val Loss: 0.0460\n",
            "Epoch 27/100, Train Loss: 0.0402, Val Loss: 0.0460\n",
            "Epoch 28/100, Train Loss: 0.0383, Val Loss: 0.0460\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0392, Val Loss: 0.0460\n",
            "Epoch 30/100, Train Loss: 0.0411, Val Loss: 0.0460\n",
            "Epoch 31/100, Train Loss: 0.0402, Val Loss: 0.0460\n",
            "Epoch 32/100, Train Loss: 0.0379, Val Loss: 0.0460\n",
            "Epoch 33/100, Train Loss: 0.0398, Val Loss: 0.0460\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0387, Val Loss: 0.0460\n",
            "Epoch 35/100, Train Loss: 0.0405, Val Loss: 0.0460\n",
            "Epoch 36/100, Train Loss: 0.0381, Val Loss: 0.0460\n",
            "Epoch 37/100, Train Loss: 0.0396, Val Loss: 0.0460\n",
            "Epoch 38/100, Train Loss: 0.0390, Val Loss: 0.0460\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0404, Val Loss: 0.0460\n",
            "Epoch 40/100, Train Loss: 0.0408, Val Loss: 0.0460\n",
            "Epoch 41/100, Train Loss: 0.0379, Val Loss: 0.0460\n",
            "Epoch 42/100, Train Loss: 0.0368, Val Loss: 0.0460\n",
            "Epoch 43/100, Train Loss: 0.0384, Val Loss: 0.0460\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0389, Val Loss: 0.0460\n",
            "Epoch 45/100, Train Loss: 0.0393, Val Loss: 0.0460\n",
            "Epoch 46/100, Train Loss: 0.0385, Val Loss: 0.0460\n",
            "Epoch 47/100, Train Loss: 0.0379, Val Loss: 0.0460\n",
            "Epoch 48/100, Train Loss: 0.0398, Val Loss: 0.0460\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0401, Val Loss: 0.0460\n",
            "Epoch 50/100, Train Loss: 0.0402, Val Loss: 0.0460\n",
            "Epoch 51/100, Train Loss: 0.0388, Val Loss: 0.0460\n",
            "Epoch 52/100, Train Loss: 0.0378, Val Loss: 0.0460\n",
            "Epoch 53/100, Train Loss: 0.0403, Val Loss: 0.0460\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0387, Val Loss: 0.0460\n",
            "Epoch 55/100, Train Loss: 0.0383, Val Loss: 0.0460\n",
            "Epoch 56/100, Train Loss: 0.0392, Val Loss: 0.0460\n",
            "Epoch 57/100, Train Loss: 0.0392, Val Loss: 0.0460\n",
            "Epoch 58/100, Train Loss: 0.0388, Val Loss: 0.0460\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0397, Val Loss: 0.0460\n",
            "Epoch 60/100, Train Loss: 0.0382, Val Loss: 0.0460\n",
            "Epoch 61/100, Train Loss: 0.0379, Val Loss: 0.0460\n",
            "Epoch 62/100, Train Loss: 0.0393, Val Loss: 0.0460\n",
            "Epoch 63/100, Train Loss: 0.0392, Val Loss: 0.0460\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0394, Val Loss: 0.0460\n",
            "Epoch 65/100, Train Loss: 0.0383, Val Loss: 0.0460\n",
            "Epoch 66/100, Train Loss: 0.0379, Val Loss: 0.0460\n",
            "Epoch 67/100, Train Loss: 0.0405, Val Loss: 0.0460\n",
            "Epoch 68/100, Train Loss: 0.0421, Val Loss: 0.0460\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0384, Val Loss: 0.0460\n",
            "Epoch 70/100, Train Loss: 0.0372, Val Loss: 0.0460\n",
            "Epoch 71/100, Train Loss: 0.0380, Val Loss: 0.0460\n",
            "Epoch 72/100, Train Loss: 0.0391, Val Loss: 0.0460\n",
            "Epoch 73/100, Train Loss: 0.0389, Val Loss: 0.0460\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0394, Val Loss: 0.0460\n",
            "Epoch 75/100, Train Loss: 0.0397, Val Loss: 0.0460\n",
            "Epoch 76/100, Train Loss: 0.0378, Val Loss: 0.0460\n",
            "Epoch 77/100, Train Loss: 0.0389, Val Loss: 0.0460\n",
            "Epoch 78/100, Train Loss: 0.0370, Val Loss: 0.0460\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0378, Val Loss: 0.0460\n",
            "Epoch 80/100, Train Loss: 0.0385, Val Loss: 0.0460\n",
            "Epoch 81/100, Train Loss: 0.0389, Val Loss: 0.0460\n",
            "Epoch 82/100, Train Loss: 0.0394, Val Loss: 0.0460\n",
            "Epoch 83/100, Train Loss: 0.0396, Val Loss: 0.0460\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0378, Val Loss: 0.0460\n",
            "Epoch 85/100, Train Loss: 0.0384, Val Loss: 0.0460\n",
            "Epoch 86/100, Train Loss: 0.0374, Val Loss: 0.0460\n",
            "Epoch 87/100, Train Loss: 0.0380, Val Loss: 0.0460\n",
            "Epoch 88/100, Train Loss: 0.0373, Val Loss: 0.0460\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0388, Val Loss: 0.0460\n",
            "Epoch 90/100, Train Loss: 0.0392, Val Loss: 0.0460\n",
            "Epoch 91/100, Train Loss: 0.0400, Val Loss: 0.0460\n",
            "Epoch 92/100, Train Loss: 0.0389, Val Loss: 0.0460\n",
            "Epoch 93/100, Train Loss: 0.0380, Val Loss: 0.0460\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0384, Val Loss: 0.0460\n",
            "Epoch 95/100, Train Loss: 0.0403, Val Loss: 0.0460\n",
            "Epoch 96/100, Train Loss: 0.0397, Val Loss: 0.0460\n",
            "Epoch 97/100, Train Loss: 0.0391, Val Loss: 0.0460\n",
            "Epoch 98/100, Train Loss: 0.0375, Val Loss: 0.0460\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0390, Val Loss: 0.0460\n",
            "Epoch 100/100, Train Loss: 0.0381, Val Loss: 0.0460\n",
            "\n",
            "Test iteration 6/28\n",
            "Current training set size: 102 samples\n",
            "Epoch 1/100, Train Loss: 0.1484, Val Loss: 0.1864\n",
            "Epoch 2/100, Train Loss: 0.1421, Val Loss: 0.1940\n",
            "Epoch 3/100, Train Loss: 0.1389, Val Loss: 0.1498\n",
            "Epoch 4/100, Train Loss: 0.1446, Val Loss: 0.1673\n",
            "Epoch 5/100, Train Loss: 0.1288, Val Loss: 0.1426\n",
            "Epoch 6/100, Train Loss: 0.0911, Val Loss: 0.0454\n",
            "Epoch 7/100, Train Loss: 0.0769, Val Loss: 0.0536\n",
            "Epoch 8/100, Train Loss: 0.0715, Val Loss: 0.0450\n",
            "Epoch 9/100, Train Loss: 0.0629, Val Loss: 0.0382\n",
            "Epoch 10/100, Train Loss: 0.0647, Val Loss: 0.0333\n",
            "Epoch 11/100, Train Loss: 0.0575, Val Loss: 0.0378\n",
            "Epoch 12/100, Train Loss: 0.0544, Val Loss: 0.0594\n",
            "Epoch 13/100, Train Loss: 0.0556, Val Loss: 0.0365\n",
            "Epoch 14/100, Train Loss: 0.0498, Val Loss: 0.0450\n",
            "Epoch 15/100, Train Loss: 0.0529, Val Loss: 0.0491\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0461, Val Loss: 0.0476\n",
            "Epoch 17/100, Train Loss: 0.0434, Val Loss: 0.0458\n",
            "Epoch 18/100, Train Loss: 0.0414, Val Loss: 0.0445\n",
            "Epoch 19/100, Train Loss: 0.0407, Val Loss: 0.0441\n",
            "Epoch 20/100, Train Loss: 0.0444, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0404, Val Loss: 0.0434\n",
            "Epoch 22/100, Train Loss: 0.0390, Val Loss: 0.0433\n",
            "Epoch 23/100, Train Loss: 0.0398, Val Loss: 0.0433\n",
            "Epoch 24/100, Train Loss: 0.0402, Val Loss: 0.0433\n",
            "Epoch 25/100, Train Loss: 0.0378, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0393, Val Loss: 0.0433\n",
            "Epoch 27/100, Train Loss: 0.0406, Val Loss: 0.0433\n",
            "Epoch 28/100, Train Loss: 0.0457, Val Loss: 0.0433\n",
            "Epoch 29/100, Train Loss: 0.0385, Val Loss: 0.0433\n",
            "Epoch 30/100, Train Loss: 0.0392, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0398, Val Loss: 0.0433\n",
            "Epoch 32/100, Train Loss: 0.0389, Val Loss: 0.0433\n",
            "Epoch 33/100, Train Loss: 0.0420, Val Loss: 0.0433\n",
            "Epoch 34/100, Train Loss: 0.0399, Val Loss: 0.0433\n",
            "Epoch 35/100, Train Loss: 0.0404, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0406, Val Loss: 0.0433\n",
            "Epoch 37/100, Train Loss: 0.0392, Val Loss: 0.0433\n",
            "Epoch 38/100, Train Loss: 0.0388, Val Loss: 0.0433\n",
            "Epoch 39/100, Train Loss: 0.0395, Val Loss: 0.0433\n",
            "Epoch 40/100, Train Loss: 0.0407, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0401, Val Loss: 0.0433\n",
            "Epoch 42/100, Train Loss: 0.0396, Val Loss: 0.0433\n",
            "Epoch 43/100, Train Loss: 0.0396, Val Loss: 0.0433\n",
            "Epoch 44/100, Train Loss: 0.0393, Val Loss: 0.0433\n",
            "Epoch 45/100, Train Loss: 0.0405, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0400, Val Loss: 0.0433\n",
            "Epoch 47/100, Train Loss: 0.0415, Val Loss: 0.0433\n",
            "Epoch 48/100, Train Loss: 0.0412, Val Loss: 0.0433\n",
            "Epoch 49/100, Train Loss: 0.0408, Val Loss: 0.0433\n",
            "Epoch 50/100, Train Loss: 0.0391, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0392, Val Loss: 0.0433\n",
            "Epoch 52/100, Train Loss: 0.0404, Val Loss: 0.0433\n",
            "Epoch 53/100, Train Loss: 0.0389, Val Loss: 0.0433\n",
            "Epoch 54/100, Train Loss: 0.0397, Val Loss: 0.0433\n",
            "Epoch 55/100, Train Loss: 0.0438, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0449, Val Loss: 0.0433\n",
            "Epoch 57/100, Train Loss: 0.0403, Val Loss: 0.0433\n",
            "Epoch 58/100, Train Loss: 0.0405, Val Loss: 0.0433\n",
            "Epoch 59/100, Train Loss: 0.0395, Val Loss: 0.0433\n",
            "Epoch 60/100, Train Loss: 0.0398, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0388, Val Loss: 0.0433\n",
            "Epoch 62/100, Train Loss: 0.0398, Val Loss: 0.0433\n",
            "Epoch 63/100, Train Loss: 0.0400, Val Loss: 0.0433\n",
            "Epoch 64/100, Train Loss: 0.0386, Val Loss: 0.0433\n",
            "Epoch 65/100, Train Loss: 0.0420, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0402, Val Loss: 0.0433\n",
            "Epoch 67/100, Train Loss: 0.0399, Val Loss: 0.0433\n",
            "Epoch 68/100, Train Loss: 0.0384, Val Loss: 0.0433\n",
            "Epoch 69/100, Train Loss: 0.0396, Val Loss: 0.0433\n",
            "Epoch 70/100, Train Loss: 0.0401, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0402, Val Loss: 0.0433\n",
            "Epoch 72/100, Train Loss: 0.0396, Val Loss: 0.0433\n",
            "Epoch 73/100, Train Loss: 0.0399, Val Loss: 0.0433\n",
            "Epoch 74/100, Train Loss: 0.0391, Val Loss: 0.0433\n",
            "Epoch 75/100, Train Loss: 0.0403, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0394, Val Loss: 0.0433\n",
            "Epoch 77/100, Train Loss: 0.0420, Val Loss: 0.0433\n",
            "Epoch 78/100, Train Loss: 0.0412, Val Loss: 0.0433\n",
            "Epoch 79/100, Train Loss: 0.0395, Val Loss: 0.0433\n",
            "Epoch 80/100, Train Loss: 0.0387, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0385, Val Loss: 0.0433\n",
            "Epoch 82/100, Train Loss: 0.0394, Val Loss: 0.0433\n",
            "Epoch 83/100, Train Loss: 0.0407, Val Loss: 0.0433\n",
            "Epoch 84/100, Train Loss: 0.0397, Val Loss: 0.0433\n",
            "Epoch 85/100, Train Loss: 0.0393, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0390, Val Loss: 0.0433\n",
            "Epoch 87/100, Train Loss: 0.0398, Val Loss: 0.0433\n",
            "Epoch 88/100, Train Loss: 0.0404, Val Loss: 0.0433\n",
            "Epoch 89/100, Train Loss: 0.0397, Val Loss: 0.0433\n",
            "Epoch 90/100, Train Loss: 0.0408, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0404, Val Loss: 0.0433\n",
            "Epoch 92/100, Train Loss: 0.0409, Val Loss: 0.0433\n",
            "Epoch 93/100, Train Loss: 0.0396, Val Loss: 0.0433\n",
            "Epoch 94/100, Train Loss: 0.0410, Val Loss: 0.0433\n",
            "Epoch 95/100, Train Loss: 0.0422, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0407, Val Loss: 0.0433\n",
            "Epoch 97/100, Train Loss: 0.0386, Val Loss: 0.0433\n",
            "Epoch 98/100, Train Loss: 0.0424, Val Loss: 0.0433\n",
            "Epoch 99/100, Train Loss: 0.0399, Val Loss: 0.0433\n",
            "Epoch 100/100, Train Loss: 0.0415, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 7/28\n",
            "Current training set size: 103 samples\n",
            "Epoch 1/100, Train Loss: 0.1567, Val Loss: 0.2076\n",
            "Epoch 2/100, Train Loss: 0.1463, Val Loss: 0.2024\n",
            "Epoch 3/100, Train Loss: 0.1388, Val Loss: 0.1742\n",
            "Epoch 4/100, Train Loss: 0.1361, Val Loss: 0.1679\n",
            "Epoch 5/100, Train Loss: 0.1169, Val Loss: 0.0469\n",
            "Epoch 6/100, Train Loss: 0.0879, Val Loss: 0.0845\n",
            "Epoch 7/100, Train Loss: 0.0735, Val Loss: 0.0999\n",
            "Epoch 8/100, Train Loss: 0.0803, Val Loss: 0.0511\n",
            "Epoch 9/100, Train Loss: 0.0595, Val Loss: 0.0326\n",
            "Epoch 10/100, Train Loss: 0.0640, Val Loss: 0.0627\n",
            "Epoch 11/100, Train Loss: 0.0605, Val Loss: 0.0574\n",
            "Epoch 12/100, Train Loss: 0.0548, Val Loss: 0.0552\n",
            "Epoch 13/100, Train Loss: 0.0551, Val Loss: 0.0502\n",
            "Epoch 14/100, Train Loss: 0.0535, Val Loss: 0.0527\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 15/100, Train Loss: 0.0454, Val Loss: 0.0477\n",
            "Epoch 16/100, Train Loss: 0.0427, Val Loss: 0.0432\n",
            "Epoch 17/100, Train Loss: 0.0419, Val Loss: 0.0399\n",
            "Epoch 18/100, Train Loss: 0.0404, Val Loss: 0.0380\n",
            "Epoch 19/100, Train Loss: 0.0397, Val Loss: 0.0364\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0386, Val Loss: 0.0364\n",
            "Epoch 21/100, Train Loss: 0.0393, Val Loss: 0.0364\n",
            "Epoch 22/100, Train Loss: 0.0407, Val Loss: 0.0364\n",
            "Epoch 23/100, Train Loss: 0.0395, Val Loss: 0.0364\n",
            "Epoch 24/100, Train Loss: 0.0396, Val Loss: 0.0364\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0399, Val Loss: 0.0364\n",
            "Epoch 26/100, Train Loss: 0.0389, Val Loss: 0.0364\n",
            "Epoch 27/100, Train Loss: 0.0399, Val Loss: 0.0364\n",
            "Epoch 28/100, Train Loss: 0.0398, Val Loss: 0.0364\n",
            "Epoch 29/100, Train Loss: 0.0394, Val Loss: 0.0364\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0406, Val Loss: 0.0364\n",
            "Epoch 31/100, Train Loss: 0.0413, Val Loss: 0.0364\n",
            "Epoch 32/100, Train Loss: 0.0399, Val Loss: 0.0364\n",
            "Epoch 33/100, Train Loss: 0.0410, Val Loss: 0.0364\n",
            "Epoch 34/100, Train Loss: 0.0412, Val Loss: 0.0364\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0391, Val Loss: 0.0364\n",
            "Epoch 36/100, Train Loss: 0.0403, Val Loss: 0.0364\n",
            "Epoch 37/100, Train Loss: 0.0390, Val Loss: 0.0364\n",
            "Epoch 38/100, Train Loss: 0.0395, Val Loss: 0.0364\n",
            "Epoch 39/100, Train Loss: 0.0389, Val Loss: 0.0364\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0395, Val Loss: 0.0364\n",
            "Epoch 41/100, Train Loss: 0.0393, Val Loss: 0.0364\n",
            "Epoch 42/100, Train Loss: 0.0404, Val Loss: 0.0364\n",
            "Epoch 43/100, Train Loss: 0.0400, Val Loss: 0.0364\n",
            "Epoch 44/100, Train Loss: 0.0405, Val Loss: 0.0364\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0398, Val Loss: 0.0364\n",
            "Epoch 46/100, Train Loss: 0.0381, Val Loss: 0.0364\n",
            "Epoch 47/100, Train Loss: 0.0373, Val Loss: 0.0364\n",
            "Epoch 48/100, Train Loss: 0.0418, Val Loss: 0.0364\n",
            "Epoch 49/100, Train Loss: 0.0416, Val Loss: 0.0364\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0388, Val Loss: 0.0364\n",
            "Epoch 51/100, Train Loss: 0.0399, Val Loss: 0.0364\n",
            "Epoch 52/100, Train Loss: 0.0402, Val Loss: 0.0364\n",
            "Epoch 53/100, Train Loss: 0.0406, Val Loss: 0.0364\n",
            "Epoch 54/100, Train Loss: 0.0404, Val Loss: 0.0364\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0391, Val Loss: 0.0364\n",
            "Epoch 56/100, Train Loss: 0.0392, Val Loss: 0.0364\n",
            "Epoch 57/100, Train Loss: 0.0417, Val Loss: 0.0364\n",
            "Epoch 58/100, Train Loss: 0.0396, Val Loss: 0.0364\n",
            "Epoch 59/100, Train Loss: 0.0394, Val Loss: 0.0364\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0403, Val Loss: 0.0364\n",
            "Epoch 61/100, Train Loss: 0.0382, Val Loss: 0.0364\n",
            "Epoch 62/100, Train Loss: 0.0388, Val Loss: 0.0364\n",
            "Epoch 63/100, Train Loss: 0.0416, Val Loss: 0.0364\n",
            "Epoch 64/100, Train Loss: 0.0416, Val Loss: 0.0364\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0394, Val Loss: 0.0364\n",
            "Epoch 66/100, Train Loss: 0.0389, Val Loss: 0.0364\n",
            "Epoch 67/100, Train Loss: 0.0406, Val Loss: 0.0364\n",
            "Epoch 68/100, Train Loss: 0.0386, Val Loss: 0.0364\n",
            "Epoch 69/100, Train Loss: 0.0379, Val Loss: 0.0364\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0401, Val Loss: 0.0364\n",
            "Epoch 71/100, Train Loss: 0.0399, Val Loss: 0.0364\n",
            "Epoch 72/100, Train Loss: 0.0393, Val Loss: 0.0364\n",
            "Epoch 73/100, Train Loss: 0.0404, Val Loss: 0.0364\n",
            "Epoch 74/100, Train Loss: 0.0387, Val Loss: 0.0364\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0404, Val Loss: 0.0364\n",
            "Epoch 76/100, Train Loss: 0.0386, Val Loss: 0.0364\n",
            "Epoch 77/100, Train Loss: 0.0411, Val Loss: 0.0364\n",
            "Epoch 78/100, Train Loss: 0.0407, Val Loss: 0.0364\n",
            "Epoch 79/100, Train Loss: 0.0398, Val Loss: 0.0364\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0394, Val Loss: 0.0364\n",
            "Epoch 81/100, Train Loss: 0.0394, Val Loss: 0.0364\n",
            "Epoch 82/100, Train Loss: 0.0394, Val Loss: 0.0364\n",
            "Epoch 83/100, Train Loss: 0.0387, Val Loss: 0.0364\n",
            "Epoch 84/100, Train Loss: 0.0403, Val Loss: 0.0364\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0387, Val Loss: 0.0364\n",
            "Epoch 86/100, Train Loss: 0.0386, Val Loss: 0.0364\n",
            "Epoch 87/100, Train Loss: 0.0397, Val Loss: 0.0364\n",
            "Epoch 88/100, Train Loss: 0.0399, Val Loss: 0.0364\n",
            "Epoch 89/100, Train Loss: 0.0394, Val Loss: 0.0364\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0409, Val Loss: 0.0364\n",
            "Epoch 91/100, Train Loss: 0.0386, Val Loss: 0.0364\n",
            "Epoch 92/100, Train Loss: 0.0391, Val Loss: 0.0364\n",
            "Epoch 93/100, Train Loss: 0.0400, Val Loss: 0.0364\n",
            "Epoch 94/100, Train Loss: 0.0394, Val Loss: 0.0364\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0402, Val Loss: 0.0364\n",
            "Epoch 96/100, Train Loss: 0.0389, Val Loss: 0.0364\n",
            "Epoch 97/100, Train Loss: 0.0401, Val Loss: 0.0364\n",
            "Epoch 98/100, Train Loss: 0.0393, Val Loss: 0.0364\n",
            "Epoch 99/100, Train Loss: 0.0408, Val Loss: 0.0364\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0388, Val Loss: 0.0364\n",
            "\n",
            "Test iteration 8/28\n",
            "Current training set size: 104 samples\n",
            "Epoch 1/100, Train Loss: 0.1600, Val Loss: 0.1948\n",
            "Epoch 2/100, Train Loss: 0.1459, Val Loss: 0.1817\n",
            "Epoch 3/100, Train Loss: 0.1401, Val Loss: 0.1871\n",
            "Epoch 4/100, Train Loss: 0.1189, Val Loss: 0.1402\n",
            "Epoch 5/100, Train Loss: 0.1064, Val Loss: 0.0519\n",
            "Epoch 6/100, Train Loss: 0.0884, Val Loss: 0.0794\n",
            "Epoch 7/100, Train Loss: 0.0867, Val Loss: 0.0869\n",
            "Epoch 8/100, Train Loss: 0.0910, Val Loss: 0.0619\n",
            "Epoch 9/100, Train Loss: 0.0742, Val Loss: 0.0407\n",
            "Epoch 10/100, Train Loss: 0.0620, Val Loss: 0.0415\n",
            "Epoch 11/100, Train Loss: 0.0533, Val Loss: 0.0405\n",
            "Epoch 12/100, Train Loss: 0.0546, Val Loss: 0.0333\n",
            "Epoch 13/100, Train Loss: 0.0497, Val Loss: 0.0442\n",
            "Epoch 14/100, Train Loss: 0.0581, Val Loss: 0.0421\n",
            "Epoch 15/100, Train Loss: 0.0410, Val Loss: 0.0425\n",
            "Epoch 16/100, Train Loss: 0.0469, Val Loss: 0.0625\n",
            "Epoch 17/100, Train Loss: 0.0475, Val Loss: 0.0479\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0534, Val Loss: 0.0450\n",
            "Epoch 19/100, Train Loss: 0.0457, Val Loss: 0.0424\n",
            "Epoch 20/100, Train Loss: 0.0404, Val Loss: 0.0411\n",
            "Epoch 21/100, Train Loss: 0.0391, Val Loss: 0.0407\n",
            "Epoch 22/100, Train Loss: 0.0379, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0363, Val Loss: 0.0401\n",
            "Epoch 24/100, Train Loss: 0.0357, Val Loss: 0.0401\n",
            "Epoch 25/100, Train Loss: 0.0358, Val Loss: 0.0401\n",
            "Epoch 26/100, Train Loss: 0.0361, Val Loss: 0.0401\n",
            "Epoch 27/100, Train Loss: 0.0376, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0356, Val Loss: 0.0401\n",
            "Epoch 29/100, Train Loss: 0.0370, Val Loss: 0.0401\n",
            "Epoch 30/100, Train Loss: 0.0360, Val Loss: 0.0401\n",
            "Epoch 31/100, Train Loss: 0.0361, Val Loss: 0.0401\n",
            "Epoch 32/100, Train Loss: 0.0379, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0360, Val Loss: 0.0401\n",
            "Epoch 34/100, Train Loss: 0.0355, Val Loss: 0.0401\n",
            "Epoch 35/100, Train Loss: 0.0367, Val Loss: 0.0401\n",
            "Epoch 36/100, Train Loss: 0.0369, Val Loss: 0.0401\n",
            "Epoch 37/100, Train Loss: 0.0359, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0363, Val Loss: 0.0401\n",
            "Epoch 39/100, Train Loss: 0.0354, Val Loss: 0.0401\n",
            "Epoch 40/100, Train Loss: 0.0360, Val Loss: 0.0401\n",
            "Epoch 41/100, Train Loss: 0.0357, Val Loss: 0.0401\n",
            "Epoch 42/100, Train Loss: 0.0373, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0354, Val Loss: 0.0401\n",
            "Epoch 44/100, Train Loss: 0.0356, Val Loss: 0.0401\n",
            "Epoch 45/100, Train Loss: 0.0365, Val Loss: 0.0401\n",
            "Epoch 46/100, Train Loss: 0.0368, Val Loss: 0.0401\n",
            "Epoch 47/100, Train Loss: 0.0375, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0368, Val Loss: 0.0401\n",
            "Epoch 49/100, Train Loss: 0.0353, Val Loss: 0.0401\n",
            "Epoch 50/100, Train Loss: 0.0358, Val Loss: 0.0401\n",
            "Epoch 51/100, Train Loss: 0.0364, Val Loss: 0.0401\n",
            "Epoch 52/100, Train Loss: 0.0370, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0365, Val Loss: 0.0401\n",
            "Epoch 54/100, Train Loss: 0.0366, Val Loss: 0.0401\n",
            "Epoch 55/100, Train Loss: 0.0367, Val Loss: 0.0401\n",
            "Epoch 56/100, Train Loss: 0.0357, Val Loss: 0.0401\n",
            "Epoch 57/100, Train Loss: 0.0366, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0366, Val Loss: 0.0401\n",
            "Epoch 59/100, Train Loss: 0.0357, Val Loss: 0.0401\n",
            "Epoch 60/100, Train Loss: 0.0366, Val Loss: 0.0401\n",
            "Epoch 61/100, Train Loss: 0.0373, Val Loss: 0.0401\n",
            "Epoch 62/100, Train Loss: 0.0375, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0366, Val Loss: 0.0401\n",
            "Epoch 64/100, Train Loss: 0.0349, Val Loss: 0.0401\n",
            "Epoch 65/100, Train Loss: 0.0371, Val Loss: 0.0401\n",
            "Epoch 66/100, Train Loss: 0.0375, Val Loss: 0.0401\n",
            "Epoch 67/100, Train Loss: 0.0345, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0351, Val Loss: 0.0401\n",
            "Epoch 69/100, Train Loss: 0.0363, Val Loss: 0.0401\n",
            "Epoch 70/100, Train Loss: 0.0357, Val Loss: 0.0401\n",
            "Epoch 71/100, Train Loss: 0.0379, Val Loss: 0.0401\n",
            "Epoch 72/100, Train Loss: 0.0370, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0376, Val Loss: 0.0401\n",
            "Epoch 74/100, Train Loss: 0.0373, Val Loss: 0.0401\n",
            "Epoch 75/100, Train Loss: 0.0371, Val Loss: 0.0401\n",
            "Epoch 76/100, Train Loss: 0.0365, Val Loss: 0.0401\n",
            "Epoch 77/100, Train Loss: 0.0364, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0373, Val Loss: 0.0401\n",
            "Epoch 79/100, Train Loss: 0.0352, Val Loss: 0.0401\n",
            "Epoch 80/100, Train Loss: 0.0374, Val Loss: 0.0401\n",
            "Epoch 81/100, Train Loss: 0.0373, Val Loss: 0.0401\n",
            "Epoch 82/100, Train Loss: 0.0362, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0362, Val Loss: 0.0401\n",
            "Epoch 84/100, Train Loss: 0.0368, Val Loss: 0.0401\n",
            "Epoch 85/100, Train Loss: 0.0364, Val Loss: 0.0401\n",
            "Epoch 86/100, Train Loss: 0.0370, Val Loss: 0.0401\n",
            "Epoch 87/100, Train Loss: 0.0375, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0381, Val Loss: 0.0401\n",
            "Epoch 89/100, Train Loss: 0.0365, Val Loss: 0.0401\n",
            "Epoch 90/100, Train Loss: 0.0360, Val Loss: 0.0401\n",
            "Epoch 91/100, Train Loss: 0.0365, Val Loss: 0.0401\n",
            "Epoch 92/100, Train Loss: 0.0360, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0366, Val Loss: 0.0401\n",
            "Epoch 94/100, Train Loss: 0.0356, Val Loss: 0.0401\n",
            "Epoch 95/100, Train Loss: 0.0354, Val Loss: 0.0401\n",
            "Epoch 96/100, Train Loss: 0.0367, Val Loss: 0.0401\n",
            "Epoch 97/100, Train Loss: 0.0375, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0367, Val Loss: 0.0401\n",
            "Epoch 99/100, Train Loss: 0.0366, Val Loss: 0.0401\n",
            "Epoch 100/100, Train Loss: 0.0383, Val Loss: 0.0401\n",
            "\n",
            "Test iteration 9/28\n",
            "Current training set size: 105 samples\n",
            "Epoch 1/100, Train Loss: 0.1592, Val Loss: 0.2141\n",
            "Epoch 2/100, Train Loss: 0.1470, Val Loss: 0.1987\n",
            "Epoch 3/100, Train Loss: 0.1317, Val Loss: 0.1649\n",
            "Epoch 4/100, Train Loss: 0.1258, Val Loss: 0.1523\n",
            "Epoch 5/100, Train Loss: 0.1030, Val Loss: 0.0795\n",
            "Epoch 6/100, Train Loss: 0.0877, Val Loss: 0.0932\n",
            "Epoch 7/100, Train Loss: 0.0875, Val Loss: 0.0583\n",
            "Epoch 8/100, Train Loss: 0.0757, Val Loss: 0.0401\n",
            "Epoch 9/100, Train Loss: 0.0653, Val Loss: 0.0333\n",
            "Epoch 10/100, Train Loss: 0.0624, Val Loss: 0.0485\n",
            "Epoch 11/100, Train Loss: 0.0554, Val Loss: 0.0713\n",
            "Epoch 12/100, Train Loss: 0.0640, Val Loss: 0.0514\n",
            "Epoch 13/100, Train Loss: 0.0472, Val Loss: 0.0351\n",
            "Epoch 14/100, Train Loss: 0.0459, Val Loss: 0.0447\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 15/100, Train Loss: 0.0441, Val Loss: 0.0440\n",
            "Epoch 16/100, Train Loss: 0.0409, Val Loss: 0.0412\n",
            "Epoch 17/100, Train Loss: 0.0389, Val Loss: 0.0390\n",
            "Epoch 18/100, Train Loss: 0.0376, Val Loss: 0.0372\n",
            "Epoch 19/100, Train Loss: 0.0392, Val Loss: 0.0371\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0381, Val Loss: 0.0371\n",
            "Epoch 21/100, Train Loss: 0.0389, Val Loss: 0.0371\n",
            "Epoch 22/100, Train Loss: 0.0385, Val Loss: 0.0371\n",
            "Epoch 23/100, Train Loss: 0.0404, Val Loss: 0.0371\n",
            "Epoch 24/100, Train Loss: 0.0385, Val Loss: 0.0371\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0432, Val Loss: 0.0371\n",
            "Epoch 26/100, Train Loss: 0.0377, Val Loss: 0.0371\n",
            "Epoch 27/100, Train Loss: 0.0376, Val Loss: 0.0371\n",
            "Epoch 28/100, Train Loss: 0.0391, Val Loss: 0.0371\n",
            "Epoch 29/100, Train Loss: 0.0378, Val Loss: 0.0371\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0394, Val Loss: 0.0371\n",
            "Epoch 31/100, Train Loss: 0.0380, Val Loss: 0.0371\n",
            "Epoch 32/100, Train Loss: 0.0366, Val Loss: 0.0371\n",
            "Epoch 33/100, Train Loss: 0.0383, Val Loss: 0.0371\n",
            "Epoch 34/100, Train Loss: 0.0380, Val Loss: 0.0371\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0378, Val Loss: 0.0371\n",
            "Epoch 36/100, Train Loss: 0.0385, Val Loss: 0.0371\n",
            "Epoch 37/100, Train Loss: 0.0388, Val Loss: 0.0371\n",
            "Epoch 38/100, Train Loss: 0.0387, Val Loss: 0.0371\n",
            "Epoch 39/100, Train Loss: 0.0389, Val Loss: 0.0371\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0379, Val Loss: 0.0371\n",
            "Epoch 41/100, Train Loss: 0.0376, Val Loss: 0.0371\n",
            "Epoch 42/100, Train Loss: 0.0377, Val Loss: 0.0371\n",
            "Epoch 43/100, Train Loss: 0.0383, Val Loss: 0.0371\n",
            "Epoch 44/100, Train Loss: 0.0371, Val Loss: 0.0371\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0379, Val Loss: 0.0371\n",
            "Epoch 46/100, Train Loss: 0.0375, Val Loss: 0.0371\n",
            "Epoch 47/100, Train Loss: 0.0380, Val Loss: 0.0371\n",
            "Epoch 48/100, Train Loss: 0.0385, Val Loss: 0.0371\n",
            "Epoch 49/100, Train Loss: 0.0363, Val Loss: 0.0371\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0402, Val Loss: 0.0371\n",
            "Epoch 51/100, Train Loss: 0.0394, Val Loss: 0.0371\n",
            "Epoch 52/100, Train Loss: 0.0384, Val Loss: 0.0371\n",
            "Epoch 53/100, Train Loss: 0.0370, Val Loss: 0.0371\n",
            "Epoch 54/100, Train Loss: 0.0395, Val Loss: 0.0371\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0395, Val Loss: 0.0371\n",
            "Epoch 56/100, Train Loss: 0.0381, Val Loss: 0.0371\n",
            "Epoch 57/100, Train Loss: 0.0382, Val Loss: 0.0371\n",
            "Epoch 58/100, Train Loss: 0.0380, Val Loss: 0.0371\n",
            "Epoch 59/100, Train Loss: 0.0367, Val Loss: 0.0371\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0418, Val Loss: 0.0371\n",
            "Epoch 61/100, Train Loss: 0.0450, Val Loss: 0.0371\n",
            "Epoch 62/100, Train Loss: 0.0396, Val Loss: 0.0371\n",
            "Epoch 63/100, Train Loss: 0.0393, Val Loss: 0.0371\n",
            "Epoch 64/100, Train Loss: 0.0391, Val Loss: 0.0371\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0386, Val Loss: 0.0371\n",
            "Epoch 66/100, Train Loss: 0.0397, Val Loss: 0.0371\n",
            "Epoch 67/100, Train Loss: 0.0395, Val Loss: 0.0371\n",
            "Epoch 68/100, Train Loss: 0.0383, Val Loss: 0.0371\n",
            "Epoch 69/100, Train Loss: 0.0393, Val Loss: 0.0371\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0393, Val Loss: 0.0371\n",
            "Epoch 71/100, Train Loss: 0.0391, Val Loss: 0.0371\n",
            "Epoch 72/100, Train Loss: 0.0382, Val Loss: 0.0371\n",
            "Epoch 73/100, Train Loss: 0.0380, Val Loss: 0.0371\n",
            "Epoch 74/100, Train Loss: 0.0385, Val Loss: 0.0371\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0378, Val Loss: 0.0371\n",
            "Epoch 76/100, Train Loss: 0.0393, Val Loss: 0.0371\n",
            "Epoch 77/100, Train Loss: 0.0391, Val Loss: 0.0371\n",
            "Epoch 78/100, Train Loss: 0.0371, Val Loss: 0.0371\n",
            "Epoch 79/100, Train Loss: 0.0379, Val Loss: 0.0371\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0390, Val Loss: 0.0371\n",
            "Epoch 81/100, Train Loss: 0.0377, Val Loss: 0.0371\n",
            "Epoch 82/100, Train Loss: 0.0390, Val Loss: 0.0371\n",
            "Epoch 83/100, Train Loss: 0.0392, Val Loss: 0.0371\n",
            "Epoch 84/100, Train Loss: 0.0378, Val Loss: 0.0371\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0390, Val Loss: 0.0371\n",
            "Epoch 86/100, Train Loss: 0.0371, Val Loss: 0.0371\n",
            "Epoch 87/100, Train Loss: 0.0387, Val Loss: 0.0371\n",
            "Epoch 88/100, Train Loss: 0.0393, Val Loss: 0.0371\n",
            "Epoch 89/100, Train Loss: 0.0396, Val Loss: 0.0371\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0389, Val Loss: 0.0371\n",
            "Epoch 91/100, Train Loss: 0.0380, Val Loss: 0.0371\n",
            "Epoch 92/100, Train Loss: 0.0396, Val Loss: 0.0371\n",
            "Epoch 93/100, Train Loss: 0.0374, Val Loss: 0.0371\n",
            "Epoch 94/100, Train Loss: 0.0393, Val Loss: 0.0371\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0383, Val Loss: 0.0371\n",
            "Epoch 96/100, Train Loss: 0.0402, Val Loss: 0.0371\n",
            "Epoch 97/100, Train Loss: 0.0387, Val Loss: 0.0371\n",
            "Epoch 98/100, Train Loss: 0.0381, Val Loss: 0.0371\n",
            "Epoch 99/100, Train Loss: 0.0370, Val Loss: 0.0371\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0409, Val Loss: 0.0371\n",
            "\n",
            "Test iteration 10/28\n",
            "Current training set size: 106 samples\n",
            "Epoch 1/100, Train Loss: 0.1626, Val Loss: 0.2103\n",
            "Epoch 2/100, Train Loss: 0.1431, Val Loss: 0.1819\n",
            "Epoch 3/100, Train Loss: 0.1388, Val Loss: 0.1386\n",
            "Epoch 4/100, Train Loss: 0.1203, Val Loss: 0.1734\n",
            "Epoch 5/100, Train Loss: 0.1133, Val Loss: 0.0982\n",
            "Epoch 6/100, Train Loss: 0.0944, Val Loss: 0.0501\n",
            "Epoch 7/100, Train Loss: 0.0749, Val Loss: 0.0351\n",
            "Epoch 8/100, Train Loss: 0.0744, Val Loss: 0.0359\n",
            "Epoch 9/100, Train Loss: 0.0743, Val Loss: 0.0729\n",
            "Epoch 10/100, Train Loss: 0.0626, Val Loss: 0.0411\n",
            "Epoch 11/100, Train Loss: 0.0614, Val Loss: 0.0347\n",
            "Epoch 12/100, Train Loss: 0.0562, Val Loss: 0.0326\n",
            "Epoch 13/100, Train Loss: 0.0546, Val Loss: 0.0425\n",
            "Epoch 14/100, Train Loss: 0.0502, Val Loss: 0.0477\n",
            "Epoch 15/100, Train Loss: 0.0478, Val Loss: 0.0418\n",
            "Epoch 16/100, Train Loss: 0.0601, Val Loss: 0.0397\n",
            "Epoch 17/100, Train Loss: 0.0417, Val Loss: 0.0436\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0379, Val Loss: 0.0398\n",
            "Epoch 19/100, Train Loss: 0.0364, Val Loss: 0.0377\n",
            "Epoch 20/100, Train Loss: 0.0348, Val Loss: 0.0374\n",
            "Epoch 21/100, Train Loss: 0.0349, Val Loss: 0.0369\n",
            "Epoch 22/100, Train Loss: 0.0357, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0355, Val Loss: 0.0370\n",
            "Epoch 24/100, Train Loss: 0.0348, Val Loss: 0.0370\n",
            "Epoch 25/100, Train Loss: 0.0354, Val Loss: 0.0370\n",
            "Epoch 26/100, Train Loss: 0.0349, Val Loss: 0.0370\n",
            "Epoch 27/100, Train Loss: 0.0342, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0354, Val Loss: 0.0370\n",
            "Epoch 29/100, Train Loss: 0.0360, Val Loss: 0.0370\n",
            "Epoch 30/100, Train Loss: 0.0357, Val Loss: 0.0370\n",
            "Epoch 31/100, Train Loss: 0.0342, Val Loss: 0.0370\n",
            "Epoch 32/100, Train Loss: 0.0399, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0350, Val Loss: 0.0370\n",
            "Epoch 34/100, Train Loss: 0.0391, Val Loss: 0.0370\n",
            "Epoch 35/100, Train Loss: 0.0354, Val Loss: 0.0370\n",
            "Epoch 36/100, Train Loss: 0.0359, Val Loss: 0.0370\n",
            "Epoch 37/100, Train Loss: 0.0343, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0354, Val Loss: 0.0370\n",
            "Epoch 39/100, Train Loss: 0.0388, Val Loss: 0.0370\n",
            "Epoch 40/100, Train Loss: 0.0359, Val Loss: 0.0370\n",
            "Epoch 41/100, Train Loss: 0.0347, Val Loss: 0.0370\n",
            "Epoch 42/100, Train Loss: 0.0342, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0359, Val Loss: 0.0370\n",
            "Epoch 44/100, Train Loss: 0.0359, Val Loss: 0.0370\n",
            "Epoch 45/100, Train Loss: 0.0361, Val Loss: 0.0370\n",
            "Epoch 46/100, Train Loss: 0.0357, Val Loss: 0.0370\n",
            "Epoch 47/100, Train Loss: 0.0367, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0362, Val Loss: 0.0370\n",
            "Epoch 49/100, Train Loss: 0.0351, Val Loss: 0.0370\n",
            "Epoch 50/100, Train Loss: 0.0351, Val Loss: 0.0370\n",
            "Epoch 51/100, Train Loss: 0.0343, Val Loss: 0.0370\n",
            "Epoch 52/100, Train Loss: 0.0355, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0340, Val Loss: 0.0370\n",
            "Epoch 54/100, Train Loss: 0.0347, Val Loss: 0.0370\n",
            "Epoch 55/100, Train Loss: 0.0339, Val Loss: 0.0370\n",
            "Epoch 56/100, Train Loss: 0.0400, Val Loss: 0.0370\n",
            "Epoch 57/100, Train Loss: 0.0351, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0349, Val Loss: 0.0370\n",
            "Epoch 59/100, Train Loss: 0.0351, Val Loss: 0.0370\n",
            "Epoch 60/100, Train Loss: 0.0338, Val Loss: 0.0370\n",
            "Epoch 61/100, Train Loss: 0.0339, Val Loss: 0.0370\n",
            "Epoch 62/100, Train Loss: 0.0340, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0360, Val Loss: 0.0370\n",
            "Epoch 64/100, Train Loss: 0.0349, Val Loss: 0.0370\n",
            "Epoch 65/100, Train Loss: 0.0351, Val Loss: 0.0370\n",
            "Epoch 66/100, Train Loss: 0.0345, Val Loss: 0.0370\n",
            "Epoch 67/100, Train Loss: 0.0336, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0331, Val Loss: 0.0370\n",
            "Epoch 69/100, Train Loss: 0.0351, Val Loss: 0.0370\n",
            "Epoch 70/100, Train Loss: 0.0360, Val Loss: 0.0370\n",
            "Epoch 71/100, Train Loss: 0.0351, Val Loss: 0.0370\n",
            "Epoch 72/100, Train Loss: 0.0359, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0344, Val Loss: 0.0370\n",
            "Epoch 74/100, Train Loss: 0.0339, Val Loss: 0.0370\n",
            "Epoch 75/100, Train Loss: 0.0363, Val Loss: 0.0370\n",
            "Epoch 76/100, Train Loss: 0.0345, Val Loss: 0.0370\n",
            "Epoch 77/100, Train Loss: 0.0342, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0364, Val Loss: 0.0370\n",
            "Epoch 79/100, Train Loss: 0.0355, Val Loss: 0.0370\n",
            "Epoch 80/100, Train Loss: 0.0356, Val Loss: 0.0370\n",
            "Epoch 81/100, Train Loss: 0.0344, Val Loss: 0.0370\n",
            "Epoch 82/100, Train Loss: 0.0359, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0334, Val Loss: 0.0370\n",
            "Epoch 84/100, Train Loss: 0.0336, Val Loss: 0.0370\n",
            "Epoch 85/100, Train Loss: 0.0390, Val Loss: 0.0370\n",
            "Epoch 86/100, Train Loss: 0.0361, Val Loss: 0.0370\n",
            "Epoch 87/100, Train Loss: 0.0350, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0344, Val Loss: 0.0370\n",
            "Epoch 89/100, Train Loss: 0.0354, Val Loss: 0.0370\n",
            "Epoch 90/100, Train Loss: 0.0345, Val Loss: 0.0370\n",
            "Epoch 91/100, Train Loss: 0.0353, Val Loss: 0.0370\n",
            "Epoch 92/100, Train Loss: 0.0360, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0354, Val Loss: 0.0370\n",
            "Epoch 94/100, Train Loss: 0.0348, Val Loss: 0.0370\n",
            "Epoch 95/100, Train Loss: 0.0354, Val Loss: 0.0370\n",
            "Epoch 96/100, Train Loss: 0.0353, Val Loss: 0.0370\n",
            "Epoch 97/100, Train Loss: 0.0351, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0355, Val Loss: 0.0370\n",
            "Epoch 99/100, Train Loss: 0.0342, Val Loss: 0.0370\n",
            "Epoch 100/100, Train Loss: 0.0347, Val Loss: 0.0370\n",
            "\n",
            "Test iteration 11/28\n",
            "Current training set size: 107 samples\n",
            "Epoch 1/100, Train Loss: 0.1484, Val Loss: 0.1842\n",
            "Epoch 2/100, Train Loss: 0.1397, Val Loss: 0.1859\n",
            "Epoch 3/100, Train Loss: 0.1287, Val Loss: 0.1376\n",
            "Epoch 4/100, Train Loss: 0.1292, Val Loss: 0.1450\n",
            "Epoch 5/100, Train Loss: 0.0963, Val Loss: 0.0597\n",
            "Epoch 6/100, Train Loss: 0.0853, Val Loss: 0.0564\n",
            "Epoch 7/100, Train Loss: 0.0777, Val Loss: 0.0403\n",
            "Epoch 8/100, Train Loss: 0.0710, Val Loss: 0.0813\n",
            "Epoch 9/100, Train Loss: 0.0836, Val Loss: 0.0509\n",
            "Epoch 10/100, Train Loss: 0.0600, Val Loss: 0.0615\n",
            "Epoch 11/100, Train Loss: 0.0627, Val Loss: 0.0518\n",
            "Epoch 12/100, Train Loss: 0.0525, Val Loss: 0.0461\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 13/100, Train Loss: 0.0495, Val Loss: 0.0422\n",
            "Epoch 14/100, Train Loss: 0.0445, Val Loss: 0.0384\n",
            "Epoch 15/100, Train Loss: 0.0418, Val Loss: 0.0380\n",
            "Epoch 16/100, Train Loss: 0.0416, Val Loss: 0.0377\n",
            "Epoch 17/100, Train Loss: 0.0430, Val Loss: 0.0376\n",
            "Epoch 18/100, Train Loss: 0.0423, Val Loss: 0.0375\n",
            "Epoch 19/100, Train Loss: 0.0414, Val Loss: 0.0375\n",
            "Epoch 20/100, Train Loss: 0.0445, Val Loss: 0.0374\n",
            "Epoch 21/100, Train Loss: 0.0415, Val Loss: 0.0370\n",
            "Epoch 22/100, Train Loss: 0.0426, Val Loss: 0.0371\n",
            "Epoch 23/100, Train Loss: 0.0418, Val Loss: 0.0369\n",
            "Epoch 24/100, Train Loss: 0.0420, Val Loss: 0.0367\n",
            "Epoch 25/100, Train Loss: 0.0410, Val Loss: 0.0368\n",
            "Epoch 26/100, Train Loss: 0.0414, Val Loss: 0.0367\n",
            "Epoch 27/100, Train Loss: 0.0407, Val Loss: 0.0367\n",
            "Epoch 28/100, Train Loss: 0.0413, Val Loss: 0.0364\n",
            "Epoch 29/100, Train Loss: 0.0412, Val Loss: 0.0361\n",
            "Epoch 30/100, Train Loss: 0.0408, Val Loss: 0.0360\n",
            "Epoch 31/100, Train Loss: 0.0413, Val Loss: 0.0357\n",
            "Epoch 32/100, Train Loss: 0.0396, Val Loss: 0.0355\n",
            "Epoch 33/100, Train Loss: 0.0420, Val Loss: 0.0354\n",
            "Epoch 34/100, Train Loss: 0.0422, Val Loss: 0.0354\n",
            "Epoch 35/100, Train Loss: 0.0416, Val Loss: 0.0353\n",
            "Epoch 36/100, Train Loss: 0.0395, Val Loss: 0.0353\n",
            "Epoch 37/100, Train Loss: 0.0399, Val Loss: 0.0353\n",
            "Epoch 38/100, Train Loss: 0.0405, Val Loss: 0.0352\n",
            "Epoch 39/100, Train Loss: 0.0407, Val Loss: 0.0350\n",
            "Epoch 40/100, Train Loss: 0.0412, Val Loss: 0.0350\n",
            "Epoch 41/100, Train Loss: 0.0385, Val Loss: 0.0350\n",
            "Epoch 42/100, Train Loss: 0.0395, Val Loss: 0.0354\n",
            "Epoch 43/100, Train Loss: 0.0395, Val Loss: 0.0350\n",
            "Epoch 44/100, Train Loss: 0.0401, Val Loss: 0.0347\n",
            "Epoch 45/100, Train Loss: 0.0398, Val Loss: 0.0346\n",
            "Epoch 46/100, Train Loss: 0.0396, Val Loss: 0.0351\n",
            "Epoch 47/100, Train Loss: 0.0388, Val Loss: 0.0350\n",
            "Epoch 48/100, Train Loss: 0.0391, Val Loss: 0.0353\n",
            "Epoch 49/100, Train Loss: 0.0396, Val Loss: 0.0354\n",
            "Epoch 50/100, Train Loss: 0.0404, Val Loss: 0.0349\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0392, Val Loss: 0.0349\n",
            "Epoch 52/100, Train Loss: 0.0382, Val Loss: 0.0349\n",
            "Epoch 53/100, Train Loss: 0.0395, Val Loss: 0.0349\n",
            "Epoch 54/100, Train Loss: 0.0394, Val Loss: 0.0349\n",
            "Epoch 55/100, Train Loss: 0.0395, Val Loss: 0.0349\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0380, Val Loss: 0.0349\n",
            "Epoch 57/100, Train Loss: 0.0386, Val Loss: 0.0349\n",
            "Epoch 58/100, Train Loss: 0.0405, Val Loss: 0.0349\n",
            "Epoch 59/100, Train Loss: 0.0391, Val Loss: 0.0349\n",
            "Epoch 60/100, Train Loss: 0.0394, Val Loss: 0.0349\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0402, Val Loss: 0.0349\n",
            "Epoch 62/100, Train Loss: 0.0394, Val Loss: 0.0349\n",
            "Epoch 63/100, Train Loss: 0.0396, Val Loss: 0.0349\n",
            "Epoch 64/100, Train Loss: 0.0383, Val Loss: 0.0349\n",
            "Epoch 65/100, Train Loss: 0.0400, Val Loss: 0.0349\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0407, Val Loss: 0.0349\n",
            "Epoch 67/100, Train Loss: 0.0400, Val Loss: 0.0349\n",
            "Epoch 68/100, Train Loss: 0.0398, Val Loss: 0.0349\n",
            "Epoch 69/100, Train Loss: 0.0409, Val Loss: 0.0349\n",
            "Epoch 70/100, Train Loss: 0.0388, Val Loss: 0.0349\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0391, Val Loss: 0.0349\n",
            "Epoch 72/100, Train Loss: 0.0395, Val Loss: 0.0349\n",
            "Epoch 73/100, Train Loss: 0.0397, Val Loss: 0.0349\n",
            "Epoch 74/100, Train Loss: 0.0394, Val Loss: 0.0349\n",
            "Epoch 75/100, Train Loss: 0.0400, Val Loss: 0.0349\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0401, Val Loss: 0.0349\n",
            "Epoch 77/100, Train Loss: 0.0395, Val Loss: 0.0349\n",
            "Epoch 78/100, Train Loss: 0.0393, Val Loss: 0.0349\n",
            "Epoch 79/100, Train Loss: 0.0405, Val Loss: 0.0349\n",
            "Epoch 80/100, Train Loss: 0.0403, Val Loss: 0.0349\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0391, Val Loss: 0.0349\n",
            "Epoch 82/100, Train Loss: 0.0399, Val Loss: 0.0349\n",
            "Epoch 83/100, Train Loss: 0.0381, Val Loss: 0.0349\n",
            "Epoch 84/100, Train Loss: 0.0401, Val Loss: 0.0349\n",
            "Epoch 85/100, Train Loss: 0.0402, Val Loss: 0.0349\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0401, Val Loss: 0.0349\n",
            "Epoch 87/100, Train Loss: 0.0398, Val Loss: 0.0349\n",
            "Epoch 88/100, Train Loss: 0.0384, Val Loss: 0.0349\n",
            "Epoch 89/100, Train Loss: 0.0387, Val Loss: 0.0349\n",
            "Epoch 90/100, Train Loss: 0.0375, Val Loss: 0.0349\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0400, Val Loss: 0.0349\n",
            "Epoch 92/100, Train Loss: 0.0399, Val Loss: 0.0349\n",
            "Epoch 93/100, Train Loss: 0.0386, Val Loss: 0.0349\n",
            "Epoch 94/100, Train Loss: 0.0373, Val Loss: 0.0349\n",
            "Epoch 95/100, Train Loss: 0.0397, Val Loss: 0.0349\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0392, Val Loss: 0.0349\n",
            "Epoch 97/100, Train Loss: 0.0396, Val Loss: 0.0349\n",
            "Epoch 98/100, Train Loss: 0.0399, Val Loss: 0.0349\n",
            "Epoch 99/100, Train Loss: 0.0374, Val Loss: 0.0349\n",
            "Epoch 100/100, Train Loss: 0.0390, Val Loss: 0.0349\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 12/28\n",
            "Current training set size: 108 samples\n",
            "Epoch 1/100, Train Loss: 0.1569, Val Loss: 0.1881\n",
            "Epoch 2/100, Train Loss: 0.1479, Val Loss: 0.1797\n",
            "Epoch 3/100, Train Loss: 0.1358, Val Loss: 0.1558\n",
            "Epoch 4/100, Train Loss: 0.1238, Val Loss: 0.1556\n",
            "Epoch 5/100, Train Loss: 0.1136, Val Loss: 0.1014\n",
            "Epoch 6/100, Train Loss: 0.1033, Val Loss: 0.0701\n",
            "Epoch 7/100, Train Loss: 0.0693, Val Loss: 0.0956\n",
            "Epoch 8/100, Train Loss: 0.0856, Val Loss: 0.0495\n",
            "Epoch 9/100, Train Loss: 0.0749, Val Loss: 0.0589\n",
            "Epoch 10/100, Train Loss: 0.0679, Val Loss: 0.0561\n",
            "Epoch 11/100, Train Loss: 0.0771, Val Loss: 0.0734\n",
            "Epoch 12/100, Train Loss: 0.0561, Val Loss: 0.0505\n",
            "Epoch 13/100, Train Loss: 0.0503, Val Loss: 0.0432\n",
            "Epoch 14/100, Train Loss: 0.0605, Val Loss: 0.0512\n",
            "Epoch 15/100, Train Loss: 0.0494, Val Loss: 0.0480\n",
            "Epoch 16/100, Train Loss: 0.0427, Val Loss: 0.0335\n",
            "Epoch 17/100, Train Loss: 0.0404, Val Loss: 0.0328\n",
            "Epoch 18/100, Train Loss: 0.0406, Val Loss: 0.0688\n",
            "Epoch 19/100, Train Loss: 0.0441, Val Loss: 0.0383\n",
            "Epoch 20/100, Train Loss: 0.0436, Val Loss: 0.0397\n",
            "Epoch 21/100, Train Loss: 0.0404, Val Loss: 0.0421\n",
            "Epoch 22/100, Train Loss: 0.0380, Val Loss: 0.0494\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0400, Val Loss: 0.0449\n",
            "Epoch 24/100, Train Loss: 0.0344, Val Loss: 0.0398\n",
            "Epoch 25/100, Train Loss: 0.0331, Val Loss: 0.0388\n",
            "Epoch 26/100, Train Loss: 0.0332, Val Loss: 0.0381\n",
            "Epoch 27/100, Train Loss: 0.0320, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0313, Val Loss: 0.0379\n",
            "Epoch 29/100, Train Loss: 0.0324, Val Loss: 0.0379\n",
            "Epoch 30/100, Train Loss: 0.0321, Val Loss: 0.0379\n",
            "Epoch 31/100, Train Loss: 0.0323, Val Loss: 0.0379\n",
            "Epoch 32/100, Train Loss: 0.0333, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0322, Val Loss: 0.0379\n",
            "Epoch 34/100, Train Loss: 0.0329, Val Loss: 0.0379\n",
            "Epoch 35/100, Train Loss: 0.0322, Val Loss: 0.0379\n",
            "Epoch 36/100, Train Loss: 0.0334, Val Loss: 0.0379\n",
            "Epoch 37/100, Train Loss: 0.0329, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0334, Val Loss: 0.0379\n",
            "Epoch 39/100, Train Loss: 0.0328, Val Loss: 0.0379\n",
            "Epoch 40/100, Train Loss: 0.0302, Val Loss: 0.0379\n",
            "Epoch 41/100, Train Loss: 0.0326, Val Loss: 0.0379\n",
            "Epoch 42/100, Train Loss: 0.0337, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0333, Val Loss: 0.0379\n",
            "Epoch 44/100, Train Loss: 0.0328, Val Loss: 0.0379\n",
            "Epoch 45/100, Train Loss: 0.0329, Val Loss: 0.0379\n",
            "Epoch 46/100, Train Loss: 0.0303, Val Loss: 0.0379\n",
            "Epoch 47/100, Train Loss: 0.0337, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0329, Val Loss: 0.0379\n",
            "Epoch 49/100, Train Loss: 0.0318, Val Loss: 0.0379\n",
            "Epoch 50/100, Train Loss: 0.0333, Val Loss: 0.0379\n",
            "Epoch 51/100, Train Loss: 0.0336, Val Loss: 0.0379\n",
            "Epoch 52/100, Train Loss: 0.0337, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0330, Val Loss: 0.0379\n",
            "Epoch 54/100, Train Loss: 0.0326, Val Loss: 0.0379\n",
            "Epoch 55/100, Train Loss: 0.0327, Val Loss: 0.0379\n",
            "Epoch 56/100, Train Loss: 0.0333, Val Loss: 0.0379\n",
            "Epoch 57/100, Train Loss: 0.0334, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0336, Val Loss: 0.0379\n",
            "Epoch 59/100, Train Loss: 0.0324, Val Loss: 0.0379\n",
            "Epoch 60/100, Train Loss: 0.0320, Val Loss: 0.0379\n",
            "Epoch 61/100, Train Loss: 0.0323, Val Loss: 0.0379\n",
            "Epoch 62/100, Train Loss: 0.0325, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0330, Val Loss: 0.0379\n",
            "Epoch 64/100, Train Loss: 0.0343, Val Loss: 0.0379\n",
            "Epoch 65/100, Train Loss: 0.0338, Val Loss: 0.0379\n",
            "Epoch 66/100, Train Loss: 0.0321, Val Loss: 0.0379\n",
            "Epoch 67/100, Train Loss: 0.0332, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0315, Val Loss: 0.0379\n",
            "Epoch 69/100, Train Loss: 0.0312, Val Loss: 0.0379\n",
            "Epoch 70/100, Train Loss: 0.0353, Val Loss: 0.0379\n",
            "Epoch 71/100, Train Loss: 0.0328, Val Loss: 0.0379\n",
            "Epoch 72/100, Train Loss: 0.0325, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0310, Val Loss: 0.0379\n",
            "Epoch 74/100, Train Loss: 0.0330, Val Loss: 0.0379\n",
            "Epoch 75/100, Train Loss: 0.0330, Val Loss: 0.0379\n",
            "Epoch 76/100, Train Loss: 0.0324, Val Loss: 0.0379\n",
            "Epoch 77/100, Train Loss: 0.0326, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0319, Val Loss: 0.0379\n",
            "Epoch 79/100, Train Loss: 0.0329, Val Loss: 0.0379\n",
            "Epoch 80/100, Train Loss: 0.0320, Val Loss: 0.0379\n",
            "Epoch 81/100, Train Loss: 0.0328, Val Loss: 0.0379\n",
            "Epoch 82/100, Train Loss: 0.0333, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0313, Val Loss: 0.0379\n",
            "Epoch 84/100, Train Loss: 0.0318, Val Loss: 0.0379\n",
            "Epoch 85/100, Train Loss: 0.0336, Val Loss: 0.0379\n",
            "Epoch 86/100, Train Loss: 0.0331, Val Loss: 0.0379\n",
            "Epoch 87/100, Train Loss: 0.0321, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0326, Val Loss: 0.0379\n",
            "Epoch 89/100, Train Loss: 0.0329, Val Loss: 0.0379\n",
            "Epoch 90/100, Train Loss: 0.0337, Val Loss: 0.0379\n",
            "Epoch 91/100, Train Loss: 0.0337, Val Loss: 0.0379\n",
            "Epoch 92/100, Train Loss: 0.0316, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0329, Val Loss: 0.0379\n",
            "Epoch 94/100, Train Loss: 0.0334, Val Loss: 0.0379\n",
            "Epoch 95/100, Train Loss: 0.0316, Val Loss: 0.0379\n",
            "Epoch 96/100, Train Loss: 0.0318, Val Loss: 0.0379\n",
            "Epoch 97/100, Train Loss: 0.0321, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0317, Val Loss: 0.0379\n",
            "Epoch 99/100, Train Loss: 0.0311, Val Loss: 0.0379\n",
            "Epoch 100/100, Train Loss: 0.0320, Val Loss: 0.0379\n",
            "\n",
            "Test iteration 13/28\n",
            "Current training set size: 109 samples\n",
            "Epoch 1/100, Train Loss: 0.1633, Val Loss: 0.1903\n",
            "Epoch 2/100, Train Loss: 0.1474, Val Loss: 0.1944\n",
            "Epoch 3/100, Train Loss: 0.1416, Val Loss: 0.1771\n",
            "Epoch 4/100, Train Loss: 0.1250, Val Loss: 0.0713\n",
            "Epoch 5/100, Train Loss: 0.0998, Val Loss: 0.0372\n",
            "Epoch 6/100, Train Loss: 0.0756, Val Loss: 0.0521\n",
            "Epoch 7/100, Train Loss: 0.0733, Val Loss: 0.0752\n",
            "Epoch 8/100, Train Loss: 0.0934, Val Loss: 0.0550\n",
            "Epoch 9/100, Train Loss: 0.0708, Val Loss: 0.0527\n",
            "Epoch 10/100, Train Loss: 0.0680, Val Loss: 0.0483\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 11/100, Train Loss: 0.0644, Val Loss: 0.0428\n",
            "Epoch 12/100, Train Loss: 0.0546, Val Loss: 0.0398\n",
            "Epoch 13/100, Train Loss: 0.0528, Val Loss: 0.0394\n",
            "Epoch 14/100, Train Loss: 0.0520, Val Loss: 0.0389\n",
            "Epoch 15/100, Train Loss: 0.0507, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0498, Val Loss: 0.0390\n",
            "Epoch 17/100, Train Loss: 0.0503, Val Loss: 0.0390\n",
            "Epoch 18/100, Train Loss: 0.0492, Val Loss: 0.0390\n",
            "Epoch 19/100, Train Loss: 0.0502, Val Loss: 0.0390\n",
            "Epoch 20/100, Train Loss: 0.0496, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0482, Val Loss: 0.0390\n",
            "Epoch 22/100, Train Loss: 0.0503, Val Loss: 0.0390\n",
            "Epoch 23/100, Train Loss: 0.0515, Val Loss: 0.0390\n",
            "Epoch 24/100, Train Loss: 0.0498, Val Loss: 0.0390\n",
            "Epoch 25/100, Train Loss: 0.0501, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0508, Val Loss: 0.0390\n",
            "Epoch 27/100, Train Loss: 0.0489, Val Loss: 0.0390\n",
            "Epoch 28/100, Train Loss: 0.0520, Val Loss: 0.0390\n",
            "Epoch 29/100, Train Loss: 0.0500, Val Loss: 0.0390\n",
            "Epoch 30/100, Train Loss: 0.0505, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0491, Val Loss: 0.0390\n",
            "Epoch 32/100, Train Loss: 0.0496, Val Loss: 0.0390\n",
            "Epoch 33/100, Train Loss: 0.0493, Val Loss: 0.0390\n",
            "Epoch 34/100, Train Loss: 0.0488, Val Loss: 0.0390\n",
            "Epoch 35/100, Train Loss: 0.0530, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0520, Val Loss: 0.0390\n",
            "Epoch 37/100, Train Loss: 0.0502, Val Loss: 0.0390\n",
            "Epoch 38/100, Train Loss: 0.0508, Val Loss: 0.0390\n",
            "Epoch 39/100, Train Loss: 0.0506, Val Loss: 0.0390\n",
            "Epoch 40/100, Train Loss: 0.0503, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0504, Val Loss: 0.0390\n",
            "Epoch 42/100, Train Loss: 0.0506, Val Loss: 0.0390\n",
            "Epoch 43/100, Train Loss: 0.0493, Val Loss: 0.0390\n",
            "Epoch 44/100, Train Loss: 0.0500, Val Loss: 0.0390\n",
            "Epoch 45/100, Train Loss: 0.0519, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0503, Val Loss: 0.0390\n",
            "Epoch 47/100, Train Loss: 0.0486, Val Loss: 0.0390\n",
            "Epoch 48/100, Train Loss: 0.0484, Val Loss: 0.0390\n",
            "Epoch 49/100, Train Loss: 0.0512, Val Loss: 0.0390\n",
            "Epoch 50/100, Train Loss: 0.0506, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0495, Val Loss: 0.0390\n",
            "Epoch 52/100, Train Loss: 0.0492, Val Loss: 0.0390\n",
            "Epoch 53/100, Train Loss: 0.0487, Val Loss: 0.0390\n",
            "Epoch 54/100, Train Loss: 0.0514, Val Loss: 0.0390\n",
            "Epoch 55/100, Train Loss: 0.0667, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0513, Val Loss: 0.0390\n",
            "Epoch 57/100, Train Loss: 0.0509, Val Loss: 0.0390\n",
            "Epoch 58/100, Train Loss: 0.0504, Val Loss: 0.0390\n",
            "Epoch 59/100, Train Loss: 0.0502, Val Loss: 0.0390\n",
            "Epoch 60/100, Train Loss: 0.0518, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0537, Val Loss: 0.0390\n",
            "Epoch 62/100, Train Loss: 0.0491, Val Loss: 0.0390\n",
            "Epoch 63/100, Train Loss: 0.0510, Val Loss: 0.0390\n",
            "Epoch 64/100, Train Loss: 0.0502, Val Loss: 0.0390\n",
            "Epoch 65/100, Train Loss: 0.0502, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0487, Val Loss: 0.0390\n",
            "Epoch 67/100, Train Loss: 0.0499, Val Loss: 0.0390\n",
            "Epoch 68/100, Train Loss: 0.0547, Val Loss: 0.0390\n",
            "Epoch 69/100, Train Loss: 0.0514, Val Loss: 0.0390\n",
            "Epoch 70/100, Train Loss: 0.0520, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0489, Val Loss: 0.0390\n",
            "Epoch 72/100, Train Loss: 0.0505, Val Loss: 0.0390\n",
            "Epoch 73/100, Train Loss: 0.0511, Val Loss: 0.0390\n",
            "Epoch 74/100, Train Loss: 0.0516, Val Loss: 0.0390\n",
            "Epoch 75/100, Train Loss: 0.0511, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0500, Val Loss: 0.0390\n",
            "Epoch 77/100, Train Loss: 0.0544, Val Loss: 0.0390\n",
            "Epoch 78/100, Train Loss: 0.0485, Val Loss: 0.0390\n",
            "Epoch 79/100, Train Loss: 0.0513, Val Loss: 0.0390\n",
            "Epoch 80/100, Train Loss: 0.0495, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0509, Val Loss: 0.0390\n",
            "Epoch 82/100, Train Loss: 0.0499, Val Loss: 0.0390\n",
            "Epoch 83/100, Train Loss: 0.0492, Val Loss: 0.0390\n",
            "Epoch 84/100, Train Loss: 0.0493, Val Loss: 0.0390\n",
            "Epoch 85/100, Train Loss: 0.0514, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0495, Val Loss: 0.0390\n",
            "Epoch 87/100, Train Loss: 0.0492, Val Loss: 0.0390\n",
            "Epoch 88/100, Train Loss: 0.0499, Val Loss: 0.0390\n",
            "Epoch 89/100, Train Loss: 0.0514, Val Loss: 0.0390\n",
            "Epoch 90/100, Train Loss: 0.0499, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0510, Val Loss: 0.0390\n",
            "Epoch 92/100, Train Loss: 0.0489, Val Loss: 0.0390\n",
            "Epoch 93/100, Train Loss: 0.0495, Val Loss: 0.0390\n",
            "Epoch 94/100, Train Loss: 0.0496, Val Loss: 0.0390\n",
            "Epoch 95/100, Train Loss: 0.0517, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0505, Val Loss: 0.0390\n",
            "Epoch 97/100, Train Loss: 0.0494, Val Loss: 0.0390\n",
            "Epoch 98/100, Train Loss: 0.0509, Val Loss: 0.0390\n",
            "Epoch 99/100, Train Loss: 0.0491, Val Loss: 0.0390\n",
            "Epoch 100/100, Train Loss: 0.0510, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 14/28\n",
            "Current training set size: 110 samples\n",
            "Epoch 1/100, Train Loss: 0.1629, Val Loss: 0.2029\n",
            "Epoch 2/100, Train Loss: 0.1502, Val Loss: 0.1852\n",
            "Epoch 3/100, Train Loss: 0.1413, Val Loss: 0.1907\n",
            "Epoch 4/100, Train Loss: 0.1261, Val Loss: 0.1684\n",
            "Epoch 5/100, Train Loss: 0.1287, Val Loss: 0.1316\n",
            "Epoch 6/100, Train Loss: 0.0879, Val Loss: 0.0811\n",
            "Epoch 7/100, Train Loss: 0.0794, Val Loss: 0.0778\n",
            "Epoch 8/100, Train Loss: 0.0697, Val Loss: 0.0330\n",
            "Epoch 9/100, Train Loss: 0.0619, Val Loss: 0.0328\n",
            "Epoch 10/100, Train Loss: 0.0537, Val Loss: 0.0339\n",
            "Epoch 11/100, Train Loss: 0.0460, Val Loss: 0.0379\n",
            "Epoch 12/100, Train Loss: 0.0484, Val Loss: 0.0394\n",
            "Epoch 13/100, Train Loss: 0.0439, Val Loss: 0.0588\n",
            "Epoch 14/100, Train Loss: 0.0595, Val Loss: 0.0666\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 15/100, Train Loss: 0.0544, Val Loss: 0.0624\n",
            "Epoch 16/100, Train Loss: 0.0512, Val Loss: 0.0583\n",
            "Epoch 17/100, Train Loss: 0.0477, Val Loss: 0.0555\n",
            "Epoch 18/100, Train Loss: 0.0441, Val Loss: 0.0533\n",
            "Epoch 19/100, Train Loss: 0.0429, Val Loss: 0.0509\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0431, Val Loss: 0.0509\n",
            "Epoch 21/100, Train Loss: 0.0428, Val Loss: 0.0509\n",
            "Epoch 22/100, Train Loss: 0.0423, Val Loss: 0.0509\n",
            "Epoch 23/100, Train Loss: 0.0434, Val Loss: 0.0509\n",
            "Epoch 24/100, Train Loss: 0.0426, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0444, Val Loss: 0.0508\n",
            "Epoch 26/100, Train Loss: 0.0420, Val Loss: 0.0508\n",
            "Epoch 27/100, Train Loss: 0.0422, Val Loss: 0.0508\n",
            "Epoch 28/100, Train Loss: 0.0417, Val Loss: 0.0508\n",
            "Epoch 29/100, Train Loss: 0.0450, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0420, Val Loss: 0.0508\n",
            "Epoch 31/100, Train Loss: 0.0434, Val Loss: 0.0508\n",
            "Epoch 32/100, Train Loss: 0.0433, Val Loss: 0.0508\n",
            "Epoch 33/100, Train Loss: 0.0440, Val Loss: 0.0508\n",
            "Epoch 34/100, Train Loss: 0.0430, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0435, Val Loss: 0.0508\n",
            "Epoch 36/100, Train Loss: 0.0457, Val Loss: 0.0508\n",
            "Epoch 37/100, Train Loss: 0.0421, Val Loss: 0.0508\n",
            "Epoch 38/100, Train Loss: 0.0432, Val Loss: 0.0508\n",
            "Epoch 39/100, Train Loss: 0.0419, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0435, Val Loss: 0.0508\n",
            "Epoch 41/100, Train Loss: 0.0432, Val Loss: 0.0508\n",
            "Epoch 42/100, Train Loss: 0.0431, Val Loss: 0.0508\n",
            "Epoch 43/100, Train Loss: 0.0438, Val Loss: 0.0508\n",
            "Epoch 44/100, Train Loss: 0.0423, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0436, Val Loss: 0.0508\n",
            "Epoch 46/100, Train Loss: 0.0437, Val Loss: 0.0508\n",
            "Epoch 47/100, Train Loss: 0.0444, Val Loss: 0.0508\n",
            "Epoch 48/100, Train Loss: 0.0440, Val Loss: 0.0508\n",
            "Epoch 49/100, Train Loss: 0.0426, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0450, Val Loss: 0.0508\n",
            "Epoch 51/100, Train Loss: 0.0435, Val Loss: 0.0508\n",
            "Epoch 52/100, Train Loss: 0.0437, Val Loss: 0.0508\n",
            "Epoch 53/100, Train Loss: 0.0468, Val Loss: 0.0508\n",
            "Epoch 54/100, Train Loss: 0.0455, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0433, Val Loss: 0.0508\n",
            "Epoch 56/100, Train Loss: 0.0438, Val Loss: 0.0508\n",
            "Epoch 57/100, Train Loss: 0.0423, Val Loss: 0.0508\n",
            "Epoch 58/100, Train Loss: 0.0421, Val Loss: 0.0508\n",
            "Epoch 59/100, Train Loss: 0.0433, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0425, Val Loss: 0.0508\n",
            "Epoch 61/100, Train Loss: 0.0444, Val Loss: 0.0508\n",
            "Epoch 62/100, Train Loss: 0.0442, Val Loss: 0.0508\n",
            "Epoch 63/100, Train Loss: 0.0436, Val Loss: 0.0508\n",
            "Epoch 64/100, Train Loss: 0.0427, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0426, Val Loss: 0.0508\n",
            "Epoch 66/100, Train Loss: 0.0423, Val Loss: 0.0508\n",
            "Epoch 67/100, Train Loss: 0.0443, Val Loss: 0.0508\n",
            "Epoch 68/100, Train Loss: 0.0443, Val Loss: 0.0508\n",
            "Epoch 69/100, Train Loss: 0.0432, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0439, Val Loss: 0.0508\n",
            "Epoch 71/100, Train Loss: 0.0420, Val Loss: 0.0508\n",
            "Epoch 72/100, Train Loss: 0.0455, Val Loss: 0.0508\n",
            "Epoch 73/100, Train Loss: 0.0433, Val Loss: 0.0508\n",
            "Epoch 74/100, Train Loss: 0.0425, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0435, Val Loss: 0.0508\n",
            "Epoch 76/100, Train Loss: 0.0428, Val Loss: 0.0508\n",
            "Epoch 77/100, Train Loss: 0.0426, Val Loss: 0.0508\n",
            "Epoch 78/100, Train Loss: 0.0411, Val Loss: 0.0508\n",
            "Epoch 79/100, Train Loss: 0.0424, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0436, Val Loss: 0.0508\n",
            "Epoch 81/100, Train Loss: 0.0437, Val Loss: 0.0508\n",
            "Epoch 82/100, Train Loss: 0.0418, Val Loss: 0.0508\n",
            "Epoch 83/100, Train Loss: 0.0426, Val Loss: 0.0508\n",
            "Epoch 84/100, Train Loss: 0.0436, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0429, Val Loss: 0.0508\n",
            "Epoch 86/100, Train Loss: 0.0427, Val Loss: 0.0508\n",
            "Epoch 87/100, Train Loss: 0.0419, Val Loss: 0.0508\n",
            "Epoch 88/100, Train Loss: 0.0424, Val Loss: 0.0508\n",
            "Epoch 89/100, Train Loss: 0.0431, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0436, Val Loss: 0.0508\n",
            "Epoch 91/100, Train Loss: 0.0436, Val Loss: 0.0508\n",
            "Epoch 92/100, Train Loss: 0.0412, Val Loss: 0.0508\n",
            "Epoch 93/100, Train Loss: 0.0426, Val Loss: 0.0508\n",
            "Epoch 94/100, Train Loss: 0.0433, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0446, Val Loss: 0.0508\n",
            "Epoch 96/100, Train Loss: 0.0453, Val Loss: 0.0508\n",
            "Epoch 97/100, Train Loss: 0.0424, Val Loss: 0.0508\n",
            "Epoch 98/100, Train Loss: 0.0433, Val Loss: 0.0508\n",
            "Epoch 99/100, Train Loss: 0.0420, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0429, Val Loss: 0.0508\n",
            "\n",
            "Test iteration 15/28\n",
            "Current training set size: 111 samples\n",
            "Epoch 1/100, Train Loss: 0.1571, Val Loss: 0.1891\n",
            "Epoch 2/100, Train Loss: 0.1438, Val Loss: 0.1904\n",
            "Epoch 3/100, Train Loss: 0.1431, Val Loss: 0.1645\n",
            "Epoch 4/100, Train Loss: 0.1208, Val Loss: 0.1139\n",
            "Epoch 5/100, Train Loss: 0.0940, Val Loss: 0.0762\n",
            "Epoch 6/100, Train Loss: 0.0745, Val Loss: 0.0822\n",
            "Epoch 7/100, Train Loss: 0.0864, Val Loss: 0.0583\n",
            "Epoch 8/100, Train Loss: 0.0720, Val Loss: 0.0479\n",
            "Epoch 9/100, Train Loss: 0.0635, Val Loss: 0.0686\n",
            "Epoch 10/100, Train Loss: 0.0561, Val Loss: 0.0476\n",
            "Epoch 11/100, Train Loss: 0.0520, Val Loss: 0.0464\n",
            "Epoch 12/100, Train Loss: 0.0450, Val Loss: 0.0410\n",
            "Epoch 13/100, Train Loss: 0.0495, Val Loss: 0.0359\n",
            "Epoch 14/100, Train Loss: 0.0474, Val Loss: 0.0617\n",
            "Epoch 15/100, Train Loss: 0.0444, Val Loss: 0.0465\n",
            "Epoch 16/100, Train Loss: 0.0443, Val Loss: 0.0411\n",
            "Epoch 17/100, Train Loss: 0.0365, Val Loss: 0.0352\n",
            "Epoch 18/100, Train Loss: 0.0362, Val Loss: 0.0433\n",
            "Epoch 19/100, Train Loss: 0.0410, Val Loss: 0.0586\n",
            "Epoch 20/100, Train Loss: 0.0413, Val Loss: 0.0371\n",
            "Epoch 21/100, Train Loss: 0.0393, Val Loss: 0.0715\n",
            "Epoch 22/100, Train Loss: 0.0409, Val Loss: 0.0412\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0457, Val Loss: 0.0390\n",
            "Epoch 24/100, Train Loss: 0.0367, Val Loss: 0.0380\n",
            "Epoch 25/100, Train Loss: 0.0347, Val Loss: 0.0378\n",
            "Epoch 26/100, Train Loss: 0.0333, Val Loss: 0.0377\n",
            "Epoch 27/100, Train Loss: 0.0332, Val Loss: 0.0380\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0322, Val Loss: 0.0380\n",
            "Epoch 29/100, Train Loss: 0.0308, Val Loss: 0.0380\n",
            "Epoch 30/100, Train Loss: 0.0313, Val Loss: 0.0380\n",
            "Epoch 31/100, Train Loss: 0.0329, Val Loss: 0.0380\n",
            "Epoch 32/100, Train Loss: 0.0317, Val Loss: 0.0380\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0315, Val Loss: 0.0380\n",
            "Epoch 34/100, Train Loss: 0.0314, Val Loss: 0.0380\n",
            "Epoch 35/100, Train Loss: 0.0317, Val Loss: 0.0380\n",
            "Epoch 36/100, Train Loss: 0.0319, Val Loss: 0.0380\n",
            "Epoch 37/100, Train Loss: 0.0319, Val Loss: 0.0380\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0312, Val Loss: 0.0380\n",
            "Epoch 39/100, Train Loss: 0.0328, Val Loss: 0.0380\n",
            "Epoch 40/100, Train Loss: 0.0310, Val Loss: 0.0380\n",
            "Epoch 41/100, Train Loss: 0.0326, Val Loss: 0.0380\n",
            "Epoch 42/100, Train Loss: 0.0326, Val Loss: 0.0380\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0315, Val Loss: 0.0380\n",
            "Epoch 44/100, Train Loss: 0.0334, Val Loss: 0.0380\n",
            "Epoch 45/100, Train Loss: 0.0324, Val Loss: 0.0380\n",
            "Epoch 46/100, Train Loss: 0.0311, Val Loss: 0.0380\n",
            "Epoch 47/100, Train Loss: 0.0330, Val Loss: 0.0380\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0315, Val Loss: 0.0380\n",
            "Epoch 49/100, Train Loss: 0.0323, Val Loss: 0.0380\n",
            "Epoch 50/100, Train Loss: 0.0321, Val Loss: 0.0380\n",
            "Epoch 51/100, Train Loss: 0.0317, Val Loss: 0.0380\n",
            "Epoch 52/100, Train Loss: 0.0333, Val Loss: 0.0380\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0317, Val Loss: 0.0380\n",
            "Epoch 54/100, Train Loss: 0.0323, Val Loss: 0.0380\n",
            "Epoch 55/100, Train Loss: 0.0334, Val Loss: 0.0380\n",
            "Epoch 56/100, Train Loss: 0.0310, Val Loss: 0.0380\n",
            "Epoch 57/100, Train Loss: 0.0314, Val Loss: 0.0380\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0321, Val Loss: 0.0380\n",
            "Epoch 59/100, Train Loss: 0.0324, Val Loss: 0.0380\n",
            "Epoch 60/100, Train Loss: 0.0313, Val Loss: 0.0380\n",
            "Epoch 61/100, Train Loss: 0.0322, Val Loss: 0.0380\n",
            "Epoch 62/100, Train Loss: 0.0326, Val Loss: 0.0380\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0330, Val Loss: 0.0380\n",
            "Epoch 64/100, Train Loss: 0.0328, Val Loss: 0.0380\n",
            "Epoch 65/100, Train Loss: 0.0320, Val Loss: 0.0380\n",
            "Epoch 66/100, Train Loss: 0.0324, Val Loss: 0.0380\n",
            "Epoch 67/100, Train Loss: 0.0325, Val Loss: 0.0380\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0329, Val Loss: 0.0380\n",
            "Epoch 69/100, Train Loss: 0.0318, Val Loss: 0.0380\n",
            "Epoch 70/100, Train Loss: 0.0320, Val Loss: 0.0380\n",
            "Epoch 71/100, Train Loss: 0.0310, Val Loss: 0.0380\n",
            "Epoch 72/100, Train Loss: 0.0323, Val Loss: 0.0380\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0321, Val Loss: 0.0380\n",
            "Epoch 74/100, Train Loss: 0.0312, Val Loss: 0.0380\n",
            "Epoch 75/100, Train Loss: 0.0328, Val Loss: 0.0380\n",
            "Epoch 76/100, Train Loss: 0.0310, Val Loss: 0.0380\n",
            "Epoch 77/100, Train Loss: 0.0333, Val Loss: 0.0380\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0317, Val Loss: 0.0380\n",
            "Epoch 79/100, Train Loss: 0.0336, Val Loss: 0.0380\n",
            "Epoch 80/100, Train Loss: 0.0322, Val Loss: 0.0380\n",
            "Epoch 81/100, Train Loss: 0.0322, Val Loss: 0.0380\n",
            "Epoch 82/100, Train Loss: 0.0314, Val Loss: 0.0380\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0330, Val Loss: 0.0380\n",
            "Epoch 84/100, Train Loss: 0.0311, Val Loss: 0.0380\n",
            "Epoch 85/100, Train Loss: 0.0327, Val Loss: 0.0380\n",
            "Epoch 86/100, Train Loss: 0.0314, Val Loss: 0.0380\n",
            "Epoch 87/100, Train Loss: 0.0325, Val Loss: 0.0380\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0312, Val Loss: 0.0380\n",
            "Epoch 89/100, Train Loss: 0.0312, Val Loss: 0.0380\n",
            "Epoch 90/100, Train Loss: 0.0334, Val Loss: 0.0380\n",
            "Epoch 91/100, Train Loss: 0.0315, Val Loss: 0.0380\n",
            "Epoch 92/100, Train Loss: 0.0312, Val Loss: 0.0380\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0323, Val Loss: 0.0380\n",
            "Epoch 94/100, Train Loss: 0.0319, Val Loss: 0.0380\n",
            "Epoch 95/100, Train Loss: 0.0313, Val Loss: 0.0380\n",
            "Epoch 96/100, Train Loss: 0.0328, Val Loss: 0.0380\n",
            "Epoch 97/100, Train Loss: 0.0316, Val Loss: 0.0380\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0344, Val Loss: 0.0380\n",
            "Epoch 99/100, Train Loss: 0.0326, Val Loss: 0.0380\n",
            "Epoch 100/100, Train Loss: 0.0317, Val Loss: 0.0380\n",
            "\n",
            "Test iteration 16/28\n",
            "Current training set size: 112 samples\n",
            "Epoch 1/100, Train Loss: 0.1532, Val Loss: 0.1960\n",
            "Epoch 2/100, Train Loss: 0.1389, Val Loss: 0.2379\n",
            "Epoch 3/100, Train Loss: 0.1351, Val Loss: 0.0742\n",
            "Epoch 4/100, Train Loss: 0.1343, Val Loss: 0.1662\n",
            "Epoch 5/100, Train Loss: 0.1102, Val Loss: 0.0606\n",
            "Epoch 6/100, Train Loss: 0.0885, Val Loss: 0.0405\n",
            "Epoch 7/100, Train Loss: 0.0681, Val Loss: 0.0393\n",
            "Epoch 8/100, Train Loss: 0.0644, Val Loss: 0.0686\n",
            "Epoch 9/100, Train Loss: 0.0633, Val Loss: 0.0362\n",
            "Epoch 10/100, Train Loss: 0.0573, Val Loss: 0.0435\n",
            "Epoch 11/100, Train Loss: 0.0579, Val Loss: 0.0315\n",
            "Epoch 12/100, Train Loss: 0.0479, Val Loss: 0.0356\n",
            "Epoch 13/100, Train Loss: 0.0540, Val Loss: 0.0715\n",
            "Epoch 14/100, Train Loss: 0.0497, Val Loss: 0.0402\n",
            "Epoch 15/100, Train Loss: 0.0472, Val Loss: 0.0387\n",
            "Epoch 16/100, Train Loss: 0.0425, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0424, Val Loss: 0.0405\n",
            "Epoch 18/100, Train Loss: 0.0372, Val Loss: 0.0394\n",
            "Epoch 19/100, Train Loss: 0.0366, Val Loss: 0.0392\n",
            "Epoch 20/100, Train Loss: 0.0375, Val Loss: 0.0393\n",
            "Epoch 21/100, Train Loss: 0.0358, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0342, Val Loss: 0.0391\n",
            "Epoch 23/100, Train Loss: 0.0362, Val Loss: 0.0391\n",
            "Epoch 24/100, Train Loss: 0.0350, Val Loss: 0.0391\n",
            "Epoch 25/100, Train Loss: 0.0359, Val Loss: 0.0391\n",
            "Epoch 26/100, Train Loss: 0.0357, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0364, Val Loss: 0.0391\n",
            "Epoch 28/100, Train Loss: 0.0352, Val Loss: 0.0391\n",
            "Epoch 29/100, Train Loss: 0.0338, Val Loss: 0.0391\n",
            "Epoch 30/100, Train Loss: 0.0358, Val Loss: 0.0391\n",
            "Epoch 31/100, Train Loss: 0.0351, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0359, Val Loss: 0.0391\n",
            "Epoch 33/100, Train Loss: 0.0335, Val Loss: 0.0391\n",
            "Epoch 34/100, Train Loss: 0.0353, Val Loss: 0.0391\n",
            "Epoch 35/100, Train Loss: 0.0351, Val Loss: 0.0391\n",
            "Epoch 36/100, Train Loss: 0.0356, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0358, Val Loss: 0.0391\n",
            "Epoch 38/100, Train Loss: 0.0345, Val Loss: 0.0391\n",
            "Epoch 39/100, Train Loss: 0.0350, Val Loss: 0.0391\n",
            "Epoch 40/100, Train Loss: 0.0362, Val Loss: 0.0391\n",
            "Epoch 41/100, Train Loss: 0.0367, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0355, Val Loss: 0.0391\n",
            "Epoch 43/100, Train Loss: 0.0346, Val Loss: 0.0391\n",
            "Epoch 44/100, Train Loss: 0.0366, Val Loss: 0.0391\n",
            "Epoch 45/100, Train Loss: 0.0352, Val Loss: 0.0391\n",
            "Epoch 46/100, Train Loss: 0.0353, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0350, Val Loss: 0.0391\n",
            "Epoch 48/100, Train Loss: 0.0353, Val Loss: 0.0391\n",
            "Epoch 49/100, Train Loss: 0.0354, Val Loss: 0.0391\n",
            "Epoch 50/100, Train Loss: 0.0357, Val Loss: 0.0391\n",
            "Epoch 51/100, Train Loss: 0.0345, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0348, Val Loss: 0.0391\n",
            "Epoch 53/100, Train Loss: 0.0346, Val Loss: 0.0391\n",
            "Epoch 54/100, Train Loss: 0.0367, Val Loss: 0.0391\n",
            "Epoch 55/100, Train Loss: 0.0339, Val Loss: 0.0391\n",
            "Epoch 56/100, Train Loss: 0.0356, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0357, Val Loss: 0.0391\n",
            "Epoch 58/100, Train Loss: 0.0350, Val Loss: 0.0391\n",
            "Epoch 59/100, Train Loss: 0.0351, Val Loss: 0.0391\n",
            "Epoch 60/100, Train Loss: 0.0353, Val Loss: 0.0391\n",
            "Epoch 61/100, Train Loss: 0.0350, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0367, Val Loss: 0.0391\n",
            "Epoch 63/100, Train Loss: 0.0352, Val Loss: 0.0391\n",
            "Epoch 64/100, Train Loss: 0.0351, Val Loss: 0.0391\n",
            "Epoch 65/100, Train Loss: 0.0366, Val Loss: 0.0391\n",
            "Epoch 66/100, Train Loss: 0.0354, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0353, Val Loss: 0.0391\n",
            "Epoch 68/100, Train Loss: 0.0344, Val Loss: 0.0391\n",
            "Epoch 69/100, Train Loss: 0.0357, Val Loss: 0.0391\n",
            "Epoch 70/100, Train Loss: 0.0346, Val Loss: 0.0391\n",
            "Epoch 71/100, Train Loss: 0.0369, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0344, Val Loss: 0.0391\n",
            "Epoch 73/100, Train Loss: 0.0351, Val Loss: 0.0391\n",
            "Epoch 74/100, Train Loss: 0.0357, Val Loss: 0.0391\n",
            "Epoch 75/100, Train Loss: 0.0348, Val Loss: 0.0391\n",
            "Epoch 76/100, Train Loss: 0.0356, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0351, Val Loss: 0.0391\n",
            "Epoch 78/100, Train Loss: 0.0351, Val Loss: 0.0391\n",
            "Epoch 79/100, Train Loss: 0.0356, Val Loss: 0.0391\n",
            "Epoch 80/100, Train Loss: 0.0356, Val Loss: 0.0391\n",
            "Epoch 81/100, Train Loss: 0.0349, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0361, Val Loss: 0.0391\n",
            "Epoch 83/100, Train Loss: 0.0348, Val Loss: 0.0391\n",
            "Epoch 84/100, Train Loss: 0.0356, Val Loss: 0.0391\n",
            "Epoch 85/100, Train Loss: 0.0356, Val Loss: 0.0391\n",
            "Epoch 86/100, Train Loss: 0.0354, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0351, Val Loss: 0.0391\n",
            "Epoch 88/100, Train Loss: 0.0352, Val Loss: 0.0391\n",
            "Epoch 89/100, Train Loss: 0.0355, Val Loss: 0.0391\n",
            "Epoch 90/100, Train Loss: 0.0353, Val Loss: 0.0391\n",
            "Epoch 91/100, Train Loss: 0.0356, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0355, Val Loss: 0.0391\n",
            "Epoch 93/100, Train Loss: 0.0358, Val Loss: 0.0391\n",
            "Epoch 94/100, Train Loss: 0.0353, Val Loss: 0.0391\n",
            "Epoch 95/100, Train Loss: 0.0354, Val Loss: 0.0391\n",
            "Epoch 96/100, Train Loss: 0.0343, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0371, Val Loss: 0.0391\n",
            "Epoch 98/100, Train Loss: 0.0354, Val Loss: 0.0391\n",
            "Epoch 99/100, Train Loss: 0.0341, Val Loss: 0.0391\n",
            "Epoch 100/100, Train Loss: 0.0348, Val Loss: 0.0391\n",
            "\n",
            "Test iteration 17/28\n",
            "Current training set size: 113 samples\n",
            "Epoch 1/100, Train Loss: 0.1538, Val Loss: 0.1869\n",
            "Epoch 2/100, Train Loss: 0.1352, Val Loss: 0.1999\n",
            "Epoch 3/100, Train Loss: 0.1326, Val Loss: 0.1585\n",
            "Epoch 4/100, Train Loss: 0.1131, Val Loss: 0.0701\n",
            "Epoch 5/100, Train Loss: 0.0771, Val Loss: 0.0647\n",
            "Epoch 6/100, Train Loss: 0.0730, Val Loss: 0.0761\n",
            "Epoch 7/100, Train Loss: 0.0709, Val Loss: 0.0583\n",
            "Epoch 8/100, Train Loss: 0.0646, Val Loss: 0.0393\n",
            "Epoch 9/100, Train Loss: 0.0637, Val Loss: 0.0569\n",
            "Epoch 10/100, Train Loss: 0.0592, Val Loss: 0.0351\n",
            "Epoch 11/100, Train Loss: 0.0455, Val Loss: 0.0921\n",
            "Epoch 12/100, Train Loss: 0.0512, Val Loss: 0.0372\n",
            "Epoch 13/100, Train Loss: 0.0429, Val Loss: 0.0439\n",
            "Epoch 14/100, Train Loss: 0.0406, Val Loss: 0.0396\n",
            "Epoch 15/100, Train Loss: 0.0595, Val Loss: 0.0397\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0360, Val Loss: 0.0388\n",
            "Epoch 17/100, Train Loss: 0.0343, Val Loss: 0.0383\n",
            "Epoch 18/100, Train Loss: 0.0341, Val Loss: 0.0388\n",
            "Epoch 19/100, Train Loss: 0.0347, Val Loss: 0.0392\n",
            "Epoch 20/100, Train Loss: 0.0334, Val Loss: 0.0380\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0345, Val Loss: 0.0380\n",
            "Epoch 22/100, Train Loss: 0.0354, Val Loss: 0.0380\n",
            "Epoch 23/100, Train Loss: 0.0350, Val Loss: 0.0380\n",
            "Epoch 24/100, Train Loss: 0.0350, Val Loss: 0.0381\n",
            "Epoch 25/100, Train Loss: 0.0335, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0348, Val Loss: 0.0381\n",
            "Epoch 27/100, Train Loss: 0.0356, Val Loss: 0.0381\n",
            "Epoch 28/100, Train Loss: 0.0351, Val Loss: 0.0381\n",
            "Epoch 29/100, Train Loss: 0.0343, Val Loss: 0.0381\n",
            "Epoch 30/100, Train Loss: 0.0329, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0340, Val Loss: 0.0381\n",
            "Epoch 32/100, Train Loss: 0.0322, Val Loss: 0.0381\n",
            "Epoch 33/100, Train Loss: 0.0363, Val Loss: 0.0381\n",
            "Epoch 34/100, Train Loss: 0.0346, Val Loss: 0.0381\n",
            "Epoch 35/100, Train Loss: 0.0331, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0347, Val Loss: 0.0381\n",
            "Epoch 37/100, Train Loss: 0.0333, Val Loss: 0.0381\n",
            "Epoch 38/100, Train Loss: 0.0338, Val Loss: 0.0381\n",
            "Epoch 39/100, Train Loss: 0.0338, Val Loss: 0.0381\n",
            "Epoch 40/100, Train Loss: 0.0337, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0336, Val Loss: 0.0381\n",
            "Epoch 42/100, Train Loss: 0.0330, Val Loss: 0.0381\n",
            "Epoch 43/100, Train Loss: 0.0327, Val Loss: 0.0381\n",
            "Epoch 44/100, Train Loss: 0.0357, Val Loss: 0.0381\n",
            "Epoch 45/100, Train Loss: 0.0352, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0343, Val Loss: 0.0381\n",
            "Epoch 47/100, Train Loss: 0.0338, Val Loss: 0.0381\n",
            "Epoch 48/100, Train Loss: 0.0332, Val Loss: 0.0381\n",
            "Epoch 49/100, Train Loss: 0.0333, Val Loss: 0.0381\n",
            "Epoch 50/100, Train Loss: 0.0334, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0330, Val Loss: 0.0381\n",
            "Epoch 52/100, Train Loss: 0.0340, Val Loss: 0.0381\n",
            "Epoch 53/100, Train Loss: 0.0354, Val Loss: 0.0381\n",
            "Epoch 54/100, Train Loss: 0.0370, Val Loss: 0.0381\n",
            "Epoch 55/100, Train Loss: 0.0341, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0337, Val Loss: 0.0381\n",
            "Epoch 57/100, Train Loss: 0.0340, Val Loss: 0.0381\n",
            "Epoch 58/100, Train Loss: 0.0353, Val Loss: 0.0381\n",
            "Epoch 59/100, Train Loss: 0.0347, Val Loss: 0.0381\n",
            "Epoch 60/100, Train Loss: 0.0328, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0347, Val Loss: 0.0381\n",
            "Epoch 62/100, Train Loss: 0.0334, Val Loss: 0.0381\n",
            "Epoch 63/100, Train Loss: 0.0337, Val Loss: 0.0381\n",
            "Epoch 64/100, Train Loss: 0.0329, Val Loss: 0.0381\n",
            "Epoch 65/100, Train Loss: 0.0350, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0344, Val Loss: 0.0381\n",
            "Epoch 67/100, Train Loss: 0.0329, Val Loss: 0.0381\n",
            "Epoch 68/100, Train Loss: 0.0334, Val Loss: 0.0381\n",
            "Epoch 69/100, Train Loss: 0.0350, Val Loss: 0.0381\n",
            "Epoch 70/100, Train Loss: 0.0353, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0337, Val Loss: 0.0381\n",
            "Epoch 72/100, Train Loss: 0.0350, Val Loss: 0.0381\n",
            "Epoch 73/100, Train Loss: 0.0338, Val Loss: 0.0381\n",
            "Epoch 74/100, Train Loss: 0.0342, Val Loss: 0.0381\n",
            "Epoch 75/100, Train Loss: 0.0329, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0334, Val Loss: 0.0381\n",
            "Epoch 77/100, Train Loss: 0.0350, Val Loss: 0.0381\n",
            "Epoch 78/100, Train Loss: 0.0351, Val Loss: 0.0381\n",
            "Epoch 79/100, Train Loss: 0.0350, Val Loss: 0.0381\n",
            "Epoch 80/100, Train Loss: 0.0358, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0330, Val Loss: 0.0381\n",
            "Epoch 82/100, Train Loss: 0.0332, Val Loss: 0.0381\n",
            "Epoch 83/100, Train Loss: 0.0342, Val Loss: 0.0381\n",
            "Epoch 84/100, Train Loss: 0.0325, Val Loss: 0.0381\n",
            "Epoch 85/100, Train Loss: 0.0341, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0342, Val Loss: 0.0381\n",
            "Epoch 87/100, Train Loss: 0.0341, Val Loss: 0.0381\n",
            "Epoch 88/100, Train Loss: 0.0331, Val Loss: 0.0381\n",
            "Epoch 89/100, Train Loss: 0.0340, Val Loss: 0.0381\n",
            "Epoch 90/100, Train Loss: 0.0350, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0333, Val Loss: 0.0381\n",
            "Epoch 92/100, Train Loss: 0.0339, Val Loss: 0.0381\n",
            "Epoch 93/100, Train Loss: 0.0405, Val Loss: 0.0381\n",
            "Epoch 94/100, Train Loss: 0.0350, Val Loss: 0.0381\n",
            "Epoch 95/100, Train Loss: 0.0346, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0345, Val Loss: 0.0381\n",
            "Epoch 97/100, Train Loss: 0.0334, Val Loss: 0.0381\n",
            "Epoch 98/100, Train Loss: 0.0361, Val Loss: 0.0381\n",
            "Epoch 99/100, Train Loss: 0.0338, Val Loss: 0.0381\n",
            "Epoch 100/100, Train Loss: 0.0337, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 18/28\n",
            "Current training set size: 114 samples\n",
            "Epoch 1/100, Train Loss: 0.1559, Val Loss: 0.2061\n",
            "Epoch 2/100, Train Loss: 0.1420, Val Loss: 0.1845\n",
            "Epoch 3/100, Train Loss: 0.1260, Val Loss: 0.0918\n",
            "Epoch 4/100, Train Loss: 0.1168, Val Loss: 0.0900\n",
            "Epoch 5/100, Train Loss: 0.0947, Val Loss: 0.0894\n",
            "Epoch 6/100, Train Loss: 0.0769, Val Loss: 0.0350\n",
            "Epoch 7/100, Train Loss: 0.0745, Val Loss: 0.0508\n",
            "Epoch 8/100, Train Loss: 0.0697, Val Loss: 0.0580\n",
            "Epoch 9/100, Train Loss: 0.0641, Val Loss: 0.0815\n",
            "Epoch 10/100, Train Loss: 0.0563, Val Loss: 0.0444\n",
            "Epoch 11/100, Train Loss: 0.0559, Val Loss: 0.0491\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 12/100, Train Loss: 0.0477, Val Loss: 0.0480\n",
            "Epoch 13/100, Train Loss: 0.0456, Val Loss: 0.0459\n",
            "Epoch 14/100, Train Loss: 0.0475, Val Loss: 0.0442\n",
            "Epoch 15/100, Train Loss: 0.0430, Val Loss: 0.0429\n",
            "Epoch 16/100, Train Loss: 0.0425, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0441, Val Loss: 0.0420\n",
            "Epoch 18/100, Train Loss: 0.0426, Val Loss: 0.0420\n",
            "Epoch 19/100, Train Loss: 0.0417, Val Loss: 0.0420\n",
            "Epoch 20/100, Train Loss: 0.0437, Val Loss: 0.0420\n",
            "Epoch 21/100, Train Loss: 0.0412, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0424, Val Loss: 0.0420\n",
            "Epoch 23/100, Train Loss: 0.0413, Val Loss: 0.0420\n",
            "Epoch 24/100, Train Loss: 0.0418, Val Loss: 0.0420\n",
            "Epoch 25/100, Train Loss: 0.0416, Val Loss: 0.0420\n",
            "Epoch 26/100, Train Loss: 0.0427, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0416, Val Loss: 0.0420\n",
            "Epoch 28/100, Train Loss: 0.0433, Val Loss: 0.0420\n",
            "Epoch 29/100, Train Loss: 0.0437, Val Loss: 0.0420\n",
            "Epoch 30/100, Train Loss: 0.0412, Val Loss: 0.0420\n",
            "Epoch 31/100, Train Loss: 0.0428, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0424, Val Loss: 0.0420\n",
            "Epoch 33/100, Train Loss: 0.0435, Val Loss: 0.0420\n",
            "Epoch 34/100, Train Loss: 0.0432, Val Loss: 0.0420\n",
            "Epoch 35/100, Train Loss: 0.0431, Val Loss: 0.0420\n",
            "Epoch 36/100, Train Loss: 0.0416, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0429, Val Loss: 0.0420\n",
            "Epoch 38/100, Train Loss: 0.0431, Val Loss: 0.0420\n",
            "Epoch 39/100, Train Loss: 0.0445, Val Loss: 0.0420\n",
            "Epoch 40/100, Train Loss: 0.0420, Val Loss: 0.0420\n",
            "Epoch 41/100, Train Loss: 0.0426, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0434, Val Loss: 0.0420\n",
            "Epoch 43/100, Train Loss: 0.0423, Val Loss: 0.0420\n",
            "Epoch 44/100, Train Loss: 0.0423, Val Loss: 0.0420\n",
            "Epoch 45/100, Train Loss: 0.0410, Val Loss: 0.0420\n",
            "Epoch 46/100, Train Loss: 0.0428, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0433, Val Loss: 0.0420\n",
            "Epoch 48/100, Train Loss: 0.0418, Val Loss: 0.0420\n",
            "Epoch 49/100, Train Loss: 0.0433, Val Loss: 0.0420\n",
            "Epoch 50/100, Train Loss: 0.0417, Val Loss: 0.0420\n",
            "Epoch 51/100, Train Loss: 0.0414, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0426, Val Loss: 0.0420\n",
            "Epoch 53/100, Train Loss: 0.0469, Val Loss: 0.0420\n",
            "Epoch 54/100, Train Loss: 0.0414, Val Loss: 0.0420\n",
            "Epoch 55/100, Train Loss: 0.0418, Val Loss: 0.0420\n",
            "Epoch 56/100, Train Loss: 0.0422, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0417, Val Loss: 0.0420\n",
            "Epoch 58/100, Train Loss: 0.0425, Val Loss: 0.0420\n",
            "Epoch 59/100, Train Loss: 0.0419, Val Loss: 0.0420\n",
            "Epoch 60/100, Train Loss: 0.0417, Val Loss: 0.0420\n",
            "Epoch 61/100, Train Loss: 0.0426, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0434, Val Loss: 0.0420\n",
            "Epoch 63/100, Train Loss: 0.0430, Val Loss: 0.0420\n",
            "Epoch 64/100, Train Loss: 0.0419, Val Loss: 0.0420\n",
            "Epoch 65/100, Train Loss: 0.0427, Val Loss: 0.0420\n",
            "Epoch 66/100, Train Loss: 0.0411, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0431, Val Loss: 0.0420\n",
            "Epoch 68/100, Train Loss: 0.0420, Val Loss: 0.0420\n",
            "Epoch 69/100, Train Loss: 0.0419, Val Loss: 0.0420\n",
            "Epoch 70/100, Train Loss: 0.0424, Val Loss: 0.0420\n",
            "Epoch 71/100, Train Loss: 0.0416, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0426, Val Loss: 0.0420\n",
            "Epoch 73/100, Train Loss: 0.0421, Val Loss: 0.0420\n",
            "Epoch 74/100, Train Loss: 0.0408, Val Loss: 0.0420\n",
            "Epoch 75/100, Train Loss: 0.0414, Val Loss: 0.0420\n",
            "Epoch 76/100, Train Loss: 0.0436, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0416, Val Loss: 0.0420\n",
            "Epoch 78/100, Train Loss: 0.0430, Val Loss: 0.0420\n",
            "Epoch 79/100, Train Loss: 0.0416, Val Loss: 0.0420\n",
            "Epoch 80/100, Train Loss: 0.0419, Val Loss: 0.0420\n",
            "Epoch 81/100, Train Loss: 0.0422, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0427, Val Loss: 0.0420\n",
            "Epoch 83/100, Train Loss: 0.0411, Val Loss: 0.0420\n",
            "Epoch 84/100, Train Loss: 0.0409, Val Loss: 0.0420\n",
            "Epoch 85/100, Train Loss: 0.0422, Val Loss: 0.0420\n",
            "Epoch 86/100, Train Loss: 0.0417, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0404, Val Loss: 0.0420\n",
            "Epoch 88/100, Train Loss: 0.0420, Val Loss: 0.0420\n",
            "Epoch 89/100, Train Loss: 0.0457, Val Loss: 0.0420\n",
            "Epoch 90/100, Train Loss: 0.0422, Val Loss: 0.0420\n",
            "Epoch 91/100, Train Loss: 0.0412, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0423, Val Loss: 0.0420\n",
            "Epoch 93/100, Train Loss: 0.0418, Val Loss: 0.0420\n",
            "Epoch 94/100, Train Loss: 0.0431, Val Loss: 0.0420\n",
            "Epoch 95/100, Train Loss: 0.0415, Val Loss: 0.0420\n",
            "Epoch 96/100, Train Loss: 0.0426, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0411, Val Loss: 0.0420\n",
            "Epoch 98/100, Train Loss: 0.0432, Val Loss: 0.0420\n",
            "Epoch 99/100, Train Loss: 0.0435, Val Loss: 0.0420\n",
            "Epoch 100/100, Train Loss: 0.0427, Val Loss: 0.0420\n",
            "\n",
            "Test iteration 19/28\n",
            "Current training set size: 115 samples\n",
            "Epoch 1/100, Train Loss: 0.1512, Val Loss: 0.1868\n",
            "Epoch 2/100, Train Loss: 0.1547, Val Loss: 0.1896\n",
            "Epoch 3/100, Train Loss: 0.1339, Val Loss: 0.1610\n",
            "Epoch 4/100, Train Loss: 0.1094, Val Loss: 0.1174\n",
            "Epoch 5/100, Train Loss: 0.0817, Val Loss: 0.0994\n",
            "Epoch 6/100, Train Loss: 0.0701, Val Loss: 0.0706\n",
            "Epoch 7/100, Train Loss: 0.0818, Val Loss: 0.0636\n",
            "Epoch 8/100, Train Loss: 0.0775, Val Loss: 0.0406\n",
            "Epoch 9/100, Train Loss: 0.0634, Val Loss: 0.0468\n",
            "Epoch 10/100, Train Loss: 0.0626, Val Loss: 0.0327\n",
            "Epoch 11/100, Train Loss: 0.0568, Val Loss: 0.0321\n",
            "Epoch 12/100, Train Loss: 0.0523, Val Loss: 0.0452\n",
            "Epoch 13/100, Train Loss: 0.0444, Val Loss: 0.0488\n",
            "Epoch 14/100, Train Loss: 0.0449, Val Loss: 0.0317\n",
            "Epoch 15/100, Train Loss: 0.0418, Val Loss: 0.0355\n",
            "Epoch 16/100, Train Loss: 0.0419, Val Loss: 0.0367\n",
            "Epoch 17/100, Train Loss: 0.0396, Val Loss: 0.0357\n",
            "Epoch 18/100, Train Loss: 0.0533, Val Loss: 0.0352\n",
            "Epoch 19/100, Train Loss: 0.0399, Val Loss: 0.0432\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0437, Val Loss: 0.0417\n",
            "Epoch 21/100, Train Loss: 0.0362, Val Loss: 0.0404\n",
            "Epoch 22/100, Train Loss: 0.0349, Val Loss: 0.0406\n",
            "Epoch 23/100, Train Loss: 0.0341, Val Loss: 0.0406\n",
            "Epoch 24/100, Train Loss: 0.0338, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0340, Val Loss: 0.0407\n",
            "Epoch 26/100, Train Loss: 0.0345, Val Loss: 0.0407\n",
            "Epoch 27/100, Train Loss: 0.0340, Val Loss: 0.0407\n",
            "Epoch 28/100, Train Loss: 0.0329, Val Loss: 0.0407\n",
            "Epoch 29/100, Train Loss: 0.0323, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0330, Val Loss: 0.0407\n",
            "Epoch 31/100, Train Loss: 0.0336, Val Loss: 0.0407\n",
            "Epoch 32/100, Train Loss: 0.0328, Val Loss: 0.0407\n",
            "Epoch 33/100, Train Loss: 0.0339, Val Loss: 0.0407\n",
            "Epoch 34/100, Train Loss: 0.0339, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0344, Val Loss: 0.0407\n",
            "Epoch 36/100, Train Loss: 0.0325, Val Loss: 0.0407\n",
            "Epoch 37/100, Train Loss: 0.0327, Val Loss: 0.0407\n",
            "Epoch 38/100, Train Loss: 0.0329, Val Loss: 0.0407\n",
            "Epoch 39/100, Train Loss: 0.0334, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0339, Val Loss: 0.0407\n",
            "Epoch 41/100, Train Loss: 0.0326, Val Loss: 0.0407\n",
            "Epoch 42/100, Train Loss: 0.0340, Val Loss: 0.0407\n",
            "Epoch 43/100, Train Loss: 0.0339, Val Loss: 0.0407\n",
            "Epoch 44/100, Train Loss: 0.0331, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0321, Val Loss: 0.0407\n",
            "Epoch 46/100, Train Loss: 0.0339, Val Loss: 0.0407\n",
            "Epoch 47/100, Train Loss: 0.0349, Val Loss: 0.0407\n",
            "Epoch 48/100, Train Loss: 0.0340, Val Loss: 0.0407\n",
            "Epoch 49/100, Train Loss: 0.0334, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0342, Val Loss: 0.0407\n",
            "Epoch 51/100, Train Loss: 0.0329, Val Loss: 0.0407\n",
            "Epoch 52/100, Train Loss: 0.0339, Val Loss: 0.0407\n",
            "Epoch 53/100, Train Loss: 0.0339, Val Loss: 0.0407\n",
            "Epoch 54/100, Train Loss: 0.0346, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0342, Val Loss: 0.0407\n",
            "Epoch 56/100, Train Loss: 0.0341, Val Loss: 0.0407\n",
            "Epoch 57/100, Train Loss: 0.0330, Val Loss: 0.0407\n",
            "Epoch 58/100, Train Loss: 0.0322, Val Loss: 0.0407\n",
            "Epoch 59/100, Train Loss: 0.0353, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0340, Val Loss: 0.0407\n",
            "Epoch 61/100, Train Loss: 0.0334, Val Loss: 0.0407\n",
            "Epoch 62/100, Train Loss: 0.0343, Val Loss: 0.0407\n",
            "Epoch 63/100, Train Loss: 0.0335, Val Loss: 0.0407\n",
            "Epoch 64/100, Train Loss: 0.0335, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0344, Val Loss: 0.0407\n",
            "Epoch 66/100, Train Loss: 0.0346, Val Loss: 0.0407\n",
            "Epoch 67/100, Train Loss: 0.0331, Val Loss: 0.0407\n",
            "Epoch 68/100, Train Loss: 0.0332, Val Loss: 0.0407\n",
            "Epoch 69/100, Train Loss: 0.0338, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0336, Val Loss: 0.0407\n",
            "Epoch 71/100, Train Loss: 0.0332, Val Loss: 0.0407\n",
            "Epoch 72/100, Train Loss: 0.0335, Val Loss: 0.0407\n",
            "Epoch 73/100, Train Loss: 0.0341, Val Loss: 0.0407\n",
            "Epoch 74/100, Train Loss: 0.0330, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0343, Val Loss: 0.0407\n",
            "Epoch 76/100, Train Loss: 0.0338, Val Loss: 0.0407\n",
            "Epoch 77/100, Train Loss: 0.0322, Val Loss: 0.0407\n",
            "Epoch 78/100, Train Loss: 0.0338, Val Loss: 0.0407\n",
            "Epoch 79/100, Train Loss: 0.0332, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0330, Val Loss: 0.0407\n",
            "Epoch 81/100, Train Loss: 0.0330, Val Loss: 0.0407\n",
            "Epoch 82/100, Train Loss: 0.0350, Val Loss: 0.0407\n",
            "Epoch 83/100, Train Loss: 0.0354, Val Loss: 0.0407\n",
            "Epoch 84/100, Train Loss: 0.0338, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0363, Val Loss: 0.0407\n",
            "Epoch 86/100, Train Loss: 0.0347, Val Loss: 0.0407\n",
            "Epoch 87/100, Train Loss: 0.0331, Val Loss: 0.0407\n",
            "Epoch 88/100, Train Loss: 0.0347, Val Loss: 0.0407\n",
            "Epoch 89/100, Train Loss: 0.0333, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0334, Val Loss: 0.0407\n",
            "Epoch 91/100, Train Loss: 0.0336, Val Loss: 0.0407\n",
            "Epoch 92/100, Train Loss: 0.0336, Val Loss: 0.0407\n",
            "Epoch 93/100, Train Loss: 0.0345, Val Loss: 0.0407\n",
            "Epoch 94/100, Train Loss: 0.0347, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0338, Val Loss: 0.0407\n",
            "Epoch 96/100, Train Loss: 0.0350, Val Loss: 0.0407\n",
            "Epoch 97/100, Train Loss: 0.0335, Val Loss: 0.0407\n",
            "Epoch 98/100, Train Loss: 0.0346, Val Loss: 0.0407\n",
            "Epoch 99/100, Train Loss: 0.0332, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0335, Val Loss: 0.0407\n",
            "\n",
            "Test iteration 20/28\n",
            "Current training set size: 116 samples\n",
            "Epoch 1/100, Train Loss: 0.1524, Val Loss: 0.1891\n",
            "Epoch 2/100, Train Loss: 0.1412, Val Loss: 0.1734\n",
            "Epoch 3/100, Train Loss: 0.1311, Val Loss: 0.1382\n",
            "Epoch 4/100, Train Loss: 0.1096, Val Loss: 0.0733\n",
            "Epoch 5/100, Train Loss: 0.0921, Val Loss: 0.0380\n",
            "Epoch 6/100, Train Loss: 0.0887, Val Loss: 0.0328\n",
            "Epoch 7/100, Train Loss: 0.0728, Val Loss: 0.0519\n",
            "Epoch 8/100, Train Loss: 0.0712, Val Loss: 0.0487\n",
            "Epoch 9/100, Train Loss: 0.0676, Val Loss: 0.0462\n",
            "Epoch 10/100, Train Loss: 0.0609, Val Loss: 0.0453\n",
            "Epoch 11/100, Train Loss: 0.0529, Val Loss: 0.0408\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 12/100, Train Loss: 0.0467, Val Loss: 0.0404\n",
            "Epoch 13/100, Train Loss: 0.0432, Val Loss: 0.0395\n",
            "Epoch 14/100, Train Loss: 0.0433, Val Loss: 0.0391\n",
            "Epoch 15/100, Train Loss: 0.0436, Val Loss: 0.0390\n",
            "Epoch 16/100, Train Loss: 0.0431, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0445, Val Loss: 0.0386\n",
            "Epoch 18/100, Train Loss: 0.0431, Val Loss: 0.0386\n",
            "Epoch 19/100, Train Loss: 0.0429, Val Loss: 0.0386\n",
            "Epoch 20/100, Train Loss: 0.0441, Val Loss: 0.0386\n",
            "Epoch 21/100, Train Loss: 0.0442, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0435, Val Loss: 0.0386\n",
            "Epoch 23/100, Train Loss: 0.0445, Val Loss: 0.0386\n",
            "Epoch 24/100, Train Loss: 0.0440, Val Loss: 0.0386\n",
            "Epoch 25/100, Train Loss: 0.0451, Val Loss: 0.0386\n",
            "Epoch 26/100, Train Loss: 0.0449, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0428, Val Loss: 0.0386\n",
            "Epoch 28/100, Train Loss: 0.0434, Val Loss: 0.0386\n",
            "Epoch 29/100, Train Loss: 0.0434, Val Loss: 0.0386\n",
            "Epoch 30/100, Train Loss: 0.0425, Val Loss: 0.0386\n",
            "Epoch 31/100, Train Loss: 0.0439, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0432, Val Loss: 0.0386\n",
            "Epoch 33/100, Train Loss: 0.0437, Val Loss: 0.0386\n",
            "Epoch 34/100, Train Loss: 0.0444, Val Loss: 0.0386\n",
            "Epoch 35/100, Train Loss: 0.0435, Val Loss: 0.0386\n",
            "Epoch 36/100, Train Loss: 0.0428, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0428, Val Loss: 0.0386\n",
            "Epoch 38/100, Train Loss: 0.0444, Val Loss: 0.0386\n",
            "Epoch 39/100, Train Loss: 0.0420, Val Loss: 0.0386\n",
            "Epoch 40/100, Train Loss: 0.0431, Val Loss: 0.0386\n",
            "Epoch 41/100, Train Loss: 0.0431, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0447, Val Loss: 0.0386\n",
            "Epoch 43/100, Train Loss: 0.0435, Val Loss: 0.0386\n",
            "Epoch 44/100, Train Loss: 0.0429, Val Loss: 0.0386\n",
            "Epoch 45/100, Train Loss: 0.0429, Val Loss: 0.0386\n",
            "Epoch 46/100, Train Loss: 0.0433, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0441, Val Loss: 0.0386\n",
            "Epoch 48/100, Train Loss: 0.0428, Val Loss: 0.0386\n",
            "Epoch 49/100, Train Loss: 0.0430, Val Loss: 0.0386\n",
            "Epoch 50/100, Train Loss: 0.0432, Val Loss: 0.0386\n",
            "Epoch 51/100, Train Loss: 0.0434, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0431, Val Loss: 0.0386\n",
            "Epoch 53/100, Train Loss: 0.0441, Val Loss: 0.0386\n",
            "Epoch 54/100, Train Loss: 0.0437, Val Loss: 0.0386\n",
            "Epoch 55/100, Train Loss: 0.0427, Val Loss: 0.0386\n",
            "Epoch 56/100, Train Loss: 0.0433, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0445, Val Loss: 0.0386\n",
            "Epoch 58/100, Train Loss: 0.0454, Val Loss: 0.0386\n",
            "Epoch 59/100, Train Loss: 0.0440, Val Loss: 0.0386\n",
            "Epoch 60/100, Train Loss: 0.0424, Val Loss: 0.0386\n",
            "Epoch 61/100, Train Loss: 0.0431, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0432, Val Loss: 0.0386\n",
            "Epoch 63/100, Train Loss: 0.0425, Val Loss: 0.0386\n",
            "Epoch 64/100, Train Loss: 0.0434, Val Loss: 0.0386\n",
            "Epoch 65/100, Train Loss: 0.0437, Val Loss: 0.0386\n",
            "Epoch 66/100, Train Loss: 0.0436, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0440, Val Loss: 0.0386\n",
            "Epoch 68/100, Train Loss: 0.0441, Val Loss: 0.0386\n",
            "Epoch 69/100, Train Loss: 0.0434, Val Loss: 0.0386\n",
            "Epoch 70/100, Train Loss: 0.0440, Val Loss: 0.0386\n",
            "Epoch 71/100, Train Loss: 0.0436, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0435, Val Loss: 0.0386\n",
            "Epoch 73/100, Train Loss: 0.0432, Val Loss: 0.0386\n",
            "Epoch 74/100, Train Loss: 0.0441, Val Loss: 0.0386\n",
            "Epoch 75/100, Train Loss: 0.0432, Val Loss: 0.0386\n",
            "Epoch 76/100, Train Loss: 0.0418, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0447, Val Loss: 0.0386\n",
            "Epoch 78/100, Train Loss: 0.0429, Val Loss: 0.0386\n",
            "Epoch 79/100, Train Loss: 0.0438, Val Loss: 0.0386\n",
            "Epoch 80/100, Train Loss: 0.0432, Val Loss: 0.0386\n",
            "Epoch 81/100, Train Loss: 0.0440, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0433, Val Loss: 0.0386\n",
            "Epoch 83/100, Train Loss: 0.0446, Val Loss: 0.0386\n",
            "Epoch 84/100, Train Loss: 0.0462, Val Loss: 0.0386\n",
            "Epoch 85/100, Train Loss: 0.0429, Val Loss: 0.0386\n",
            "Epoch 86/100, Train Loss: 0.0443, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0423, Val Loss: 0.0386\n",
            "Epoch 88/100, Train Loss: 0.0449, Val Loss: 0.0386\n",
            "Epoch 89/100, Train Loss: 0.0438, Val Loss: 0.0386\n",
            "Epoch 90/100, Train Loss: 0.0438, Val Loss: 0.0386\n",
            "Epoch 91/100, Train Loss: 0.0440, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0444, Val Loss: 0.0386\n",
            "Epoch 93/100, Train Loss: 0.0439, Val Loss: 0.0386\n",
            "Epoch 94/100, Train Loss: 0.0450, Val Loss: 0.0386\n",
            "Epoch 95/100, Train Loss: 0.0429, Val Loss: 0.0386\n",
            "Epoch 96/100, Train Loss: 0.0429, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0437, Val Loss: 0.0386\n",
            "Epoch 98/100, Train Loss: 0.0430, Val Loss: 0.0386\n",
            "Epoch 99/100, Train Loss: 0.0430, Val Loss: 0.0386\n",
            "Epoch 100/100, Train Loss: 0.0425, Val Loss: 0.0386\n",
            "\n",
            "Test iteration 21/28\n",
            "Current training set size: 117 samples\n",
            "Epoch 1/100, Train Loss: 0.1550, Val Loss: 0.1873\n",
            "Epoch 2/100, Train Loss: 0.1463, Val Loss: 0.1722\n",
            "Epoch 3/100, Train Loss: 0.1489, Val Loss: 0.1691\n",
            "Epoch 4/100, Train Loss: 0.1186, Val Loss: 0.0574\n",
            "Epoch 5/100, Train Loss: 0.0850, Val Loss: 0.0604\n",
            "Epoch 6/100, Train Loss: 0.0788, Val Loss: 0.0405\n",
            "Epoch 7/100, Train Loss: 0.0808, Val Loss: 0.0662\n",
            "Epoch 8/100, Train Loss: 0.0822, Val Loss: 0.0662\n",
            "Epoch 9/100, Train Loss: 0.0685, Val Loss: 0.0541\n",
            "Epoch 10/100, Train Loss: 0.0607, Val Loss: 0.0559\n",
            "Epoch 11/100, Train Loss: 0.0575, Val Loss: 0.0781\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 12/100, Train Loss: 0.0712, Val Loss: 0.0631\n",
            "Epoch 13/100, Train Loss: 0.0579, Val Loss: 0.0524\n",
            "Epoch 14/100, Train Loss: 0.0531, Val Loss: 0.0457\n",
            "Epoch 15/100, Train Loss: 0.0563, Val Loss: 0.0412\n",
            "Epoch 16/100, Train Loss: 0.0483, Val Loss: 0.0371\n",
            "Epoch 17/100, Train Loss: 0.0475, Val Loss: 0.0344\n",
            "Epoch 18/100, Train Loss: 0.0454, Val Loss: 0.0337\n",
            "Epoch 19/100, Train Loss: 0.0452, Val Loss: 0.0330\n",
            "Epoch 20/100, Train Loss: 0.0458, Val Loss: 0.0333\n",
            "Epoch 21/100, Train Loss: 0.0454, Val Loss: 0.0336\n",
            "Epoch 22/100, Train Loss: 0.0440, Val Loss: 0.0340\n",
            "Epoch 23/100, Train Loss: 0.0438, Val Loss: 0.0342\n",
            "Epoch 24/100, Train Loss: 0.0450, Val Loss: 0.0341\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0441, Val Loss: 0.0341\n",
            "Epoch 26/100, Train Loss: 0.0453, Val Loss: 0.0341\n",
            "Epoch 27/100, Train Loss: 0.0463, Val Loss: 0.0341\n",
            "Epoch 28/100, Train Loss: 0.0455, Val Loss: 0.0341\n",
            "Epoch 29/100, Train Loss: 0.0437, Val Loss: 0.0341\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0451, Val Loss: 0.0341\n",
            "Epoch 31/100, Train Loss: 0.0439, Val Loss: 0.0341\n",
            "Epoch 32/100, Train Loss: 0.0459, Val Loss: 0.0341\n",
            "Epoch 33/100, Train Loss: 0.0446, Val Loss: 0.0341\n",
            "Epoch 34/100, Train Loss: 0.0441, Val Loss: 0.0341\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0436, Val Loss: 0.0341\n",
            "Epoch 36/100, Train Loss: 0.0443, Val Loss: 0.0341\n",
            "Epoch 37/100, Train Loss: 0.0451, Val Loss: 0.0341\n",
            "Epoch 38/100, Train Loss: 0.0456, Val Loss: 0.0341\n",
            "Epoch 39/100, Train Loss: 0.0451, Val Loss: 0.0341\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0446, Val Loss: 0.0341\n",
            "Epoch 41/100, Train Loss: 0.0447, Val Loss: 0.0341\n",
            "Epoch 42/100, Train Loss: 0.0444, Val Loss: 0.0341\n",
            "Epoch 43/100, Train Loss: 0.0447, Val Loss: 0.0341\n",
            "Epoch 44/100, Train Loss: 0.0448, Val Loss: 0.0341\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0444, Val Loss: 0.0341\n",
            "Epoch 46/100, Train Loss: 0.0501, Val Loss: 0.0341\n",
            "Epoch 47/100, Train Loss: 0.0471, Val Loss: 0.0341\n",
            "Epoch 48/100, Train Loss: 0.0444, Val Loss: 0.0341\n",
            "Epoch 49/100, Train Loss: 0.0431, Val Loss: 0.0341\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0493, Val Loss: 0.0341\n",
            "Epoch 51/100, Train Loss: 0.0432, Val Loss: 0.0341\n",
            "Epoch 52/100, Train Loss: 0.0437, Val Loss: 0.0341\n",
            "Epoch 53/100, Train Loss: 0.0514, Val Loss: 0.0341\n",
            "Epoch 54/100, Train Loss: 0.0437, Val Loss: 0.0341\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0448, Val Loss: 0.0341\n",
            "Epoch 56/100, Train Loss: 0.0498, Val Loss: 0.0341\n",
            "Epoch 57/100, Train Loss: 0.0445, Val Loss: 0.0341\n",
            "Epoch 58/100, Train Loss: 0.0447, Val Loss: 0.0341\n",
            "Epoch 59/100, Train Loss: 0.0439, Val Loss: 0.0341\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0445, Val Loss: 0.0341\n",
            "Epoch 61/100, Train Loss: 0.0452, Val Loss: 0.0341\n",
            "Epoch 62/100, Train Loss: 0.0439, Val Loss: 0.0341\n",
            "Epoch 63/100, Train Loss: 0.0443, Val Loss: 0.0341\n",
            "Epoch 64/100, Train Loss: 0.0442, Val Loss: 0.0341\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0450, Val Loss: 0.0341\n",
            "Epoch 66/100, Train Loss: 0.0456, Val Loss: 0.0341\n",
            "Epoch 67/100, Train Loss: 0.0453, Val Loss: 0.0341\n",
            "Epoch 68/100, Train Loss: 0.0475, Val Loss: 0.0341\n",
            "Epoch 69/100, Train Loss: 0.0463, Val Loss: 0.0341\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0457, Val Loss: 0.0341\n",
            "Epoch 71/100, Train Loss: 0.0453, Val Loss: 0.0341\n",
            "Epoch 72/100, Train Loss: 0.0461, Val Loss: 0.0341\n",
            "Epoch 73/100, Train Loss: 0.0445, Val Loss: 0.0341\n",
            "Epoch 74/100, Train Loss: 0.0442, Val Loss: 0.0341\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0493, Val Loss: 0.0341\n",
            "Epoch 76/100, Train Loss: 0.0452, Val Loss: 0.0341\n",
            "Epoch 77/100, Train Loss: 0.0452, Val Loss: 0.0341\n",
            "Epoch 78/100, Train Loss: 0.0500, Val Loss: 0.0341\n",
            "Epoch 79/100, Train Loss: 0.0460, Val Loss: 0.0341\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0435, Val Loss: 0.0341\n",
            "Epoch 81/100, Train Loss: 0.0450, Val Loss: 0.0341\n",
            "Epoch 82/100, Train Loss: 0.0441, Val Loss: 0.0341\n",
            "Epoch 83/100, Train Loss: 0.0436, Val Loss: 0.0341\n",
            "Epoch 84/100, Train Loss: 0.0462, Val Loss: 0.0341\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0459, Val Loss: 0.0341\n",
            "Epoch 86/100, Train Loss: 0.0454, Val Loss: 0.0341\n",
            "Epoch 87/100, Train Loss: 0.0452, Val Loss: 0.0341\n",
            "Epoch 88/100, Train Loss: 0.0447, Val Loss: 0.0341\n",
            "Epoch 89/100, Train Loss: 0.0448, Val Loss: 0.0341\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0456, Val Loss: 0.0341\n",
            "Epoch 91/100, Train Loss: 0.0450, Val Loss: 0.0341\n",
            "Epoch 92/100, Train Loss: 0.0450, Val Loss: 0.0341\n",
            "Epoch 93/100, Train Loss: 0.0443, Val Loss: 0.0341\n",
            "Epoch 94/100, Train Loss: 0.0446, Val Loss: 0.0341\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0444, Val Loss: 0.0341\n",
            "Epoch 96/100, Train Loss: 0.0442, Val Loss: 0.0341\n",
            "Epoch 97/100, Train Loss: 0.0438, Val Loss: 0.0341\n",
            "Epoch 98/100, Train Loss: 0.0437, Val Loss: 0.0341\n",
            "Epoch 99/100, Train Loss: 0.0436, Val Loss: 0.0341\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0460, Val Loss: 0.0341\n",
            "\n",
            "Test iteration 22/28\n",
            "Current training set size: 118 samples\n",
            "Epoch 1/100, Train Loss: 0.1520, Val Loss: 0.1886\n",
            "Epoch 2/100, Train Loss: 0.1520, Val Loss: 0.1859\n",
            "Epoch 3/100, Train Loss: 0.1397, Val Loss: 0.1879\n",
            "Epoch 4/100, Train Loss: 0.1415, Val Loss: 0.1709\n",
            "Epoch 5/100, Train Loss: 0.1303, Val Loss: 0.1254\n",
            "Epoch 6/100, Train Loss: 0.1028, Val Loss: 0.1246\n",
            "Epoch 7/100, Train Loss: 0.0817, Val Loss: 0.0538\n",
            "Epoch 8/100, Train Loss: 0.0685, Val Loss: 0.0560\n",
            "Epoch 9/100, Train Loss: 0.0608, Val Loss: 0.0342\n",
            "Epoch 10/100, Train Loss: 0.0591, Val Loss: 0.0433\n",
            "Epoch 11/100, Train Loss: 0.0625, Val Loss: 0.0771\n",
            "Epoch 12/100, Train Loss: 0.0665, Val Loss: 0.0466\n",
            "Epoch 13/100, Train Loss: 0.0458, Val Loss: 0.0378\n",
            "Epoch 14/100, Train Loss: 0.0467, Val Loss: 0.0479\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 15/100, Train Loss: 0.0626, Val Loss: 0.0426\n",
            "Epoch 16/100, Train Loss: 0.0516, Val Loss: 0.0393\n",
            "Epoch 17/100, Train Loss: 0.0443, Val Loss: 0.0390\n",
            "Epoch 18/100, Train Loss: 0.0418, Val Loss: 0.0388\n",
            "Epoch 19/100, Train Loss: 0.0410, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0397, Val Loss: 0.0386\n",
            "Epoch 21/100, Train Loss: 0.0403, Val Loss: 0.0386\n",
            "Epoch 22/100, Train Loss: 0.0417, Val Loss: 0.0386\n",
            "Epoch 23/100, Train Loss: 0.0412, Val Loss: 0.0386\n",
            "Epoch 24/100, Train Loss: 0.0391, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0398, Val Loss: 0.0386\n",
            "Epoch 26/100, Train Loss: 0.0404, Val Loss: 0.0386\n",
            "Epoch 27/100, Train Loss: 0.0459, Val Loss: 0.0386\n",
            "Epoch 28/100, Train Loss: 0.0400, Val Loss: 0.0386\n",
            "Epoch 29/100, Train Loss: 0.0398, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0409, Val Loss: 0.0386\n",
            "Epoch 31/100, Train Loss: 0.0409, Val Loss: 0.0386\n",
            "Epoch 32/100, Train Loss: 0.0410, Val Loss: 0.0386\n",
            "Epoch 33/100, Train Loss: 0.0401, Val Loss: 0.0386\n",
            "Epoch 34/100, Train Loss: 0.0422, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0405, Val Loss: 0.0386\n",
            "Epoch 36/100, Train Loss: 0.0400, Val Loss: 0.0386\n",
            "Epoch 37/100, Train Loss: 0.0393, Val Loss: 0.0386\n",
            "Epoch 38/100, Train Loss: 0.0421, Val Loss: 0.0386\n",
            "Epoch 39/100, Train Loss: 0.0401, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0400, Val Loss: 0.0386\n",
            "Epoch 41/100, Train Loss: 0.0411, Val Loss: 0.0386\n",
            "Epoch 42/100, Train Loss: 0.0413, Val Loss: 0.0386\n",
            "Epoch 43/100, Train Loss: 0.0407, Val Loss: 0.0386\n",
            "Epoch 44/100, Train Loss: 0.0409, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0409, Val Loss: 0.0386\n",
            "Epoch 46/100, Train Loss: 0.0402, Val Loss: 0.0386\n",
            "Epoch 47/100, Train Loss: 0.0408, Val Loss: 0.0386\n",
            "Epoch 48/100, Train Loss: 0.0417, Val Loss: 0.0386\n",
            "Epoch 49/100, Train Loss: 0.0414, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0397, Val Loss: 0.0386\n",
            "Epoch 51/100, Train Loss: 0.0408, Val Loss: 0.0386\n",
            "Epoch 52/100, Train Loss: 0.0408, Val Loss: 0.0386\n",
            "Epoch 53/100, Train Loss: 0.0413, Val Loss: 0.0386\n",
            "Epoch 54/100, Train Loss: 0.0402, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0407, Val Loss: 0.0386\n",
            "Epoch 56/100, Train Loss: 0.0452, Val Loss: 0.0386\n",
            "Epoch 57/100, Train Loss: 0.0400, Val Loss: 0.0386\n",
            "Epoch 58/100, Train Loss: 0.0397, Val Loss: 0.0386\n",
            "Epoch 59/100, Train Loss: 0.0415, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0411, Val Loss: 0.0386\n",
            "Epoch 61/100, Train Loss: 0.0402, Val Loss: 0.0386\n",
            "Epoch 62/100, Train Loss: 0.0394, Val Loss: 0.0386\n",
            "Epoch 63/100, Train Loss: 0.0409, Val Loss: 0.0386\n",
            "Epoch 64/100, Train Loss: 0.0397, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0400, Val Loss: 0.0386\n",
            "Epoch 66/100, Train Loss: 0.0396, Val Loss: 0.0386\n",
            "Epoch 67/100, Train Loss: 0.0410, Val Loss: 0.0386\n",
            "Epoch 68/100, Train Loss: 0.0413, Val Loss: 0.0386\n",
            "Epoch 69/100, Train Loss: 0.0416, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0410, Val Loss: 0.0386\n",
            "Epoch 71/100, Train Loss: 0.0401, Val Loss: 0.0386\n",
            "Epoch 72/100, Train Loss: 0.0413, Val Loss: 0.0386\n",
            "Epoch 73/100, Train Loss: 0.0411, Val Loss: 0.0386\n",
            "Epoch 74/100, Train Loss: 0.0401, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0408, Val Loss: 0.0386\n",
            "Epoch 76/100, Train Loss: 0.0406, Val Loss: 0.0386\n",
            "Epoch 77/100, Train Loss: 0.0449, Val Loss: 0.0386\n",
            "Epoch 78/100, Train Loss: 0.0412, Val Loss: 0.0386\n",
            "Epoch 79/100, Train Loss: 0.0409, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0398, Val Loss: 0.0386\n",
            "Epoch 81/100, Train Loss: 0.0435, Val Loss: 0.0386\n",
            "Epoch 82/100, Train Loss: 0.0397, Val Loss: 0.0386\n",
            "Epoch 83/100, Train Loss: 0.0415, Val Loss: 0.0386\n",
            "Epoch 84/100, Train Loss: 0.0405, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0413, Val Loss: 0.0386\n",
            "Epoch 86/100, Train Loss: 0.0408, Val Loss: 0.0386\n",
            "Epoch 87/100, Train Loss: 0.0401, Val Loss: 0.0386\n",
            "Epoch 88/100, Train Loss: 0.0413, Val Loss: 0.0386\n",
            "Epoch 89/100, Train Loss: 0.0406, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0406, Val Loss: 0.0386\n",
            "Epoch 91/100, Train Loss: 0.0397, Val Loss: 0.0386\n",
            "Epoch 92/100, Train Loss: 0.0417, Val Loss: 0.0386\n",
            "Epoch 93/100, Train Loss: 0.0414, Val Loss: 0.0386\n",
            "Epoch 94/100, Train Loss: 0.0416, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0412, Val Loss: 0.0386\n",
            "Epoch 96/100, Train Loss: 0.0414, Val Loss: 0.0386\n",
            "Epoch 97/100, Train Loss: 0.0431, Val Loss: 0.0386\n",
            "Epoch 98/100, Train Loss: 0.0403, Val Loss: 0.0386\n",
            "Epoch 99/100, Train Loss: 0.0404, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0399, Val Loss: 0.0386\n",
            "\n",
            "Test iteration 23/28\n",
            "Current training set size: 119 samples\n",
            "Epoch 1/100, Train Loss: 0.1547, Val Loss: 0.2002\n",
            "Epoch 2/100, Train Loss: 0.1476, Val Loss: 0.1826\n",
            "Epoch 3/100, Train Loss: 0.1388, Val Loss: 0.1378\n",
            "Epoch 4/100, Train Loss: 0.1174, Val Loss: 0.0766\n",
            "Epoch 5/100, Train Loss: 0.1000, Val Loss: 0.0654\n",
            "Epoch 6/100, Train Loss: 0.0768, Val Loss: 0.0533\n",
            "Epoch 7/100, Train Loss: 0.0727, Val Loss: 0.0447\n",
            "Epoch 8/100, Train Loss: 0.0656, Val Loss: 0.0425\n",
            "Epoch 9/100, Train Loss: 0.0653, Val Loss: 0.0360\n",
            "Epoch 10/100, Train Loss: 0.0566, Val Loss: 0.0437\n",
            "Epoch 11/100, Train Loss: 0.0570, Val Loss: 0.0705\n",
            "Epoch 12/100, Train Loss: 0.0606, Val Loss: 0.0369\n",
            "Epoch 13/100, Train Loss: 0.0541, Val Loss: 0.0407\n",
            "Epoch 14/100, Train Loss: 0.0486, Val Loss: 0.0388\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 15/100, Train Loss: 0.0409, Val Loss: 0.0386\n",
            "Epoch 16/100, Train Loss: 0.0404, Val Loss: 0.0386\n",
            "Epoch 17/100, Train Loss: 0.0403, Val Loss: 0.0376\n",
            "Epoch 18/100, Train Loss: 0.0405, Val Loss: 0.0377\n",
            "Epoch 19/100, Train Loss: 0.0402, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0398, Val Loss: 0.0377\n",
            "Epoch 21/100, Train Loss: 0.0397, Val Loss: 0.0377\n",
            "Epoch 22/100, Train Loss: 0.0395, Val Loss: 0.0377\n",
            "Epoch 23/100, Train Loss: 0.0395, Val Loss: 0.0377\n",
            "Epoch 24/100, Train Loss: 0.0409, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0399, Val Loss: 0.0377\n",
            "Epoch 26/100, Train Loss: 0.0418, Val Loss: 0.0377\n",
            "Epoch 27/100, Train Loss: 0.0395, Val Loss: 0.0377\n",
            "Epoch 28/100, Train Loss: 0.0395, Val Loss: 0.0377\n",
            "Epoch 29/100, Train Loss: 0.0394, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0393, Val Loss: 0.0377\n",
            "Epoch 31/100, Train Loss: 0.0397, Val Loss: 0.0377\n",
            "Epoch 32/100, Train Loss: 0.0404, Val Loss: 0.0377\n",
            "Epoch 33/100, Train Loss: 0.0411, Val Loss: 0.0377\n",
            "Epoch 34/100, Train Loss: 0.0406, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0409, Val Loss: 0.0377\n",
            "Epoch 36/100, Train Loss: 0.0393, Val Loss: 0.0377\n",
            "Epoch 37/100, Train Loss: 0.0392, Val Loss: 0.0377\n",
            "Epoch 38/100, Train Loss: 0.0384, Val Loss: 0.0377\n",
            "Epoch 39/100, Train Loss: 0.0416, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0402, Val Loss: 0.0377\n",
            "Epoch 41/100, Train Loss: 0.0396, Val Loss: 0.0377\n",
            "Epoch 42/100, Train Loss: 0.0383, Val Loss: 0.0377\n",
            "Epoch 43/100, Train Loss: 0.0403, Val Loss: 0.0377\n",
            "Epoch 44/100, Train Loss: 0.0395, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0402, Val Loss: 0.0377\n",
            "Epoch 46/100, Train Loss: 0.0410, Val Loss: 0.0377\n",
            "Epoch 47/100, Train Loss: 0.0413, Val Loss: 0.0377\n",
            "Epoch 48/100, Train Loss: 0.0404, Val Loss: 0.0377\n",
            "Epoch 49/100, Train Loss: 0.0403, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0401, Val Loss: 0.0377\n",
            "Epoch 51/100, Train Loss: 0.0407, Val Loss: 0.0377\n",
            "Epoch 52/100, Train Loss: 0.0413, Val Loss: 0.0377\n",
            "Epoch 53/100, Train Loss: 0.0412, Val Loss: 0.0377\n",
            "Epoch 54/100, Train Loss: 0.0401, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0403, Val Loss: 0.0377\n",
            "Epoch 56/100, Train Loss: 0.0390, Val Loss: 0.0377\n",
            "Epoch 57/100, Train Loss: 0.0407, Val Loss: 0.0377\n",
            "Epoch 58/100, Train Loss: 0.0406, Val Loss: 0.0377\n",
            "Epoch 59/100, Train Loss: 0.0411, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0408, Val Loss: 0.0377\n",
            "Epoch 61/100, Train Loss: 0.0407, Val Loss: 0.0377\n",
            "Epoch 62/100, Train Loss: 0.0409, Val Loss: 0.0377\n",
            "Epoch 63/100, Train Loss: 0.0390, Val Loss: 0.0377\n",
            "Epoch 64/100, Train Loss: 0.0395, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0389, Val Loss: 0.0377\n",
            "Epoch 66/100, Train Loss: 0.0405, Val Loss: 0.0377\n",
            "Epoch 67/100, Train Loss: 0.0388, Val Loss: 0.0377\n",
            "Epoch 68/100, Train Loss: 0.0401, Val Loss: 0.0377\n",
            "Epoch 69/100, Train Loss: 0.0406, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0396, Val Loss: 0.0377\n",
            "Epoch 71/100, Train Loss: 0.0392, Val Loss: 0.0377\n",
            "Epoch 72/100, Train Loss: 0.0402, Val Loss: 0.0377\n",
            "Epoch 73/100, Train Loss: 0.0419, Val Loss: 0.0377\n",
            "Epoch 74/100, Train Loss: 0.0398, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0393, Val Loss: 0.0377\n",
            "Epoch 76/100, Train Loss: 0.0393, Val Loss: 0.0377\n",
            "Epoch 77/100, Train Loss: 0.0395, Val Loss: 0.0377\n",
            "Epoch 78/100, Train Loss: 0.0389, Val Loss: 0.0377\n",
            "Epoch 79/100, Train Loss: 0.0391, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0393, Val Loss: 0.0377\n",
            "Epoch 81/100, Train Loss: 0.0409, Val Loss: 0.0377\n",
            "Epoch 82/100, Train Loss: 0.0395, Val Loss: 0.0377\n",
            "Epoch 83/100, Train Loss: 0.0395, Val Loss: 0.0377\n",
            "Epoch 84/100, Train Loss: 0.0398, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0397, Val Loss: 0.0377\n",
            "Epoch 86/100, Train Loss: 0.0398, Val Loss: 0.0377\n",
            "Epoch 87/100, Train Loss: 0.0411, Val Loss: 0.0377\n",
            "Epoch 88/100, Train Loss: 0.0412, Val Loss: 0.0377\n",
            "Epoch 89/100, Train Loss: 0.0390, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0394, Val Loss: 0.0377\n",
            "Epoch 91/100, Train Loss: 0.0413, Val Loss: 0.0377\n",
            "Epoch 92/100, Train Loss: 0.0397, Val Loss: 0.0377\n",
            "Epoch 93/100, Train Loss: 0.0404, Val Loss: 0.0377\n",
            "Epoch 94/100, Train Loss: 0.0405, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0408, Val Loss: 0.0377\n",
            "Epoch 96/100, Train Loss: 0.0417, Val Loss: 0.0377\n",
            "Epoch 97/100, Train Loss: 0.0409, Val Loss: 0.0377\n",
            "Epoch 98/100, Train Loss: 0.0397, Val Loss: 0.0377\n",
            "Epoch 99/100, Train Loss: 0.0401, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0401, Val Loss: 0.0377\n",
            "\n",
            "Test iteration 24/28\n",
            "Current training set size: 120 samples\n",
            "Epoch 1/100, Train Loss: 0.1588, Val Loss: 0.1890\n",
            "Epoch 2/100, Train Loss: 0.1420, Val Loss: 0.1834\n",
            "Epoch 3/100, Train Loss: 0.1417, Val Loss: 0.1618\n",
            "Epoch 4/100, Train Loss: 0.1113, Val Loss: 0.0961\n",
            "Epoch 5/100, Train Loss: 0.0947, Val Loss: 0.0531\n",
            "Epoch 6/100, Train Loss: 0.0714, Val Loss: 0.0429\n",
            "Epoch 7/100, Train Loss: 0.0734, Val Loss: 0.0698\n",
            "Epoch 8/100, Train Loss: 0.0803, Val Loss: 0.0386\n",
            "Epoch 9/100, Train Loss: 0.0620, Val Loss: 0.0370\n",
            "Epoch 10/100, Train Loss: 0.0681, Val Loss: 0.0431\n",
            "Epoch 11/100, Train Loss: 0.0608, Val Loss: 0.0436\n",
            "Epoch 12/100, Train Loss: 0.0545, Val Loss: 0.0469\n",
            "Epoch 13/100, Train Loss: 0.0512, Val Loss: 0.0631\n",
            "Epoch 14/100, Train Loss: 0.0533, Val Loss: 0.0491\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 15/100, Train Loss: 0.0442, Val Loss: 0.0415\n",
            "Epoch 16/100, Train Loss: 0.0411, Val Loss: 0.0381\n",
            "Epoch 17/100, Train Loss: 0.0380, Val Loss: 0.0367\n",
            "Epoch 18/100, Train Loss: 0.0393, Val Loss: 0.0366\n",
            "Epoch 19/100, Train Loss: 0.0395, Val Loss: 0.0366\n",
            "Epoch 20/100, Train Loss: 0.0383, Val Loss: 0.0366\n",
            "Epoch 21/100, Train Loss: 0.0388, Val Loss: 0.0366\n",
            "Epoch 22/100, Train Loss: 0.0391, Val Loss: 0.0368\n",
            "Epoch 23/100, Train Loss: 0.0391, Val Loss: 0.0370\n",
            "Epoch 24/100, Train Loss: 0.0386, Val Loss: 0.0373\n",
            "Epoch 25/100, Train Loss: 0.0396, Val Loss: 0.0368\n",
            "Epoch 26/100, Train Loss: 0.0381, Val Loss: 0.0372\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0388, Val Loss: 0.0372\n",
            "Epoch 28/100, Train Loss: 0.0375, Val Loss: 0.0372\n",
            "Epoch 29/100, Train Loss: 0.0400, Val Loss: 0.0372\n",
            "Epoch 30/100, Train Loss: 0.0378, Val Loss: 0.0372\n",
            "Epoch 31/100, Train Loss: 0.0380, Val Loss: 0.0372\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0398, Val Loss: 0.0372\n",
            "Epoch 33/100, Train Loss: 0.0395, Val Loss: 0.0372\n",
            "Epoch 34/100, Train Loss: 0.0384, Val Loss: 0.0372\n",
            "Epoch 35/100, Train Loss: 0.0395, Val Loss: 0.0372\n",
            "Epoch 36/100, Train Loss: 0.0391, Val Loss: 0.0372\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0388, Val Loss: 0.0372\n",
            "Epoch 38/100, Train Loss: 0.0384, Val Loss: 0.0372\n",
            "Epoch 39/100, Train Loss: 0.0392, Val Loss: 0.0372\n",
            "Epoch 40/100, Train Loss: 0.0380, Val Loss: 0.0372\n",
            "Epoch 41/100, Train Loss: 0.0380, Val Loss: 0.0372\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0389, Val Loss: 0.0372\n",
            "Epoch 43/100, Train Loss: 0.0388, Val Loss: 0.0372\n",
            "Epoch 44/100, Train Loss: 0.0387, Val Loss: 0.0372\n",
            "Epoch 45/100, Train Loss: 0.0392, Val Loss: 0.0372\n",
            "Epoch 46/100, Train Loss: 0.0390, Val Loss: 0.0372\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0387, Val Loss: 0.0372\n",
            "Epoch 48/100, Train Loss: 0.0379, Val Loss: 0.0372\n",
            "Epoch 49/100, Train Loss: 0.0391, Val Loss: 0.0372\n",
            "Epoch 50/100, Train Loss: 0.0383, Val Loss: 0.0372\n",
            "Epoch 51/100, Train Loss: 0.0388, Val Loss: 0.0372\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0376, Val Loss: 0.0372\n",
            "Epoch 53/100, Train Loss: 0.0392, Val Loss: 0.0372\n",
            "Epoch 54/100, Train Loss: 0.0377, Val Loss: 0.0372\n",
            "Epoch 55/100, Train Loss: 0.0384, Val Loss: 0.0372\n",
            "Epoch 56/100, Train Loss: 0.0390, Val Loss: 0.0372\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0377, Val Loss: 0.0372\n",
            "Epoch 58/100, Train Loss: 0.0388, Val Loss: 0.0372\n",
            "Epoch 59/100, Train Loss: 0.0384, Val Loss: 0.0372\n",
            "Epoch 60/100, Train Loss: 0.0387, Val Loss: 0.0372\n",
            "Epoch 61/100, Train Loss: 0.0380, Val Loss: 0.0372\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0382, Val Loss: 0.0372\n",
            "Epoch 63/100, Train Loss: 0.0394, Val Loss: 0.0372\n",
            "Epoch 64/100, Train Loss: 0.0390, Val Loss: 0.0372\n",
            "Epoch 65/100, Train Loss: 0.0383, Val Loss: 0.0372\n",
            "Epoch 66/100, Train Loss: 0.0382, Val Loss: 0.0372\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0375, Val Loss: 0.0372\n",
            "Epoch 68/100, Train Loss: 0.0382, Val Loss: 0.0372\n",
            "Epoch 69/100, Train Loss: 0.0397, Val Loss: 0.0372\n",
            "Epoch 70/100, Train Loss: 0.0377, Val Loss: 0.0372\n",
            "Epoch 71/100, Train Loss: 0.0381, Val Loss: 0.0372\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0389, Val Loss: 0.0372\n",
            "Epoch 73/100, Train Loss: 0.0384, Val Loss: 0.0372\n",
            "Epoch 74/100, Train Loss: 0.0378, Val Loss: 0.0372\n",
            "Epoch 75/100, Train Loss: 0.0379, Val Loss: 0.0372\n",
            "Epoch 76/100, Train Loss: 0.0373, Val Loss: 0.0372\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0380, Val Loss: 0.0372\n",
            "Epoch 78/100, Train Loss: 0.0398, Val Loss: 0.0372\n",
            "Epoch 79/100, Train Loss: 0.0382, Val Loss: 0.0372\n",
            "Epoch 80/100, Train Loss: 0.0373, Val Loss: 0.0372\n",
            "Epoch 81/100, Train Loss: 0.0384, Val Loss: 0.0372\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0384, Val Loss: 0.0372\n",
            "Epoch 83/100, Train Loss: 0.0386, Val Loss: 0.0372\n",
            "Epoch 84/100, Train Loss: 0.0382, Val Loss: 0.0372\n",
            "Epoch 85/100, Train Loss: 0.0381, Val Loss: 0.0372\n",
            "Epoch 86/100, Train Loss: 0.0394, Val Loss: 0.0372\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0384, Val Loss: 0.0372\n",
            "Epoch 88/100, Train Loss: 0.0380, Val Loss: 0.0372\n",
            "Epoch 89/100, Train Loss: 0.0388, Val Loss: 0.0372\n",
            "Epoch 90/100, Train Loss: 0.0386, Val Loss: 0.0372\n",
            "Epoch 91/100, Train Loss: 0.0385, Val Loss: 0.0372\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0372, Val Loss: 0.0372\n",
            "Epoch 93/100, Train Loss: 0.0390, Val Loss: 0.0372\n",
            "Epoch 94/100, Train Loss: 0.0395, Val Loss: 0.0372\n",
            "Epoch 95/100, Train Loss: 0.0378, Val Loss: 0.0372\n",
            "Epoch 96/100, Train Loss: 0.0384, Val Loss: 0.0372\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0386, Val Loss: 0.0372\n",
            "Epoch 98/100, Train Loss: 0.0385, Val Loss: 0.0372\n",
            "Epoch 99/100, Train Loss: 0.0380, Val Loss: 0.0372\n",
            "Epoch 100/100, Train Loss: 0.0394, Val Loss: 0.0372\n",
            "\n",
            "Test iteration 25/28\n",
            "Current training set size: 121 samples\n",
            "Epoch 1/100, Train Loss: 0.1647, Val Loss: 0.1898\n",
            "Epoch 2/100, Train Loss: 0.1545, Val Loss: 0.1885\n",
            "Epoch 3/100, Train Loss: 0.1377, Val Loss: 0.1882\n",
            "Epoch 4/100, Train Loss: 0.1293, Val Loss: 0.1079\n",
            "Epoch 5/100, Train Loss: 0.0967, Val Loss: 0.0736\n",
            "Epoch 6/100, Train Loss: 0.0780, Val Loss: 0.0590\n",
            "Epoch 7/100, Train Loss: 0.0760, Val Loss: 0.0603\n",
            "Epoch 8/100, Train Loss: 0.0673, Val Loss: 0.0819\n",
            "Epoch 9/100, Train Loss: 0.0685, Val Loss: 0.0396\n",
            "Epoch 10/100, Train Loss: 0.0563, Val Loss: 0.0659\n",
            "Epoch 11/100, Train Loss: 0.0587, Val Loss: 0.0485\n",
            "Epoch 12/100, Train Loss: 0.0454, Val Loss: 0.0383\n",
            "Epoch 13/100, Train Loss: 0.0458, Val Loss: 0.0564\n",
            "Epoch 14/100, Train Loss: 0.0530, Val Loss: 0.0516\n",
            "Epoch 15/100, Train Loss: 0.0460, Val Loss: 0.0496\n",
            "Epoch 16/100, Train Loss: 0.0459, Val Loss: 0.0609\n",
            "Epoch 17/100, Train Loss: 0.0459, Val Loss: 0.0480\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0502, Val Loss: 0.0451\n",
            "Epoch 19/100, Train Loss: 0.0411, Val Loss: 0.0453\n",
            "Epoch 20/100, Train Loss: 0.0371, Val Loss: 0.0469\n",
            "Epoch 21/100, Train Loss: 0.0376, Val Loss: 0.0475\n",
            "Epoch 22/100, Train Loss: 0.0383, Val Loss: 0.0476\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0361, Val Loss: 0.0476\n",
            "Epoch 24/100, Train Loss: 0.0385, Val Loss: 0.0476\n",
            "Epoch 25/100, Train Loss: 0.0379, Val Loss: 0.0476\n",
            "Epoch 26/100, Train Loss: 0.0374, Val Loss: 0.0476\n",
            "Epoch 27/100, Train Loss: 0.0363, Val Loss: 0.0476\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0381, Val Loss: 0.0476\n",
            "Epoch 29/100, Train Loss: 0.0376, Val Loss: 0.0476\n",
            "Epoch 30/100, Train Loss: 0.0375, Val Loss: 0.0476\n",
            "Epoch 31/100, Train Loss: 0.0381, Val Loss: 0.0476\n",
            "Epoch 32/100, Train Loss: 0.0386, Val Loss: 0.0476\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0389, Val Loss: 0.0476\n",
            "Epoch 34/100, Train Loss: 0.0376, Val Loss: 0.0476\n",
            "Epoch 35/100, Train Loss: 0.0376, Val Loss: 0.0476\n",
            "Epoch 36/100, Train Loss: 0.0382, Val Loss: 0.0476\n",
            "Epoch 37/100, Train Loss: 0.0387, Val Loss: 0.0476\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0360, Val Loss: 0.0476\n",
            "Epoch 39/100, Train Loss: 0.0368, Val Loss: 0.0476\n",
            "Epoch 40/100, Train Loss: 0.0374, Val Loss: 0.0476\n",
            "Epoch 41/100, Train Loss: 0.0366, Val Loss: 0.0476\n",
            "Epoch 42/100, Train Loss: 0.0453, Val Loss: 0.0476\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0361, Val Loss: 0.0476\n",
            "Epoch 44/100, Train Loss: 0.0385, Val Loss: 0.0476\n",
            "Epoch 45/100, Train Loss: 0.0402, Val Loss: 0.0476\n",
            "Epoch 46/100, Train Loss: 0.0372, Val Loss: 0.0476\n",
            "Epoch 47/100, Train Loss: 0.0386, Val Loss: 0.0476\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0383, Val Loss: 0.0476\n",
            "Epoch 49/100, Train Loss: 0.0382, Val Loss: 0.0476\n",
            "Epoch 50/100, Train Loss: 0.0459, Val Loss: 0.0476\n",
            "Epoch 51/100, Train Loss: 0.0380, Val Loss: 0.0476\n",
            "Epoch 52/100, Train Loss: 0.0370, Val Loss: 0.0476\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0364, Val Loss: 0.0476\n",
            "Epoch 54/100, Train Loss: 0.0386, Val Loss: 0.0476\n",
            "Epoch 55/100, Train Loss: 0.0357, Val Loss: 0.0476\n",
            "Epoch 56/100, Train Loss: 0.0363, Val Loss: 0.0476\n",
            "Epoch 57/100, Train Loss: 0.0491, Val Loss: 0.0476\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0370, Val Loss: 0.0476\n",
            "Epoch 59/100, Train Loss: 0.0382, Val Loss: 0.0476\n",
            "Epoch 60/100, Train Loss: 0.0360, Val Loss: 0.0476\n",
            "Epoch 61/100, Train Loss: 0.0362, Val Loss: 0.0476\n",
            "Epoch 62/100, Train Loss: 0.0375, Val Loss: 0.0476\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0373, Val Loss: 0.0476\n",
            "Epoch 64/100, Train Loss: 0.0376, Val Loss: 0.0476\n",
            "Epoch 65/100, Train Loss: 0.0371, Val Loss: 0.0476\n",
            "Epoch 66/100, Train Loss: 0.0372, Val Loss: 0.0476\n",
            "Epoch 67/100, Train Loss: 0.0391, Val Loss: 0.0476\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0395, Val Loss: 0.0476\n",
            "Epoch 69/100, Train Loss: 0.0395, Val Loss: 0.0476\n",
            "Epoch 70/100, Train Loss: 0.0369, Val Loss: 0.0476\n",
            "Epoch 71/100, Train Loss: 0.0389, Val Loss: 0.0476\n",
            "Epoch 72/100, Train Loss: 0.0374, Val Loss: 0.0476\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0374, Val Loss: 0.0476\n",
            "Epoch 74/100, Train Loss: 0.0369, Val Loss: 0.0476\n",
            "Epoch 75/100, Train Loss: 0.0375, Val Loss: 0.0476\n",
            "Epoch 76/100, Train Loss: 0.0392, Val Loss: 0.0476\n",
            "Epoch 77/100, Train Loss: 0.0394, Val Loss: 0.0476\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0508, Val Loss: 0.0476\n",
            "Epoch 79/100, Train Loss: 0.0373, Val Loss: 0.0476\n",
            "Epoch 80/100, Train Loss: 0.0384, Val Loss: 0.0476\n",
            "Epoch 81/100, Train Loss: 0.0358, Val Loss: 0.0476\n",
            "Epoch 82/100, Train Loss: 0.0373, Val Loss: 0.0476\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0361, Val Loss: 0.0476\n",
            "Epoch 84/100, Train Loss: 0.0385, Val Loss: 0.0476\n",
            "Epoch 85/100, Train Loss: 0.0375, Val Loss: 0.0476\n",
            "Epoch 86/100, Train Loss: 0.0387, Val Loss: 0.0476\n",
            "Epoch 87/100, Train Loss: 0.0357, Val Loss: 0.0476\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0357, Val Loss: 0.0476\n",
            "Epoch 89/100, Train Loss: 0.0371, Val Loss: 0.0476\n",
            "Epoch 90/100, Train Loss: 0.0376, Val Loss: 0.0476\n",
            "Epoch 91/100, Train Loss: 0.0375, Val Loss: 0.0476\n",
            "Epoch 92/100, Train Loss: 0.0381, Val Loss: 0.0476\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0369, Val Loss: 0.0476\n",
            "Epoch 94/100, Train Loss: 0.0370, Val Loss: 0.0476\n",
            "Epoch 95/100, Train Loss: 0.0385, Val Loss: 0.0476\n",
            "Epoch 96/100, Train Loss: 0.0371, Val Loss: 0.0476\n",
            "Epoch 97/100, Train Loss: 0.0371, Val Loss: 0.0476\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0366, Val Loss: 0.0476\n",
            "Epoch 99/100, Train Loss: 0.0371, Val Loss: 0.0476\n",
            "Epoch 100/100, Train Loss: 0.0371, Val Loss: 0.0476\n",
            "\n",
            "Test iteration 26/28\n",
            "Current training set size: 122 samples\n",
            "Epoch 1/100, Train Loss: 0.1556, Val Loss: 0.2060\n",
            "Epoch 2/100, Train Loss: 0.1509, Val Loss: 0.1837\n",
            "Epoch 3/100, Train Loss: 0.1458, Val Loss: 0.1705\n",
            "Epoch 4/100, Train Loss: 0.1192, Val Loss: 0.1092\n",
            "Epoch 5/100, Train Loss: 0.0881, Val Loss: 0.0355\n",
            "Epoch 6/100, Train Loss: 0.0772, Val Loss: 0.0427\n",
            "Epoch 7/100, Train Loss: 0.0789, Val Loss: 0.0429\n",
            "Epoch 8/100, Train Loss: 0.0654, Val Loss: 0.0394\n",
            "Epoch 9/100, Train Loss: 0.0590, Val Loss: 0.0372\n",
            "Epoch 10/100, Train Loss: 0.0555, Val Loss: 0.0352\n",
            "Epoch 11/100, Train Loss: 0.0517, Val Loss: 0.0412\n",
            "Epoch 12/100, Train Loss: 0.0553, Val Loss: 0.0323\n",
            "Epoch 13/100, Train Loss: 0.0438, Val Loss: 0.0404\n",
            "Epoch 14/100, Train Loss: 0.0477, Val Loss: 0.0397\n",
            "Epoch 15/100, Train Loss: 0.0504, Val Loss: 0.0504\n",
            "Epoch 16/100, Train Loss: 0.0448, Val Loss: 0.0413\n",
            "Epoch 17/100, Train Loss: 0.0469, Val Loss: 0.0496\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0579, Val Loss: 0.0457\n",
            "Epoch 19/100, Train Loss: 0.0488, Val Loss: 0.0440\n",
            "Epoch 20/100, Train Loss: 0.0416, Val Loss: 0.0444\n",
            "Epoch 21/100, Train Loss: 0.0399, Val Loss: 0.0447\n",
            "Epoch 22/100, Train Loss: 0.0377, Val Loss: 0.0451\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0378, Val Loss: 0.0451\n",
            "Epoch 24/100, Train Loss: 0.0383, Val Loss: 0.0451\n",
            "Epoch 25/100, Train Loss: 0.0374, Val Loss: 0.0452\n",
            "Epoch 26/100, Train Loss: 0.0366, Val Loss: 0.0452\n",
            "Epoch 27/100, Train Loss: 0.0375, Val Loss: 0.0452\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0374, Val Loss: 0.0452\n",
            "Epoch 29/100, Train Loss: 0.0368, Val Loss: 0.0452\n",
            "Epoch 30/100, Train Loss: 0.0385, Val Loss: 0.0452\n",
            "Epoch 31/100, Train Loss: 0.0362, Val Loss: 0.0452\n",
            "Epoch 32/100, Train Loss: 0.0363, Val Loss: 0.0452\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0397, Val Loss: 0.0452\n",
            "Epoch 34/100, Train Loss: 0.0371, Val Loss: 0.0452\n",
            "Epoch 35/100, Train Loss: 0.0346, Val Loss: 0.0452\n",
            "Epoch 36/100, Train Loss: 0.0447, Val Loss: 0.0452\n",
            "Epoch 37/100, Train Loss: 0.0376, Val Loss: 0.0452\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0383, Val Loss: 0.0452\n",
            "Epoch 39/100, Train Loss: 0.0382, Val Loss: 0.0452\n",
            "Epoch 40/100, Train Loss: 0.0373, Val Loss: 0.0452\n",
            "Epoch 41/100, Train Loss: 0.0368, Val Loss: 0.0452\n",
            "Epoch 42/100, Train Loss: 0.0383, Val Loss: 0.0452\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0369, Val Loss: 0.0452\n",
            "Epoch 44/100, Train Loss: 0.0381, Val Loss: 0.0452\n",
            "Epoch 45/100, Train Loss: 0.0379, Val Loss: 0.0452\n",
            "Epoch 46/100, Train Loss: 0.0359, Val Loss: 0.0452\n",
            "Epoch 47/100, Train Loss: 0.0371, Val Loss: 0.0452\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0375, Val Loss: 0.0452\n",
            "Epoch 49/100, Train Loss: 0.0388, Val Loss: 0.0452\n",
            "Epoch 50/100, Train Loss: 0.0372, Val Loss: 0.0452\n",
            "Epoch 51/100, Train Loss: 0.0401, Val Loss: 0.0452\n",
            "Epoch 52/100, Train Loss: 0.0348, Val Loss: 0.0452\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0387, Val Loss: 0.0452\n",
            "Epoch 54/100, Train Loss: 0.0369, Val Loss: 0.0452\n",
            "Epoch 55/100, Train Loss: 0.0364, Val Loss: 0.0452\n",
            "Epoch 56/100, Train Loss: 0.0384, Val Loss: 0.0452\n",
            "Epoch 57/100, Train Loss: 0.0374, Val Loss: 0.0452\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0372, Val Loss: 0.0452\n",
            "Epoch 59/100, Train Loss: 0.0367, Val Loss: 0.0452\n",
            "Epoch 60/100, Train Loss: 0.0362, Val Loss: 0.0452\n",
            "Epoch 61/100, Train Loss: 0.0382, Val Loss: 0.0452\n",
            "Epoch 62/100, Train Loss: 0.0367, Val Loss: 0.0452\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0369, Val Loss: 0.0452\n",
            "Epoch 64/100, Train Loss: 0.0360, Val Loss: 0.0452\n",
            "Epoch 65/100, Train Loss: 0.0382, Val Loss: 0.0452\n",
            "Epoch 66/100, Train Loss: 0.0363, Val Loss: 0.0452\n",
            "Epoch 67/100, Train Loss: 0.0372, Val Loss: 0.0452\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0376, Val Loss: 0.0452\n",
            "Epoch 69/100, Train Loss: 0.0370, Val Loss: 0.0452\n",
            "Epoch 70/100, Train Loss: 0.0363, Val Loss: 0.0452\n",
            "Epoch 71/100, Train Loss: 0.0360, Val Loss: 0.0452\n",
            "Epoch 72/100, Train Loss: 0.0366, Val Loss: 0.0452\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0381, Val Loss: 0.0452\n",
            "Epoch 74/100, Train Loss: 0.0367, Val Loss: 0.0452\n",
            "Epoch 75/100, Train Loss: 0.0369, Val Loss: 0.0452\n",
            "Epoch 76/100, Train Loss: 0.0371, Val Loss: 0.0452\n",
            "Epoch 77/100, Train Loss: 0.0376, Val Loss: 0.0452\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0364, Val Loss: 0.0452\n",
            "Epoch 79/100, Train Loss: 0.0378, Val Loss: 0.0452\n",
            "Epoch 80/100, Train Loss: 0.0378, Val Loss: 0.0452\n",
            "Epoch 81/100, Train Loss: 0.0385, Val Loss: 0.0452\n",
            "Epoch 82/100, Train Loss: 0.0374, Val Loss: 0.0452\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0378, Val Loss: 0.0452\n",
            "Epoch 84/100, Train Loss: 0.0378, Val Loss: 0.0452\n",
            "Epoch 85/100, Train Loss: 0.0377, Val Loss: 0.0452\n",
            "Epoch 86/100, Train Loss: 0.0386, Val Loss: 0.0452\n",
            "Epoch 87/100, Train Loss: 0.0428, Val Loss: 0.0452\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0369, Val Loss: 0.0452\n",
            "Epoch 89/100, Train Loss: 0.0372, Val Loss: 0.0452\n",
            "Epoch 90/100, Train Loss: 0.0360, Val Loss: 0.0452\n",
            "Epoch 91/100, Train Loss: 0.0374, Val Loss: 0.0452\n",
            "Epoch 92/100, Train Loss: 0.0383, Val Loss: 0.0452\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0387, Val Loss: 0.0452\n",
            "Epoch 94/100, Train Loss: 0.0383, Val Loss: 0.0452\n",
            "Epoch 95/100, Train Loss: 0.0378, Val Loss: 0.0452\n",
            "Epoch 96/100, Train Loss: 0.0376, Val Loss: 0.0452\n",
            "Epoch 97/100, Train Loss: 0.0373, Val Loss: 0.0452\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0379, Val Loss: 0.0452\n",
            "Epoch 99/100, Train Loss: 0.0369, Val Loss: 0.0452\n",
            "Epoch 100/100, Train Loss: 0.0392, Val Loss: 0.0452\n",
            "\n",
            "Test iteration 27/28\n",
            "Current training set size: 123 samples\n",
            "Epoch 1/100, Train Loss: 0.1471, Val Loss: 0.1834\n",
            "Epoch 2/100, Train Loss: 0.1483, Val Loss: 0.1922\n",
            "Epoch 3/100, Train Loss: 0.1374, Val Loss: 0.1218\n",
            "Epoch 4/100, Train Loss: 0.1392, Val Loss: 0.1607\n",
            "Epoch 5/100, Train Loss: 0.0994, Val Loss: 0.0381\n",
            "Epoch 6/100, Train Loss: 0.0845, Val Loss: 0.0583\n",
            "Epoch 7/100, Train Loss: 0.0748, Val Loss: 0.0533\n",
            "Epoch 8/100, Train Loss: 0.0751, Val Loss: 0.0557\n",
            "Epoch 9/100, Train Loss: 0.0725, Val Loss: 0.0502\n",
            "Epoch 10/100, Train Loss: 0.0646, Val Loss: 0.0639\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 11/100, Train Loss: 0.0620, Val Loss: 0.0610\n",
            "Epoch 12/100, Train Loss: 0.0600, Val Loss: 0.0576\n",
            "Epoch 13/100, Train Loss: 0.0565, Val Loss: 0.0552\n",
            "Epoch 14/100, Train Loss: 0.0562, Val Loss: 0.0532\n",
            "Epoch 15/100, Train Loss: 0.0559, Val Loss: 0.0513\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0544, Val Loss: 0.0513\n",
            "Epoch 17/100, Train Loss: 0.0531, Val Loss: 0.0513\n",
            "Epoch 18/100, Train Loss: 0.0537, Val Loss: 0.0513\n",
            "Epoch 19/100, Train Loss: 0.0538, Val Loss: 0.0513\n",
            "Epoch 20/100, Train Loss: 0.0546, Val Loss: 0.0513\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0538, Val Loss: 0.0513\n",
            "Epoch 22/100, Train Loss: 0.0540, Val Loss: 0.0513\n",
            "Epoch 23/100, Train Loss: 0.0530, Val Loss: 0.0513\n",
            "Epoch 24/100, Train Loss: 0.0542, Val Loss: 0.0513\n",
            "Epoch 25/100, Train Loss: 0.0541, Val Loss: 0.0513\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0540, Val Loss: 0.0513\n",
            "Epoch 27/100, Train Loss: 0.0531, Val Loss: 0.0513\n",
            "Epoch 28/100, Train Loss: 0.0542, Val Loss: 0.0513\n",
            "Epoch 29/100, Train Loss: 0.0540, Val Loss: 0.0513\n",
            "Epoch 30/100, Train Loss: 0.0539, Val Loss: 0.0513\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0545, Val Loss: 0.0513\n",
            "Epoch 32/100, Train Loss: 0.0549, Val Loss: 0.0513\n",
            "Epoch 33/100, Train Loss: 0.0534, Val Loss: 0.0513\n",
            "Epoch 34/100, Train Loss: 0.0534, Val Loss: 0.0513\n",
            "Epoch 35/100, Train Loss: 0.0529, Val Loss: 0.0513\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0555, Val Loss: 0.0513\n",
            "Epoch 37/100, Train Loss: 0.0538, Val Loss: 0.0513\n",
            "Epoch 38/100, Train Loss: 0.0542, Val Loss: 0.0513\n",
            "Epoch 39/100, Train Loss: 0.0536, Val Loss: 0.0513\n",
            "Epoch 40/100, Train Loss: 0.0534, Val Loss: 0.0513\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0556, Val Loss: 0.0513\n",
            "Epoch 42/100, Train Loss: 0.0530, Val Loss: 0.0513\n",
            "Epoch 43/100, Train Loss: 0.0531, Val Loss: 0.0513\n",
            "Epoch 44/100, Train Loss: 0.0536, Val Loss: 0.0513\n",
            "Epoch 45/100, Train Loss: 0.0561, Val Loss: 0.0513\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0537, Val Loss: 0.0513\n",
            "Epoch 47/100, Train Loss: 0.0529, Val Loss: 0.0513\n",
            "Epoch 48/100, Train Loss: 0.0541, Val Loss: 0.0513\n",
            "Epoch 49/100, Train Loss: 0.0528, Val Loss: 0.0513\n",
            "Epoch 50/100, Train Loss: 0.0543, Val Loss: 0.0513\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0547, Val Loss: 0.0513\n",
            "Epoch 52/100, Train Loss: 0.0531, Val Loss: 0.0513\n",
            "Epoch 53/100, Train Loss: 0.0536, Val Loss: 0.0513\n",
            "Epoch 54/100, Train Loss: 0.0543, Val Loss: 0.0513\n",
            "Epoch 55/100, Train Loss: 0.0539, Val Loss: 0.0513\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0537, Val Loss: 0.0513\n",
            "Epoch 57/100, Train Loss: 0.0545, Val Loss: 0.0513\n",
            "Epoch 58/100, Train Loss: 0.0542, Val Loss: 0.0513\n",
            "Epoch 59/100, Train Loss: 0.0540, Val Loss: 0.0513\n",
            "Epoch 60/100, Train Loss: 0.0536, Val Loss: 0.0513\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0550, Val Loss: 0.0513\n",
            "Epoch 62/100, Train Loss: 0.0554, Val Loss: 0.0513\n",
            "Epoch 63/100, Train Loss: 0.0535, Val Loss: 0.0513\n",
            "Epoch 64/100, Train Loss: 0.0541, Val Loss: 0.0513\n",
            "Epoch 65/100, Train Loss: 0.0528, Val Loss: 0.0513\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0535, Val Loss: 0.0513\n",
            "Epoch 67/100, Train Loss: 0.0543, Val Loss: 0.0513\n",
            "Epoch 68/100, Train Loss: 0.0539, Val Loss: 0.0513\n",
            "Epoch 69/100, Train Loss: 0.0543, Val Loss: 0.0513\n",
            "Epoch 70/100, Train Loss: 0.0537, Val Loss: 0.0513\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0531, Val Loss: 0.0513\n",
            "Epoch 72/100, Train Loss: 0.0541, Val Loss: 0.0513\n",
            "Epoch 73/100, Train Loss: 0.0548, Val Loss: 0.0513\n",
            "Epoch 74/100, Train Loss: 0.0533, Val Loss: 0.0513\n",
            "Epoch 75/100, Train Loss: 0.0532, Val Loss: 0.0513\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0538, Val Loss: 0.0513\n",
            "Epoch 77/100, Train Loss: 0.0538, Val Loss: 0.0513\n",
            "Epoch 78/100, Train Loss: 0.0545, Val Loss: 0.0513\n",
            "Epoch 79/100, Train Loss: 0.0544, Val Loss: 0.0513\n",
            "Epoch 80/100, Train Loss: 0.0540, Val Loss: 0.0513\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0540, Val Loss: 0.0513\n",
            "Epoch 82/100, Train Loss: 0.0532, Val Loss: 0.0513\n",
            "Epoch 83/100, Train Loss: 0.0537, Val Loss: 0.0513\n",
            "Epoch 84/100, Train Loss: 0.0530, Val Loss: 0.0513\n",
            "Epoch 85/100, Train Loss: 0.0536, Val Loss: 0.0513\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0532, Val Loss: 0.0513\n",
            "Epoch 87/100, Train Loss: 0.0552, Val Loss: 0.0513\n",
            "Epoch 88/100, Train Loss: 0.0550, Val Loss: 0.0513\n",
            "Epoch 89/100, Train Loss: 0.0539, Val Loss: 0.0513\n",
            "Epoch 90/100, Train Loss: 0.0530, Val Loss: 0.0513\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0542, Val Loss: 0.0513\n",
            "Epoch 92/100, Train Loss: 0.0546, Val Loss: 0.0513\n",
            "Epoch 93/100, Train Loss: 0.0540, Val Loss: 0.0513\n",
            "Epoch 94/100, Train Loss: 0.0533, Val Loss: 0.0513\n",
            "Epoch 95/100, Train Loss: 0.0556, Val Loss: 0.0513\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0530, Val Loss: 0.0513\n",
            "Epoch 97/100, Train Loss: 0.0537, Val Loss: 0.0513\n",
            "Epoch 98/100, Train Loss: 0.0533, Val Loss: 0.0513\n",
            "Epoch 99/100, Train Loss: 0.0545, Val Loss: 0.0513\n",
            "Epoch 100/100, Train Loss: 0.0537, Val Loss: 0.0513\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 28/28\n",
            "Current training set size: 124 samples\n",
            "Epoch 1/100, Train Loss: 0.1755, Val Loss: 0.1967\n",
            "Epoch 2/100, Train Loss: 0.1484, Val Loss: 0.2032\n",
            "Epoch 3/100, Train Loss: 0.1306, Val Loss: 0.0987\n",
            "Epoch 4/100, Train Loss: 0.0976, Val Loss: 0.1160\n",
            "Epoch 5/100, Train Loss: 0.0979, Val Loss: 0.0495\n",
            "Epoch 6/100, Train Loss: 0.0895, Val Loss: 0.0383\n",
            "Epoch 7/100, Train Loss: 0.0792, Val Loss: 0.0790\n",
            "Epoch 8/100, Train Loss: 0.0724, Val Loss: 0.0659\n",
            "Epoch 9/100, Train Loss: 0.0708, Val Loss: 0.0434\n",
            "Epoch 10/100, Train Loss: 0.0658, Val Loss: 0.0334\n",
            "Epoch 11/100, Train Loss: 0.0531, Val Loss: 0.0382\n",
            "Epoch 12/100, Train Loss: 0.0512, Val Loss: 0.0435\n",
            "Epoch 13/100, Train Loss: 0.0610, Val Loss: 0.0802\n",
            "Epoch 14/100, Train Loss: 0.0571, Val Loss: 0.0336\n",
            "Epoch 15/100, Train Loss: 0.0511, Val Loss: 0.0361\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0483, Val Loss: 0.0351\n",
            "Epoch 17/100, Train Loss: 0.0438, Val Loss: 0.0344\n",
            "Epoch 18/100, Train Loss: 0.0439, Val Loss: 0.0342\n",
            "Epoch 19/100, Train Loss: 0.0420, Val Loss: 0.0344\n",
            "Epoch 20/100, Train Loss: 0.0418, Val Loss: 0.0346\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0422, Val Loss: 0.0346\n",
            "Epoch 22/100, Train Loss: 0.0431, Val Loss: 0.0346\n",
            "Epoch 23/100, Train Loss: 0.0429, Val Loss: 0.0346\n",
            "Epoch 24/100, Train Loss: 0.0421, Val Loss: 0.0346\n",
            "Epoch 25/100, Train Loss: 0.0422, Val Loss: 0.0346\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0426, Val Loss: 0.0346\n",
            "Epoch 27/100, Train Loss: 0.0423, Val Loss: 0.0346\n",
            "Epoch 28/100, Train Loss: 0.0439, Val Loss: 0.0346\n",
            "Epoch 29/100, Train Loss: 0.0418, Val Loss: 0.0346\n",
            "Epoch 30/100, Train Loss: 0.0426, Val Loss: 0.0346\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0422, Val Loss: 0.0346\n",
            "Epoch 32/100, Train Loss: 0.0438, Val Loss: 0.0346\n",
            "Epoch 33/100, Train Loss: 0.0426, Val Loss: 0.0346\n",
            "Epoch 34/100, Train Loss: 0.0435, Val Loss: 0.0346\n",
            "Epoch 35/100, Train Loss: 0.0423, Val Loss: 0.0346\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0431, Val Loss: 0.0346\n",
            "Epoch 37/100, Train Loss: 0.0435, Val Loss: 0.0346\n",
            "Epoch 38/100, Train Loss: 0.0422, Val Loss: 0.0346\n",
            "Epoch 39/100, Train Loss: 0.0425, Val Loss: 0.0346\n",
            "Epoch 40/100, Train Loss: 0.0423, Val Loss: 0.0346\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0435, Val Loss: 0.0346\n",
            "Epoch 42/100, Train Loss: 0.0415, Val Loss: 0.0346\n",
            "Epoch 43/100, Train Loss: 0.0419, Val Loss: 0.0346\n",
            "Epoch 44/100, Train Loss: 0.0438, Val Loss: 0.0346\n",
            "Epoch 45/100, Train Loss: 0.0426, Val Loss: 0.0346\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0428, Val Loss: 0.0346\n",
            "Epoch 47/100, Train Loss: 0.0422, Val Loss: 0.0346\n",
            "Epoch 48/100, Train Loss: 0.0426, Val Loss: 0.0346\n",
            "Epoch 49/100, Train Loss: 0.0426, Val Loss: 0.0346\n",
            "Epoch 50/100, Train Loss: 0.0430, Val Loss: 0.0346\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0430, Val Loss: 0.0346\n",
            "Epoch 52/100, Train Loss: 0.0437, Val Loss: 0.0346\n",
            "Epoch 53/100, Train Loss: 0.0424, Val Loss: 0.0346\n",
            "Epoch 54/100, Train Loss: 0.0428, Val Loss: 0.0346\n",
            "Epoch 55/100, Train Loss: 0.0441, Val Loss: 0.0346\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0430, Val Loss: 0.0346\n",
            "Epoch 57/100, Train Loss: 0.0430, Val Loss: 0.0346\n",
            "Epoch 58/100, Train Loss: 0.0433, Val Loss: 0.0346\n",
            "Epoch 59/100, Train Loss: 0.0428, Val Loss: 0.0346\n",
            "Epoch 60/100, Train Loss: 0.0440, Val Loss: 0.0346\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0430, Val Loss: 0.0346\n",
            "Epoch 62/100, Train Loss: 0.0422, Val Loss: 0.0346\n",
            "Epoch 63/100, Train Loss: 0.0423, Val Loss: 0.0346\n",
            "Epoch 64/100, Train Loss: 0.0414, Val Loss: 0.0346\n",
            "Epoch 65/100, Train Loss: 0.0434, Val Loss: 0.0346\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0438, Val Loss: 0.0346\n",
            "Epoch 67/100, Train Loss: 0.0422, Val Loss: 0.0346\n",
            "Epoch 68/100, Train Loss: 0.0437, Val Loss: 0.0346\n",
            "Epoch 69/100, Train Loss: 0.0415, Val Loss: 0.0346\n",
            "Epoch 70/100, Train Loss: 0.0433, Val Loss: 0.0346\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0430, Val Loss: 0.0346\n",
            "Epoch 72/100, Train Loss: 0.0432, Val Loss: 0.0346\n",
            "Epoch 73/100, Train Loss: 0.0423, Val Loss: 0.0346\n",
            "Epoch 74/100, Train Loss: 0.0420, Val Loss: 0.0346\n",
            "Epoch 75/100, Train Loss: 0.0422, Val Loss: 0.0346\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0431, Val Loss: 0.0346\n",
            "Epoch 77/100, Train Loss: 0.0431, Val Loss: 0.0346\n",
            "Epoch 78/100, Train Loss: 0.0432, Val Loss: 0.0346\n",
            "Epoch 79/100, Train Loss: 0.0436, Val Loss: 0.0346\n",
            "Epoch 80/100, Train Loss: 0.0429, Val Loss: 0.0346\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0430, Val Loss: 0.0346\n",
            "Epoch 82/100, Train Loss: 0.0437, Val Loss: 0.0346\n",
            "Epoch 83/100, Train Loss: 0.0427, Val Loss: 0.0346\n",
            "Epoch 84/100, Train Loss: 0.0412, Val Loss: 0.0346\n",
            "Epoch 85/100, Train Loss: 0.0435, Val Loss: 0.0346\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0429, Val Loss: 0.0346\n",
            "Epoch 87/100, Train Loss: 0.0432, Val Loss: 0.0346\n",
            "Epoch 88/100, Train Loss: 0.0424, Val Loss: 0.0346\n",
            "Epoch 89/100, Train Loss: 0.0417, Val Loss: 0.0346\n",
            "Epoch 90/100, Train Loss: 0.0435, Val Loss: 0.0346\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0427, Val Loss: 0.0346\n",
            "Epoch 92/100, Train Loss: 0.0428, Val Loss: 0.0346\n",
            "Epoch 93/100, Train Loss: 0.0421, Val Loss: 0.0346\n",
            "Epoch 94/100, Train Loss: 0.0415, Val Loss: 0.0346\n",
            "Epoch 95/100, Train Loss: 0.0427, Val Loss: 0.0346\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0431, Val Loss: 0.0346\n",
            "Epoch 97/100, Train Loss: 0.0431, Val Loss: 0.0346\n",
            "Epoch 98/100, Train Loss: 0.0413, Val Loss: 0.0346\n",
            "Epoch 99/100, Train Loss: 0.0429, Val Loss: 0.0346\n",
            "Epoch 100/100, Train Loss: 0.0423, Val Loss: 0.0346\n",
            "Reducing learning rate by factor of 0.01\n",
            "Results:\n",
            "             RMSE            MAE       MAPE model_type     units  drop_rate  \\\n",
            "0   249784.188102  118509.607143  17.488050     hybrid   L256_G8        0.1   \n",
            "1   234333.508475  104441.356027   5.870110     hybrid   L256_G8        0.1   \n",
            "2   235183.289575  115937.969866   5.857494     hybrid   L256_G8        0.2   \n",
            "3   245912.508899  103145.171875   6.485597     hybrid   L256_G8        0.2   \n",
            "4   247919.828892  108043.342634  12.699153     hybrid  L256_G16        0.1   \n",
            "5   238335.144596  107060.200893  24.382057     hybrid  L256_G16        0.1   \n",
            "6   238051.025397  112082.593750   8.406826     hybrid  L256_G16        0.2   \n",
            "7   237953.615242  108688.574777  11.245000     hybrid  L256_G16        0.2   \n",
            "8   236853.597343  111330.300223   8.276025     hybrid   L512_G8        0.1   \n",
            "9   232910.681323  102174.939732   8.317327     hybrid   L512_G8        0.1   \n",
            "10  241585.605414  105644.007812   9.601500     hybrid   L512_G8        0.2   \n",
            "11  241800.961336  109211.174107   4.517188     hybrid   L512_G8        0.2   \n",
            "12  234155.516224  111280.965402  23.784697     hybrid  L512_G16        0.1   \n",
            "13  221942.417351  101441.725446  11.462436     hybrid  L512_G16        0.1   \n",
            "14  231306.184846  111478.608259  14.620383     hybrid  L512_G16        0.2   \n",
            "15  252241.391991  112930.821429  12.135194     hybrid  L512_G16        0.2   \n",
            "\n",
            "    dense_unit  batch_size  epochs  \n",
            "0           32           4     100  \n",
            "1           64           4     100  \n",
            "2           32           4     100  \n",
            "3           64           4     100  \n",
            "4           32           4     100  \n",
            "5           64           4     100  \n",
            "6           32           4     100  \n",
            "7           64           4     100  \n",
            "8           32           4     100  \n",
            "9           64           4     100  \n",
            "10          32           4     100  \n",
            "11          64           4     100  \n",
            "12          32           4     100  \n",
            "13          64           4     100  \n",
            "14          32           4     100  \n",
            "15          64           4     100  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adjusted_valuelist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "oXmddMCgx26U",
        "outputId": "35dd6f33-a22b-41b2-b72d-bb58f38cf225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             RMSE            MAE       MAPE model_type     units  drop_rate  \\\n",
              "0   249784.188102  118509.607143  17.488050     hybrid   L256_G8        0.1   \n",
              "1   234333.508475  104441.356027   5.870110     hybrid   L256_G8        0.1   \n",
              "2   235183.289575  115937.969866   5.857494     hybrid   L256_G8        0.2   \n",
              "3   245912.508899  103145.171875   6.485597     hybrid   L256_G8        0.2   \n",
              "4   247919.828892  108043.342634  12.699153     hybrid  L256_G16        0.1   \n",
              "5   238335.144596  107060.200893  24.382057     hybrid  L256_G16        0.1   \n",
              "6   238051.025397  112082.593750   8.406826     hybrid  L256_G16        0.2   \n",
              "7   237953.615242  108688.574777  11.245000     hybrid  L256_G16        0.2   \n",
              "8   236853.597343  111330.300223   8.276025     hybrid   L512_G8        0.1   \n",
              "9   232910.681323  102174.939732   8.317327     hybrid   L512_G8        0.1   \n",
              "10  241585.605414  105644.007812   9.601500     hybrid   L512_G8        0.2   \n",
              "11  241800.961336  109211.174107   4.517188     hybrid   L512_G8        0.2   \n",
              "12  234155.516224  111280.965402  23.784697     hybrid  L512_G16        0.1   \n",
              "13  221942.417351  101441.725446  11.462436     hybrid  L512_G16        0.1   \n",
              "14  231306.184846  111478.608259  14.620383     hybrid  L512_G16        0.2   \n",
              "15  252241.391991  112930.821429  12.135194     hybrid  L512_G16        0.2   \n",
              "\n",
              "    dense_unit  batch_size  epochs  \n",
              "0           32           4     100  \n",
              "1           64           4     100  \n",
              "2           32           4     100  \n",
              "3           64           4     100  \n",
              "4           32           4     100  \n",
              "5           64           4     100  \n",
              "6           32           4     100  \n",
              "7           64           4     100  \n",
              "8           32           4     100  \n",
              "9           64           4     100  \n",
              "10          32           4     100  \n",
              "11          64           4     100  \n",
              "12          32           4     100  \n",
              "13          64           4     100  \n",
              "14          32           4     100  \n",
              "15          64           4     100  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0a9730d9-ae8a-41bf-b755-86dbbe55b436\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAE</th>\n",
              "      <th>MAPE</th>\n",
              "      <th>model_type</th>\n",
              "      <th>units</th>\n",
              "      <th>drop_rate</th>\n",
              "      <th>dense_unit</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>epochs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>249784.188102</td>\n",
              "      <td>118509.607143</td>\n",
              "      <td>17.488050</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L256_G8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>234333.508475</td>\n",
              "      <td>104441.356027</td>\n",
              "      <td>5.870110</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L256_G8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>235183.289575</td>\n",
              "      <td>115937.969866</td>\n",
              "      <td>5.857494</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L256_G8</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>245912.508899</td>\n",
              "      <td>103145.171875</td>\n",
              "      <td>6.485597</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L256_G8</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>247919.828892</td>\n",
              "      <td>108043.342634</td>\n",
              "      <td>12.699153</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L256_G16</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>238335.144596</td>\n",
              "      <td>107060.200893</td>\n",
              "      <td>24.382057</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L256_G16</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>238051.025397</td>\n",
              "      <td>112082.593750</td>\n",
              "      <td>8.406826</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L256_G16</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>237953.615242</td>\n",
              "      <td>108688.574777</td>\n",
              "      <td>11.245000</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L256_G16</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>236853.597343</td>\n",
              "      <td>111330.300223</td>\n",
              "      <td>8.276025</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L512_G8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>232910.681323</td>\n",
              "      <td>102174.939732</td>\n",
              "      <td>8.317327</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L512_G8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>241585.605414</td>\n",
              "      <td>105644.007812</td>\n",
              "      <td>9.601500</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L512_G8</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>241800.961336</td>\n",
              "      <td>109211.174107</td>\n",
              "      <td>4.517188</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L512_G8</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>234155.516224</td>\n",
              "      <td>111280.965402</td>\n",
              "      <td>23.784697</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L512_G16</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>221942.417351</td>\n",
              "      <td>101441.725446</td>\n",
              "      <td>11.462436</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L512_G16</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>231306.184846</td>\n",
              "      <td>111478.608259</td>\n",
              "      <td>14.620383</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L512_G16</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>252241.391991</td>\n",
              "      <td>112930.821429</td>\n",
              "      <td>12.135194</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L512_G16</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0a9730d9-ae8a-41bf-b755-86dbbe55b436')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0a9730d9-ae8a-41bf-b755-86dbbe55b436 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0a9730d9-ae8a-41bf-b755-86dbbe55b436');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ace94ecd-580b-400d-83b3-ecd930a191e9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ace94ecd-580b-400d-83b3-ecd930a191e9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ace94ecd-580b-400d-83b3-ecd930a191e9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_36c22e2c-e530-47b2-8e48-ab3d0ea84952\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('adjusted_valuelist')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_36c22e2c-e530-47b2-8e48-ab3d0ea84952 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('adjusted_valuelist');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "adjusted_valuelist",
              "summary": "{\n  \"name\": \"adjusted_valuelist\",\n  \"rows\": 16,\n  \"fields\": [\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7688.662435546456,\n        \"min\": 221942.4173507385,\n        \"max\": 252241.39199099963,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          249784.18810237854,\n          234333.50847508435,\n          238335.14459627212\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4865.713274200072,\n        \"min\": 101441.72544642857,\n        \"max\": 118509.60714285714,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          118509.60714285714,\n          104441.35602678571,\n          107060.20089285714\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          17.48805046081543,\n          5.870110034942627,\n          24.382057189941406\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"hybrid\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"units\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"L256_G16\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"drop_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.051639777949432225,\n        \"min\": 0.1,\n        \"max\": 0.2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dense_unit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16,\n        \"min\": 32,\n        \"max\": 64,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 4,\n        \"max\": 4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"epochs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 100,\n        \"max\": 100,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          100\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_adjusted_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUqrwEStx3bD",
        "outputId": "4e80642b-a44b-4132-b525-66b9588d5025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'hybrid_lstmUnit256_gruUnit8_drop0.1_dense32_batch4_epochs100': array([      0.  ,  663497.3 ,  467259.1 , 1330277.  ,  591774.7 ,\n",
              "         416874.5 ,  588709.8 ,  699454.8 , 1355296.8 ,  619002.7 ,\n",
              "        1245848.2 , 1438250.2 ,  619844.94,  416420.2 ,  796438.75,\n",
              "         613419.4 ,  400426.78,  451312.84, 1602284.1 , 1172899.6 ,\n",
              "         553084.25, 1312636.1 ,  603316.5 ,  414175.16,  677476.3 ,\n",
              "         473814.56, 1658738.  ,  765384.94], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit8_drop0.1_dense64_batch4_epochs100': array([      0.  ,  597886.6 ,  486088.3 , 1410906.  ,  532848.1 ,\n",
              "         406753.1 ,  593534.94,  559717.8 , 1362451.6 ,  594334.4 ,\n",
              "        1217381.8 , 1494933.5 ,  635722.4 ,  430412.12,  738596.75,\n",
              "         641263.25,  383769.97,  452027.6 , 1579113.6 , 1354288.2 ,\n",
              "         577189.8 , 1322430.1 ,  606942.4 ,  374071.88,  646270.94,\n",
              "         461265.  , 1728911.2 ,  750296.1 ], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit8_drop0.2_dense32_batch4_epochs100': array([      0.  ,  597815.4 ,  495620.62, 1297223.8 ,  540607.9 ,\n",
              "         390867.78,  584452.25,  680140.  , 1424594.8 ,  616422.4 ,\n",
              "        1156949.  , 1638438.5 ,  687892.7 ,  446411.6 ,  811747.56,\n",
              "         639317.56,  398205.7 ,  480689.  , 1535408.6 , 1146850.4 ,\n",
              "         537048.8 , 1351056.9 ,  676674.4 ,  384307.56,  625274.56,\n",
              "         439640.34, 1608525.4 ,  745447.4 ], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit8_drop0.2_dense64_batch4_epochs100': array([      0.  ,  601362.5 ,  508837.5 , 1300140.9 ,  536615.1 ,\n",
              "         414781.75,  592513.3 ,  556209.  , 1344223.1 ,  622543.75,\n",
              "        1225416.6 , 1650676.4 ,  680245.56,  415348.1 ,  780855.75,\n",
              "         621976.  ,  373067.56,  453084.06, 1639065.1 , 1191799.8 ,\n",
              "         555610.4 , 1279050.2 ,  571720.75,  355788.2 ,  657645.7 ,\n",
              "         443521.72, 1616372.8 ,  763934.5 ], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit16_drop0.1_dense32_batch4_epochs100': array([      0.  ,  636452.7 ,  462352.06, 1312462.6 ,  525421.94,\n",
              "         378470.  ,  580433.2 ,  684090.2 , 1317455.9 ,  612539.75,\n",
              "        1164753.4 , 1430775.8 ,  645421.94,  453923.8 ,  803119.3 ,\n",
              "         597681.6 ,  385385.1 ,  451406.94, 1605211.2 , 1331727.6 ,\n",
              "         528537.25, 1298346.9 ,  613740.8 ,  393714.66,  703221.44,\n",
              "         471971.47, 1588675.4 ,  795125.1 ], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit16_drop0.1_dense64_batch4_epochs100': array([      0.  ,  702430.25,  460654.22, 1371148.1 ,  520782.56,\n",
              "         423719.4 ,  587833.44,  561585.2 , 1341694.6 ,  613219.06,\n",
              "        1193211.2 , 1571971.1 ,  694785.56,  407541.3 ,  812218.5 ,\n",
              "         626468.9 ,  406433.25,  450951.94, 1598634.8 , 1325059.4 ,\n",
              "         537803.3 , 1341370.  ,  604950.4 ,  371219.75,  704258.1 ,\n",
              "         476742.7 , 1660528.  ,  727554.1 ], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit16_drop0.2_dense32_batch4_epochs100': array([      0.  ,  612212.4 ,  478177.66, 1386117.9 ,  562375.9 ,\n",
              "         406226.53,  549346.1 ,  599902.2 , 1334888.4 ,  627738.  ,\n",
              "        1177696.9 , 1375252.  ,  611355.56,  430598.16,  757813.75,\n",
              "         545704.4 ,  419124.22,  453236.62, 1529790.1 , 1279525.9 ,\n",
              "         526047.5 , 1342872.1 ,  616090.1 ,  372063.1 ,  654752.1 ,\n",
              "         419375.66, 1648166.  ,  777391.1 ], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit16_drop0.2_dense64_batch4_epochs100': array([      0.  ,  628240.56,  481515.7 , 1314334.2 ,  516691.25,\n",
              "         394951.25,  566626.7 ,  583109.75, 1350315.5 ,  614559.06,\n",
              "        1200656.  , 1501159.8 ,  704917.6 ,  402611.84,  737435.06,\n",
              "         633944.06,  396432.1 ,  429332.44, 1590955.5 , 1332516.1 ,\n",
              "         524208.84, 1308556.1 ,  600478.56,  374738.6 ,  677218.94,\n",
              "         469615.9 , 1685902.9 ,  705451.7 ], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit8_drop0.1_dense32_batch4_epochs100': array([      0.  ,  611473.7 ,  454768.53, 1365855.4 ,  550230.3 ,\n",
              "         406201.72,  599253.06,  620316.3 , 1422671.8 ,  622096.3 ,\n",
              "        1102056.  , 1636824.9 ,  643692.94,  451776.22,  772956.1 ,\n",
              "         644677.44,  424962.1 ,  446771.12, 1602125.6 , 1280734.8 ,\n",
              "         530595.9 , 1381318.1 ,  615781.9 ,  389237.9 ,  657506.56,\n",
              "         455144.88, 1726766.6 ,  729026.7 ], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit8_drop0.1_dense64_batch4_epochs100': array([      0.  ,  611706.94,  481826.47, 1373349.1 ,  567469.75,\n",
              "         430428.5 ,  593961.75,  646047.44, 1391327.1 ,  622256.25,\n",
              "        1187565.2 , 1662244.6 ,  649414.56,  457362.1 ,  790607.2 ,\n",
              "         649299.25,  395870.6 ,  450439.2 , 1597829.  , 1329094.6 ,\n",
              "         543930.25, 1294226.1 ,  553788.1 ,  374347.6 ,  667988.2 ,\n",
              "         466616.62, 1694276.  ,  754553.6 ], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit8_drop0.2_dense32_batch4_epochs100': array([      0.  ,  618959.1 ,  463272.1 , 1378962.4 ,  546617.75,\n",
              "         433512.12,  603973.7 ,  596238.5 , 1259523.9 ,  619350.94,\n",
              "        1209276.8 , 1547537.8 ,  653978.44,  431119.12,  807683.2 ,\n",
              "         651867.  ,  445473.72,  440215.1 , 1640826.9 , 1334547.4 ,\n",
              "         531231.1 , 1283231.4 ,  570167.7 ,  380844.78,  683657.  ,\n",
              "         461306.28, 1756462.8 ,  711768.75], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit8_drop0.2_dense64_batch4_epochs100': array([      0.  ,  590246.2 ,  482172.47, 1387252.2 ,  517431.72,\n",
              "         437327.75,  585872.5 ,  594655.75, 1386289.4 ,  620675.06,\n",
              "        1215725.1 , 1462718.  ,  692485.9 ,  427824.38,  829035.6 ,\n",
              "         617184.94,  418663.66,  439758.94, 1588393.1 , 1252120.6 ,\n",
              "         535444.94, 1249805.  ,  592594.4 ,  373906.22,  679151.06,\n",
              "         457360.94, 1645526.8 ,  703306.3 ], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit16_drop0.1_dense32_batch4_epochs100': array([      0.  ,  699056.75,  473082.2 , 1341093.5 ,  521016.66,\n",
              "         437743.8 ,  575668.7 ,  608225.6 , 1333220.5 ,  622264.2 ,\n",
              "        1191807.4 , 1439087.2 ,  627385.2 ,  443160.5 ,  837079.  ,\n",
              "         646376.75,  394427.  ,  443622.12, 1535201.2 , 1188082.6 ,\n",
              "         551080.7 , 1362734.1 ,  580527.2 ,  389870.62,  665516.4 ,\n",
              "         446122.75, 1743905.9 ,  708593.56], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit16_drop0.1_dense64_batch4_epochs100': array([      0.  ,  629468.5 ,  511637.72, 1324518.2 ,  549397.44,\n",
              "         422983.84,  572303.4 ,  627011.3 , 1411047.5 ,  613189.7 ,\n",
              "        1189188.5 , 1649025.1 ,  684660.4 ,  430939.5 ,  761608.6 ,\n",
              "         676729.56,  400156.  ,  463208.66, 1530082.6 , 1284364.4 ,\n",
              "         547694.44, 1315123.1 ,  577638.25,  378381.34,  659944.75,\n",
              "         457203.62, 1718531.  ,  786722.9 ], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit16_drop0.2_dense32_batch4_epochs100': array([      0.  ,  647302.56,  467683.16, 1390110.2 ,  545132.06,\n",
              "         402686.1 ,  592743.3 ,  662672.75, 1393177.4 ,  608150.8 ,\n",
              "        1193446.1 , 1470502.6 ,  632422.75,  463199.8 ,  793529.7 ,\n",
              "         583918.44,  426823.6 ,  451211.66, 1532911.  , 1225842.4 ,\n",
              "         547992.  , 1311452.6 ,  573090.75,  400491.6 ,  676083.3 ,\n",
              "         455563.25, 1712395.  ,  762921.75], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit16_drop0.2_dense64_batch4_epochs100': array([      0.  ,  633267.8 ,  452770.22, 1345595.1 ,  537253.5 ,\n",
              "         427136.8 ,  587997.  ,  596348.1 , 1355342.9 ,  617536.6 ,\n",
              "        1187246.2 , 1627740.1 ,  687301.8 ,  456278.25,  823499.  ,\n",
              "         622477.94,  388213.  ,  478613.28, 1629211.1 , 1260411.4 ,\n",
              "         511042.88, 1337936.6 ,  571130.8 ,  369514.44,  667078.7 ,\n",
              "         452939.94, 1500745.8 ,  743019.44], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_ground_truths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lft0mtzyx-iW",
        "outputId": "6deebc0d-0e67-4ce2-b47e-26ceedcd807f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'hybrid_lstmUnit256_gruUnit8_drop0.1_dense32_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit8_drop0.1_dense64_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit8_drop0.2_dense32_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit8_drop0.2_dense64_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit16_drop0.1_dense32_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit16_drop0.1_dense64_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit16_drop0.2_dense32_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit16_drop0.2_dense64_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit8_drop0.1_dense32_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit8_drop0.1_dense64_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit8_drop0.2_dense32_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit8_drop0.2_dense64_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit16_drop0.1_dense32_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit16_drop0.1_dense64_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit16_drop0.2_dense32_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit16_drop0.2_dense64_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adjusted_valuelist.to_csv( '/content/drive/MyDrive/HybridLSTMGRU_random.csv', index=False)\n",
        "pd.DataFrame(all_adjusted_predictions).to_csv(\"/content/drive/MyDrive/all_adjusted_predictions_random.csv\", index=False)\n",
        "pd.DataFrame(all_ground_truths).to_csv(\"/content/drive/MyDrive/all_ground_truths_random.csv\", index=False)"
      ],
      "metadata": {
        "id": "NEiXJz7EyFIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_key = 'hybrid_lstmUnit512_gruUnit8_drop0.2_dense64_batch4_epochs100'\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(all_ground_truths[config_key], label='Ground Truth')\n",
        "plt.plot(all_adjusted_predictions[config_key], label='GRU Predictions')\n",
        "plt.title(f'GRU Performance: {config_key}')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "ts_Lj-NryLYz",
        "outputId": "74ca72ce-0f7a-4498-9288-8d38a1e0a769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GRU"
      ],
      "metadata": {
        "id": "WJkpp2jydsm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Giảm số lượng/GRU unit, dense unit, epochs và sử dụng batch size nhỏ hơn để huấn luyện nhanh hơn.\n",
        "# model_types = ['multi-scale']\n",
        "# lstm_unit = [128, 256, 512]\n",
        "# gru_unit = [8, 16, 32]\n",
        "# drop_rate = [0.1, 0.2]\n",
        "# dense_unit = [16, 32, 64]\n",
        "# batch_size_num = [2, 4]\n",
        "# epochs = [100]\n",
        "\n",
        "model_types = ['gru']\n",
        "lstm_unit = [256,512]\n",
        "gru_unit = [8,16]\n",
        "drop_rate = [0.1,0.2]\n",
        "dense_unit = [32,64]\n",
        "batch_size_num = [4]\n",
        "epochs = [100]\n",
        "\n",
        "# # Replace the current parameter definitions\n",
        "# model_types = ['hybrid', 'sequential', 'stacked', 'bidirectional', 'cnn-rnn', 'multi-scale', 'transformer-rnn', 'ensemble', 'lstm', 'gru']\n",
        "# lstm_unit = [128]\n",
        "# gru_unit = [8]\n",
        "# drop_rate = [0.1]\n",
        "# dense_unit = [64]\n",
        "# batch_size_num = [2]\n",
        "# epochs = [100]\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import concatenate\n",
        "import itertools\n",
        "from math import sqrt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "class AttentionGRU(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, gru_units, dropout_rate, dense_units):\n",
        "        super(AttentionGRU, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention1 = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # GRU layer\n",
        "        self.gru = nn.GRU(input_dim, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear(gru_units * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, time_steps, input_dim)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.attention1(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)  # (batch_size, time_steps, input_dim)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # GRU layer - output: (batch, seq, hidden_size)\n",
        "        gru_out, _ = self.gru(attention_mul)\n",
        "\n",
        "        # Flatten and Dense layers\n",
        "        flattened = self.flatten(gru_out)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class AttentionLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, dropout_rate, dense_units):\n",
        "        super(AttentionLSTM, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention1 = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear(lstm_units * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, time_steps, input_dim)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.attention1(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)  # (batch_size, time_steps, input_dim)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # LSTM layer - output: (batch, seq, hidden_size)\n",
        "        lstm_out, _ = self.lstm(attention_mul)\n",
        "\n",
        "        # Flatten and Dense layers\n",
        "        flattened = self.flatten(lstm_out)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class HybridLSTM_GRU(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(HybridLSTM_GRU, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention1 = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # LSTM and GRU layers\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "        self.gru = nn.GRU(input_dim, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        # Combined size from both LSTM and GRU\n",
        "        self.dense1 = nn.Linear((lstm_units + gru_units) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, time_steps, input_dim)\n",
        "\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.attention1(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)  # (batch_size, time_steps, input_dim)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # LSTM and GRU layers\n",
        "        lstm_out, _ = self.lstm(attention_mul)\n",
        "        gru_out, _ = self.gru(attention_mul)\n",
        "\n",
        "        # Concatenate LSTM and GRU outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Flatten and Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class SequentialHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(SequentialHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # LSTM followed by GRU\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "        self.gru = nn.GRU(lstm_units, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear(gru_units * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)\n",
        "        a = self.attention(a)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # Sequential processing: LSTM then GRU\n",
        "        lstm_out, _ = self.lstm(attention_mul)\n",
        "        gru_out, _ = self.gru(lstm_out)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(gru_out)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class StackedHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(StackedHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention\n",
        "        self.attention = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # Two stacked LSTM layers\n",
        "        self.lstm1 = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(lstm_units, lstm_units//2, batch_first=True)\n",
        "\n",
        "        # Two stacked GRU layers\n",
        "        self.gru1 = nn.GRU(input_dim, gru_units, batch_first=True)\n",
        "        self.gru2 = nn.GRU(gru_units, gru_units//2, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units//2 + gru_units//2) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)\n",
        "        a = self.attention(a)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # Stacked LSTM path\n",
        "        lstm_out1, _ = self.lstm1(attention_mul)\n",
        "        lstm_out2, _ = self.lstm2(lstm_out1)\n",
        "\n",
        "        # Stacked GRU path\n",
        "        gru_out1, _ = self.gru1(attention_mul)\n",
        "        gru_out2, _ = self.gru2(gru_out1)\n",
        "\n",
        "        # Concatenate final outputs\n",
        "        combined = torch.cat((lstm_out2, gru_out2), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class BidirectionalHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(BidirectionalHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # Bidirectional LSTM and GRU\n",
        "        self.bilstm = nn.LSTM(input_dim, lstm_units, batch_first=True, bidirectional=True)\n",
        "        self.bigru = nn.GRU(input_dim, gru_units, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units*2 + gru_units*2) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)\n",
        "        a = self.attention(a)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # Bidirectional networks\n",
        "        lstm_out, _ = self.bilstm(attention_mul)\n",
        "        gru_out, _ = self.bigru(attention_mul)\n",
        "\n",
        "        # Concatenate outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class CNNRNNHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(CNNRNNHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # 1D CNN for feature extraction\n",
        "        self.conv1 = nn.Conv1d(input_dim, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # RNN layers\n",
        "        self.lstm = nn.LSTM(64, lstm_units, batch_first=True)\n",
        "        self.gru = nn.GRU(64, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units + gru_units) * (time_steps-1), dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # CNN feature extraction\n",
        "        x = x.permute(0, 2, 1)  # (batch, features, seq_len)\n",
        "        cnn_out = self.relu(self.conv1(x))\n",
        "        cnn_out = self.maxpool(cnn_out)\n",
        "        cnn_out = self.relu(self.conv2(cnn_out))\n",
        "        cnn_out = cnn_out.permute(0, 2, 1)  # (batch, seq_len, features)\n",
        "\n",
        "        # RNN processing\n",
        "        lstm_out, _ = self.lstm(cnn_out)\n",
        "        gru_out, _ = self.gru(cnn_out)\n",
        "\n",
        "        # Combine outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class MultiScaleHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(MultiScaleHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # LSTM for long-term dependencies\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "\n",
        "        # GRU for shorter-term dependencies (operating on windows)\n",
        "        self.gru = nn.GRU(input_dim, gru_units, batch_first=True)\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units + gru_units) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Full sequence for LSTM (long-term)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Attention mechanism for GRU input\n",
        "        a = x.permute(0, 2, 1)\n",
        "        a = self.attention(a)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)\n",
        "\n",
        "        # Apply attention weights for GRU (short-term focus)\n",
        "        gru_input = torch.mul(x, a)\n",
        "        gru_out, _ = self.gru(gru_input)\n",
        "\n",
        "        # Combine outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class TransformerRNNHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units, nhead=4):\n",
        "        super(TransformerRNNHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Input projection for transformer\n",
        "        self.input_proj = nn.Linear(input_dim, 64)\n",
        "\n",
        "        # Transformer encoder layer\n",
        "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=64, nhead=nhead, dropout=dropout_rate, batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layer, num_layers=2)\n",
        "\n",
        "        # RNN layers\n",
        "        self.lstm = nn.LSTM(64, lstm_units, batch_first=True)\n",
        "        self.gru = nn.GRU(64, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units + gru_units) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Project input to transformer dimension\n",
        "        x_proj = self.input_proj(x)\n",
        "\n",
        "        # Apply transformer encoder\n",
        "        transformer_out = self.transformer_encoder(x_proj)\n",
        "\n",
        "        # Process with RNNs\n",
        "        lstm_out, _ = self.lstm(transformer_out)\n",
        "        gru_out, _ = self.gru(transformer_out)\n",
        "\n",
        "        # Combine outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class EnsembleHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(EnsembleHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Individual models\n",
        "        self.lstm_model = AttentionLSTM(input_dim, time_steps, lstm_units, dropout_rate, dense_units)\n",
        "        self.gru_model = AttentionGRU(input_dim, time_steps, gru_units, dropout_rate, dense_units)\n",
        "\n",
        "        # Combination layer\n",
        "        self.combine = nn.Linear(2, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get predictions from each model\n",
        "        lstm_pred = self.lstm_model(x)\n",
        "        gru_pred = self.gru_model(x)\n",
        "\n",
        "        # Combine predictions (learnable weights)\n",
        "        combined = torch.cat((lstm_pred, gru_pred), dim=1)\n",
        "        output = self.final_activation(self.combine(combined))\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "def build_model(train_X, train_Y, val_X, val_Y, model_type='gru', lstm_units=128, gru_units=128, drop_rate=0.3, dense_unit=64, batch_size=32, epochs=100):\n",
        "    # Print training parameters\n",
        "    train_X_tensor = torch.FloatTensor(train_X)\n",
        "    train_Y_tensor = torch.FloatTensor(train_Y.reshape(-1, 1))\n",
        "    val_X_tensor = torch.FloatTensor(val_X)\n",
        "    val_Y_tensor = torch.FloatTensor(val_Y.reshape(-1, 1))\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
        "    val_dataset = TensorDataset(val_X_tensor, val_Y_tensor)\n",
        "\n",
        "    # Create reproducible DataLoaders with fixed seeds\n",
        "    train_generator = torch.Generator()\n",
        "    train_generator.manual_seed(SEED)\n",
        "    val_generator = torch.Generator()\n",
        "    val_generator.manual_seed(SEED)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=train_generator)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, generator=val_generator)\n",
        "\n",
        "    # Initialize model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    time_steps = train_X.shape[1]\n",
        "    input_dim = train_X.shape[2]\n",
        "\n",
        "    # Initialize model with fixed initial weights\n",
        "    torch.manual_seed(SEED)\n",
        "\n",
        "    if model_type == 'gru':\n",
        "        model = AttentionGRU(input_dim, time_steps, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'lstm':\n",
        "        model = AttentionLSTM(input_dim, time_steps, lstm_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'hybrid':\n",
        "        model = HybridLSTM_GRU(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'sequential':\n",
        "        model = SequentialHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'stacked':\n",
        "        model = StackedHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'bidirectional':\n",
        "        model = BidirectionalHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'cnn-rnn':\n",
        "        model = CNNRNNHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'multi-scale':\n",
        "        model = MultiScaleHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'transformer-rnn':\n",
        "        model = TransformerRNNHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'ensemble':\n",
        "        model = EnsembleHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "    # Initialize optimizer and loss function\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    criterion = nn.L1Loss()  # MAE loss\n",
        "\n",
        "    # Training loop\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    patience = 5\n",
        "    lr_factor = 0.01\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        # Print progress\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
        "\n",
        "        # Learning rate schedule based on validation loss\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] *= lr_factor\n",
        "                patience_counter = 0\n",
        "                print(f'Reducing learning rate by factor of {lr_factor}')\n",
        "\n",
        "    return model\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    mask = y_true != 0\n",
        "    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "    return mape\n",
        "\n",
        "def walk_forward(train_X, train_Y, val_X, val_Y, test_X, test_Y, grid_search, scaler):\n",
        "    r, f, c = test_X.shape\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    all_predictions = {}\n",
        "    all_adjusted_predictions = {}\n",
        "    all_ground_truths = {}\n",
        "\n",
        "    # Create lists to store all evaluation results\n",
        "    original_valuelists = []\n",
        "    adjusted_valuelists = []\n",
        "\n",
        "    for x in grid_search:\n",
        "        history_x = np.array([x for x in train_X])\n",
        "        history_y = np.array([y for y in train_Y])\n",
        "        predictions = list()\n",
        "        adjusted_predictions = list()\n",
        "        groundtrue = list()\n",
        "\n",
        "        # Extract model type first to determine how to unpack the rest\n",
        "        model_type = x[0]\n",
        "\n",
        "        # Create the appropriate config_key and extract parameters based on model type\n",
        "        if model_type in ['hybrid', 'sequential', 'stacked', 'bidirectional', 'cnn-rnn', 'multi-scale', 'transformer-rnn', 'ensemble']:\n",
        "            # Hybrid model has 7 parameters\n",
        "            model_type, lstm_unit_val, gru_unit_val, drop, dense, batch, epoch = x\n",
        "            units = f\"L{lstm_unit_val}_G{gru_unit_val}\"  # For logging\n",
        "            config_key = f\"{model_type}_lstmUnit{lstm_unit_val}_gruUnit{gru_unit_val}_drop{drop}_dense{dense}_batch{batch}_epochs{epoch}\"\n",
        "        else:\n",
        "            # LSTM and GRU models have 6 parameters\n",
        "            model_type, units, drop, dense, batch, epoch = x\n",
        "            config_key = f\"{model_type}_unit{units}_drop{drop}_dense{dense}_batch{batch}_epochs{epoch}\"\n",
        "\n",
        "        print(\"\\n\" + \"*\"*50)\n",
        "        print(f\"Starting walk-forward validation with parameters:\")\n",
        "        print(f\"Model Type: {model_type}, Units: {units}, Dropout: {drop}, Dense Units: {dense}\")\n",
        "        print(f\"Batch Size: {batch}, Epochs: {epoch}\")\n",
        "        print(f\"Device: {device}\")\n",
        "        print(f\"Total test samples: {len(test_X)}\")\n",
        "        print(\"*\"*50 + \"\\n\")\n",
        "\n",
        "        for i in range(len(test_X)):\n",
        "            print(f\"\\nTest iteration {i+1}/{len(test_X)}\")\n",
        "            print(f\"Current training set size: {history_x.shape[0]} samples\")\n",
        "\n",
        "            if model_type in ['hybrid', 'sequential', 'stacked', 'bidirectional', 'cnn-rnn', 'multi-scale', 'transformer-rnn', 'ensemble']:\n",
        "                model = build_model(history_x, history_y, val_X, val_Y, model_type=model_type,\n",
        "                                lstm_units=lstm_unit_val, gru_units=gru_unit_val, drop_rate=drop,\n",
        "                                dense_unit=dense, batch_size=batch, epochs=epoch)\n",
        "            else:\n",
        "                model = build_model(history_x, history_y, val_X, val_Y, model_type=model_type,\n",
        "                                lstm_units=units if model_type == 'lstm' else 128,\n",
        "                                gru_units=units if model_type == 'gru' else 128,\n",
        "                                drop_rate=drop, dense_unit=dense, batch_size=batch, epochs=epoch)\n",
        "\n",
        "            # Rest of the function remains the same\n",
        "            model.eval()\n",
        "\n",
        "            # Convert test data to tensor\n",
        "            test_tensor = torch.FloatTensor(test_X[i].reshape(1, f, c)).to(device)\n",
        "\n",
        "            # Predict\n",
        "            with torch.no_grad():\n",
        "                yhat = model(test_tensor).cpu().numpy()\n",
        "\n",
        "            inv_yhat, inv_y = inverscale(yhat, test_X[i], test_Y[i], scaler)\n",
        "            prev_month_lockdown = test_X[i][11][5]\n",
        "            adjusted_inv_yhat = inv_yhat * (1 - prev_month_lockdown)\n",
        "            predictions.append(inv_yhat)\n",
        "            adjusted_predictions.append(adjusted_inv_yhat)\n",
        "            groundtrue.append(inv_y)\n",
        "\n",
        "            # Observation\n",
        "            obs_x = test_X[i]\n",
        "            obs_y = test_Y[i]\n",
        "\n",
        "            history_x = np.append(history_x, [obs_x], axis=0)\n",
        "            history_y = np.append(history_y, obs_y)\n",
        "\n",
        "        # Store predictions and ground truth for this configuration\n",
        "        all_predictions[config_key] = np.array(predictions).flatten()\n",
        "        all_adjusted_predictions[config_key] = np.array(adjusted_predictions).flatten()\n",
        "        all_ground_truths[config_key] = np.array(groundtrue).flatten()\n",
        "\n",
        "        original_valuelist = evalue(predictions, groundtrue)\n",
        "        original_valuelist['model_type'] = model_type\n",
        "        original_valuelist['units'] = units\n",
        "        original_valuelist['drop_rate'] = drop\n",
        "        original_valuelist['dense_unit'] = dense\n",
        "        original_valuelist['batch_size'] = batch\n",
        "        original_valuelist['epochs'] = epoch\n",
        "\n",
        "        # Evaluate with adjusted predictions\n",
        "        adjusted_valuelist = evalue(adjusted_predictions, groundtrue)\n",
        "        adjusted_valuelist['model_type'] = model_type\n",
        "        adjusted_valuelist['units'] = units\n",
        "        adjusted_valuelist['drop_rate'] = drop\n",
        "        adjusted_valuelist['dense_unit'] = dense\n",
        "        adjusted_valuelist['batch_size'] = batch\n",
        "        adjusted_valuelist['epochs'] = epoch\n",
        "\n",
        "        # Append to the lists of results\n",
        "        original_valuelists.append(original_valuelist)\n",
        "        adjusted_valuelists.append(adjusted_valuelist)\n",
        "\n",
        "    # Combine all results\n",
        "    all_original_valuelist = pd.concat(original_valuelists, ignore_index=True)\n",
        "    all_adjusted_valuelist = pd.concat(adjusted_valuelists, ignore_index=True)\n",
        "\n",
        "    return all_original_valuelist, all_adjusted_valuelist, all_predictions, all_ground_truths, all_adjusted_predictions\n",
        "\n",
        "def evalue(yhat, inv_y):\n",
        "    valuelist = {}\n",
        "    DLM_rmse = sqrt(mean_squared_error(inv_y, yhat))\n",
        "    valuelist.update({'RMSE': {'DLM': DLM_rmse}})\n",
        "    DLM_mae = mean_absolute_error(inv_y, yhat)\n",
        "    valuelist.update({'MAE': {'DLM': DLM_mae}})\n",
        "    DLM_mape = mean_absolute_percentage_error(inv_y, yhat)\n",
        "    valuelist.update({'MAPE': {'DLM': DLM_mape}})\n",
        "    return pd.DataFrame(valuelist)\n",
        "\n",
        "def inverscale(yhat, test_X, test_Y, scaler):\n",
        "    feature = len(scaler.scale_)\n",
        "    test_Y = np.array(test_Y)\n",
        "    test_X = test_X[1, 0:feature]\n",
        "    test_X = test_X.reshape(1, test_X.shape[0])\n",
        "\n",
        "    if len(yhat.shape) == 1:\n",
        "        yhat = yhat.reshape(len(yhat), 1)\n",
        "\n",
        "    inv_yhat = concatenate((yhat, test_X[:, :-1]), axis=1)\n",
        "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "    inv_yhat = inv_yhat[:, 0]\n",
        "\n",
        "    test_Y = test_Y.reshape(1, 1)\n",
        "    inv_y = concatenate((test_Y, test_X[:, :-1]), axis=1)\n",
        "    inv_y = scaler.inverse_transform(inv_y)\n",
        "    inv_y = inv_y[:, 0]\n",
        "    return inv_yhat, inv_y\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    values = reframed.values\n",
        "    reframed_with_dates_values = reframed_with_dates.values\n",
        "\n",
        "    # Import train_test_split for random splitting\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    # Drop the date column for the splitting but keep track of indices\n",
        "    reframed_without_dates = reframed.copy()\n",
        "\n",
        "    # First split: 80% train+val, 20% test\n",
        "    train_val_indices, test_indices = train_test_split(\n",
        "        np.arange(len(reframed_without_dates)),\n",
        "        test_size=0.2,\n",
        "        random_state=SEED\n",
        "    )\n",
        "\n",
        "    # Second split: From the 80%, use 7/8 for train (70% of total) and 1/8 for val (10% of total)\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        train_val_indices,\n",
        "        test_size=0.125,  # 0.125 * 0.8 = 0.1 (10% of total)\n",
        "        random_state=SEED\n",
        "    )\n",
        "\n",
        "    # Extract values for train, validation, and test sets\n",
        "    train_data = reframed.iloc[train_indices].values\n",
        "    val_data = reframed.iloc[val_indices].values\n",
        "    test_data = reframed.iloc[test_indices].values\n",
        "\n",
        "    # Store the corresponding dates for reference\n",
        "    train_dates = reframed_with_dates.iloc[train_indices]['date']\n",
        "    val_dates = reframed_with_dates.iloc[val_indices]['date']\n",
        "    test_dates = reframed_with_dates.iloc[test_indices]['date']\n",
        "\n",
        "    # Split into X and Y\n",
        "    train_X, train_Y = train_data[:, :-1], train_data[:, -1]\n",
        "    val_X, val_Y = val_data[:, :-1], val_data[:, -1]\n",
        "    test_X, test_Y = test_data[:, :-1], test_data[:, -1]\n",
        "\n",
        "    # Reshape input to be 3D [samples, timesteps, features]\n",
        "    train_X = train_X.reshape(train_X.shape[0], 12, int(train_X.shape[1]/12))\n",
        "    val_X = val_X.reshape(val_X.shape[0], 12, int(val_X.shape[1]/12))\n",
        "    test_X = test_X.reshape(test_X.shape[0], 12, int(test_X.shape[1]/12))\n",
        "\n",
        "    # Modified grid search creation for all model types\n",
        "    grid_search = []\n",
        "    for model_type in model_types:\n",
        "        if model_type == 'lstm':\n",
        "            grid_search.extend(\n",
        "                list(itertools.product([model_type], lstm_unit, drop_rate, dense_unit, batch_size_num, epochs))\n",
        "            )\n",
        "        elif model_type == 'gru':\n",
        "            grid_search.extend(\n",
        "                list(itertools.product([model_type], gru_unit, drop_rate, dense_unit, batch_size_num, epochs))\n",
        "            )\n",
        "        else:\n",
        "            # All other models (hybrid, sequential, stacked, etc.) need both LSTM and GRU units\n",
        "            grid_search.extend(\n",
        "                list(itertools.product([model_type], lstm_unit, gru_unit, drop_rate, dense_unit, batch_size_num, epochs))\n",
        "            )\n",
        "\n",
        "    original_valuelist, adjusted_valuelist, all_predictions, all_ground_truths, all_adjusted_predictions = walk_forward(train_X, train_Y, val_X, val_Y, test_X, test_Y, grid_search, scaler)\n",
        "\n",
        "    # Group results by model type\n",
        "    # gru_results = adjusted_valuelist[adjusted_valuelist['model_type'] == 'gru']\n",
        "    # lstm_results = adjusted_valuelist[adjusted_valuelist['model_type'] == 'lstm']\n",
        "\n",
        "    print(\"Results:\")\n",
        "    print(adjusted_valuelist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK7asyU1duIx",
        "outputId": "53aa06fc-76a5-4a2b-a869-007d214c9ab5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 68/100, Train Loss: 0.0609, Val Loss: 0.0432\n",
            "Epoch 69/100, Train Loss: 0.0601, Val Loss: 0.0432\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0612, Val Loss: 0.0432\n",
            "Epoch 71/100, Train Loss: 0.0620, Val Loss: 0.0432\n",
            "Epoch 72/100, Train Loss: 0.0609, Val Loss: 0.0432\n",
            "Epoch 73/100, Train Loss: 0.0621, Val Loss: 0.0432\n",
            "Epoch 74/100, Train Loss: 0.0617, Val Loss: 0.0432\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0621, Val Loss: 0.0432\n",
            "Epoch 76/100, Train Loss: 0.0614, Val Loss: 0.0432\n",
            "Epoch 77/100, Train Loss: 0.0621, Val Loss: 0.0432\n",
            "Epoch 78/100, Train Loss: 0.0635, Val Loss: 0.0432\n",
            "Epoch 79/100, Train Loss: 0.0617, Val Loss: 0.0432\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0610, Val Loss: 0.0432\n",
            "Epoch 81/100, Train Loss: 0.0634, Val Loss: 0.0432\n",
            "Epoch 82/100, Train Loss: 0.0633, Val Loss: 0.0432\n",
            "Epoch 83/100, Train Loss: 0.0613, Val Loss: 0.0432\n",
            "Epoch 84/100, Train Loss: 0.0601, Val Loss: 0.0432\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0647, Val Loss: 0.0432\n",
            "Epoch 86/100, Train Loss: 0.0588, Val Loss: 0.0432\n",
            "Epoch 87/100, Train Loss: 0.0626, Val Loss: 0.0432\n",
            "Epoch 88/100, Train Loss: 0.0629, Val Loss: 0.0432\n",
            "Epoch 89/100, Train Loss: 0.0619, Val Loss: 0.0432\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0604, Val Loss: 0.0432\n",
            "Epoch 91/100, Train Loss: 0.0608, Val Loss: 0.0432\n",
            "Epoch 92/100, Train Loss: 0.0631, Val Loss: 0.0432\n",
            "Epoch 93/100, Train Loss: 0.0636, Val Loss: 0.0432\n",
            "Epoch 94/100, Train Loss: 0.0618, Val Loss: 0.0432\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0603, Val Loss: 0.0432\n",
            "Epoch 96/100, Train Loss: 0.0623, Val Loss: 0.0432\n",
            "Epoch 97/100, Train Loss: 0.0633, Val Loss: 0.0432\n",
            "Epoch 98/100, Train Loss: 0.0615, Val Loss: 0.0432\n",
            "Epoch 99/100, Train Loss: 0.0601, Val Loss: 0.0432\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0622, Val Loss: 0.0432\n",
            "\n",
            "Test iteration 16/28\n",
            "Current training set size: 112 samples\n",
            "Epoch 1/100, Train Loss: 0.1584, Val Loss: 0.2045\n",
            "Epoch 2/100, Train Loss: 0.1474, Val Loss: 0.1892\n",
            "Epoch 3/100, Train Loss: 0.1471, Val Loss: 0.1859\n",
            "Epoch 4/100, Train Loss: 0.1404, Val Loss: 0.1849\n",
            "Epoch 5/100, Train Loss: 0.1366, Val Loss: 0.1831\n",
            "Epoch 6/100, Train Loss: 0.1264, Val Loss: 0.1357\n",
            "Epoch 7/100, Train Loss: 0.0998, Val Loss: 0.0699\n",
            "Epoch 8/100, Train Loss: 0.0724, Val Loss: 0.0453\n",
            "Epoch 9/100, Train Loss: 0.0729, Val Loss: 0.0408\n",
            "Epoch 10/100, Train Loss: 0.0696, Val Loss: 0.0365\n",
            "Epoch 11/100, Train Loss: 0.0688, Val Loss: 0.0377\n",
            "Epoch 12/100, Train Loss: 0.0671, Val Loss: 0.0387\n",
            "Epoch 13/100, Train Loss: 0.0677, Val Loss: 0.0420\n",
            "Epoch 14/100, Train Loss: 0.0633, Val Loss: 0.0423\n",
            "Epoch 15/100, Train Loss: 0.0608, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0599, Val Loss: 0.0442\n",
            "Epoch 17/100, Train Loss: 0.0577, Val Loss: 0.0438\n",
            "Epoch 18/100, Train Loss: 0.0611, Val Loss: 0.0436\n",
            "Epoch 19/100, Train Loss: 0.0606, Val Loss: 0.0435\n",
            "Epoch 20/100, Train Loss: 0.0627, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0597, Val Loss: 0.0434\n",
            "Epoch 22/100, Train Loss: 0.0614, Val Loss: 0.0433\n",
            "Epoch 23/100, Train Loss: 0.0601, Val Loss: 0.0433\n",
            "Epoch 24/100, Train Loss: 0.0591, Val Loss: 0.0433\n",
            "Epoch 25/100, Train Loss: 0.0612, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0601, Val Loss: 0.0433\n",
            "Epoch 27/100, Train Loss: 0.0621, Val Loss: 0.0433\n",
            "Epoch 28/100, Train Loss: 0.0628, Val Loss: 0.0433\n",
            "Epoch 29/100, Train Loss: 0.0599, Val Loss: 0.0433\n",
            "Epoch 30/100, Train Loss: 0.0601, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0565, Val Loss: 0.0433\n",
            "Epoch 32/100, Train Loss: 0.0619, Val Loss: 0.0433\n",
            "Epoch 33/100, Train Loss: 0.0607, Val Loss: 0.0433\n",
            "Epoch 34/100, Train Loss: 0.0607, Val Loss: 0.0433\n",
            "Epoch 35/100, Train Loss: 0.0620, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0613, Val Loss: 0.0433\n",
            "Epoch 37/100, Train Loss: 0.0591, Val Loss: 0.0433\n",
            "Epoch 38/100, Train Loss: 0.0597, Val Loss: 0.0433\n",
            "Epoch 39/100, Train Loss: 0.0602, Val Loss: 0.0433\n",
            "Epoch 40/100, Train Loss: 0.0618, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0603, Val Loss: 0.0433\n",
            "Epoch 42/100, Train Loss: 0.0605, Val Loss: 0.0433\n",
            "Epoch 43/100, Train Loss: 0.0590, Val Loss: 0.0433\n",
            "Epoch 44/100, Train Loss: 0.0612, Val Loss: 0.0433\n",
            "Epoch 45/100, Train Loss: 0.0598, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0585, Val Loss: 0.0433\n",
            "Epoch 47/100, Train Loss: 0.0595, Val Loss: 0.0433\n",
            "Epoch 48/100, Train Loss: 0.0644, Val Loss: 0.0433\n",
            "Epoch 49/100, Train Loss: 0.0600, Val Loss: 0.0433\n",
            "Epoch 50/100, Train Loss: 0.0610, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0630, Val Loss: 0.0433\n",
            "Epoch 52/100, Train Loss: 0.0582, Val Loss: 0.0433\n",
            "Epoch 53/100, Train Loss: 0.0616, Val Loss: 0.0433\n",
            "Epoch 54/100, Train Loss: 0.0584, Val Loss: 0.0433\n",
            "Epoch 55/100, Train Loss: 0.0612, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0609, Val Loss: 0.0433\n",
            "Epoch 57/100, Train Loss: 0.0594, Val Loss: 0.0433\n",
            "Epoch 58/100, Train Loss: 0.0598, Val Loss: 0.0433\n",
            "Epoch 59/100, Train Loss: 0.0614, Val Loss: 0.0433\n",
            "Epoch 60/100, Train Loss: 0.0599, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0610, Val Loss: 0.0433\n",
            "Epoch 62/100, Train Loss: 0.0606, Val Loss: 0.0433\n",
            "Epoch 63/100, Train Loss: 0.0618, Val Loss: 0.0433\n",
            "Epoch 64/100, Train Loss: 0.0592, Val Loss: 0.0433\n",
            "Epoch 65/100, Train Loss: 0.0603, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0602, Val Loss: 0.0433\n",
            "Epoch 67/100, Train Loss: 0.0604, Val Loss: 0.0433\n",
            "Epoch 68/100, Train Loss: 0.0623, Val Loss: 0.0433\n",
            "Epoch 69/100, Train Loss: 0.0607, Val Loss: 0.0433\n",
            "Epoch 70/100, Train Loss: 0.0596, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0595, Val Loss: 0.0433\n",
            "Epoch 72/100, Train Loss: 0.0607, Val Loss: 0.0433\n",
            "Epoch 73/100, Train Loss: 0.0616, Val Loss: 0.0433\n",
            "Epoch 74/100, Train Loss: 0.0594, Val Loss: 0.0433\n",
            "Epoch 75/100, Train Loss: 0.0593, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0617, Val Loss: 0.0433\n",
            "Epoch 77/100, Train Loss: 0.0598, Val Loss: 0.0433\n",
            "Epoch 78/100, Train Loss: 0.0592, Val Loss: 0.0433\n",
            "Epoch 79/100, Train Loss: 0.0623, Val Loss: 0.0433\n",
            "Epoch 80/100, Train Loss: 0.0603, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0610, Val Loss: 0.0433\n",
            "Epoch 82/100, Train Loss: 0.0588, Val Loss: 0.0433\n",
            "Epoch 83/100, Train Loss: 0.0603, Val Loss: 0.0433\n",
            "Epoch 84/100, Train Loss: 0.0613, Val Loss: 0.0433\n",
            "Epoch 85/100, Train Loss: 0.0617, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0620, Val Loss: 0.0433\n",
            "Epoch 87/100, Train Loss: 0.0594, Val Loss: 0.0433\n",
            "Epoch 88/100, Train Loss: 0.0595, Val Loss: 0.0433\n",
            "Epoch 89/100, Train Loss: 0.0611, Val Loss: 0.0433\n",
            "Epoch 90/100, Train Loss: 0.0604, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0626, Val Loss: 0.0433\n",
            "Epoch 92/100, Train Loss: 0.0598, Val Loss: 0.0433\n",
            "Epoch 93/100, Train Loss: 0.0582, Val Loss: 0.0433\n",
            "Epoch 94/100, Train Loss: 0.0580, Val Loss: 0.0433\n",
            "Epoch 95/100, Train Loss: 0.0624, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0609, Val Loss: 0.0433\n",
            "Epoch 97/100, Train Loss: 0.0599, Val Loss: 0.0433\n",
            "Epoch 98/100, Train Loss: 0.0619, Val Loss: 0.0433\n",
            "Epoch 99/100, Train Loss: 0.0570, Val Loss: 0.0433\n",
            "Epoch 100/100, Train Loss: 0.0589, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 17/28\n",
            "Current training set size: 113 samples\n",
            "Epoch 1/100, Train Loss: 0.1605, Val Loss: 0.2130\n",
            "Epoch 2/100, Train Loss: 0.1518, Val Loss: 0.1905\n",
            "Epoch 3/100, Train Loss: 0.1469, Val Loss: 0.1874\n",
            "Epoch 4/100, Train Loss: 0.1385, Val Loss: 0.1858\n",
            "Epoch 5/100, Train Loss: 0.1363, Val Loss: 0.1717\n",
            "Epoch 6/100, Train Loss: 0.1281, Val Loss: 0.1573\n",
            "Epoch 7/100, Train Loss: 0.1051, Val Loss: 0.0886\n",
            "Epoch 8/100, Train Loss: 0.0748, Val Loss: 0.0454\n",
            "Epoch 9/100, Train Loss: 0.0669, Val Loss: 0.0354\n",
            "Epoch 10/100, Train Loss: 0.0715, Val Loss: 0.0417\n",
            "Epoch 11/100, Train Loss: 0.0666, Val Loss: 0.0428\n",
            "Epoch 12/100, Train Loss: 0.0641, Val Loss: 0.0378\n",
            "Epoch 13/100, Train Loss: 0.0653, Val Loss: 0.0489\n",
            "Epoch 14/100, Train Loss: 0.0630, Val Loss: 0.0529\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 15/100, Train Loss: 0.0630, Val Loss: 0.0523\n",
            "Epoch 16/100, Train Loss: 0.0682, Val Loss: 0.0512\n",
            "Epoch 17/100, Train Loss: 0.0706, Val Loss: 0.0501\n",
            "Epoch 18/100, Train Loss: 0.0601, Val Loss: 0.0490\n",
            "Epoch 19/100, Train Loss: 0.0610, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0622, Val Loss: 0.0484\n",
            "Epoch 21/100, Train Loss: 0.0614, Val Loss: 0.0484\n",
            "Epoch 22/100, Train Loss: 0.0602, Val Loss: 0.0484\n",
            "Epoch 23/100, Train Loss: 0.0594, Val Loss: 0.0484\n",
            "Epoch 24/100, Train Loss: 0.0633, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0592, Val Loss: 0.0484\n",
            "Epoch 26/100, Train Loss: 0.0615, Val Loss: 0.0484\n",
            "Epoch 27/100, Train Loss: 0.0602, Val Loss: 0.0484\n",
            "Epoch 28/100, Train Loss: 0.0602, Val Loss: 0.0484\n",
            "Epoch 29/100, Train Loss: 0.0653, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0613, Val Loss: 0.0484\n",
            "Epoch 31/100, Train Loss: 0.0624, Val Loss: 0.0484\n",
            "Epoch 32/100, Train Loss: 0.0617, Val Loss: 0.0484\n",
            "Epoch 33/100, Train Loss: 0.0602, Val Loss: 0.0484\n",
            "Epoch 34/100, Train Loss: 0.0603, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0619, Val Loss: 0.0484\n",
            "Epoch 36/100, Train Loss: 0.0593, Val Loss: 0.0484\n",
            "Epoch 37/100, Train Loss: 0.0624, Val Loss: 0.0484\n",
            "Epoch 38/100, Train Loss: 0.0625, Val Loss: 0.0484\n",
            "Epoch 39/100, Train Loss: 0.0623, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0626, Val Loss: 0.0484\n",
            "Epoch 41/100, Train Loss: 0.0632, Val Loss: 0.0484\n",
            "Epoch 42/100, Train Loss: 0.0597, Val Loss: 0.0484\n",
            "Epoch 43/100, Train Loss: 0.0595, Val Loss: 0.0484\n",
            "Epoch 44/100, Train Loss: 0.0638, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0613, Val Loss: 0.0484\n",
            "Epoch 46/100, Train Loss: 0.0610, Val Loss: 0.0484\n",
            "Epoch 47/100, Train Loss: 0.0613, Val Loss: 0.0484\n",
            "Epoch 48/100, Train Loss: 0.0589, Val Loss: 0.0484\n",
            "Epoch 49/100, Train Loss: 0.0602, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0635, Val Loss: 0.0484\n",
            "Epoch 51/100, Train Loss: 0.0607, Val Loss: 0.0484\n",
            "Epoch 52/100, Train Loss: 0.0627, Val Loss: 0.0484\n",
            "Epoch 53/100, Train Loss: 0.0631, Val Loss: 0.0484\n",
            "Epoch 54/100, Train Loss: 0.0624, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0612, Val Loss: 0.0484\n",
            "Epoch 56/100, Train Loss: 0.0609, Val Loss: 0.0484\n",
            "Epoch 57/100, Train Loss: 0.0718, Val Loss: 0.0484\n",
            "Epoch 58/100, Train Loss: 0.0610, Val Loss: 0.0484\n",
            "Epoch 59/100, Train Loss: 0.0605, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0611, Val Loss: 0.0484\n",
            "Epoch 61/100, Train Loss: 0.0600, Val Loss: 0.0484\n",
            "Epoch 62/100, Train Loss: 0.0592, Val Loss: 0.0484\n",
            "Epoch 63/100, Train Loss: 0.0626, Val Loss: 0.0484\n",
            "Epoch 64/100, Train Loss: 0.0604, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0610, Val Loss: 0.0484\n",
            "Epoch 66/100, Train Loss: 0.0595, Val Loss: 0.0484\n",
            "Epoch 67/100, Train Loss: 0.0587, Val Loss: 0.0484\n",
            "Epoch 68/100, Train Loss: 0.0579, Val Loss: 0.0484\n",
            "Epoch 69/100, Train Loss: 0.0601, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0594, Val Loss: 0.0484\n",
            "Epoch 71/100, Train Loss: 0.0596, Val Loss: 0.0484\n",
            "Epoch 72/100, Train Loss: 0.0612, Val Loss: 0.0484\n",
            "Epoch 73/100, Train Loss: 0.0619, Val Loss: 0.0484\n",
            "Epoch 74/100, Train Loss: 0.0611, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0596, Val Loss: 0.0484\n",
            "Epoch 76/100, Train Loss: 0.0616, Val Loss: 0.0484\n",
            "Epoch 77/100, Train Loss: 0.0607, Val Loss: 0.0484\n",
            "Epoch 78/100, Train Loss: 0.0602, Val Loss: 0.0484\n",
            "Epoch 79/100, Train Loss: 0.0610, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0603, Val Loss: 0.0484\n",
            "Epoch 81/100, Train Loss: 0.0607, Val Loss: 0.0484\n",
            "Epoch 82/100, Train Loss: 0.0609, Val Loss: 0.0484\n",
            "Epoch 83/100, Train Loss: 0.0635, Val Loss: 0.0484\n",
            "Epoch 84/100, Train Loss: 0.0623, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0629, Val Loss: 0.0484\n",
            "Epoch 86/100, Train Loss: 0.0627, Val Loss: 0.0484\n",
            "Epoch 87/100, Train Loss: 0.0599, Val Loss: 0.0484\n",
            "Epoch 88/100, Train Loss: 0.0597, Val Loss: 0.0484\n",
            "Epoch 89/100, Train Loss: 0.0591, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0617, Val Loss: 0.0484\n",
            "Epoch 91/100, Train Loss: 0.0612, Val Loss: 0.0484\n",
            "Epoch 92/100, Train Loss: 0.0595, Val Loss: 0.0484\n",
            "Epoch 93/100, Train Loss: 0.0637, Val Loss: 0.0484\n",
            "Epoch 94/100, Train Loss: 0.0597, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0629, Val Loss: 0.0484\n",
            "Epoch 96/100, Train Loss: 0.0618, Val Loss: 0.0484\n",
            "Epoch 97/100, Train Loss: 0.0601, Val Loss: 0.0484\n",
            "Epoch 98/100, Train Loss: 0.0604, Val Loss: 0.0484\n",
            "Epoch 99/100, Train Loss: 0.0616, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0593, Val Loss: 0.0484\n",
            "\n",
            "Test iteration 18/28\n",
            "Current training set size: 114 samples\n",
            "Epoch 1/100, Train Loss: 0.1611, Val Loss: 0.2097\n",
            "Epoch 2/100, Train Loss: 0.1486, Val Loss: 0.1892\n",
            "Epoch 3/100, Train Loss: 0.1414, Val Loss: 0.1889\n",
            "Epoch 4/100, Train Loss: 0.1409, Val Loss: 0.1830\n",
            "Epoch 5/100, Train Loss: 0.1324, Val Loss: 0.1621\n",
            "Epoch 6/100, Train Loss: 0.1236, Val Loss: 0.1297\n",
            "Epoch 7/100, Train Loss: 0.0925, Val Loss: 0.0570\n",
            "Epoch 8/100, Train Loss: 0.0765, Val Loss: 0.0741\n",
            "Epoch 9/100, Train Loss: 0.0748, Val Loss: 0.0428\n",
            "Epoch 10/100, Train Loss: 0.0713, Val Loss: 0.0435\n",
            "Epoch 11/100, Train Loss: 0.0660, Val Loss: 0.0356\n",
            "Epoch 12/100, Train Loss: 0.0733, Val Loss: 0.0360\n",
            "Epoch 13/100, Train Loss: 0.0632, Val Loss: 0.0451\n",
            "Epoch 14/100, Train Loss: 0.0704, Val Loss: 0.0452\n",
            "Epoch 15/100, Train Loss: 0.0596, Val Loss: 0.0395\n",
            "Epoch 16/100, Train Loss: 0.0602, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0568, Val Loss: 0.0448\n",
            "Epoch 18/100, Train Loss: 0.0590, Val Loss: 0.0442\n",
            "Epoch 19/100, Train Loss: 0.0585, Val Loss: 0.0439\n",
            "Epoch 20/100, Train Loss: 0.0618, Val Loss: 0.0436\n",
            "Epoch 21/100, Train Loss: 0.0594, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0570, Val Loss: 0.0434\n",
            "Epoch 23/100, Train Loss: 0.0555, Val Loss: 0.0434\n",
            "Epoch 24/100, Train Loss: 0.0583, Val Loss: 0.0434\n",
            "Epoch 25/100, Train Loss: 0.0579, Val Loss: 0.0434\n",
            "Epoch 26/100, Train Loss: 0.0569, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0570, Val Loss: 0.0434\n",
            "Epoch 28/100, Train Loss: 0.0557, Val Loss: 0.0434\n",
            "Epoch 29/100, Train Loss: 0.0605, Val Loss: 0.0434\n",
            "Epoch 30/100, Train Loss: 0.0580, Val Loss: 0.0434\n",
            "Epoch 31/100, Train Loss: 0.0572, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0568, Val Loss: 0.0434\n",
            "Epoch 33/100, Train Loss: 0.0579, Val Loss: 0.0434\n",
            "Epoch 34/100, Train Loss: 0.0582, Val Loss: 0.0434\n",
            "Epoch 35/100, Train Loss: 0.0580, Val Loss: 0.0434\n",
            "Epoch 36/100, Train Loss: 0.0552, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0567, Val Loss: 0.0434\n",
            "Epoch 38/100, Train Loss: 0.0591, Val Loss: 0.0434\n",
            "Epoch 39/100, Train Loss: 0.0575, Val Loss: 0.0434\n",
            "Epoch 40/100, Train Loss: 0.0568, Val Loss: 0.0434\n",
            "Epoch 41/100, Train Loss: 0.0570, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0553, Val Loss: 0.0434\n",
            "Epoch 43/100, Train Loss: 0.0564, Val Loss: 0.0434\n",
            "Epoch 44/100, Train Loss: 0.0554, Val Loss: 0.0434\n",
            "Epoch 45/100, Train Loss: 0.0575, Val Loss: 0.0434\n",
            "Epoch 46/100, Train Loss: 0.0567, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0568, Val Loss: 0.0434\n",
            "Epoch 48/100, Train Loss: 0.0571, Val Loss: 0.0434\n",
            "Epoch 49/100, Train Loss: 0.0588, Val Loss: 0.0434\n",
            "Epoch 50/100, Train Loss: 0.0589, Val Loss: 0.0434\n",
            "Epoch 51/100, Train Loss: 0.0566, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0588, Val Loss: 0.0434\n",
            "Epoch 53/100, Train Loss: 0.0609, Val Loss: 0.0434\n",
            "Epoch 54/100, Train Loss: 0.0576, Val Loss: 0.0434\n",
            "Epoch 55/100, Train Loss: 0.0578, Val Loss: 0.0434\n",
            "Epoch 56/100, Train Loss: 0.0559, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0554, Val Loss: 0.0434\n",
            "Epoch 58/100, Train Loss: 0.0567, Val Loss: 0.0434\n",
            "Epoch 59/100, Train Loss: 0.0572, Val Loss: 0.0434\n",
            "Epoch 60/100, Train Loss: 0.0575, Val Loss: 0.0434\n",
            "Epoch 61/100, Train Loss: 0.0568, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0589, Val Loss: 0.0434\n",
            "Epoch 63/100, Train Loss: 0.0552, Val Loss: 0.0434\n",
            "Epoch 64/100, Train Loss: 0.0576, Val Loss: 0.0434\n",
            "Epoch 65/100, Train Loss: 0.0562, Val Loss: 0.0434\n",
            "Epoch 66/100, Train Loss: 0.0580, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0575, Val Loss: 0.0434\n",
            "Epoch 68/100, Train Loss: 0.0609, Val Loss: 0.0434\n",
            "Epoch 69/100, Train Loss: 0.0590, Val Loss: 0.0434\n",
            "Epoch 70/100, Train Loss: 0.0574, Val Loss: 0.0434\n",
            "Epoch 71/100, Train Loss: 0.0591, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0589, Val Loss: 0.0434\n",
            "Epoch 73/100, Train Loss: 0.0566, Val Loss: 0.0434\n",
            "Epoch 74/100, Train Loss: 0.0585, Val Loss: 0.0434\n",
            "Epoch 75/100, Train Loss: 0.0583, Val Loss: 0.0434\n",
            "Epoch 76/100, Train Loss: 0.0613, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0569, Val Loss: 0.0434\n",
            "Epoch 78/100, Train Loss: 0.0559, Val Loss: 0.0434\n",
            "Epoch 79/100, Train Loss: 0.0573, Val Loss: 0.0434\n",
            "Epoch 80/100, Train Loss: 0.0558, Val Loss: 0.0434\n",
            "Epoch 81/100, Train Loss: 0.0569, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0569, Val Loss: 0.0434\n",
            "Epoch 83/100, Train Loss: 0.0587, Val Loss: 0.0434\n",
            "Epoch 84/100, Train Loss: 0.0602, Val Loss: 0.0434\n",
            "Epoch 85/100, Train Loss: 0.0567, Val Loss: 0.0434\n",
            "Epoch 86/100, Train Loss: 0.0600, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0569, Val Loss: 0.0434\n",
            "Epoch 88/100, Train Loss: 0.0593, Val Loss: 0.0434\n",
            "Epoch 89/100, Train Loss: 0.0644, Val Loss: 0.0434\n",
            "Epoch 90/100, Train Loss: 0.0589, Val Loss: 0.0434\n",
            "Epoch 91/100, Train Loss: 0.0590, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0557, Val Loss: 0.0434\n",
            "Epoch 93/100, Train Loss: 0.0585, Val Loss: 0.0434\n",
            "Epoch 94/100, Train Loss: 0.0581, Val Loss: 0.0434\n",
            "Epoch 95/100, Train Loss: 0.0558, Val Loss: 0.0434\n",
            "Epoch 96/100, Train Loss: 0.0588, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0565, Val Loss: 0.0434\n",
            "Epoch 98/100, Train Loss: 0.0571, Val Loss: 0.0434\n",
            "Epoch 99/100, Train Loss: 0.0592, Val Loss: 0.0434\n",
            "Epoch 100/100, Train Loss: 0.0593, Val Loss: 0.0434\n",
            "\n",
            "Test iteration 19/28\n",
            "Current training set size: 115 samples\n",
            "Epoch 1/100, Train Loss: 0.1622, Val Loss: 0.1966\n",
            "Epoch 2/100, Train Loss: 0.1466, Val Loss: 0.1914\n",
            "Epoch 3/100, Train Loss: 0.1415, Val Loss: 0.1877\n",
            "Epoch 4/100, Train Loss: 0.1393, Val Loss: 0.1838\n",
            "Epoch 5/100, Train Loss: 0.1341, Val Loss: 0.1639\n",
            "Epoch 6/100, Train Loss: 0.1263, Val Loss: 0.1458\n",
            "Epoch 7/100, Train Loss: 0.1049, Val Loss: 0.0553\n",
            "Epoch 8/100, Train Loss: 0.0725, Val Loss: 0.0493\n",
            "Epoch 9/100, Train Loss: 0.0727, Val Loss: 0.0621\n",
            "Epoch 10/100, Train Loss: 0.0677, Val Loss: 0.0359\n",
            "Epoch 11/100, Train Loss: 0.0668, Val Loss: 0.0414\n",
            "Epoch 12/100, Train Loss: 0.0686, Val Loss: 0.0412\n",
            "Epoch 13/100, Train Loss: 0.0605, Val Loss: 0.0365\n",
            "Epoch 14/100, Train Loss: 0.0643, Val Loss: 0.0353\n",
            "Epoch 15/100, Train Loss: 0.0628, Val Loss: 0.0493\n",
            "Epoch 16/100, Train Loss: 0.0654, Val Loss: 0.0385\n",
            "Epoch 17/100, Train Loss: 0.0615, Val Loss: 0.0361\n",
            "Epoch 18/100, Train Loss: 0.0619, Val Loss: 0.0507\n",
            "Epoch 19/100, Train Loss: 0.0573, Val Loss: 0.0409\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0622, Val Loss: 0.0395\n",
            "Epoch 21/100, Train Loss: 0.0603, Val Loss: 0.0375\n",
            "Epoch 22/100, Train Loss: 0.0547, Val Loss: 0.0363\n",
            "Epoch 23/100, Train Loss: 0.0561, Val Loss: 0.0358\n",
            "Epoch 24/100, Train Loss: 0.0559, Val Loss: 0.0358\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0557, Val Loss: 0.0358\n",
            "Epoch 26/100, Train Loss: 0.0563, Val Loss: 0.0358\n",
            "Epoch 27/100, Train Loss: 0.0560, Val Loss: 0.0358\n",
            "Epoch 28/100, Train Loss: 0.0548, Val Loss: 0.0358\n",
            "Epoch 29/100, Train Loss: 0.0557, Val Loss: 0.0358\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0555, Val Loss: 0.0358\n",
            "Epoch 31/100, Train Loss: 0.0535, Val Loss: 0.0358\n",
            "Epoch 32/100, Train Loss: 0.0563, Val Loss: 0.0358\n",
            "Epoch 33/100, Train Loss: 0.0585, Val Loss: 0.0358\n",
            "Epoch 34/100, Train Loss: 0.0541, Val Loss: 0.0358\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0532, Val Loss: 0.0358\n",
            "Epoch 36/100, Train Loss: 0.0544, Val Loss: 0.0358\n",
            "Epoch 37/100, Train Loss: 0.0544, Val Loss: 0.0358\n",
            "Epoch 38/100, Train Loss: 0.0540, Val Loss: 0.0358\n",
            "Epoch 39/100, Train Loss: 0.0537, Val Loss: 0.0358\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0553, Val Loss: 0.0358\n",
            "Epoch 41/100, Train Loss: 0.0557, Val Loss: 0.0358\n",
            "Epoch 42/100, Train Loss: 0.0560, Val Loss: 0.0358\n",
            "Epoch 43/100, Train Loss: 0.0567, Val Loss: 0.0358\n",
            "Epoch 44/100, Train Loss: 0.0564, Val Loss: 0.0358\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0574, Val Loss: 0.0358\n",
            "Epoch 46/100, Train Loss: 0.0556, Val Loss: 0.0358\n",
            "Epoch 47/100, Train Loss: 0.0548, Val Loss: 0.0358\n",
            "Epoch 48/100, Train Loss: 0.0573, Val Loss: 0.0358\n",
            "Epoch 49/100, Train Loss: 0.0554, Val Loss: 0.0358\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0568, Val Loss: 0.0358\n",
            "Epoch 51/100, Train Loss: 0.0557, Val Loss: 0.0358\n",
            "Epoch 52/100, Train Loss: 0.0550, Val Loss: 0.0358\n",
            "Epoch 53/100, Train Loss: 0.0555, Val Loss: 0.0358\n",
            "Epoch 54/100, Train Loss: 0.0563, Val Loss: 0.0358\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0554, Val Loss: 0.0358\n",
            "Epoch 56/100, Train Loss: 0.0572, Val Loss: 0.0358\n",
            "Epoch 57/100, Train Loss: 0.0574, Val Loss: 0.0358\n",
            "Epoch 58/100, Train Loss: 0.0538, Val Loss: 0.0358\n",
            "Epoch 59/100, Train Loss: 0.0552, Val Loss: 0.0358\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0550, Val Loss: 0.0358\n",
            "Epoch 61/100, Train Loss: 0.0536, Val Loss: 0.0358\n",
            "Epoch 62/100, Train Loss: 0.0552, Val Loss: 0.0358\n",
            "Epoch 63/100, Train Loss: 0.0572, Val Loss: 0.0358\n",
            "Epoch 64/100, Train Loss: 0.0547, Val Loss: 0.0358\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0556, Val Loss: 0.0358\n",
            "Epoch 66/100, Train Loss: 0.0546, Val Loss: 0.0358\n",
            "Epoch 67/100, Train Loss: 0.0546, Val Loss: 0.0358\n",
            "Epoch 68/100, Train Loss: 0.0543, Val Loss: 0.0358\n",
            "Epoch 69/100, Train Loss: 0.0538, Val Loss: 0.0358\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0543, Val Loss: 0.0358\n",
            "Epoch 71/100, Train Loss: 0.0547, Val Loss: 0.0358\n",
            "Epoch 72/100, Train Loss: 0.0567, Val Loss: 0.0358\n",
            "Epoch 73/100, Train Loss: 0.0549, Val Loss: 0.0358\n",
            "Epoch 74/100, Train Loss: 0.0569, Val Loss: 0.0358\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0573, Val Loss: 0.0358\n",
            "Epoch 76/100, Train Loss: 0.0556, Val Loss: 0.0358\n",
            "Epoch 77/100, Train Loss: 0.0535, Val Loss: 0.0358\n",
            "Epoch 78/100, Train Loss: 0.0542, Val Loss: 0.0358\n",
            "Epoch 79/100, Train Loss: 0.0559, Val Loss: 0.0358\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0575, Val Loss: 0.0358\n",
            "Epoch 81/100, Train Loss: 0.0547, Val Loss: 0.0358\n",
            "Epoch 82/100, Train Loss: 0.0569, Val Loss: 0.0358\n",
            "Epoch 83/100, Train Loss: 0.0553, Val Loss: 0.0358\n",
            "Epoch 84/100, Train Loss: 0.0562, Val Loss: 0.0358\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0548, Val Loss: 0.0358\n",
            "Epoch 86/100, Train Loss: 0.0550, Val Loss: 0.0358\n",
            "Epoch 87/100, Train Loss: 0.0558, Val Loss: 0.0358\n",
            "Epoch 88/100, Train Loss: 0.0549, Val Loss: 0.0358\n",
            "Epoch 89/100, Train Loss: 0.0566, Val Loss: 0.0358\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0547, Val Loss: 0.0358\n",
            "Epoch 91/100, Train Loss: 0.0546, Val Loss: 0.0358\n",
            "Epoch 92/100, Train Loss: 0.0527, Val Loss: 0.0358\n",
            "Epoch 93/100, Train Loss: 0.0571, Val Loss: 0.0358\n",
            "Epoch 94/100, Train Loss: 0.0562, Val Loss: 0.0358\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0577, Val Loss: 0.0358\n",
            "Epoch 96/100, Train Loss: 0.0563, Val Loss: 0.0358\n",
            "Epoch 97/100, Train Loss: 0.0565, Val Loss: 0.0358\n",
            "Epoch 98/100, Train Loss: 0.0564, Val Loss: 0.0358\n",
            "Epoch 99/100, Train Loss: 0.0553, Val Loss: 0.0358\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0544, Val Loss: 0.0358\n",
            "\n",
            "Test iteration 20/28\n",
            "Current training set size: 116 samples\n",
            "Epoch 1/100, Train Loss: 0.1612, Val Loss: 0.2079\n",
            "Epoch 2/100, Train Loss: 0.1454, Val Loss: 0.1892\n",
            "Epoch 3/100, Train Loss: 0.1442, Val Loss: 0.1896\n",
            "Epoch 4/100, Train Loss: 0.1387, Val Loss: 0.1866\n",
            "Epoch 5/100, Train Loss: 0.1328, Val Loss: 0.1651\n",
            "Epoch 6/100, Train Loss: 0.1247, Val Loss: 0.1345\n",
            "Epoch 7/100, Train Loss: 0.0987, Val Loss: 0.0580\n",
            "Epoch 8/100, Train Loss: 0.0812, Val Loss: 0.0419\n",
            "Epoch 9/100, Train Loss: 0.0772, Val Loss: 0.0468\n",
            "Epoch 10/100, Train Loss: 0.0768, Val Loss: 0.0459\n",
            "Epoch 11/100, Train Loss: 0.0734, Val Loss: 0.0473\n",
            "Epoch 12/100, Train Loss: 0.0688, Val Loss: 0.0632\n",
            "Epoch 13/100, Train Loss: 0.0712, Val Loss: 0.0518\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 14/100, Train Loss: 0.0692, Val Loss: 0.0500\n",
            "Epoch 15/100, Train Loss: 0.0704, Val Loss: 0.0487\n",
            "Epoch 16/100, Train Loss: 0.0666, Val Loss: 0.0477\n",
            "Epoch 17/100, Train Loss: 0.0663, Val Loss: 0.0473\n",
            "Epoch 18/100, Train Loss: 0.0679, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0660, Val Loss: 0.0466\n",
            "Epoch 20/100, Train Loss: 0.0672, Val Loss: 0.0466\n",
            "Epoch 21/100, Train Loss: 0.0675, Val Loss: 0.0466\n",
            "Epoch 22/100, Train Loss: 0.0662, Val Loss: 0.0465\n",
            "Epoch 23/100, Train Loss: 0.0660, Val Loss: 0.0465\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0667, Val Loss: 0.0465\n",
            "Epoch 25/100, Train Loss: 0.0665, Val Loss: 0.0465\n",
            "Epoch 26/100, Train Loss: 0.0657, Val Loss: 0.0465\n",
            "Epoch 27/100, Train Loss: 0.0657, Val Loss: 0.0465\n",
            "Epoch 28/100, Train Loss: 0.0669, Val Loss: 0.0465\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0667, Val Loss: 0.0465\n",
            "Epoch 30/100, Train Loss: 0.0681, Val Loss: 0.0465\n",
            "Epoch 31/100, Train Loss: 0.0672, Val Loss: 0.0465\n",
            "Epoch 32/100, Train Loss: 0.0664, Val Loss: 0.0465\n",
            "Epoch 33/100, Train Loss: 0.0676, Val Loss: 0.0465\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0680, Val Loss: 0.0465\n",
            "Epoch 35/100, Train Loss: 0.0665, Val Loss: 0.0465\n",
            "Epoch 36/100, Train Loss: 0.0646, Val Loss: 0.0465\n",
            "Epoch 37/100, Train Loss: 0.0690, Val Loss: 0.0465\n",
            "Epoch 38/100, Train Loss: 0.0669, Val Loss: 0.0465\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0673, Val Loss: 0.0465\n",
            "Epoch 40/100, Train Loss: 0.0673, Val Loss: 0.0465\n",
            "Epoch 41/100, Train Loss: 0.0635, Val Loss: 0.0465\n",
            "Epoch 42/100, Train Loss: 0.0667, Val Loss: 0.0465\n",
            "Epoch 43/100, Train Loss: 0.0633, Val Loss: 0.0465\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0667, Val Loss: 0.0465\n",
            "Epoch 45/100, Train Loss: 0.0670, Val Loss: 0.0465\n",
            "Epoch 46/100, Train Loss: 0.0678, Val Loss: 0.0465\n",
            "Epoch 47/100, Train Loss: 0.0658, Val Loss: 0.0465\n",
            "Epoch 48/100, Train Loss: 0.0666, Val Loss: 0.0465\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0678, Val Loss: 0.0465\n",
            "Epoch 50/100, Train Loss: 0.0697, Val Loss: 0.0465\n",
            "Epoch 51/100, Train Loss: 0.0654, Val Loss: 0.0465\n",
            "Epoch 52/100, Train Loss: 0.0656, Val Loss: 0.0465\n",
            "Epoch 53/100, Train Loss: 0.0687, Val Loss: 0.0465\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0658, Val Loss: 0.0465\n",
            "Epoch 55/100, Train Loss: 0.0692, Val Loss: 0.0465\n",
            "Epoch 56/100, Train Loss: 0.0666, Val Loss: 0.0465\n",
            "Epoch 57/100, Train Loss: 0.0640, Val Loss: 0.0465\n",
            "Epoch 58/100, Train Loss: 0.0670, Val Loss: 0.0465\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0668, Val Loss: 0.0465\n",
            "Epoch 60/100, Train Loss: 0.0669, Val Loss: 0.0465\n",
            "Epoch 61/100, Train Loss: 0.0633, Val Loss: 0.0465\n",
            "Epoch 62/100, Train Loss: 0.0680, Val Loss: 0.0465\n",
            "Epoch 63/100, Train Loss: 0.0691, Val Loss: 0.0465\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0657, Val Loss: 0.0465\n",
            "Epoch 65/100, Train Loss: 0.0677, Val Loss: 0.0465\n",
            "Epoch 66/100, Train Loss: 0.0650, Val Loss: 0.0465\n",
            "Epoch 67/100, Train Loss: 0.0667, Val Loss: 0.0465\n",
            "Epoch 68/100, Train Loss: 0.0659, Val Loss: 0.0465\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0657, Val Loss: 0.0465\n",
            "Epoch 70/100, Train Loss: 0.0641, Val Loss: 0.0465\n",
            "Epoch 71/100, Train Loss: 0.0669, Val Loss: 0.0465\n",
            "Epoch 72/100, Train Loss: 0.0680, Val Loss: 0.0465\n",
            "Epoch 73/100, Train Loss: 0.0672, Val Loss: 0.0465\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0676, Val Loss: 0.0465\n",
            "Epoch 75/100, Train Loss: 0.0644, Val Loss: 0.0465\n",
            "Epoch 76/100, Train Loss: 0.0677, Val Loss: 0.0465\n",
            "Epoch 77/100, Train Loss: 0.0669, Val Loss: 0.0465\n",
            "Epoch 78/100, Train Loss: 0.0655, Val Loss: 0.0465\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0679, Val Loss: 0.0465\n",
            "Epoch 80/100, Train Loss: 0.0651, Val Loss: 0.0465\n",
            "Epoch 81/100, Train Loss: 0.0651, Val Loss: 0.0465\n",
            "Epoch 82/100, Train Loss: 0.0669, Val Loss: 0.0465\n",
            "Epoch 83/100, Train Loss: 0.0667, Val Loss: 0.0465\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0674, Val Loss: 0.0465\n",
            "Epoch 85/100, Train Loss: 0.0670, Val Loss: 0.0465\n",
            "Epoch 86/100, Train Loss: 0.0676, Val Loss: 0.0465\n",
            "Epoch 87/100, Train Loss: 0.0667, Val Loss: 0.0465\n",
            "Epoch 88/100, Train Loss: 0.0662, Val Loss: 0.0465\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0671, Val Loss: 0.0465\n",
            "Epoch 90/100, Train Loss: 0.0642, Val Loss: 0.0465\n",
            "Epoch 91/100, Train Loss: 0.0668, Val Loss: 0.0465\n",
            "Epoch 92/100, Train Loss: 0.0651, Val Loss: 0.0465\n",
            "Epoch 93/100, Train Loss: 0.0661, Val Loss: 0.0465\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0673, Val Loss: 0.0465\n",
            "Epoch 95/100, Train Loss: 0.0648, Val Loss: 0.0465\n",
            "Epoch 96/100, Train Loss: 0.0652, Val Loss: 0.0465\n",
            "Epoch 97/100, Train Loss: 0.0691, Val Loss: 0.0465\n",
            "Epoch 98/100, Train Loss: 0.0660, Val Loss: 0.0465\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0664, Val Loss: 0.0465\n",
            "Epoch 100/100, Train Loss: 0.0653, Val Loss: 0.0465\n",
            "\n",
            "Test iteration 21/28\n",
            "Current training set size: 117 samples\n",
            "Epoch 1/100, Train Loss: 0.1682, Val Loss: 0.2154\n",
            "Epoch 2/100, Train Loss: 0.1488, Val Loss: 0.1902\n",
            "Epoch 3/100, Train Loss: 0.1467, Val Loss: 0.1948\n",
            "Epoch 4/100, Train Loss: 0.1472, Val Loss: 0.1817\n",
            "Epoch 5/100, Train Loss: 0.1357, Val Loss: 0.1778\n",
            "Epoch 6/100, Train Loss: 0.1323, Val Loss: 0.1682\n",
            "Epoch 7/100, Train Loss: 0.1118, Val Loss: 0.0960\n",
            "Epoch 8/100, Train Loss: 0.0850, Val Loss: 0.0510\n",
            "Epoch 9/100, Train Loss: 0.0729, Val Loss: 0.0424\n",
            "Epoch 10/100, Train Loss: 0.0712, Val Loss: 0.0420\n",
            "Epoch 11/100, Train Loss: 0.0666, Val Loss: 0.0426\n",
            "Epoch 12/100, Train Loss: 0.0691, Val Loss: 0.0391\n",
            "Epoch 13/100, Train Loss: 0.0754, Val Loss: 0.0386\n",
            "Epoch 14/100, Train Loss: 0.0659, Val Loss: 0.0405\n",
            "Epoch 15/100, Train Loss: 0.0851, Val Loss: 0.0407\n",
            "Epoch 16/100, Train Loss: 0.0681, Val Loss: 0.0468\n",
            "Epoch 17/100, Train Loss: 0.0657, Val Loss: 0.0347\n",
            "Epoch 18/100, Train Loss: 0.0608, Val Loss: 0.0385\n",
            "Epoch 19/100, Train Loss: 0.0630, Val Loss: 0.0388\n",
            "Epoch 20/100, Train Loss: 0.0618, Val Loss: 0.0428\n",
            "Epoch 21/100, Train Loss: 0.0584, Val Loss: 0.0412\n",
            "Epoch 22/100, Train Loss: 0.0568, Val Loss: 0.0406\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0576, Val Loss: 0.0403\n",
            "Epoch 24/100, Train Loss: 0.0553, Val Loss: 0.0400\n",
            "Epoch 25/100, Train Loss: 0.0565, Val Loss: 0.0398\n",
            "Epoch 26/100, Train Loss: 0.0543, Val Loss: 0.0396\n",
            "Epoch 27/100, Train Loss: 0.0568, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0534, Val Loss: 0.0393\n",
            "Epoch 29/100, Train Loss: 0.0543, Val Loss: 0.0393\n",
            "Epoch 30/100, Train Loss: 0.0540, Val Loss: 0.0393\n",
            "Epoch 31/100, Train Loss: 0.0558, Val Loss: 0.0393\n",
            "Epoch 32/100, Train Loss: 0.0556, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0546, Val Loss: 0.0393\n",
            "Epoch 34/100, Train Loss: 0.0528, Val Loss: 0.0393\n",
            "Epoch 35/100, Train Loss: 0.0547, Val Loss: 0.0393\n",
            "Epoch 36/100, Train Loss: 0.0547, Val Loss: 0.0393\n",
            "Epoch 37/100, Train Loss: 0.0542, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0532, Val Loss: 0.0393\n",
            "Epoch 39/100, Train Loss: 0.0550, Val Loss: 0.0393\n",
            "Epoch 40/100, Train Loss: 0.0533, Val Loss: 0.0393\n",
            "Epoch 41/100, Train Loss: 0.0574, Val Loss: 0.0393\n",
            "Epoch 42/100, Train Loss: 0.0554, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0552, Val Loss: 0.0393\n",
            "Epoch 44/100, Train Loss: 0.0548, Val Loss: 0.0393\n",
            "Epoch 45/100, Train Loss: 0.0555, Val Loss: 0.0393\n",
            "Epoch 46/100, Train Loss: 0.0591, Val Loss: 0.0393\n",
            "Epoch 47/100, Train Loss: 0.0561, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0541, Val Loss: 0.0393\n",
            "Epoch 49/100, Train Loss: 0.0549, Val Loss: 0.0393\n",
            "Epoch 50/100, Train Loss: 0.0660, Val Loss: 0.0393\n",
            "Epoch 51/100, Train Loss: 0.0556, Val Loss: 0.0393\n",
            "Epoch 52/100, Train Loss: 0.0562, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0591, Val Loss: 0.0393\n",
            "Epoch 54/100, Train Loss: 0.0554, Val Loss: 0.0393\n",
            "Epoch 55/100, Train Loss: 0.0565, Val Loss: 0.0393\n",
            "Epoch 56/100, Train Loss: 0.0587, Val Loss: 0.0393\n",
            "Epoch 57/100, Train Loss: 0.0539, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0540, Val Loss: 0.0393\n",
            "Epoch 59/100, Train Loss: 0.0548, Val Loss: 0.0393\n",
            "Epoch 60/100, Train Loss: 0.0524, Val Loss: 0.0393\n",
            "Epoch 61/100, Train Loss: 0.0532, Val Loss: 0.0393\n",
            "Epoch 62/100, Train Loss: 0.0570, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0555, Val Loss: 0.0393\n",
            "Epoch 64/100, Train Loss: 0.0547, Val Loss: 0.0393\n",
            "Epoch 65/100, Train Loss: 0.0525, Val Loss: 0.0393\n",
            "Epoch 66/100, Train Loss: 0.0529, Val Loss: 0.0393\n",
            "Epoch 67/100, Train Loss: 0.0556, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0554, Val Loss: 0.0393\n",
            "Epoch 69/100, Train Loss: 0.0581, Val Loss: 0.0393\n",
            "Epoch 70/100, Train Loss: 0.0551, Val Loss: 0.0393\n",
            "Epoch 71/100, Train Loss: 0.0577, Val Loss: 0.0393\n",
            "Epoch 72/100, Train Loss: 0.0545, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0530, Val Loss: 0.0393\n",
            "Epoch 74/100, Train Loss: 0.0538, Val Loss: 0.0393\n",
            "Epoch 75/100, Train Loss: 0.0578, Val Loss: 0.0393\n",
            "Epoch 76/100, Train Loss: 0.0604, Val Loss: 0.0393\n",
            "Epoch 77/100, Train Loss: 0.0520, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0703, Val Loss: 0.0393\n",
            "Epoch 79/100, Train Loss: 0.0550, Val Loss: 0.0393\n",
            "Epoch 80/100, Train Loss: 0.0553, Val Loss: 0.0393\n",
            "Epoch 81/100, Train Loss: 0.0519, Val Loss: 0.0393\n",
            "Epoch 82/100, Train Loss: 0.0548, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0544, Val Loss: 0.0393\n",
            "Epoch 84/100, Train Loss: 0.0657, Val Loss: 0.0393\n",
            "Epoch 85/100, Train Loss: 0.0532, Val Loss: 0.0393\n",
            "Epoch 86/100, Train Loss: 0.0579, Val Loss: 0.0393\n",
            "Epoch 87/100, Train Loss: 0.0561, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0577, Val Loss: 0.0393\n",
            "Epoch 89/100, Train Loss: 0.0544, Val Loss: 0.0393\n",
            "Epoch 90/100, Train Loss: 0.0541, Val Loss: 0.0393\n",
            "Epoch 91/100, Train Loss: 0.0546, Val Loss: 0.0393\n",
            "Epoch 92/100, Train Loss: 0.0564, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0549, Val Loss: 0.0393\n",
            "Epoch 94/100, Train Loss: 0.0520, Val Loss: 0.0393\n",
            "Epoch 95/100, Train Loss: 0.0534, Val Loss: 0.0393\n",
            "Epoch 96/100, Train Loss: 0.0554, Val Loss: 0.0393\n",
            "Epoch 97/100, Train Loss: 0.0550, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0530, Val Loss: 0.0393\n",
            "Epoch 99/100, Train Loss: 0.0553, Val Loss: 0.0393\n",
            "Epoch 100/100, Train Loss: 0.0545, Val Loss: 0.0393\n",
            "\n",
            "Test iteration 22/28\n",
            "Current training set size: 118 samples\n",
            "Epoch 1/100, Train Loss: 0.1676, Val Loss: 0.1924\n",
            "Epoch 2/100, Train Loss: 0.1474, Val Loss: 0.1896\n",
            "Epoch 3/100, Train Loss: 0.1426, Val Loss: 0.2054\n",
            "Epoch 4/100, Train Loss: 0.1439, Val Loss: 0.1863\n",
            "Epoch 5/100, Train Loss: 0.1426, Val Loss: 0.1715\n",
            "Epoch 6/100, Train Loss: 0.1278, Val Loss: 0.1562\n",
            "Epoch 7/100, Train Loss: 0.1071, Val Loss: 0.0845\n",
            "Epoch 8/100, Train Loss: 0.0791, Val Loss: 0.0365\n",
            "Epoch 9/100, Train Loss: 0.0783, Val Loss: 0.0556\n",
            "Epoch 10/100, Train Loss: 0.0678, Val Loss: 0.0364\n",
            "Epoch 11/100, Train Loss: 0.0782, Val Loss: 0.0557\n",
            "Epoch 12/100, Train Loss: 0.0718, Val Loss: 0.0443\n",
            "Epoch 13/100, Train Loss: 0.0673, Val Loss: 0.0497\n",
            "Epoch 14/100, Train Loss: 0.0729, Val Loss: 0.0388\n",
            "Epoch 15/100, Train Loss: 0.0665, Val Loss: 0.0411\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0603, Val Loss: 0.0407\n",
            "Epoch 17/100, Train Loss: 0.0600, Val Loss: 0.0407\n",
            "Epoch 18/100, Train Loss: 0.0619, Val Loss: 0.0406\n",
            "Epoch 19/100, Train Loss: 0.0611, Val Loss: 0.0405\n",
            "Epoch 20/100, Train Loss: 0.0611, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0636, Val Loss: 0.0404\n",
            "Epoch 22/100, Train Loss: 0.0593, Val Loss: 0.0404\n",
            "Epoch 23/100, Train Loss: 0.0611, Val Loss: 0.0404\n",
            "Epoch 24/100, Train Loss: 0.0614, Val Loss: 0.0404\n",
            "Epoch 25/100, Train Loss: 0.0624, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0609, Val Loss: 0.0404\n",
            "Epoch 27/100, Train Loss: 0.0690, Val Loss: 0.0404\n",
            "Epoch 28/100, Train Loss: 0.0620, Val Loss: 0.0404\n",
            "Epoch 29/100, Train Loss: 0.0626, Val Loss: 0.0404\n",
            "Epoch 30/100, Train Loss: 0.0603, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0621, Val Loss: 0.0404\n",
            "Epoch 32/100, Train Loss: 0.0587, Val Loss: 0.0404\n",
            "Epoch 33/100, Train Loss: 0.0604, Val Loss: 0.0404\n",
            "Epoch 34/100, Train Loss: 0.0619, Val Loss: 0.0404\n",
            "Epoch 35/100, Train Loss: 0.0608, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0627, Val Loss: 0.0404\n",
            "Epoch 37/100, Train Loss: 0.0612, Val Loss: 0.0404\n",
            "Epoch 38/100, Train Loss: 0.0627, Val Loss: 0.0404\n",
            "Epoch 39/100, Train Loss: 0.0622, Val Loss: 0.0404\n",
            "Epoch 40/100, Train Loss: 0.0622, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0636, Val Loss: 0.0404\n",
            "Epoch 42/100, Train Loss: 0.0644, Val Loss: 0.0404\n",
            "Epoch 43/100, Train Loss: 0.0605, Val Loss: 0.0404\n",
            "Epoch 44/100, Train Loss: 0.0621, Val Loss: 0.0404\n",
            "Epoch 45/100, Train Loss: 0.0622, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0617, Val Loss: 0.0404\n",
            "Epoch 47/100, Train Loss: 0.0602, Val Loss: 0.0404\n",
            "Epoch 48/100, Train Loss: 0.0627, Val Loss: 0.0404\n",
            "Epoch 49/100, Train Loss: 0.0620, Val Loss: 0.0404\n",
            "Epoch 50/100, Train Loss: 0.0600, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0650, Val Loss: 0.0404\n",
            "Epoch 52/100, Train Loss: 0.0618, Val Loss: 0.0404\n",
            "Epoch 53/100, Train Loss: 0.0641, Val Loss: 0.0404\n",
            "Epoch 54/100, Train Loss: 0.0650, Val Loss: 0.0404\n",
            "Epoch 55/100, Train Loss: 0.0618, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0680, Val Loss: 0.0404\n",
            "Epoch 57/100, Train Loss: 0.0617, Val Loss: 0.0404\n",
            "Epoch 58/100, Train Loss: 0.0600, Val Loss: 0.0404\n",
            "Epoch 59/100, Train Loss: 0.0660, Val Loss: 0.0404\n",
            "Epoch 60/100, Train Loss: 0.0622, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0619, Val Loss: 0.0404\n",
            "Epoch 62/100, Train Loss: 0.0631, Val Loss: 0.0404\n",
            "Epoch 63/100, Train Loss: 0.0639, Val Loss: 0.0404\n",
            "Epoch 64/100, Train Loss: 0.0615, Val Loss: 0.0404\n",
            "Epoch 65/100, Train Loss: 0.0607, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0607, Val Loss: 0.0404\n",
            "Epoch 67/100, Train Loss: 0.0616, Val Loss: 0.0404\n",
            "Epoch 68/100, Train Loss: 0.0626, Val Loss: 0.0404\n",
            "Epoch 69/100, Train Loss: 0.0623, Val Loss: 0.0404\n",
            "Epoch 70/100, Train Loss: 0.0635, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0626, Val Loss: 0.0404\n",
            "Epoch 72/100, Train Loss: 0.0628, Val Loss: 0.0404\n",
            "Epoch 73/100, Train Loss: 0.0638, Val Loss: 0.0404\n",
            "Epoch 74/100, Train Loss: 0.0611, Val Loss: 0.0404\n",
            "Epoch 75/100, Train Loss: 0.0607, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0615, Val Loss: 0.0404\n",
            "Epoch 77/100, Train Loss: 0.0663, Val Loss: 0.0404\n",
            "Epoch 78/100, Train Loss: 0.0613, Val Loss: 0.0404\n",
            "Epoch 79/100, Train Loss: 0.0626, Val Loss: 0.0404\n",
            "Epoch 80/100, Train Loss: 0.0632, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0619, Val Loss: 0.0404\n",
            "Epoch 82/100, Train Loss: 0.0629, Val Loss: 0.0404\n",
            "Epoch 83/100, Train Loss: 0.0631, Val Loss: 0.0404\n",
            "Epoch 84/100, Train Loss: 0.0611, Val Loss: 0.0404\n",
            "Epoch 85/100, Train Loss: 0.0599, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0620, Val Loss: 0.0404\n",
            "Epoch 87/100, Train Loss: 0.0613, Val Loss: 0.0404\n",
            "Epoch 88/100, Train Loss: 0.0614, Val Loss: 0.0404\n",
            "Epoch 89/100, Train Loss: 0.0596, Val Loss: 0.0404\n",
            "Epoch 90/100, Train Loss: 0.0625, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0622, Val Loss: 0.0404\n",
            "Epoch 92/100, Train Loss: 0.0639, Val Loss: 0.0404\n",
            "Epoch 93/100, Train Loss: 0.0612, Val Loss: 0.0404\n",
            "Epoch 94/100, Train Loss: 0.0631, Val Loss: 0.0404\n",
            "Epoch 95/100, Train Loss: 0.0643, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0631, Val Loss: 0.0404\n",
            "Epoch 97/100, Train Loss: 0.0606, Val Loss: 0.0404\n",
            "Epoch 98/100, Train Loss: 0.0605, Val Loss: 0.0404\n",
            "Epoch 99/100, Train Loss: 0.0625, Val Loss: 0.0404\n",
            "Epoch 100/100, Train Loss: 0.0627, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 23/28\n",
            "Current training set size: 119 samples\n",
            "Epoch 1/100, Train Loss: 0.1583, Val Loss: 0.2147\n",
            "Epoch 2/100, Train Loss: 0.1509, Val Loss: 0.1912\n",
            "Epoch 3/100, Train Loss: 0.1447, Val Loss: 0.1919\n",
            "Epoch 4/100, Train Loss: 0.1428, Val Loss: 0.1826\n",
            "Epoch 5/100, Train Loss: 0.1416, Val Loss: 0.1766\n",
            "Epoch 6/100, Train Loss: 0.1312, Val Loss: 0.1460\n",
            "Epoch 7/100, Train Loss: 0.1131, Val Loss: 0.0914\n",
            "Epoch 8/100, Train Loss: 0.0788, Val Loss: 0.0408\n",
            "Epoch 9/100, Train Loss: 0.0766, Val Loss: 0.0385\n",
            "Epoch 10/100, Train Loss: 0.0684, Val Loss: 0.0326\n",
            "Epoch 11/100, Train Loss: 0.0691, Val Loss: 0.0461\n",
            "Epoch 12/100, Train Loss: 0.0678, Val Loss: 0.0484\n",
            "Epoch 13/100, Train Loss: 0.0713, Val Loss: 0.0458\n",
            "Epoch 14/100, Train Loss: 0.0658, Val Loss: 0.0415\n",
            "Epoch 15/100, Train Loss: 0.0665, Val Loss: 0.0478\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0663, Val Loss: 0.0471\n",
            "Epoch 17/100, Train Loss: 0.0638, Val Loss: 0.0462\n",
            "Epoch 18/100, Train Loss: 0.0623, Val Loss: 0.0456\n",
            "Epoch 19/100, Train Loss: 0.0635, Val Loss: 0.0453\n",
            "Epoch 20/100, Train Loss: 0.0610, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0643, Val Loss: 0.0449\n",
            "Epoch 22/100, Train Loss: 0.0628, Val Loss: 0.0449\n",
            "Epoch 23/100, Train Loss: 0.0655, Val Loss: 0.0449\n",
            "Epoch 24/100, Train Loss: 0.0602, Val Loss: 0.0449\n",
            "Epoch 25/100, Train Loss: 0.0628, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0622, Val Loss: 0.0449\n",
            "Epoch 27/100, Train Loss: 0.0616, Val Loss: 0.0449\n",
            "Epoch 28/100, Train Loss: 0.0624, Val Loss: 0.0449\n",
            "Epoch 29/100, Train Loss: 0.0655, Val Loss: 0.0449\n",
            "Epoch 30/100, Train Loss: 0.0623, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0625, Val Loss: 0.0449\n",
            "Epoch 32/100, Train Loss: 0.0644, Val Loss: 0.0449\n",
            "Epoch 33/100, Train Loss: 0.0623, Val Loss: 0.0449\n",
            "Epoch 34/100, Train Loss: 0.0629, Val Loss: 0.0449\n",
            "Epoch 35/100, Train Loss: 0.0619, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0635, Val Loss: 0.0449\n",
            "Epoch 37/100, Train Loss: 0.0641, Val Loss: 0.0449\n",
            "Epoch 38/100, Train Loss: 0.0637, Val Loss: 0.0449\n",
            "Epoch 39/100, Train Loss: 0.0641, Val Loss: 0.0449\n",
            "Epoch 40/100, Train Loss: 0.0650, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0645, Val Loss: 0.0449\n",
            "Epoch 42/100, Train Loss: 0.0628, Val Loss: 0.0449\n",
            "Epoch 43/100, Train Loss: 0.0643, Val Loss: 0.0449\n",
            "Epoch 44/100, Train Loss: 0.0619, Val Loss: 0.0449\n",
            "Epoch 45/100, Train Loss: 0.0659, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0628, Val Loss: 0.0449\n",
            "Epoch 47/100, Train Loss: 0.0613, Val Loss: 0.0449\n",
            "Epoch 48/100, Train Loss: 0.0622, Val Loss: 0.0449\n",
            "Epoch 49/100, Train Loss: 0.0636, Val Loss: 0.0449\n",
            "Epoch 50/100, Train Loss: 0.0624, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0653, Val Loss: 0.0449\n",
            "Epoch 52/100, Train Loss: 0.0629, Val Loss: 0.0449\n",
            "Epoch 53/100, Train Loss: 0.0636, Val Loss: 0.0449\n",
            "Epoch 54/100, Train Loss: 0.0632, Val Loss: 0.0449\n",
            "Epoch 55/100, Train Loss: 0.0627, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0638, Val Loss: 0.0449\n",
            "Epoch 57/100, Train Loss: 0.0622, Val Loss: 0.0449\n",
            "Epoch 58/100, Train Loss: 0.0634, Val Loss: 0.0449\n",
            "Epoch 59/100, Train Loss: 0.0658, Val Loss: 0.0449\n",
            "Epoch 60/100, Train Loss: 0.0647, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0627, Val Loss: 0.0449\n",
            "Epoch 62/100, Train Loss: 0.0635, Val Loss: 0.0449\n",
            "Epoch 63/100, Train Loss: 0.0640, Val Loss: 0.0449\n",
            "Epoch 64/100, Train Loss: 0.0645, Val Loss: 0.0449\n",
            "Epoch 65/100, Train Loss: 0.0627, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0659, Val Loss: 0.0449\n",
            "Epoch 67/100, Train Loss: 0.0630, Val Loss: 0.0449\n",
            "Epoch 68/100, Train Loss: 0.0613, Val Loss: 0.0449\n",
            "Epoch 69/100, Train Loss: 0.0627, Val Loss: 0.0449\n",
            "Epoch 70/100, Train Loss: 0.0650, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0623, Val Loss: 0.0449\n",
            "Epoch 72/100, Train Loss: 0.0636, Val Loss: 0.0449\n",
            "Epoch 73/100, Train Loss: 0.0651, Val Loss: 0.0449\n",
            "Epoch 74/100, Train Loss: 0.0646, Val Loss: 0.0449\n",
            "Epoch 75/100, Train Loss: 0.0611, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0649, Val Loss: 0.0449\n",
            "Epoch 77/100, Train Loss: 0.0632, Val Loss: 0.0449\n",
            "Epoch 78/100, Train Loss: 0.0627, Val Loss: 0.0449\n",
            "Epoch 79/100, Train Loss: 0.0657, Val Loss: 0.0449\n",
            "Epoch 80/100, Train Loss: 0.0631, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0634, Val Loss: 0.0449\n",
            "Epoch 82/100, Train Loss: 0.0629, Val Loss: 0.0449\n",
            "Epoch 83/100, Train Loss: 0.0646, Val Loss: 0.0449\n",
            "Epoch 84/100, Train Loss: 0.0625, Val Loss: 0.0449\n",
            "Epoch 85/100, Train Loss: 0.0641, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0638, Val Loss: 0.0449\n",
            "Epoch 87/100, Train Loss: 0.0643, Val Loss: 0.0449\n",
            "Epoch 88/100, Train Loss: 0.0608, Val Loss: 0.0449\n",
            "Epoch 89/100, Train Loss: 0.0628, Val Loss: 0.0449\n",
            "Epoch 90/100, Train Loss: 0.0631, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0638, Val Loss: 0.0449\n",
            "Epoch 92/100, Train Loss: 0.0638, Val Loss: 0.0449\n",
            "Epoch 93/100, Train Loss: 0.0649, Val Loss: 0.0449\n",
            "Epoch 94/100, Train Loss: 0.0639, Val Loss: 0.0449\n",
            "Epoch 95/100, Train Loss: 0.0635, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0665, Val Loss: 0.0449\n",
            "Epoch 97/100, Train Loss: 0.0628, Val Loss: 0.0449\n",
            "Epoch 98/100, Train Loss: 0.0647, Val Loss: 0.0449\n",
            "Epoch 99/100, Train Loss: 0.0652, Val Loss: 0.0449\n",
            "Epoch 100/100, Train Loss: 0.0639, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 24/28\n",
            "Current training set size: 120 samples\n",
            "Epoch 1/100, Train Loss: 0.1616, Val Loss: 0.2030\n",
            "Epoch 2/100, Train Loss: 0.1419, Val Loss: 0.1908\n",
            "Epoch 3/100, Train Loss: 0.1480, Val Loss: 0.1902\n",
            "Epoch 4/100, Train Loss: 0.1418, Val Loss: 0.1846\n",
            "Epoch 5/100, Train Loss: 0.1397, Val Loss: 0.1755\n",
            "Epoch 6/100, Train Loss: 0.1268, Val Loss: 0.1411\n",
            "Epoch 7/100, Train Loss: 0.1254, Val Loss: 0.0910\n",
            "Epoch 8/100, Train Loss: 0.0848, Val Loss: 0.0340\n",
            "Epoch 9/100, Train Loss: 0.0753, Val Loss: 0.0449\n",
            "Epoch 10/100, Train Loss: 0.0730, Val Loss: 0.0440\n",
            "Epoch 11/100, Train Loss: 0.0710, Val Loss: 0.0430\n",
            "Epoch 12/100, Train Loss: 0.0690, Val Loss: 0.0332\n",
            "Epoch 13/100, Train Loss: 0.0662, Val Loss: 0.0508\n",
            "Epoch 14/100, Train Loss: 0.0701, Val Loss: 0.0405\n",
            "Epoch 15/100, Train Loss: 0.0669, Val Loss: 0.0447\n",
            "Epoch 16/100, Train Loss: 0.0629, Val Loss: 0.0404\n",
            "Epoch 17/100, Train Loss: 0.0649, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0598, Val Loss: 0.0397\n",
            "Epoch 19/100, Train Loss: 0.0619, Val Loss: 0.0397\n",
            "Epoch 20/100, Train Loss: 0.0624, Val Loss: 0.0397\n",
            "Epoch 21/100, Train Loss: 0.0588, Val Loss: 0.0395\n",
            "Epoch 22/100, Train Loss: 0.0609, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0593, Val Loss: 0.0395\n",
            "Epoch 24/100, Train Loss: 0.0611, Val Loss: 0.0395\n",
            "Epoch 25/100, Train Loss: 0.0600, Val Loss: 0.0395\n",
            "Epoch 26/100, Train Loss: 0.0589, Val Loss: 0.0395\n",
            "Epoch 27/100, Train Loss: 0.0596, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0623, Val Loss: 0.0395\n",
            "Epoch 29/100, Train Loss: 0.0622, Val Loss: 0.0395\n",
            "Epoch 30/100, Train Loss: 0.0614, Val Loss: 0.0395\n",
            "Epoch 31/100, Train Loss: 0.0603, Val Loss: 0.0395\n",
            "Epoch 32/100, Train Loss: 0.0612, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0603, Val Loss: 0.0395\n",
            "Epoch 34/100, Train Loss: 0.0595, Val Loss: 0.0395\n",
            "Epoch 35/100, Train Loss: 0.0607, Val Loss: 0.0395\n",
            "Epoch 36/100, Train Loss: 0.0612, Val Loss: 0.0395\n",
            "Epoch 37/100, Train Loss: 0.0615, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0632, Val Loss: 0.0395\n",
            "Epoch 39/100, Train Loss: 0.0624, Val Loss: 0.0395\n",
            "Epoch 40/100, Train Loss: 0.0616, Val Loss: 0.0395\n",
            "Epoch 41/100, Train Loss: 0.0621, Val Loss: 0.0395\n",
            "Epoch 42/100, Train Loss: 0.0608, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0575, Val Loss: 0.0395\n",
            "Epoch 44/100, Train Loss: 0.0629, Val Loss: 0.0395\n",
            "Epoch 45/100, Train Loss: 0.0607, Val Loss: 0.0395\n",
            "Epoch 46/100, Train Loss: 0.0618, Val Loss: 0.0395\n",
            "Epoch 47/100, Train Loss: 0.0600, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0612, Val Loss: 0.0395\n",
            "Epoch 49/100, Train Loss: 0.0595, Val Loss: 0.0395\n",
            "Epoch 50/100, Train Loss: 0.0621, Val Loss: 0.0395\n",
            "Epoch 51/100, Train Loss: 0.0611, Val Loss: 0.0395\n",
            "Epoch 52/100, Train Loss: 0.0601, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0588, Val Loss: 0.0395\n",
            "Epoch 54/100, Train Loss: 0.0590, Val Loss: 0.0395\n",
            "Epoch 55/100, Train Loss: 0.0592, Val Loss: 0.0395\n",
            "Epoch 56/100, Train Loss: 0.0608, Val Loss: 0.0395\n",
            "Epoch 57/100, Train Loss: 0.0630, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0621, Val Loss: 0.0395\n",
            "Epoch 59/100, Train Loss: 0.0601, Val Loss: 0.0395\n",
            "Epoch 60/100, Train Loss: 0.0605, Val Loss: 0.0395\n",
            "Epoch 61/100, Train Loss: 0.0607, Val Loss: 0.0395\n",
            "Epoch 62/100, Train Loss: 0.0591, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0611, Val Loss: 0.0395\n",
            "Epoch 64/100, Train Loss: 0.0601, Val Loss: 0.0395\n",
            "Epoch 65/100, Train Loss: 0.0603, Val Loss: 0.0395\n",
            "Epoch 66/100, Train Loss: 0.0607, Val Loss: 0.0395\n",
            "Epoch 67/100, Train Loss: 0.0601, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0606, Val Loss: 0.0395\n",
            "Epoch 69/100, Train Loss: 0.0612, Val Loss: 0.0395\n",
            "Epoch 70/100, Train Loss: 0.0603, Val Loss: 0.0395\n",
            "Epoch 71/100, Train Loss: 0.0588, Val Loss: 0.0395\n",
            "Epoch 72/100, Train Loss: 0.0600, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0610, Val Loss: 0.0395\n",
            "Epoch 74/100, Train Loss: 0.0601, Val Loss: 0.0395\n",
            "Epoch 75/100, Train Loss: 0.0606, Val Loss: 0.0395\n",
            "Epoch 76/100, Train Loss: 0.0602, Val Loss: 0.0395\n",
            "Epoch 77/100, Train Loss: 0.0591, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0597, Val Loss: 0.0395\n",
            "Epoch 79/100, Train Loss: 0.0604, Val Loss: 0.0395\n",
            "Epoch 80/100, Train Loss: 0.0611, Val Loss: 0.0395\n",
            "Epoch 81/100, Train Loss: 0.0620, Val Loss: 0.0395\n",
            "Epoch 82/100, Train Loss: 0.0622, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0633, Val Loss: 0.0395\n",
            "Epoch 84/100, Train Loss: 0.0618, Val Loss: 0.0395\n",
            "Epoch 85/100, Train Loss: 0.0628, Val Loss: 0.0395\n",
            "Epoch 86/100, Train Loss: 0.0622, Val Loss: 0.0395\n",
            "Epoch 87/100, Train Loss: 0.0602, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0610, Val Loss: 0.0395\n",
            "Epoch 89/100, Train Loss: 0.0597, Val Loss: 0.0395\n",
            "Epoch 90/100, Train Loss: 0.0602, Val Loss: 0.0395\n",
            "Epoch 91/100, Train Loss: 0.0596, Val Loss: 0.0395\n",
            "Epoch 92/100, Train Loss: 0.0614, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0617, Val Loss: 0.0395\n",
            "Epoch 94/100, Train Loss: 0.0602, Val Loss: 0.0395\n",
            "Epoch 95/100, Train Loss: 0.0608, Val Loss: 0.0395\n",
            "Epoch 96/100, Train Loss: 0.0607, Val Loss: 0.0395\n",
            "Epoch 97/100, Train Loss: 0.0624, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0605, Val Loss: 0.0395\n",
            "Epoch 99/100, Train Loss: 0.0584, Val Loss: 0.0395\n",
            "Epoch 100/100, Train Loss: 0.0608, Val Loss: 0.0395\n",
            "\n",
            "Test iteration 25/28\n",
            "Current training set size: 121 samples\n",
            "Epoch 1/100, Train Loss: 0.1628, Val Loss: 0.2061\n",
            "Epoch 2/100, Train Loss: 0.1515, Val Loss: 0.1900\n",
            "Epoch 3/100, Train Loss: 0.1424, Val Loss: 0.1920\n",
            "Epoch 4/100, Train Loss: 0.1417, Val Loss: 0.1809\n",
            "Epoch 5/100, Train Loss: 0.1391, Val Loss: 0.1733\n",
            "Epoch 6/100, Train Loss: 0.1243, Val Loss: 0.1501\n",
            "Epoch 7/100, Train Loss: 0.1040, Val Loss: 0.0653\n",
            "Epoch 8/100, Train Loss: 0.0722, Val Loss: 0.0498\n",
            "Epoch 9/100, Train Loss: 0.0729, Val Loss: 0.0439\n",
            "Epoch 10/100, Train Loss: 0.0724, Val Loss: 0.0420\n",
            "Epoch 11/100, Train Loss: 0.0691, Val Loss: 0.0418\n",
            "Epoch 12/100, Train Loss: 0.0733, Val Loss: 0.0473\n",
            "Epoch 13/100, Train Loss: 0.0722, Val Loss: 0.0438\n",
            "Epoch 14/100, Train Loss: 0.0681, Val Loss: 0.0427\n",
            "Epoch 15/100, Train Loss: 0.0677, Val Loss: 0.0356\n",
            "Epoch 16/100, Train Loss: 0.0648, Val Loss: 0.0442\n",
            "Epoch 17/100, Train Loss: 0.0631, Val Loss: 0.0423\n",
            "Epoch 18/100, Train Loss: 0.0591, Val Loss: 0.0364\n",
            "Epoch 19/100, Train Loss: 0.0632, Val Loss: 0.0341\n",
            "Epoch 20/100, Train Loss: 0.0597, Val Loss: 0.0435\n",
            "Epoch 21/100, Train Loss: 0.0556, Val Loss: 0.0393\n",
            "Epoch 22/100, Train Loss: 0.0571, Val Loss: 0.0462\n",
            "Epoch 23/100, Train Loss: 0.0575, Val Loss: 0.0421\n",
            "Epoch 24/100, Train Loss: 0.0542, Val Loss: 0.0373\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0518, Val Loss: 0.0377\n",
            "Epoch 26/100, Train Loss: 0.0516, Val Loss: 0.0379\n",
            "Epoch 27/100, Train Loss: 0.0529, Val Loss: 0.0381\n",
            "Epoch 28/100, Train Loss: 0.0518, Val Loss: 0.0383\n",
            "Epoch 29/100, Train Loss: 0.0492, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0501, Val Loss: 0.0385\n",
            "Epoch 31/100, Train Loss: 0.0489, Val Loss: 0.0385\n",
            "Epoch 32/100, Train Loss: 0.0524, Val Loss: 0.0385\n",
            "Epoch 33/100, Train Loss: 0.0615, Val Loss: 0.0385\n",
            "Epoch 34/100, Train Loss: 0.0530, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0501, Val Loss: 0.0385\n",
            "Epoch 36/100, Train Loss: 0.0505, Val Loss: 0.0385\n",
            "Epoch 37/100, Train Loss: 0.0607, Val Loss: 0.0385\n",
            "Epoch 38/100, Train Loss: 0.0495, Val Loss: 0.0385\n",
            "Epoch 39/100, Train Loss: 0.0499, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0528, Val Loss: 0.0385\n",
            "Epoch 41/100, Train Loss: 0.0496, Val Loss: 0.0385\n",
            "Epoch 42/100, Train Loss: 0.0674, Val Loss: 0.0385\n",
            "Epoch 43/100, Train Loss: 0.0485, Val Loss: 0.0385\n",
            "Epoch 44/100, Train Loss: 0.0532, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0518, Val Loss: 0.0385\n",
            "Epoch 46/100, Train Loss: 0.0507, Val Loss: 0.0385\n",
            "Epoch 47/100, Train Loss: 0.0517, Val Loss: 0.0385\n",
            "Epoch 48/100, Train Loss: 0.0534, Val Loss: 0.0385\n",
            "Epoch 49/100, Train Loss: 0.0505, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0697, Val Loss: 0.0385\n",
            "Epoch 51/100, Train Loss: 0.0506, Val Loss: 0.0385\n",
            "Epoch 52/100, Train Loss: 0.0507, Val Loss: 0.0385\n",
            "Epoch 53/100, Train Loss: 0.0526, Val Loss: 0.0385\n",
            "Epoch 54/100, Train Loss: 0.0505, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0499, Val Loss: 0.0385\n",
            "Epoch 56/100, Train Loss: 0.0524, Val Loss: 0.0385\n",
            "Epoch 57/100, Train Loss: 0.0650, Val Loss: 0.0385\n",
            "Epoch 58/100, Train Loss: 0.0504, Val Loss: 0.0385\n",
            "Epoch 59/100, Train Loss: 0.0515, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0506, Val Loss: 0.0385\n",
            "Epoch 61/100, Train Loss: 0.0484, Val Loss: 0.0385\n",
            "Epoch 62/100, Train Loss: 0.0529, Val Loss: 0.0385\n",
            "Epoch 63/100, Train Loss: 0.0493, Val Loss: 0.0385\n",
            "Epoch 64/100, Train Loss: 0.0505, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0491, Val Loss: 0.0385\n",
            "Epoch 66/100, Train Loss: 0.0537, Val Loss: 0.0385\n",
            "Epoch 67/100, Train Loss: 0.0497, Val Loss: 0.0385\n",
            "Epoch 68/100, Train Loss: 0.0611, Val Loss: 0.0385\n",
            "Epoch 69/100, Train Loss: 0.0511, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0516, Val Loss: 0.0385\n",
            "Epoch 71/100, Train Loss: 0.0529, Val Loss: 0.0385\n",
            "Epoch 72/100, Train Loss: 0.0510, Val Loss: 0.0385\n",
            "Epoch 73/100, Train Loss: 0.0523, Val Loss: 0.0385\n",
            "Epoch 74/100, Train Loss: 0.0505, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0492, Val Loss: 0.0385\n",
            "Epoch 76/100, Train Loss: 0.0537, Val Loss: 0.0385\n",
            "Epoch 77/100, Train Loss: 0.0492, Val Loss: 0.0385\n",
            "Epoch 78/100, Train Loss: 0.0636, Val Loss: 0.0385\n",
            "Epoch 79/100, Train Loss: 0.0506, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0529, Val Loss: 0.0385\n",
            "Epoch 81/100, Train Loss: 0.0531, Val Loss: 0.0385\n",
            "Epoch 82/100, Train Loss: 0.0504, Val Loss: 0.0385\n",
            "Epoch 83/100, Train Loss: 0.0541, Val Loss: 0.0385\n",
            "Epoch 84/100, Train Loss: 0.0500, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0498, Val Loss: 0.0385\n",
            "Epoch 86/100, Train Loss: 0.0533, Val Loss: 0.0385\n",
            "Epoch 87/100, Train Loss: 0.0504, Val Loss: 0.0385\n",
            "Epoch 88/100, Train Loss: 0.0520, Val Loss: 0.0385\n",
            "Epoch 89/100, Train Loss: 0.0514, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0516, Val Loss: 0.0385\n",
            "Epoch 91/100, Train Loss: 0.0571, Val Loss: 0.0385\n",
            "Epoch 92/100, Train Loss: 0.0519, Val Loss: 0.0385\n",
            "Epoch 93/100, Train Loss: 0.0513, Val Loss: 0.0385\n",
            "Epoch 94/100, Train Loss: 0.0526, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0509, Val Loss: 0.0385\n",
            "Epoch 96/100, Train Loss: 0.0527, Val Loss: 0.0385\n",
            "Epoch 97/100, Train Loss: 0.0513, Val Loss: 0.0385\n",
            "Epoch 98/100, Train Loss: 0.0526, Val Loss: 0.0385\n",
            "Epoch 99/100, Train Loss: 0.0537, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0514, Val Loss: 0.0385\n",
            "\n",
            "Test iteration 26/28\n",
            "Current training set size: 122 samples\n",
            "Epoch 1/100, Train Loss: 0.1565, Val Loss: 0.2062\n",
            "Epoch 2/100, Train Loss: 0.1470, Val Loss: 0.1902\n",
            "Epoch 3/100, Train Loss: 0.1446, Val Loss: 0.1872\n",
            "Epoch 4/100, Train Loss: 0.1405, Val Loss: 0.1841\n",
            "Epoch 5/100, Train Loss: 0.1325, Val Loss: 0.1606\n",
            "Epoch 6/100, Train Loss: 0.1241, Val Loss: 0.1256\n",
            "Epoch 7/100, Train Loss: 0.1026, Val Loss: 0.0540\n",
            "Epoch 8/100, Train Loss: 0.0746, Val Loss: 0.0505\n",
            "Epoch 9/100, Train Loss: 0.0732, Val Loss: 0.0372\n",
            "Epoch 10/100, Train Loss: 0.0708, Val Loss: 0.0381\n",
            "Epoch 11/100, Train Loss: 0.0685, Val Loss: 0.0447\n",
            "Epoch 12/100, Train Loss: 0.0734, Val Loss: 0.0536\n",
            "Epoch 13/100, Train Loss: 0.0740, Val Loss: 0.0419\n",
            "Epoch 14/100, Train Loss: 0.0737, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 15/100, Train Loss: 0.0760, Val Loss: 0.0423\n",
            "Epoch 16/100, Train Loss: 0.0718, Val Loss: 0.0421\n",
            "Epoch 17/100, Train Loss: 0.0685, Val Loss: 0.0419\n",
            "Epoch 18/100, Train Loss: 0.0648, Val Loss: 0.0418\n",
            "Epoch 19/100, Train Loss: 0.0638, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0658, Val Loss: 0.0417\n",
            "Epoch 21/100, Train Loss: 0.0639, Val Loss: 0.0417\n",
            "Epoch 22/100, Train Loss: 0.0661, Val Loss: 0.0417\n",
            "Epoch 23/100, Train Loss: 0.0645, Val Loss: 0.0417\n",
            "Epoch 24/100, Train Loss: 0.0652, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0632, Val Loss: 0.0417\n",
            "Epoch 26/100, Train Loss: 0.0707, Val Loss: 0.0417\n",
            "Epoch 27/100, Train Loss: 0.0633, Val Loss: 0.0417\n",
            "Epoch 28/100, Train Loss: 0.0655, Val Loss: 0.0417\n",
            "Epoch 29/100, Train Loss: 0.0643, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0654, Val Loss: 0.0417\n",
            "Epoch 31/100, Train Loss: 0.0659, Val Loss: 0.0417\n",
            "Epoch 32/100, Train Loss: 0.0648, Val Loss: 0.0417\n",
            "Epoch 33/100, Train Loss: 0.0654, Val Loss: 0.0417\n",
            "Epoch 34/100, Train Loss: 0.0641, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0649, Val Loss: 0.0417\n",
            "Epoch 36/100, Train Loss: 0.0732, Val Loss: 0.0417\n",
            "Epoch 37/100, Train Loss: 0.0640, Val Loss: 0.0417\n",
            "Epoch 38/100, Train Loss: 0.0649, Val Loss: 0.0417\n",
            "Epoch 39/100, Train Loss: 0.0661, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0636, Val Loss: 0.0417\n",
            "Epoch 41/100, Train Loss: 0.0645, Val Loss: 0.0417\n",
            "Epoch 42/100, Train Loss: 0.0648, Val Loss: 0.0417\n",
            "Epoch 43/100, Train Loss: 0.0645, Val Loss: 0.0417\n",
            "Epoch 44/100, Train Loss: 0.0628, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0634, Val Loss: 0.0417\n",
            "Epoch 46/100, Train Loss: 0.0640, Val Loss: 0.0417\n",
            "Epoch 47/100, Train Loss: 0.0625, Val Loss: 0.0417\n",
            "Epoch 48/100, Train Loss: 0.0650, Val Loss: 0.0417\n",
            "Epoch 49/100, Train Loss: 0.0634, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0649, Val Loss: 0.0417\n",
            "Epoch 51/100, Train Loss: 0.0705, Val Loss: 0.0417\n",
            "Epoch 52/100, Train Loss: 0.0666, Val Loss: 0.0417\n",
            "Epoch 53/100, Train Loss: 0.0639, Val Loss: 0.0417\n",
            "Epoch 54/100, Train Loss: 0.0649, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0650, Val Loss: 0.0417\n",
            "Epoch 56/100, Train Loss: 0.0625, Val Loss: 0.0417\n",
            "Epoch 57/100, Train Loss: 0.0659, Val Loss: 0.0417\n",
            "Epoch 58/100, Train Loss: 0.0641, Val Loss: 0.0417\n",
            "Epoch 59/100, Train Loss: 0.0648, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0691, Val Loss: 0.0417\n",
            "Epoch 61/100, Train Loss: 0.0628, Val Loss: 0.0417\n",
            "Epoch 62/100, Train Loss: 0.0628, Val Loss: 0.0417\n",
            "Epoch 63/100, Train Loss: 0.0654, Val Loss: 0.0417\n",
            "Epoch 64/100, Train Loss: 0.0659, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0650, Val Loss: 0.0417\n",
            "Epoch 66/100, Train Loss: 0.0625, Val Loss: 0.0417\n",
            "Epoch 67/100, Train Loss: 0.0648, Val Loss: 0.0417\n",
            "Epoch 68/100, Train Loss: 0.0654, Val Loss: 0.0417\n",
            "Epoch 69/100, Train Loss: 0.0634, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0636, Val Loss: 0.0417\n",
            "Epoch 71/100, Train Loss: 0.0647, Val Loss: 0.0417\n",
            "Epoch 72/100, Train Loss: 0.0632, Val Loss: 0.0417\n",
            "Epoch 73/100, Train Loss: 0.0633, Val Loss: 0.0417\n",
            "Epoch 74/100, Train Loss: 0.0640, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0659, Val Loss: 0.0417\n",
            "Epoch 76/100, Train Loss: 0.0644, Val Loss: 0.0417\n",
            "Epoch 77/100, Train Loss: 0.0642, Val Loss: 0.0417\n",
            "Epoch 78/100, Train Loss: 0.0659, Val Loss: 0.0417\n",
            "Epoch 79/100, Train Loss: 0.0642, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0664, Val Loss: 0.0417\n",
            "Epoch 81/100, Train Loss: 0.0637, Val Loss: 0.0417\n",
            "Epoch 82/100, Train Loss: 0.0648, Val Loss: 0.0417\n",
            "Epoch 83/100, Train Loss: 0.0650, Val Loss: 0.0417\n",
            "Epoch 84/100, Train Loss: 0.0656, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0637, Val Loss: 0.0417\n",
            "Epoch 86/100, Train Loss: 0.0633, Val Loss: 0.0417\n",
            "Epoch 87/100, Train Loss: 0.0678, Val Loss: 0.0417\n",
            "Epoch 88/100, Train Loss: 0.0649, Val Loss: 0.0417\n",
            "Epoch 89/100, Train Loss: 0.0631, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0623, Val Loss: 0.0417\n",
            "Epoch 91/100, Train Loss: 0.0630, Val Loss: 0.0417\n",
            "Epoch 92/100, Train Loss: 0.0662, Val Loss: 0.0417\n",
            "Epoch 93/100, Train Loss: 0.0641, Val Loss: 0.0417\n",
            "Epoch 94/100, Train Loss: 0.0646, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0623, Val Loss: 0.0417\n",
            "Epoch 96/100, Train Loss: 0.0674, Val Loss: 0.0417\n",
            "Epoch 97/100, Train Loss: 0.0654, Val Loss: 0.0417\n",
            "Epoch 98/100, Train Loss: 0.0666, Val Loss: 0.0417\n",
            "Epoch 99/100, Train Loss: 0.0613, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0626, Val Loss: 0.0417\n",
            "\n",
            "Test iteration 27/28\n",
            "Current training set size: 123 samples\n",
            "Epoch 1/100, Train Loss: 0.1539, Val Loss: 0.2150\n",
            "Epoch 2/100, Train Loss: 0.1469, Val Loss: 0.1903\n",
            "Epoch 3/100, Train Loss: 0.1432, Val Loss: 0.1940\n",
            "Epoch 4/100, Train Loss: 0.1406, Val Loss: 0.2047\n",
            "Epoch 5/100, Train Loss: 0.1387, Val Loss: 0.1821\n",
            "Epoch 6/100, Train Loss: 0.1254, Val Loss: 0.1530\n",
            "Epoch 7/100, Train Loss: 0.0954, Val Loss: 0.0709\n",
            "Epoch 8/100, Train Loss: 0.0719, Val Loss: 0.0374\n",
            "Epoch 9/100, Train Loss: 0.0750, Val Loss: 0.0404\n",
            "Epoch 10/100, Train Loss: 0.0707, Val Loss: 0.0445\n",
            "Epoch 11/100, Train Loss: 0.0677, Val Loss: 0.0396\n",
            "Epoch 12/100, Train Loss: 0.0718, Val Loss: 0.0432\n",
            "Epoch 13/100, Train Loss: 0.0697, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 14/100, Train Loss: 0.0632, Val Loss: 0.0395\n",
            "Epoch 15/100, Train Loss: 0.0636, Val Loss: 0.0395\n",
            "Epoch 16/100, Train Loss: 0.0645, Val Loss: 0.0396\n",
            "Epoch 17/100, Train Loss: 0.0664, Val Loss: 0.0396\n",
            "Epoch 18/100, Train Loss: 0.0628, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0655, Val Loss: 0.0395\n",
            "Epoch 20/100, Train Loss: 0.0631, Val Loss: 0.0395\n",
            "Epoch 21/100, Train Loss: 0.0656, Val Loss: 0.0395\n",
            "Epoch 22/100, Train Loss: 0.0631, Val Loss: 0.0395\n",
            "Epoch 23/100, Train Loss: 0.0631, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0644, Val Loss: 0.0395\n",
            "Epoch 25/100, Train Loss: 0.0628, Val Loss: 0.0395\n",
            "Epoch 26/100, Train Loss: 0.0633, Val Loss: 0.0395\n",
            "Epoch 27/100, Train Loss: 0.0670, Val Loss: 0.0395\n",
            "Epoch 28/100, Train Loss: 0.0633, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0643, Val Loss: 0.0395\n",
            "Epoch 30/100, Train Loss: 0.0657, Val Loss: 0.0395\n",
            "Epoch 31/100, Train Loss: 0.0642, Val Loss: 0.0395\n",
            "Epoch 32/100, Train Loss: 0.0640, Val Loss: 0.0395\n",
            "Epoch 33/100, Train Loss: 0.0640, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0630, Val Loss: 0.0395\n",
            "Epoch 35/100, Train Loss: 0.0657, Val Loss: 0.0395\n",
            "Epoch 36/100, Train Loss: 0.0628, Val Loss: 0.0395\n",
            "Epoch 37/100, Train Loss: 0.0644, Val Loss: 0.0395\n",
            "Epoch 38/100, Train Loss: 0.0651, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0640, Val Loss: 0.0395\n",
            "Epoch 40/100, Train Loss: 0.0621, Val Loss: 0.0395\n",
            "Epoch 41/100, Train Loss: 0.0643, Val Loss: 0.0395\n",
            "Epoch 42/100, Train Loss: 0.0650, Val Loss: 0.0395\n",
            "Epoch 43/100, Train Loss: 0.0629, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0622, Val Loss: 0.0395\n",
            "Epoch 45/100, Train Loss: 0.0661, Val Loss: 0.0395\n",
            "Epoch 46/100, Train Loss: 0.0637, Val Loss: 0.0395\n",
            "Epoch 47/100, Train Loss: 0.0632, Val Loss: 0.0395\n",
            "Epoch 48/100, Train Loss: 0.0636, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0641, Val Loss: 0.0395\n",
            "Epoch 50/100, Train Loss: 0.0625, Val Loss: 0.0395\n",
            "Epoch 51/100, Train Loss: 0.0646, Val Loss: 0.0395\n",
            "Epoch 52/100, Train Loss: 0.0641, Val Loss: 0.0395\n",
            "Epoch 53/100, Train Loss: 0.0646, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0629, Val Loss: 0.0395\n",
            "Epoch 55/100, Train Loss: 0.0656, Val Loss: 0.0395\n",
            "Epoch 56/100, Train Loss: 0.0631, Val Loss: 0.0395\n",
            "Epoch 57/100, Train Loss: 0.0631, Val Loss: 0.0395\n",
            "Epoch 58/100, Train Loss: 0.0657, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0646, Val Loss: 0.0395\n",
            "Epoch 60/100, Train Loss: 0.0648, Val Loss: 0.0395\n",
            "Epoch 61/100, Train Loss: 0.0642, Val Loss: 0.0395\n",
            "Epoch 62/100, Train Loss: 0.0658, Val Loss: 0.0395\n",
            "Epoch 63/100, Train Loss: 0.0649, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0643, Val Loss: 0.0395\n",
            "Epoch 65/100, Train Loss: 0.0645, Val Loss: 0.0395\n",
            "Epoch 66/100, Train Loss: 0.0640, Val Loss: 0.0395\n",
            "Epoch 67/100, Train Loss: 0.0632, Val Loss: 0.0395\n",
            "Epoch 68/100, Train Loss: 0.0634, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0647, Val Loss: 0.0395\n",
            "Epoch 70/100, Train Loss: 0.0622, Val Loss: 0.0395\n",
            "Epoch 71/100, Train Loss: 0.0642, Val Loss: 0.0395\n",
            "Epoch 72/100, Train Loss: 0.0641, Val Loss: 0.0395\n",
            "Epoch 73/100, Train Loss: 0.0644, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0655, Val Loss: 0.0395\n",
            "Epoch 75/100, Train Loss: 0.0628, Val Loss: 0.0395\n",
            "Epoch 76/100, Train Loss: 0.0644, Val Loss: 0.0395\n",
            "Epoch 77/100, Train Loss: 0.0654, Val Loss: 0.0395\n",
            "Epoch 78/100, Train Loss: 0.0630, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0645, Val Loss: 0.0395\n",
            "Epoch 80/100, Train Loss: 0.0620, Val Loss: 0.0395\n",
            "Epoch 81/100, Train Loss: 0.0630, Val Loss: 0.0395\n",
            "Epoch 82/100, Train Loss: 0.0639, Val Loss: 0.0395\n",
            "Epoch 83/100, Train Loss: 0.0646, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0622, Val Loss: 0.0395\n",
            "Epoch 85/100, Train Loss: 0.0669, Val Loss: 0.0395\n",
            "Epoch 86/100, Train Loss: 0.0622, Val Loss: 0.0395\n",
            "Epoch 87/100, Train Loss: 0.0623, Val Loss: 0.0395\n",
            "Epoch 88/100, Train Loss: 0.0652, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0648, Val Loss: 0.0395\n",
            "Epoch 90/100, Train Loss: 0.0627, Val Loss: 0.0395\n",
            "Epoch 91/100, Train Loss: 0.0636, Val Loss: 0.0395\n",
            "Epoch 92/100, Train Loss: 0.0647, Val Loss: 0.0395\n",
            "Epoch 93/100, Train Loss: 0.0643, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0630, Val Loss: 0.0395\n",
            "Epoch 95/100, Train Loss: 0.0655, Val Loss: 0.0395\n",
            "Epoch 96/100, Train Loss: 0.0655, Val Loss: 0.0395\n",
            "Epoch 97/100, Train Loss: 0.0620, Val Loss: 0.0395\n",
            "Epoch 98/100, Train Loss: 0.0639, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0635, Val Loss: 0.0395\n",
            "Epoch 100/100, Train Loss: 0.0641, Val Loss: 0.0395\n",
            "\n",
            "Test iteration 28/28\n",
            "Current training set size: 124 samples\n",
            "Epoch 1/100, Train Loss: 0.1667, Val Loss: 0.1968\n",
            "Epoch 2/100, Train Loss: 0.1489, Val Loss: 0.1973\n",
            "Epoch 3/100, Train Loss: 0.1457, Val Loss: 0.1840\n",
            "Epoch 4/100, Train Loss: 0.1441, Val Loss: 0.1828\n",
            "Epoch 5/100, Train Loss: 0.1361, Val Loss: 0.1775\n",
            "Epoch 6/100, Train Loss: 0.1232, Val Loss: 0.1310\n",
            "Epoch 7/100, Train Loss: 0.0878, Val Loss: 0.0478\n",
            "Epoch 8/100, Train Loss: 0.0719, Val Loss: 0.0520\n",
            "Epoch 9/100, Train Loss: 0.0763, Val Loss: 0.0455\n",
            "Epoch 10/100, Train Loss: 0.0671, Val Loss: 0.0353\n",
            "Epoch 11/100, Train Loss: 0.0710, Val Loss: 0.0452\n",
            "Epoch 12/100, Train Loss: 0.0737, Val Loss: 0.0497\n",
            "Epoch 13/100, Train Loss: 0.0675, Val Loss: 0.0443\n",
            "Epoch 14/100, Train Loss: 0.0662, Val Loss: 0.0403\n",
            "Epoch 15/100, Train Loss: 0.0705, Val Loss: 0.0412\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0704, Val Loss: 0.0401\n",
            "Epoch 17/100, Train Loss: 0.0658, Val Loss: 0.0395\n",
            "Epoch 18/100, Train Loss: 0.0659, Val Loss: 0.0390\n",
            "Epoch 19/100, Train Loss: 0.0655, Val Loss: 0.0389\n",
            "Epoch 20/100, Train Loss: 0.0647, Val Loss: 0.0388\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0650, Val Loss: 0.0388\n",
            "Epoch 22/100, Train Loss: 0.0650, Val Loss: 0.0388\n",
            "Epoch 23/100, Train Loss: 0.0621, Val Loss: 0.0388\n",
            "Epoch 24/100, Train Loss: 0.0662, Val Loss: 0.0388\n",
            "Epoch 25/100, Train Loss: 0.0630, Val Loss: 0.0388\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0643, Val Loss: 0.0388\n",
            "Epoch 27/100, Train Loss: 0.0644, Val Loss: 0.0388\n",
            "Epoch 28/100, Train Loss: 0.0652, Val Loss: 0.0388\n",
            "Epoch 29/100, Train Loss: 0.0636, Val Loss: 0.0388\n",
            "Epoch 30/100, Train Loss: 0.0650, Val Loss: 0.0388\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0639, Val Loss: 0.0388\n",
            "Epoch 32/100, Train Loss: 0.0641, Val Loss: 0.0388\n",
            "Epoch 33/100, Train Loss: 0.0636, Val Loss: 0.0388\n",
            "Epoch 34/100, Train Loss: 0.0627, Val Loss: 0.0388\n",
            "Epoch 35/100, Train Loss: 0.0629, Val Loss: 0.0388\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0665, Val Loss: 0.0388\n",
            "Epoch 37/100, Train Loss: 0.0648, Val Loss: 0.0388\n",
            "Epoch 38/100, Train Loss: 0.0644, Val Loss: 0.0388\n",
            "Epoch 39/100, Train Loss: 0.0648, Val Loss: 0.0388\n",
            "Epoch 40/100, Train Loss: 0.0649, Val Loss: 0.0388\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0641, Val Loss: 0.0388\n",
            "Epoch 42/100, Train Loss: 0.0620, Val Loss: 0.0388\n",
            "Epoch 43/100, Train Loss: 0.0656, Val Loss: 0.0388\n",
            "Epoch 44/100, Train Loss: 0.0648, Val Loss: 0.0388\n",
            "Epoch 45/100, Train Loss: 0.0644, Val Loss: 0.0388\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0625, Val Loss: 0.0388\n",
            "Epoch 47/100, Train Loss: 0.0663, Val Loss: 0.0388\n",
            "Epoch 48/100, Train Loss: 0.0637, Val Loss: 0.0388\n",
            "Epoch 49/100, Train Loss: 0.0621, Val Loss: 0.0388\n",
            "Epoch 50/100, Train Loss: 0.0650, Val Loss: 0.0388\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0654, Val Loss: 0.0388\n",
            "Epoch 52/100, Train Loss: 0.0624, Val Loss: 0.0388\n",
            "Epoch 53/100, Train Loss: 0.0651, Val Loss: 0.0388\n",
            "Epoch 54/100, Train Loss: 0.0649, Val Loss: 0.0388\n",
            "Epoch 55/100, Train Loss: 0.0641, Val Loss: 0.0388\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0652, Val Loss: 0.0388\n",
            "Epoch 57/100, Train Loss: 0.0629, Val Loss: 0.0388\n",
            "Epoch 58/100, Train Loss: 0.0624, Val Loss: 0.0388\n",
            "Epoch 59/100, Train Loss: 0.0640, Val Loss: 0.0388\n",
            "Epoch 60/100, Train Loss: 0.0639, Val Loss: 0.0388\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0649, Val Loss: 0.0388\n",
            "Epoch 62/100, Train Loss: 0.0647, Val Loss: 0.0388\n",
            "Epoch 63/100, Train Loss: 0.0660, Val Loss: 0.0388\n",
            "Epoch 64/100, Train Loss: 0.0626, Val Loss: 0.0388\n",
            "Epoch 65/100, Train Loss: 0.0634, Val Loss: 0.0388\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0651, Val Loss: 0.0388\n",
            "Epoch 67/100, Train Loss: 0.0642, Val Loss: 0.0388\n",
            "Epoch 68/100, Train Loss: 0.0641, Val Loss: 0.0388\n",
            "Epoch 69/100, Train Loss: 0.0649, Val Loss: 0.0388\n",
            "Epoch 70/100, Train Loss: 0.0647, Val Loss: 0.0388\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0657, Val Loss: 0.0388\n",
            "Epoch 72/100, Train Loss: 0.0624, Val Loss: 0.0388\n",
            "Epoch 73/100, Train Loss: 0.0614, Val Loss: 0.0388\n",
            "Epoch 74/100, Train Loss: 0.0640, Val Loss: 0.0388\n",
            "Epoch 75/100, Train Loss: 0.0634, Val Loss: 0.0388\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0663, Val Loss: 0.0388\n",
            "Epoch 77/100, Train Loss: 0.0628, Val Loss: 0.0388\n",
            "Epoch 78/100, Train Loss: 0.0633, Val Loss: 0.0388\n",
            "Epoch 79/100, Train Loss: 0.0636, Val Loss: 0.0388\n",
            "Epoch 80/100, Train Loss: 0.0651, Val Loss: 0.0388\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0634, Val Loss: 0.0388\n",
            "Epoch 82/100, Train Loss: 0.0625, Val Loss: 0.0388\n",
            "Epoch 83/100, Train Loss: 0.0638, Val Loss: 0.0388\n",
            "Epoch 84/100, Train Loss: 0.0633, Val Loss: 0.0388\n",
            "Epoch 85/100, Train Loss: 0.0626, Val Loss: 0.0388\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0649, Val Loss: 0.0388\n",
            "Epoch 87/100, Train Loss: 0.0639, Val Loss: 0.0388\n",
            "Epoch 88/100, Train Loss: 0.0650, Val Loss: 0.0388\n",
            "Epoch 89/100, Train Loss: 0.0658, Val Loss: 0.0388\n",
            "Epoch 90/100, Train Loss: 0.0644, Val Loss: 0.0388\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0651, Val Loss: 0.0388\n",
            "Epoch 92/100, Train Loss: 0.0630, Val Loss: 0.0388\n",
            "Epoch 93/100, Train Loss: 0.0653, Val Loss: 0.0388\n",
            "Epoch 94/100, Train Loss: 0.0631, Val Loss: 0.0388\n",
            "Epoch 95/100, Train Loss: 0.0649, Val Loss: 0.0388\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0623, Val Loss: 0.0388\n",
            "Epoch 97/100, Train Loss: 0.0630, Val Loss: 0.0388\n",
            "Epoch 98/100, Train Loss: 0.0632, Val Loss: 0.0388\n",
            "Epoch 99/100, Train Loss: 0.0638, Val Loss: 0.0388\n",
            "Epoch 100/100, Train Loss: 0.0647, Val Loss: 0.0388\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: gru, Units: 16, Dropout: 0.2, Dense Units: 64\n",
            "Batch Size: 4, Epochs: 100\n",
            "Device: cuda\n",
            "Total test samples: 28\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/28\n",
            "Current training set size: 97 samples\n",
            "Epoch 1/100, Train Loss: 0.1468, Val Loss: 0.2083\n",
            "Epoch 2/100, Train Loss: 0.1479, Val Loss: 0.1901\n",
            "Epoch 3/100, Train Loss: 0.1537, Val Loss: 0.1867\n",
            "Epoch 4/100, Train Loss: 0.1487, Val Loss: 0.1803\n",
            "Epoch 5/100, Train Loss: 0.1316, Val Loss: 0.1695\n",
            "Epoch 6/100, Train Loss: 0.1304, Val Loss: 0.1823\n",
            "Epoch 7/100, Train Loss: 0.1224, Val Loss: 0.1309\n",
            "Epoch 8/100, Train Loss: 0.0910, Val Loss: 0.0702\n",
            "Epoch 9/100, Train Loss: 0.0744, Val Loss: 0.0446\n",
            "Epoch 10/100, Train Loss: 0.0631, Val Loss: 0.0384\n",
            "Epoch 11/100, Train Loss: 0.0695, Val Loss: 0.0377\n",
            "Epoch 12/100, Train Loss: 0.0613, Val Loss: 0.0372\n",
            "Epoch 13/100, Train Loss: 0.0603, Val Loss: 0.0302\n",
            "Epoch 14/100, Train Loss: 0.0707, Val Loss: 0.0389\n",
            "Epoch 15/100, Train Loss: 0.0650, Val Loss: 0.0367\n",
            "Epoch 16/100, Train Loss: 0.0579, Val Loss: 0.0304\n",
            "Epoch 17/100, Train Loss: 0.0566, Val Loss: 0.0361\n",
            "Epoch 18/100, Train Loss: 0.0592, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0534, Val Loss: 0.0389\n",
            "Epoch 20/100, Train Loss: 0.0518, Val Loss: 0.0384\n",
            "Epoch 21/100, Train Loss: 0.0601, Val Loss: 0.0381\n",
            "Epoch 22/100, Train Loss: 0.0530, Val Loss: 0.0377\n",
            "Epoch 23/100, Train Loss: 0.0503, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0528, Val Loss: 0.0377\n",
            "Epoch 25/100, Train Loss: 0.0525, Val Loss: 0.0377\n",
            "Epoch 26/100, Train Loss: 0.0553, Val Loss: 0.0377\n",
            "Epoch 27/100, Train Loss: 0.0574, Val Loss: 0.0377\n",
            "Epoch 28/100, Train Loss: 0.0554, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0522, Val Loss: 0.0377\n",
            "Epoch 30/100, Train Loss: 0.0532, Val Loss: 0.0377\n",
            "Epoch 31/100, Train Loss: 0.0520, Val Loss: 0.0377\n",
            "Epoch 32/100, Train Loss: 0.0522, Val Loss: 0.0377\n",
            "Epoch 33/100, Train Loss: 0.0505, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0535, Val Loss: 0.0377\n",
            "Epoch 35/100, Train Loss: 0.0552, Val Loss: 0.0377\n",
            "Epoch 36/100, Train Loss: 0.0510, Val Loss: 0.0377\n",
            "Epoch 37/100, Train Loss: 0.0669, Val Loss: 0.0377\n",
            "Epoch 38/100, Train Loss: 0.0512, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0522, Val Loss: 0.0377\n",
            "Epoch 40/100, Train Loss: 0.0553, Val Loss: 0.0377\n",
            "Epoch 41/100, Train Loss: 0.0544, Val Loss: 0.0377\n",
            "Epoch 42/100, Train Loss: 0.0494, Val Loss: 0.0377\n",
            "Epoch 43/100, Train Loss: 0.0540, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0500, Val Loss: 0.0377\n",
            "Epoch 45/100, Train Loss: 0.0551, Val Loss: 0.0377\n",
            "Epoch 46/100, Train Loss: 0.0542, Val Loss: 0.0377\n",
            "Epoch 47/100, Train Loss: 0.0523, Val Loss: 0.0377\n",
            "Epoch 48/100, Train Loss: 0.0506, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0555, Val Loss: 0.0377\n",
            "Epoch 50/100, Train Loss: 0.0512, Val Loss: 0.0377\n",
            "Epoch 51/100, Train Loss: 0.0525, Val Loss: 0.0377\n",
            "Epoch 52/100, Train Loss: 0.0501, Val Loss: 0.0377\n",
            "Epoch 53/100, Train Loss: 0.0541, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0532, Val Loss: 0.0377\n",
            "Epoch 55/100, Train Loss: 0.0522, Val Loss: 0.0377\n",
            "Epoch 56/100, Train Loss: 0.0551, Val Loss: 0.0377\n",
            "Epoch 57/100, Train Loss: 0.0554, Val Loss: 0.0377\n",
            "Epoch 58/100, Train Loss: 0.0544, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0518, Val Loss: 0.0377\n",
            "Epoch 60/100, Train Loss: 0.0547, Val Loss: 0.0377\n",
            "Epoch 61/100, Train Loss: 0.0550, Val Loss: 0.0377\n",
            "Epoch 62/100, Train Loss: 0.0516, Val Loss: 0.0377\n",
            "Epoch 63/100, Train Loss: 0.0513, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0525, Val Loss: 0.0377\n",
            "Epoch 65/100, Train Loss: 0.0532, Val Loss: 0.0377\n",
            "Epoch 66/100, Train Loss: 0.0513, Val Loss: 0.0377\n",
            "Epoch 67/100, Train Loss: 0.0559, Val Loss: 0.0377\n",
            "Epoch 68/100, Train Loss: 0.0512, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0532, Val Loss: 0.0377\n",
            "Epoch 70/100, Train Loss: 0.0524, Val Loss: 0.0377\n",
            "Epoch 71/100, Train Loss: 0.0542, Val Loss: 0.0377\n",
            "Epoch 72/100, Train Loss: 0.0524, Val Loss: 0.0377\n",
            "Epoch 73/100, Train Loss: 0.0518, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0525, Val Loss: 0.0377\n",
            "Epoch 75/100, Train Loss: 0.0569, Val Loss: 0.0377\n",
            "Epoch 76/100, Train Loss: 0.0567, Val Loss: 0.0377\n",
            "Epoch 77/100, Train Loss: 0.0536, Val Loss: 0.0377\n",
            "Epoch 78/100, Train Loss: 0.0553, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0538, Val Loss: 0.0377\n",
            "Epoch 80/100, Train Loss: 0.0533, Val Loss: 0.0377\n",
            "Epoch 81/100, Train Loss: 0.0524, Val Loss: 0.0377\n",
            "Epoch 82/100, Train Loss: 0.0497, Val Loss: 0.0377\n",
            "Epoch 83/100, Train Loss: 0.0521, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0539, Val Loss: 0.0377\n",
            "Epoch 85/100, Train Loss: 0.0531, Val Loss: 0.0377\n",
            "Epoch 86/100, Train Loss: 0.0521, Val Loss: 0.0377\n",
            "Epoch 87/100, Train Loss: 0.0501, Val Loss: 0.0377\n",
            "Epoch 88/100, Train Loss: 0.0544, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0554, Val Loss: 0.0377\n",
            "Epoch 90/100, Train Loss: 0.0549, Val Loss: 0.0377\n",
            "Epoch 91/100, Train Loss: 0.0517, Val Loss: 0.0377\n",
            "Epoch 92/100, Train Loss: 0.0547, Val Loss: 0.0377\n",
            "Epoch 93/100, Train Loss: 0.0547, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0565, Val Loss: 0.0377\n",
            "Epoch 95/100, Train Loss: 0.0505, Val Loss: 0.0377\n",
            "Epoch 96/100, Train Loss: 0.0637, Val Loss: 0.0377\n",
            "Epoch 97/100, Train Loss: 0.0516, Val Loss: 0.0377\n",
            "Epoch 98/100, Train Loss: 0.0549, Val Loss: 0.0377\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0550, Val Loss: 0.0377\n",
            "Epoch 100/100, Train Loss: 0.0540, Val Loss: 0.0377\n",
            "\n",
            "Test iteration 2/28\n",
            "Current training set size: 98 samples\n",
            "Epoch 1/100, Train Loss: 0.1604, Val Loss: 0.1925\n",
            "Epoch 2/100, Train Loss: 0.1478, Val Loss: 0.1962\n",
            "Epoch 3/100, Train Loss: 0.1462, Val Loss: 0.1864\n",
            "Epoch 4/100, Train Loss: 0.1439, Val Loss: 0.1821\n",
            "Epoch 5/100, Train Loss: 0.1394, Val Loss: 0.1869\n",
            "Epoch 6/100, Train Loss: 0.1320, Val Loss: 0.1597\n",
            "Epoch 7/100, Train Loss: 0.1199, Val Loss: 0.1297\n",
            "Epoch 8/100, Train Loss: 0.0884, Val Loss: 0.0512\n",
            "Epoch 9/100, Train Loss: 0.0693, Val Loss: 0.0378\n",
            "Epoch 10/100, Train Loss: 0.0709, Val Loss: 0.0641\n",
            "Epoch 11/100, Train Loss: 0.0710, Val Loss: 0.0473\n",
            "Epoch 12/100, Train Loss: 0.0798, Val Loss: 0.0371\n",
            "Epoch 13/100, Train Loss: 0.0631, Val Loss: 0.0366\n",
            "Epoch 14/100, Train Loss: 0.0667, Val Loss: 0.0343\n",
            "Epoch 15/100, Train Loss: 0.0645, Val Loss: 0.0357\n",
            "Epoch 16/100, Train Loss: 0.0691, Val Loss: 0.0369\n",
            "Epoch 17/100, Train Loss: 0.0637, Val Loss: 0.0422\n",
            "Epoch 18/100, Train Loss: 0.0581, Val Loss: 0.0333\n",
            "Epoch 19/100, Train Loss: 0.0553, Val Loss: 0.0388\n",
            "Epoch 20/100, Train Loss: 0.0592, Val Loss: 0.0329\n",
            "Epoch 21/100, Train Loss: 0.0605, Val Loss: 0.0402\n",
            "Epoch 22/100, Train Loss: 0.0532, Val Loss: 0.0391\n",
            "Epoch 23/100, Train Loss: 0.0604, Val Loss: 0.0506\n",
            "Epoch 24/100, Train Loss: 0.0514, Val Loss: 0.0383\n",
            "Epoch 25/100, Train Loss: 0.0569, Val Loss: 0.0367\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0464, Val Loss: 0.0368\n",
            "Epoch 27/100, Train Loss: 0.0511, Val Loss: 0.0370\n",
            "Epoch 28/100, Train Loss: 0.0497, Val Loss: 0.0371\n",
            "Epoch 29/100, Train Loss: 0.0488, Val Loss: 0.0370\n",
            "Epoch 30/100, Train Loss: 0.0494, Val Loss: 0.0368\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0455, Val Loss: 0.0368\n",
            "Epoch 32/100, Train Loss: 0.0493, Val Loss: 0.0368\n",
            "Epoch 33/100, Train Loss: 0.0488, Val Loss: 0.0368\n",
            "Epoch 34/100, Train Loss: 0.0472, Val Loss: 0.0368\n",
            "Epoch 35/100, Train Loss: 0.0463, Val Loss: 0.0368\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0470, Val Loss: 0.0368\n",
            "Epoch 37/100, Train Loss: 0.0469, Val Loss: 0.0368\n",
            "Epoch 38/100, Train Loss: 0.0465, Val Loss: 0.0368\n",
            "Epoch 39/100, Train Loss: 0.0490, Val Loss: 0.0368\n",
            "Epoch 40/100, Train Loss: 0.0522, Val Loss: 0.0368\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0449, Val Loss: 0.0368\n",
            "Epoch 42/100, Train Loss: 0.0464, Val Loss: 0.0368\n",
            "Epoch 43/100, Train Loss: 0.0476, Val Loss: 0.0368\n",
            "Epoch 44/100, Train Loss: 0.0438, Val Loss: 0.0368\n",
            "Epoch 45/100, Train Loss: 0.0509, Val Loss: 0.0368\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0526, Val Loss: 0.0368\n",
            "Epoch 47/100, Train Loss: 0.0453, Val Loss: 0.0368\n",
            "Epoch 48/100, Train Loss: 0.0469, Val Loss: 0.0368\n",
            "Epoch 49/100, Train Loss: 0.0522, Val Loss: 0.0368\n",
            "Epoch 50/100, Train Loss: 0.0491, Val Loss: 0.0368\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0461, Val Loss: 0.0368\n",
            "Epoch 52/100, Train Loss: 0.0458, Val Loss: 0.0368\n",
            "Epoch 53/100, Train Loss: 0.0477, Val Loss: 0.0368\n",
            "Epoch 54/100, Train Loss: 0.0469, Val Loss: 0.0368\n",
            "Epoch 55/100, Train Loss: 0.0477, Val Loss: 0.0368\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0481, Val Loss: 0.0368\n",
            "Epoch 57/100, Train Loss: 0.0485, Val Loss: 0.0368\n",
            "Epoch 58/100, Train Loss: 0.0478, Val Loss: 0.0368\n",
            "Epoch 59/100, Train Loss: 0.0480, Val Loss: 0.0368\n",
            "Epoch 60/100, Train Loss: 0.0457, Val Loss: 0.0368\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0477, Val Loss: 0.0368\n",
            "Epoch 62/100, Train Loss: 0.0491, Val Loss: 0.0368\n",
            "Epoch 63/100, Train Loss: 0.0494, Val Loss: 0.0368\n",
            "Epoch 64/100, Train Loss: 0.0462, Val Loss: 0.0368\n",
            "Epoch 65/100, Train Loss: 0.0468, Val Loss: 0.0368\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0455, Val Loss: 0.0368\n",
            "Epoch 67/100, Train Loss: 0.0469, Val Loss: 0.0368\n",
            "Epoch 68/100, Train Loss: 0.0471, Val Loss: 0.0368\n",
            "Epoch 69/100, Train Loss: 0.0469, Val Loss: 0.0368\n",
            "Epoch 70/100, Train Loss: 0.0491, Val Loss: 0.0368\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0564, Val Loss: 0.0368\n",
            "Epoch 72/100, Train Loss: 0.0508, Val Loss: 0.0368\n",
            "Epoch 73/100, Train Loss: 0.0492, Val Loss: 0.0368\n",
            "Epoch 74/100, Train Loss: 0.0467, Val Loss: 0.0368\n",
            "Epoch 75/100, Train Loss: 0.0481, Val Loss: 0.0368\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0470, Val Loss: 0.0368\n",
            "Epoch 77/100, Train Loss: 0.0490, Val Loss: 0.0368\n",
            "Epoch 78/100, Train Loss: 0.0454, Val Loss: 0.0368\n",
            "Epoch 79/100, Train Loss: 0.0463, Val Loss: 0.0368\n",
            "Epoch 80/100, Train Loss: 0.0480, Val Loss: 0.0368\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0499, Val Loss: 0.0368\n",
            "Epoch 82/100, Train Loss: 0.0452, Val Loss: 0.0368\n",
            "Epoch 83/100, Train Loss: 0.0482, Val Loss: 0.0368\n",
            "Epoch 84/100, Train Loss: 0.0460, Val Loss: 0.0368\n",
            "Epoch 85/100, Train Loss: 0.0478, Val Loss: 0.0368\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0464, Val Loss: 0.0368\n",
            "Epoch 87/100, Train Loss: 0.0494, Val Loss: 0.0368\n",
            "Epoch 88/100, Train Loss: 0.0490, Val Loss: 0.0368\n",
            "Epoch 89/100, Train Loss: 0.0477, Val Loss: 0.0368\n",
            "Epoch 90/100, Train Loss: 0.0453, Val Loss: 0.0368\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0506, Val Loss: 0.0368\n",
            "Epoch 92/100, Train Loss: 0.0485, Val Loss: 0.0368\n",
            "Epoch 93/100, Train Loss: 0.0462, Val Loss: 0.0368\n",
            "Epoch 94/100, Train Loss: 0.0477, Val Loss: 0.0368\n",
            "Epoch 95/100, Train Loss: 0.0524, Val Loss: 0.0368\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0493, Val Loss: 0.0368\n",
            "Epoch 97/100, Train Loss: 0.0503, Val Loss: 0.0368\n",
            "Epoch 98/100, Train Loss: 0.0452, Val Loss: 0.0368\n",
            "Epoch 99/100, Train Loss: 0.0460, Val Loss: 0.0368\n",
            "Epoch 100/100, Train Loss: 0.0470, Val Loss: 0.0368\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 3/28\n",
            "Current training set size: 99 samples\n",
            "Epoch 1/100, Train Loss: 0.1619, Val Loss: 0.1926\n",
            "Epoch 2/100, Train Loss: 0.1462, Val Loss: 0.1981\n",
            "Epoch 3/100, Train Loss: 0.1449, Val Loss: 0.1926\n",
            "Epoch 4/100, Train Loss: 0.1408, Val Loss: 0.1845\n",
            "Epoch 5/100, Train Loss: 0.1372, Val Loss: 0.1795\n",
            "Epoch 6/100, Train Loss: 0.1331, Val Loss: 0.1550\n",
            "Epoch 7/100, Train Loss: 0.1186, Val Loss: 0.1077\n",
            "Epoch 8/100, Train Loss: 0.0834, Val Loss: 0.0512\n",
            "Epoch 9/100, Train Loss: 0.0722, Val Loss: 0.0397\n",
            "Epoch 10/100, Train Loss: 0.0734, Val Loss: 0.0458\n",
            "Epoch 11/100, Train Loss: 0.0681, Val Loss: 0.0424\n",
            "Epoch 12/100, Train Loss: 0.0690, Val Loss: 0.0405\n",
            "Epoch 13/100, Train Loss: 0.0698, Val Loss: 0.0354\n",
            "Epoch 14/100, Train Loss: 0.0613, Val Loss: 0.0348\n",
            "Epoch 15/100, Train Loss: 0.0621, Val Loss: 0.0369\n",
            "Epoch 16/100, Train Loss: 0.0656, Val Loss: 0.0389\n",
            "Epoch 17/100, Train Loss: 0.0707, Val Loss: 0.0556\n",
            "Epoch 18/100, Train Loss: 0.0618, Val Loss: 0.0377\n",
            "Epoch 19/100, Train Loss: 0.0587, Val Loss: 0.0392\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0555, Val Loss: 0.0390\n",
            "Epoch 21/100, Train Loss: 0.0585, Val Loss: 0.0387\n",
            "Epoch 22/100, Train Loss: 0.0541, Val Loss: 0.0385\n",
            "Epoch 23/100, Train Loss: 0.0579, Val Loss: 0.0383\n",
            "Epoch 24/100, Train Loss: 0.0554, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0532, Val Loss: 0.0381\n",
            "Epoch 26/100, Train Loss: 0.0540, Val Loss: 0.0381\n",
            "Epoch 27/100, Train Loss: 0.0552, Val Loss: 0.0381\n",
            "Epoch 28/100, Train Loss: 0.0517, Val Loss: 0.0381\n",
            "Epoch 29/100, Train Loss: 0.0541, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0562, Val Loss: 0.0381\n",
            "Epoch 31/100, Train Loss: 0.0540, Val Loss: 0.0381\n",
            "Epoch 32/100, Train Loss: 0.0545, Val Loss: 0.0381\n",
            "Epoch 33/100, Train Loss: 0.0554, Val Loss: 0.0381\n",
            "Epoch 34/100, Train Loss: 0.0541, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0549, Val Loss: 0.0381\n",
            "Epoch 36/100, Train Loss: 0.0562, Val Loss: 0.0381\n",
            "Epoch 37/100, Train Loss: 0.0548, Val Loss: 0.0381\n",
            "Epoch 38/100, Train Loss: 0.0578, Val Loss: 0.0381\n",
            "Epoch 39/100, Train Loss: 0.0527, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0519, Val Loss: 0.0381\n",
            "Epoch 41/100, Train Loss: 0.0539, Val Loss: 0.0381\n",
            "Epoch 42/100, Train Loss: 0.0558, Val Loss: 0.0381\n",
            "Epoch 43/100, Train Loss: 0.0530, Val Loss: 0.0381\n",
            "Epoch 44/100, Train Loss: 0.0534, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0575, Val Loss: 0.0381\n",
            "Epoch 46/100, Train Loss: 0.0535, Val Loss: 0.0381\n",
            "Epoch 47/100, Train Loss: 0.0543, Val Loss: 0.0381\n",
            "Epoch 48/100, Train Loss: 0.0541, Val Loss: 0.0381\n",
            "Epoch 49/100, Train Loss: 0.0575, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0536, Val Loss: 0.0381\n",
            "Epoch 51/100, Train Loss: 0.0565, Val Loss: 0.0381\n",
            "Epoch 52/100, Train Loss: 0.0568, Val Loss: 0.0381\n",
            "Epoch 53/100, Train Loss: 0.0542, Val Loss: 0.0381\n",
            "Epoch 54/100, Train Loss: 0.0561, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0597, Val Loss: 0.0381\n",
            "Epoch 56/100, Train Loss: 0.0557, Val Loss: 0.0381\n",
            "Epoch 57/100, Train Loss: 0.0553, Val Loss: 0.0381\n",
            "Epoch 58/100, Train Loss: 0.0560, Val Loss: 0.0381\n",
            "Epoch 59/100, Train Loss: 0.0547, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0568, Val Loss: 0.0381\n",
            "Epoch 61/100, Train Loss: 0.0547, Val Loss: 0.0381\n",
            "Epoch 62/100, Train Loss: 0.0543, Val Loss: 0.0381\n",
            "Epoch 63/100, Train Loss: 0.0545, Val Loss: 0.0381\n",
            "Epoch 64/100, Train Loss: 0.0550, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0552, Val Loss: 0.0381\n",
            "Epoch 66/100, Train Loss: 0.0519, Val Loss: 0.0381\n",
            "Epoch 67/100, Train Loss: 0.0565, Val Loss: 0.0381\n",
            "Epoch 68/100, Train Loss: 0.0553, Val Loss: 0.0381\n",
            "Epoch 69/100, Train Loss: 0.0541, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0541, Val Loss: 0.0381\n",
            "Epoch 71/100, Train Loss: 0.0562, Val Loss: 0.0381\n",
            "Epoch 72/100, Train Loss: 0.0532, Val Loss: 0.0381\n",
            "Epoch 73/100, Train Loss: 0.0517, Val Loss: 0.0381\n",
            "Epoch 74/100, Train Loss: 0.0536, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0545, Val Loss: 0.0381\n",
            "Epoch 76/100, Train Loss: 0.0554, Val Loss: 0.0381\n",
            "Epoch 77/100, Train Loss: 0.0567, Val Loss: 0.0381\n",
            "Epoch 78/100, Train Loss: 0.0542, Val Loss: 0.0381\n",
            "Epoch 79/100, Train Loss: 0.0547, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0526, Val Loss: 0.0381\n",
            "Epoch 81/100, Train Loss: 0.0542, Val Loss: 0.0381\n",
            "Epoch 82/100, Train Loss: 0.0550, Val Loss: 0.0381\n",
            "Epoch 83/100, Train Loss: 0.0554, Val Loss: 0.0381\n",
            "Epoch 84/100, Train Loss: 0.0549, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0544, Val Loss: 0.0381\n",
            "Epoch 86/100, Train Loss: 0.0542, Val Loss: 0.0381\n",
            "Epoch 87/100, Train Loss: 0.0574, Val Loss: 0.0381\n",
            "Epoch 88/100, Train Loss: 0.0552, Val Loss: 0.0381\n",
            "Epoch 89/100, Train Loss: 0.0576, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0524, Val Loss: 0.0381\n",
            "Epoch 91/100, Train Loss: 0.0540, Val Loss: 0.0381\n",
            "Epoch 92/100, Train Loss: 0.0546, Val Loss: 0.0381\n",
            "Epoch 93/100, Train Loss: 0.0554, Val Loss: 0.0381\n",
            "Epoch 94/100, Train Loss: 0.0544, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0550, Val Loss: 0.0381\n",
            "Epoch 96/100, Train Loss: 0.0535, Val Loss: 0.0381\n",
            "Epoch 97/100, Train Loss: 0.0561, Val Loss: 0.0381\n",
            "Epoch 98/100, Train Loss: 0.0550, Val Loss: 0.0381\n",
            "Epoch 99/100, Train Loss: 0.0541, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0533, Val Loss: 0.0381\n",
            "\n",
            "Test iteration 4/28\n",
            "Current training set size: 100 samples\n",
            "Epoch 1/100, Train Loss: 0.1522, Val Loss: 0.1929\n",
            "Epoch 2/100, Train Loss: 0.1435, Val Loss: 0.1948\n",
            "Epoch 3/100, Train Loss: 0.1433, Val Loss: 0.1884\n",
            "Epoch 4/100, Train Loss: 0.1404, Val Loss: 0.1877\n",
            "Epoch 5/100, Train Loss: 0.1328, Val Loss: 0.1669\n",
            "Epoch 6/100, Train Loss: 0.1289, Val Loss: 0.1421\n",
            "Epoch 7/100, Train Loss: 0.1189, Val Loss: 0.1025\n",
            "Epoch 8/100, Train Loss: 0.0942, Val Loss: 0.0462\n",
            "Epoch 9/100, Train Loss: 0.0770, Val Loss: 0.0318\n",
            "Epoch 10/100, Train Loss: 0.0685, Val Loss: 0.0437\n",
            "Epoch 11/100, Train Loss: 0.0717, Val Loss: 0.0649\n",
            "Epoch 12/100, Train Loss: 0.0744, Val Loss: 0.0324\n",
            "Epoch 13/100, Train Loss: 0.0707, Val Loss: 0.0385\n",
            "Epoch 14/100, Train Loss: 0.0653, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 15/100, Train Loss: 0.0703, Val Loss: 0.0395\n",
            "Epoch 16/100, Train Loss: 0.0652, Val Loss: 0.0393\n",
            "Epoch 17/100, Train Loss: 0.0641, Val Loss: 0.0392\n",
            "Epoch 18/100, Train Loss: 0.0605, Val Loss: 0.0392\n",
            "Epoch 19/100, Train Loss: 0.0627, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0630, Val Loss: 0.0393\n",
            "Epoch 21/100, Train Loss: 0.0617, Val Loss: 0.0393\n",
            "Epoch 22/100, Train Loss: 0.0645, Val Loss: 0.0393\n",
            "Epoch 23/100, Train Loss: 0.0613, Val Loss: 0.0393\n",
            "Epoch 24/100, Train Loss: 0.0634, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0637, Val Loss: 0.0393\n",
            "Epoch 26/100, Train Loss: 0.0629, Val Loss: 0.0393\n",
            "Epoch 27/100, Train Loss: 0.0625, Val Loss: 0.0393\n",
            "Epoch 28/100, Train Loss: 0.0637, Val Loss: 0.0393\n",
            "Epoch 29/100, Train Loss: 0.0636, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0643, Val Loss: 0.0393\n",
            "Epoch 31/100, Train Loss: 0.0614, Val Loss: 0.0393\n",
            "Epoch 32/100, Train Loss: 0.0648, Val Loss: 0.0393\n",
            "Epoch 33/100, Train Loss: 0.0621, Val Loss: 0.0393\n",
            "Epoch 34/100, Train Loss: 0.0592, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0643, Val Loss: 0.0393\n",
            "Epoch 36/100, Train Loss: 0.0615, Val Loss: 0.0393\n",
            "Epoch 37/100, Train Loss: 0.0627, Val Loss: 0.0393\n",
            "Epoch 38/100, Train Loss: 0.0640, Val Loss: 0.0393\n",
            "Epoch 39/100, Train Loss: 0.0610, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0640, Val Loss: 0.0393\n",
            "Epoch 41/100, Train Loss: 0.0615, Val Loss: 0.0393\n",
            "Epoch 42/100, Train Loss: 0.0641, Val Loss: 0.0393\n",
            "Epoch 43/100, Train Loss: 0.0618, Val Loss: 0.0393\n",
            "Epoch 44/100, Train Loss: 0.0646, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0629, Val Loss: 0.0393\n",
            "Epoch 46/100, Train Loss: 0.0636, Val Loss: 0.0393\n",
            "Epoch 47/100, Train Loss: 0.0634, Val Loss: 0.0393\n",
            "Epoch 48/100, Train Loss: 0.0637, Val Loss: 0.0393\n",
            "Epoch 49/100, Train Loss: 0.0628, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0630, Val Loss: 0.0393\n",
            "Epoch 51/100, Train Loss: 0.0627, Val Loss: 0.0393\n",
            "Epoch 52/100, Train Loss: 0.0636, Val Loss: 0.0393\n",
            "Epoch 53/100, Train Loss: 0.0642, Val Loss: 0.0393\n",
            "Epoch 54/100, Train Loss: 0.0656, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0641, Val Loss: 0.0393\n",
            "Epoch 56/100, Train Loss: 0.0641, Val Loss: 0.0393\n",
            "Epoch 57/100, Train Loss: 0.0632, Val Loss: 0.0393\n",
            "Epoch 58/100, Train Loss: 0.0635, Val Loss: 0.0393\n",
            "Epoch 59/100, Train Loss: 0.0652, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0634, Val Loss: 0.0393\n",
            "Epoch 61/100, Train Loss: 0.0604, Val Loss: 0.0393\n",
            "Epoch 62/100, Train Loss: 0.0619, Val Loss: 0.0393\n",
            "Epoch 63/100, Train Loss: 0.0656, Val Loss: 0.0393\n",
            "Epoch 64/100, Train Loss: 0.0612, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0641, Val Loss: 0.0393\n",
            "Epoch 66/100, Train Loss: 0.0617, Val Loss: 0.0393\n",
            "Epoch 67/100, Train Loss: 0.0663, Val Loss: 0.0393\n",
            "Epoch 68/100, Train Loss: 0.0651, Val Loss: 0.0393\n",
            "Epoch 69/100, Train Loss: 0.0622, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0635, Val Loss: 0.0393\n",
            "Epoch 71/100, Train Loss: 0.0641, Val Loss: 0.0393\n",
            "Epoch 72/100, Train Loss: 0.0638, Val Loss: 0.0393\n",
            "Epoch 73/100, Train Loss: 0.0653, Val Loss: 0.0393\n",
            "Epoch 74/100, Train Loss: 0.0615, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0636, Val Loss: 0.0393\n",
            "Epoch 76/100, Train Loss: 0.0625, Val Loss: 0.0393\n",
            "Epoch 77/100, Train Loss: 0.0627, Val Loss: 0.0393\n",
            "Epoch 78/100, Train Loss: 0.0637, Val Loss: 0.0393\n",
            "Epoch 79/100, Train Loss: 0.0640, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0629, Val Loss: 0.0393\n",
            "Epoch 81/100, Train Loss: 0.0619, Val Loss: 0.0393\n",
            "Epoch 82/100, Train Loss: 0.0623, Val Loss: 0.0393\n",
            "Epoch 83/100, Train Loss: 0.0597, Val Loss: 0.0393\n",
            "Epoch 84/100, Train Loss: 0.0630, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0632, Val Loss: 0.0393\n",
            "Epoch 86/100, Train Loss: 0.0631, Val Loss: 0.0393\n",
            "Epoch 87/100, Train Loss: 0.0596, Val Loss: 0.0393\n",
            "Epoch 88/100, Train Loss: 0.0620, Val Loss: 0.0393\n",
            "Epoch 89/100, Train Loss: 0.0640, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0649, Val Loss: 0.0393\n",
            "Epoch 91/100, Train Loss: 0.0639, Val Loss: 0.0393\n",
            "Epoch 92/100, Train Loss: 0.0645, Val Loss: 0.0393\n",
            "Epoch 93/100, Train Loss: 0.0635, Val Loss: 0.0393\n",
            "Epoch 94/100, Train Loss: 0.0628, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0607, Val Loss: 0.0393\n",
            "Epoch 96/100, Train Loss: 0.0638, Val Loss: 0.0393\n",
            "Epoch 97/100, Train Loss: 0.0636, Val Loss: 0.0393\n",
            "Epoch 98/100, Train Loss: 0.0617, Val Loss: 0.0393\n",
            "Epoch 99/100, Train Loss: 0.0605, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0624, Val Loss: 0.0393\n",
            "\n",
            "Test iteration 5/28\n",
            "Current training set size: 101 samples\n",
            "Epoch 1/100, Train Loss: 0.1608, Val Loss: 0.1921\n",
            "Epoch 2/100, Train Loss: 0.1447, Val Loss: 0.1941\n",
            "Epoch 3/100, Train Loss: 0.1421, Val Loss: 0.1884\n",
            "Epoch 4/100, Train Loss: 0.1396, Val Loss: 0.1836\n",
            "Epoch 5/100, Train Loss: 0.1377, Val Loss: 0.1753\n",
            "Epoch 6/100, Train Loss: 0.1310, Val Loss: 0.1570\n",
            "Epoch 7/100, Train Loss: 0.1106, Val Loss: 0.1060\n",
            "Epoch 8/100, Train Loss: 0.0818, Val Loss: 0.0480\n",
            "Epoch 9/100, Train Loss: 0.0847, Val Loss: 0.0412\n",
            "Epoch 10/100, Train Loss: 0.0664, Val Loss: 0.0373\n",
            "Epoch 11/100, Train Loss: 0.0674, Val Loss: 0.0413\n",
            "Epoch 12/100, Train Loss: 0.0715, Val Loss: 0.0428\n",
            "Epoch 13/100, Train Loss: 0.0902, Val Loss: 0.0420\n",
            "Epoch 14/100, Train Loss: 0.0650, Val Loss: 0.0419\n",
            "Epoch 15/100, Train Loss: 0.0682, Val Loss: 0.0364\n",
            "Epoch 16/100, Train Loss: 0.0627, Val Loss: 0.0425\n",
            "Epoch 17/100, Train Loss: 0.0581, Val Loss: 0.0419\n",
            "Epoch 18/100, Train Loss: 0.0637, Val Loss: 0.0422\n",
            "Epoch 19/100, Train Loss: 0.0573, Val Loss: 0.0394\n",
            "Epoch 20/100, Train Loss: 0.0604, Val Loss: 0.0386\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0574, Val Loss: 0.0385\n",
            "Epoch 22/100, Train Loss: 0.0578, Val Loss: 0.0383\n",
            "Epoch 23/100, Train Loss: 0.0563, Val Loss: 0.0380\n",
            "Epoch 24/100, Train Loss: 0.0560, Val Loss: 0.0381\n",
            "Epoch 25/100, Train Loss: 0.0536, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0558, Val Loss: 0.0381\n",
            "Epoch 27/100, Train Loss: 0.0566, Val Loss: 0.0381\n",
            "Epoch 28/100, Train Loss: 0.0542, Val Loss: 0.0381\n",
            "Epoch 29/100, Train Loss: 0.0550, Val Loss: 0.0381\n",
            "Epoch 30/100, Train Loss: 0.0557, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0527, Val Loss: 0.0381\n",
            "Epoch 32/100, Train Loss: 0.0496, Val Loss: 0.0381\n",
            "Epoch 33/100, Train Loss: 0.0538, Val Loss: 0.0381\n",
            "Epoch 34/100, Train Loss: 0.0521, Val Loss: 0.0381\n",
            "Epoch 35/100, Train Loss: 0.0519, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0550, Val Loss: 0.0381\n",
            "Epoch 37/100, Train Loss: 0.0563, Val Loss: 0.0381\n",
            "Epoch 38/100, Train Loss: 0.0545, Val Loss: 0.0381\n",
            "Epoch 39/100, Train Loss: 0.0521, Val Loss: 0.0381\n",
            "Epoch 40/100, Train Loss: 0.0536, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0557, Val Loss: 0.0381\n",
            "Epoch 42/100, Train Loss: 0.0550, Val Loss: 0.0381\n",
            "Epoch 43/100, Train Loss: 0.0576, Val Loss: 0.0381\n",
            "Epoch 44/100, Train Loss: 0.0543, Val Loss: 0.0381\n",
            "Epoch 45/100, Train Loss: 0.0528, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0544, Val Loss: 0.0381\n",
            "Epoch 47/100, Train Loss: 0.0539, Val Loss: 0.0381\n",
            "Epoch 48/100, Train Loss: 0.0571, Val Loss: 0.0381\n",
            "Epoch 49/100, Train Loss: 0.0687, Val Loss: 0.0381\n",
            "Epoch 50/100, Train Loss: 0.0686, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0546, Val Loss: 0.0381\n",
            "Epoch 52/100, Train Loss: 0.0513, Val Loss: 0.0381\n",
            "Epoch 53/100, Train Loss: 0.0697, Val Loss: 0.0381\n",
            "Epoch 54/100, Train Loss: 0.0549, Val Loss: 0.0381\n",
            "Epoch 55/100, Train Loss: 0.0549, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0569, Val Loss: 0.0381\n",
            "Epoch 57/100, Train Loss: 0.0564, Val Loss: 0.0381\n",
            "Epoch 58/100, Train Loss: 0.0527, Val Loss: 0.0381\n",
            "Epoch 59/100, Train Loss: 0.0552, Val Loss: 0.0381\n",
            "Epoch 60/100, Train Loss: 0.0565, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0547, Val Loss: 0.0381\n",
            "Epoch 62/100, Train Loss: 0.0553, Val Loss: 0.0381\n",
            "Epoch 63/100, Train Loss: 0.0556, Val Loss: 0.0381\n",
            "Epoch 64/100, Train Loss: 0.0543, Val Loss: 0.0381\n",
            "Epoch 65/100, Train Loss: 0.0533, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0529, Val Loss: 0.0381\n",
            "Epoch 67/100, Train Loss: 0.0558, Val Loss: 0.0381\n",
            "Epoch 68/100, Train Loss: 0.0569, Val Loss: 0.0381\n",
            "Epoch 69/100, Train Loss: 0.0540, Val Loss: 0.0381\n",
            "Epoch 70/100, Train Loss: 0.0526, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0611, Val Loss: 0.0381\n",
            "Epoch 72/100, Train Loss: 0.0527, Val Loss: 0.0381\n",
            "Epoch 73/100, Train Loss: 0.0551, Val Loss: 0.0381\n",
            "Epoch 74/100, Train Loss: 0.0532, Val Loss: 0.0381\n",
            "Epoch 75/100, Train Loss: 0.0528, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0530, Val Loss: 0.0381\n",
            "Epoch 77/100, Train Loss: 0.0536, Val Loss: 0.0381\n",
            "Epoch 78/100, Train Loss: 0.0557, Val Loss: 0.0381\n",
            "Epoch 79/100, Train Loss: 0.0538, Val Loss: 0.0381\n",
            "Epoch 80/100, Train Loss: 0.0537, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0529, Val Loss: 0.0381\n",
            "Epoch 82/100, Train Loss: 0.0534, Val Loss: 0.0381\n",
            "Epoch 83/100, Train Loss: 0.0540, Val Loss: 0.0381\n",
            "Epoch 84/100, Train Loss: 0.0568, Val Loss: 0.0381\n",
            "Epoch 85/100, Train Loss: 0.0543, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0538, Val Loss: 0.0381\n",
            "Epoch 87/100, Train Loss: 0.0537, Val Loss: 0.0381\n",
            "Epoch 88/100, Train Loss: 0.0549, Val Loss: 0.0381\n",
            "Epoch 89/100, Train Loss: 0.0514, Val Loss: 0.0381\n",
            "Epoch 90/100, Train Loss: 0.0540, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0549, Val Loss: 0.0381\n",
            "Epoch 92/100, Train Loss: 0.0541, Val Loss: 0.0381\n",
            "Epoch 93/100, Train Loss: 0.0562, Val Loss: 0.0381\n",
            "Epoch 94/100, Train Loss: 0.0550, Val Loss: 0.0381\n",
            "Epoch 95/100, Train Loss: 0.0546, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0522, Val Loss: 0.0381\n",
            "Epoch 97/100, Train Loss: 0.0519, Val Loss: 0.0381\n",
            "Epoch 98/100, Train Loss: 0.0550, Val Loss: 0.0381\n",
            "Epoch 99/100, Train Loss: 0.0523, Val Loss: 0.0381\n",
            "Epoch 100/100, Train Loss: 0.0541, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 6/28\n",
            "Current training set size: 102 samples\n",
            "Epoch 1/100, Train Loss: 0.1499, Val Loss: 0.2000\n",
            "Epoch 2/100, Train Loss: 0.1462, Val Loss: 0.2060\n",
            "Epoch 3/100, Train Loss: 0.1471, Val Loss: 0.1875\n",
            "Epoch 4/100, Train Loss: 0.1440, Val Loss: 0.1887\n",
            "Epoch 5/100, Train Loss: 0.1380, Val Loss: 0.1702\n",
            "Epoch 6/100, Train Loss: 0.1288, Val Loss: 0.1438\n",
            "Epoch 7/100, Train Loss: 0.1253, Val Loss: 0.1326\n",
            "Epoch 8/100, Train Loss: 0.0842, Val Loss: 0.0481\n",
            "Epoch 9/100, Train Loss: 0.0757, Val Loss: 0.0509\n",
            "Epoch 10/100, Train Loss: 0.0768, Val Loss: 0.0345\n",
            "Epoch 11/100, Train Loss: 0.0690, Val Loss: 0.0380\n",
            "Epoch 12/100, Train Loss: 0.0707, Val Loss: 0.0355\n",
            "Epoch 13/100, Train Loss: 0.0698, Val Loss: 0.0449\n",
            "Epoch 14/100, Train Loss: 0.0675, Val Loss: 0.0407\n",
            "Epoch 15/100, Train Loss: 0.0621, Val Loss: 0.0353\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0596, Val Loss: 0.0353\n",
            "Epoch 17/100, Train Loss: 0.0598, Val Loss: 0.0353\n",
            "Epoch 18/100, Train Loss: 0.0582, Val Loss: 0.0352\n",
            "Epoch 19/100, Train Loss: 0.0642, Val Loss: 0.0352\n",
            "Epoch 20/100, Train Loss: 0.0671, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0599, Val Loss: 0.0351\n",
            "Epoch 22/100, Train Loss: 0.0581, Val Loss: 0.0351\n",
            "Epoch 23/100, Train Loss: 0.0558, Val Loss: 0.0351\n",
            "Epoch 24/100, Train Loss: 0.0597, Val Loss: 0.0351\n",
            "Epoch 25/100, Train Loss: 0.0586, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0604, Val Loss: 0.0351\n",
            "Epoch 27/100, Train Loss: 0.0594, Val Loss: 0.0351\n",
            "Epoch 28/100, Train Loss: 0.0698, Val Loss: 0.0351\n",
            "Epoch 29/100, Train Loss: 0.0577, Val Loss: 0.0351\n",
            "Epoch 30/100, Train Loss: 0.0607, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0590, Val Loss: 0.0351\n",
            "Epoch 32/100, Train Loss: 0.0598, Val Loss: 0.0351\n",
            "Epoch 33/100, Train Loss: 0.0587, Val Loss: 0.0351\n",
            "Epoch 34/100, Train Loss: 0.0580, Val Loss: 0.0351\n",
            "Epoch 35/100, Train Loss: 0.0596, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0619, Val Loss: 0.0351\n",
            "Epoch 37/100, Train Loss: 0.0615, Val Loss: 0.0351\n",
            "Epoch 38/100, Train Loss: 0.0598, Val Loss: 0.0351\n",
            "Epoch 39/100, Train Loss: 0.0618, Val Loss: 0.0351\n",
            "Epoch 40/100, Train Loss: 0.0584, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0586, Val Loss: 0.0351\n",
            "Epoch 42/100, Train Loss: 0.0616, Val Loss: 0.0351\n",
            "Epoch 43/100, Train Loss: 0.0567, Val Loss: 0.0351\n",
            "Epoch 44/100, Train Loss: 0.0601, Val Loss: 0.0351\n",
            "Epoch 45/100, Train Loss: 0.0593, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0575, Val Loss: 0.0351\n",
            "Epoch 47/100, Train Loss: 0.0609, Val Loss: 0.0351\n",
            "Epoch 48/100, Train Loss: 0.0584, Val Loss: 0.0351\n",
            "Epoch 49/100, Train Loss: 0.0598, Val Loss: 0.0351\n",
            "Epoch 50/100, Train Loss: 0.0577, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0581, Val Loss: 0.0351\n",
            "Epoch 52/100, Train Loss: 0.0599, Val Loss: 0.0351\n",
            "Epoch 53/100, Train Loss: 0.0585, Val Loss: 0.0351\n",
            "Epoch 54/100, Train Loss: 0.0586, Val Loss: 0.0351\n",
            "Epoch 55/100, Train Loss: 0.0650, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0707, Val Loss: 0.0351\n",
            "Epoch 57/100, Train Loss: 0.0614, Val Loss: 0.0351\n",
            "Epoch 58/100, Train Loss: 0.0606, Val Loss: 0.0351\n",
            "Epoch 59/100, Train Loss: 0.0601, Val Loss: 0.0351\n",
            "Epoch 60/100, Train Loss: 0.0604, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0589, Val Loss: 0.0351\n",
            "Epoch 62/100, Train Loss: 0.0608, Val Loss: 0.0351\n",
            "Epoch 63/100, Train Loss: 0.0606, Val Loss: 0.0351\n",
            "Epoch 64/100, Train Loss: 0.0564, Val Loss: 0.0351\n",
            "Epoch 65/100, Train Loss: 0.0588, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0598, Val Loss: 0.0351\n",
            "Epoch 67/100, Train Loss: 0.0589, Val Loss: 0.0351\n",
            "Epoch 68/100, Train Loss: 0.0573, Val Loss: 0.0351\n",
            "Epoch 69/100, Train Loss: 0.0569, Val Loss: 0.0351\n",
            "Epoch 70/100, Train Loss: 0.0588, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0590, Val Loss: 0.0351\n",
            "Epoch 72/100, Train Loss: 0.0591, Val Loss: 0.0351\n",
            "Epoch 73/100, Train Loss: 0.0639, Val Loss: 0.0351\n",
            "Epoch 74/100, Train Loss: 0.0598, Val Loss: 0.0351\n",
            "Epoch 75/100, Train Loss: 0.0622, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0596, Val Loss: 0.0351\n",
            "Epoch 77/100, Train Loss: 0.0598, Val Loss: 0.0351\n",
            "Epoch 78/100, Train Loss: 0.0583, Val Loss: 0.0351\n",
            "Epoch 79/100, Train Loss: 0.0611, Val Loss: 0.0351\n",
            "Epoch 80/100, Train Loss: 0.0569, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0592, Val Loss: 0.0351\n",
            "Epoch 82/100, Train Loss: 0.0591, Val Loss: 0.0351\n",
            "Epoch 83/100, Train Loss: 0.0595, Val Loss: 0.0351\n",
            "Epoch 84/100, Train Loss: 0.0583, Val Loss: 0.0351\n",
            "Epoch 85/100, Train Loss: 0.0606, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0597, Val Loss: 0.0351\n",
            "Epoch 87/100, Train Loss: 0.0583, Val Loss: 0.0351\n",
            "Epoch 88/100, Train Loss: 0.0605, Val Loss: 0.0351\n",
            "Epoch 89/100, Train Loss: 0.0596, Val Loss: 0.0351\n",
            "Epoch 90/100, Train Loss: 0.0575, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0604, Val Loss: 0.0351\n",
            "Epoch 92/100, Train Loss: 0.0598, Val Loss: 0.0351\n",
            "Epoch 93/100, Train Loss: 0.0614, Val Loss: 0.0351\n",
            "Epoch 94/100, Train Loss: 0.0602, Val Loss: 0.0351\n",
            "Epoch 95/100, Train Loss: 0.0587, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0630, Val Loss: 0.0351\n",
            "Epoch 97/100, Train Loss: 0.0596, Val Loss: 0.0351\n",
            "Epoch 98/100, Train Loss: 0.0615, Val Loss: 0.0351\n",
            "Epoch 99/100, Train Loss: 0.0597, Val Loss: 0.0351\n",
            "Epoch 100/100, Train Loss: 0.0606, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 7/28\n",
            "Current training set size: 103 samples\n",
            "Epoch 1/100, Train Loss: 0.1488, Val Loss: 0.2124\n",
            "Epoch 2/100, Train Loss: 0.1476, Val Loss: 0.1915\n",
            "Epoch 3/100, Train Loss: 0.1415, Val Loss: 0.1851\n",
            "Epoch 4/100, Train Loss: 0.1392, Val Loss: 0.1912\n",
            "Epoch 5/100, Train Loss: 0.1373, Val Loss: 0.1659\n",
            "Epoch 6/100, Train Loss: 0.1252, Val Loss: 0.1397\n",
            "Epoch 7/100, Train Loss: 0.1074, Val Loss: 0.0868\n",
            "Epoch 8/100, Train Loss: 0.0775, Val Loss: 0.0595\n",
            "Epoch 9/100, Train Loss: 0.0764, Val Loss: 0.0479\n",
            "Epoch 10/100, Train Loss: 0.0723, Val Loss: 0.0324\n",
            "Epoch 11/100, Train Loss: 0.0782, Val Loss: 0.0366\n",
            "Epoch 12/100, Train Loss: 0.0700, Val Loss: 0.0369\n",
            "Epoch 13/100, Train Loss: 0.0672, Val Loss: 0.0442\n",
            "Epoch 14/100, Train Loss: 0.0716, Val Loss: 0.0460\n",
            "Epoch 15/100, Train Loss: 0.0695, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0604, Val Loss: 0.0393\n",
            "Epoch 17/100, Train Loss: 0.0615, Val Loss: 0.0394\n",
            "Epoch 18/100, Train Loss: 0.0614, Val Loss: 0.0393\n",
            "Epoch 19/100, Train Loss: 0.0606, Val Loss: 0.0392\n",
            "Epoch 20/100, Train Loss: 0.0609, Val Loss: 0.0392\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0623, Val Loss: 0.0392\n",
            "Epoch 22/100, Train Loss: 0.0623, Val Loss: 0.0391\n",
            "Epoch 23/100, Train Loss: 0.0599, Val Loss: 0.0391\n",
            "Epoch 24/100, Train Loss: 0.0584, Val Loss: 0.0391\n",
            "Epoch 25/100, Train Loss: 0.0593, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0577, Val Loss: 0.0391\n",
            "Epoch 27/100, Train Loss: 0.0596, Val Loss: 0.0391\n",
            "Epoch 28/100, Train Loss: 0.0594, Val Loss: 0.0391\n",
            "Epoch 29/100, Train Loss: 0.0576, Val Loss: 0.0391\n",
            "Epoch 30/100, Train Loss: 0.0608, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0596, Val Loss: 0.0391\n",
            "Epoch 32/100, Train Loss: 0.0600, Val Loss: 0.0391\n",
            "Epoch 33/100, Train Loss: 0.0597, Val Loss: 0.0391\n",
            "Epoch 34/100, Train Loss: 0.0625, Val Loss: 0.0391\n",
            "Epoch 35/100, Train Loss: 0.0568, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0598, Val Loss: 0.0391\n",
            "Epoch 37/100, Train Loss: 0.0625, Val Loss: 0.0391\n",
            "Epoch 38/100, Train Loss: 0.0602, Val Loss: 0.0391\n",
            "Epoch 39/100, Train Loss: 0.0616, Val Loss: 0.0391\n",
            "Epoch 40/100, Train Loss: 0.0598, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0587, Val Loss: 0.0391\n",
            "Epoch 42/100, Train Loss: 0.0626, Val Loss: 0.0391\n",
            "Epoch 43/100, Train Loss: 0.0589, Val Loss: 0.0391\n",
            "Epoch 44/100, Train Loss: 0.0601, Val Loss: 0.0391\n",
            "Epoch 45/100, Train Loss: 0.0613, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0600, Val Loss: 0.0391\n",
            "Epoch 47/100, Train Loss: 0.0599, Val Loss: 0.0391\n",
            "Epoch 48/100, Train Loss: 0.0614, Val Loss: 0.0391\n",
            "Epoch 49/100, Train Loss: 0.0585, Val Loss: 0.0391\n",
            "Epoch 50/100, Train Loss: 0.0611, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0594, Val Loss: 0.0391\n",
            "Epoch 52/100, Train Loss: 0.0603, Val Loss: 0.0391\n",
            "Epoch 53/100, Train Loss: 0.0608, Val Loss: 0.0391\n",
            "Epoch 54/100, Train Loss: 0.0600, Val Loss: 0.0391\n",
            "Epoch 55/100, Train Loss: 0.0623, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0618, Val Loss: 0.0391\n",
            "Epoch 57/100, Train Loss: 0.0597, Val Loss: 0.0391\n",
            "Epoch 58/100, Train Loss: 0.0602, Val Loss: 0.0391\n",
            "Epoch 59/100, Train Loss: 0.0633, Val Loss: 0.0391\n",
            "Epoch 60/100, Train Loss: 0.0611, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0613, Val Loss: 0.0391\n",
            "Epoch 62/100, Train Loss: 0.0595, Val Loss: 0.0391\n",
            "Epoch 63/100, Train Loss: 0.0591, Val Loss: 0.0391\n",
            "Epoch 64/100, Train Loss: 0.0615, Val Loss: 0.0391\n",
            "Epoch 65/100, Train Loss: 0.0616, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0619, Val Loss: 0.0391\n",
            "Epoch 67/100, Train Loss: 0.0604, Val Loss: 0.0391\n",
            "Epoch 68/100, Train Loss: 0.0604, Val Loss: 0.0391\n",
            "Epoch 69/100, Train Loss: 0.0592, Val Loss: 0.0391\n",
            "Epoch 70/100, Train Loss: 0.0585, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0613, Val Loss: 0.0391\n",
            "Epoch 72/100, Train Loss: 0.0609, Val Loss: 0.0391\n",
            "Epoch 73/100, Train Loss: 0.0589, Val Loss: 0.0391\n",
            "Epoch 74/100, Train Loss: 0.0599, Val Loss: 0.0391\n",
            "Epoch 75/100, Train Loss: 0.0604, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0589, Val Loss: 0.0391\n",
            "Epoch 77/100, Train Loss: 0.0604, Val Loss: 0.0391\n",
            "Epoch 78/100, Train Loss: 0.0614, Val Loss: 0.0391\n",
            "Epoch 79/100, Train Loss: 0.0605, Val Loss: 0.0391\n",
            "Epoch 80/100, Train Loss: 0.0602, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0614, Val Loss: 0.0391\n",
            "Epoch 82/100, Train Loss: 0.0606, Val Loss: 0.0391\n",
            "Epoch 83/100, Train Loss: 0.0619, Val Loss: 0.0391\n",
            "Epoch 84/100, Train Loss: 0.0584, Val Loss: 0.0391\n",
            "Epoch 85/100, Train Loss: 0.0593, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0611, Val Loss: 0.0391\n",
            "Epoch 87/100, Train Loss: 0.0587, Val Loss: 0.0391\n",
            "Epoch 88/100, Train Loss: 0.0594, Val Loss: 0.0391\n",
            "Epoch 89/100, Train Loss: 0.0593, Val Loss: 0.0391\n",
            "Epoch 90/100, Train Loss: 0.0608, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0593, Val Loss: 0.0391\n",
            "Epoch 92/100, Train Loss: 0.0621, Val Loss: 0.0391\n",
            "Epoch 93/100, Train Loss: 0.0601, Val Loss: 0.0391\n",
            "Epoch 94/100, Train Loss: 0.0589, Val Loss: 0.0391\n",
            "Epoch 95/100, Train Loss: 0.0604, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0589, Val Loss: 0.0391\n",
            "Epoch 97/100, Train Loss: 0.0606, Val Loss: 0.0391\n",
            "Epoch 98/100, Train Loss: 0.0606, Val Loss: 0.0391\n",
            "Epoch 99/100, Train Loss: 0.0599, Val Loss: 0.0391\n",
            "Epoch 100/100, Train Loss: 0.0617, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 8/28\n",
            "Current training set size: 104 samples\n",
            "Epoch 1/100, Train Loss: 0.1501, Val Loss: 0.1956\n",
            "Epoch 2/100, Train Loss: 0.1468, Val Loss: 0.1902\n",
            "Epoch 3/100, Train Loss: 0.1424, Val Loss: 0.1873\n",
            "Epoch 4/100, Train Loss: 0.1414, Val Loss: 0.1838\n",
            "Epoch 5/100, Train Loss: 0.1348, Val Loss: 0.1709\n",
            "Epoch 6/100, Train Loss: 0.1274, Val Loss: 0.1608\n",
            "Epoch 7/100, Train Loss: 0.1214, Val Loss: 0.0997\n",
            "Epoch 8/100, Train Loss: 0.0859, Val Loss: 0.0447\n",
            "Epoch 9/100, Train Loss: 0.0824, Val Loss: 0.0777\n",
            "Epoch 10/100, Train Loss: 0.0753, Val Loss: 0.0456\n",
            "Epoch 11/100, Train Loss: 0.0634, Val Loss: 0.0316\n",
            "Epoch 12/100, Train Loss: 0.0677, Val Loss: 0.0412\n",
            "Epoch 13/100, Train Loss: 0.0674, Val Loss: 0.0472\n",
            "Epoch 14/100, Train Loss: 0.0674, Val Loss: 0.0394\n",
            "Epoch 15/100, Train Loss: 0.0690, Val Loss: 0.0362\n",
            "Epoch 16/100, Train Loss: 0.0687, Val Loss: 0.0406\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0608, Val Loss: 0.0399\n",
            "Epoch 18/100, Train Loss: 0.0603, Val Loss: 0.0390\n",
            "Epoch 19/100, Train Loss: 0.0604, Val Loss: 0.0385\n",
            "Epoch 20/100, Train Loss: 0.0576, Val Loss: 0.0384\n",
            "Epoch 21/100, Train Loss: 0.0603, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0572, Val Loss: 0.0384\n",
            "Epoch 23/100, Train Loss: 0.0602, Val Loss: 0.0384\n",
            "Epoch 24/100, Train Loss: 0.0599, Val Loss: 0.0384\n",
            "Epoch 25/100, Train Loss: 0.0589, Val Loss: 0.0384\n",
            "Epoch 26/100, Train Loss: 0.0577, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0621, Val Loss: 0.0384\n",
            "Epoch 28/100, Train Loss: 0.0597, Val Loss: 0.0384\n",
            "Epoch 29/100, Train Loss: 0.0594, Val Loss: 0.0384\n",
            "Epoch 30/100, Train Loss: 0.0585, Val Loss: 0.0384\n",
            "Epoch 31/100, Train Loss: 0.0600, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0591, Val Loss: 0.0384\n",
            "Epoch 33/100, Train Loss: 0.0604, Val Loss: 0.0384\n",
            "Epoch 34/100, Train Loss: 0.0584, Val Loss: 0.0384\n",
            "Epoch 35/100, Train Loss: 0.0606, Val Loss: 0.0384\n",
            "Epoch 36/100, Train Loss: 0.0591, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0593, Val Loss: 0.0384\n",
            "Epoch 38/100, Train Loss: 0.0596, Val Loss: 0.0384\n",
            "Epoch 39/100, Train Loss: 0.0576, Val Loss: 0.0384\n",
            "Epoch 40/100, Train Loss: 0.0597, Val Loss: 0.0384\n",
            "Epoch 41/100, Train Loss: 0.0573, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0587, Val Loss: 0.0384\n",
            "Epoch 43/100, Train Loss: 0.0577, Val Loss: 0.0384\n",
            "Epoch 44/100, Train Loss: 0.0598, Val Loss: 0.0384\n",
            "Epoch 45/100, Train Loss: 0.0588, Val Loss: 0.0384\n",
            "Epoch 46/100, Train Loss: 0.0576, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0588, Val Loss: 0.0384\n",
            "Epoch 48/100, Train Loss: 0.0578, Val Loss: 0.0384\n",
            "Epoch 49/100, Train Loss: 0.0601, Val Loss: 0.0384\n",
            "Epoch 50/100, Train Loss: 0.0580, Val Loss: 0.0384\n",
            "Epoch 51/100, Train Loss: 0.0566, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0609, Val Loss: 0.0384\n",
            "Epoch 53/100, Train Loss: 0.0595, Val Loss: 0.0384\n",
            "Epoch 54/100, Train Loss: 0.0576, Val Loss: 0.0384\n",
            "Epoch 55/100, Train Loss: 0.0600, Val Loss: 0.0384\n",
            "Epoch 56/100, Train Loss: 0.0620, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0619, Val Loss: 0.0384\n",
            "Epoch 58/100, Train Loss: 0.0583, Val Loss: 0.0384\n",
            "Epoch 59/100, Train Loss: 0.0577, Val Loss: 0.0384\n",
            "Epoch 60/100, Train Loss: 0.0605, Val Loss: 0.0384\n",
            "Epoch 61/100, Train Loss: 0.0618, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0583, Val Loss: 0.0384\n",
            "Epoch 63/100, Train Loss: 0.0583, Val Loss: 0.0384\n",
            "Epoch 64/100, Train Loss: 0.0568, Val Loss: 0.0384\n",
            "Epoch 65/100, Train Loss: 0.0605, Val Loss: 0.0384\n",
            "Epoch 66/100, Train Loss: 0.0587, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0603, Val Loss: 0.0384\n",
            "Epoch 68/100, Train Loss: 0.0594, Val Loss: 0.0384\n",
            "Epoch 69/100, Train Loss: 0.0602, Val Loss: 0.0384\n",
            "Epoch 70/100, Train Loss: 0.0604, Val Loss: 0.0384\n",
            "Epoch 71/100, Train Loss: 0.0569, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0608, Val Loss: 0.0384\n",
            "Epoch 73/100, Train Loss: 0.0596, Val Loss: 0.0384\n",
            "Epoch 74/100, Train Loss: 0.0591, Val Loss: 0.0384\n",
            "Epoch 75/100, Train Loss: 0.0599, Val Loss: 0.0384\n",
            "Epoch 76/100, Train Loss: 0.0595, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0566, Val Loss: 0.0384\n",
            "Epoch 78/100, Train Loss: 0.0597, Val Loss: 0.0384\n",
            "Epoch 79/100, Train Loss: 0.0577, Val Loss: 0.0384\n",
            "Epoch 80/100, Train Loss: 0.0597, Val Loss: 0.0384\n",
            "Epoch 81/100, Train Loss: 0.0607, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0616, Val Loss: 0.0384\n",
            "Epoch 83/100, Train Loss: 0.0599, Val Loss: 0.0384\n",
            "Epoch 84/100, Train Loss: 0.0594, Val Loss: 0.0384\n",
            "Epoch 85/100, Train Loss: 0.0594, Val Loss: 0.0384\n",
            "Epoch 86/100, Train Loss: 0.0569, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0588, Val Loss: 0.0384\n",
            "Epoch 88/100, Train Loss: 0.0581, Val Loss: 0.0384\n",
            "Epoch 89/100, Train Loss: 0.0587, Val Loss: 0.0384\n",
            "Epoch 90/100, Train Loss: 0.0575, Val Loss: 0.0384\n",
            "Epoch 91/100, Train Loss: 0.0595, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0596, Val Loss: 0.0384\n",
            "Epoch 93/100, Train Loss: 0.0601, Val Loss: 0.0384\n",
            "Epoch 94/100, Train Loss: 0.0595, Val Loss: 0.0384\n",
            "Epoch 95/100, Train Loss: 0.0615, Val Loss: 0.0384\n",
            "Epoch 96/100, Train Loss: 0.0608, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0579, Val Loss: 0.0384\n",
            "Epoch 98/100, Train Loss: 0.0589, Val Loss: 0.0384\n",
            "Epoch 99/100, Train Loss: 0.0597, Val Loss: 0.0384\n",
            "Epoch 100/100, Train Loss: 0.0613, Val Loss: 0.0384\n",
            "\n",
            "Test iteration 9/28\n",
            "Current training set size: 105 samples\n",
            "Epoch 1/100, Train Loss: 0.1427, Val Loss: 0.2073\n",
            "Epoch 2/100, Train Loss: 0.1409, Val Loss: 0.1964\n",
            "Epoch 3/100, Train Loss: 0.1357, Val Loss: 0.1863\n",
            "Epoch 4/100, Train Loss: 0.1378, Val Loss: 0.1929\n",
            "Epoch 5/100, Train Loss: 0.1337, Val Loss: 0.1701\n",
            "Epoch 6/100, Train Loss: 0.1393, Val Loss: 0.1611\n",
            "Epoch 7/100, Train Loss: 0.1280, Val Loss: 0.1470\n",
            "Epoch 8/100, Train Loss: 0.1002, Val Loss: 0.0531\n",
            "Epoch 9/100, Train Loss: 0.0798, Val Loss: 0.0445\n",
            "Epoch 10/100, Train Loss: 0.0693, Val Loss: 0.0435\n",
            "Epoch 11/100, Train Loss: 0.0730, Val Loss: 0.0427\n",
            "Epoch 12/100, Train Loss: 0.0662, Val Loss: 0.0391\n",
            "Epoch 13/100, Train Loss: 0.0652, Val Loss: 0.0384\n",
            "Epoch 14/100, Train Loss: 0.0644, Val Loss: 0.0342\n",
            "Epoch 15/100, Train Loss: 0.0705, Val Loss: 0.0359\n",
            "Epoch 16/100, Train Loss: 0.0566, Val Loss: 0.0410\n",
            "Epoch 17/100, Train Loss: 0.0569, Val Loss: 0.0350\n",
            "Epoch 18/100, Train Loss: 0.0566, Val Loss: 0.0364\n",
            "Epoch 19/100, Train Loss: 0.0557, Val Loss: 0.0375\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0551, Val Loss: 0.0373\n",
            "Epoch 21/100, Train Loss: 0.0556, Val Loss: 0.0368\n",
            "Epoch 22/100, Train Loss: 0.0550, Val Loss: 0.0366\n",
            "Epoch 23/100, Train Loss: 0.0538, Val Loss: 0.0365\n",
            "Epoch 24/100, Train Loss: 0.0509, Val Loss: 0.0363\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0601, Val Loss: 0.0363\n",
            "Epoch 26/100, Train Loss: 0.0510, Val Loss: 0.0363\n",
            "Epoch 27/100, Train Loss: 0.0526, Val Loss: 0.0363\n",
            "Epoch 28/100, Train Loss: 0.0538, Val Loss: 0.0363\n",
            "Epoch 29/100, Train Loss: 0.0522, Val Loss: 0.0363\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0527, Val Loss: 0.0363\n",
            "Epoch 31/100, Train Loss: 0.0532, Val Loss: 0.0363\n",
            "Epoch 32/100, Train Loss: 0.0502, Val Loss: 0.0363\n",
            "Epoch 33/100, Train Loss: 0.0530, Val Loss: 0.0363\n",
            "Epoch 34/100, Train Loss: 0.0540, Val Loss: 0.0363\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0527, Val Loss: 0.0363\n",
            "Epoch 36/100, Train Loss: 0.0538, Val Loss: 0.0363\n",
            "Epoch 37/100, Train Loss: 0.0511, Val Loss: 0.0363\n",
            "Epoch 38/100, Train Loss: 0.0538, Val Loss: 0.0363\n",
            "Epoch 39/100, Train Loss: 0.0570, Val Loss: 0.0363\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0526, Val Loss: 0.0363\n",
            "Epoch 41/100, Train Loss: 0.0519, Val Loss: 0.0363\n",
            "Epoch 42/100, Train Loss: 0.0507, Val Loss: 0.0363\n",
            "Epoch 43/100, Train Loss: 0.0531, Val Loss: 0.0363\n",
            "Epoch 44/100, Train Loss: 0.0521, Val Loss: 0.0363\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0517, Val Loss: 0.0363\n",
            "Epoch 46/100, Train Loss: 0.0541, Val Loss: 0.0363\n",
            "Epoch 47/100, Train Loss: 0.0523, Val Loss: 0.0363\n",
            "Epoch 48/100, Train Loss: 0.0537, Val Loss: 0.0363\n",
            "Epoch 49/100, Train Loss: 0.0550, Val Loss: 0.0363\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0638, Val Loss: 0.0363\n",
            "Epoch 51/100, Train Loss: 0.0538, Val Loss: 0.0363\n",
            "Epoch 52/100, Train Loss: 0.0521, Val Loss: 0.0363\n",
            "Epoch 53/100, Train Loss: 0.0535, Val Loss: 0.0363\n",
            "Epoch 54/100, Train Loss: 0.0517, Val Loss: 0.0363\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0566, Val Loss: 0.0363\n",
            "Epoch 56/100, Train Loss: 0.0522, Val Loss: 0.0363\n",
            "Epoch 57/100, Train Loss: 0.0553, Val Loss: 0.0363\n",
            "Epoch 58/100, Train Loss: 0.0518, Val Loss: 0.0363\n",
            "Epoch 59/100, Train Loss: 0.0528, Val Loss: 0.0363\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0669, Val Loss: 0.0363\n",
            "Epoch 61/100, Train Loss: 0.0579, Val Loss: 0.0363\n",
            "Epoch 62/100, Train Loss: 0.0535, Val Loss: 0.0363\n",
            "Epoch 63/100, Train Loss: 0.0527, Val Loss: 0.0363\n",
            "Epoch 64/100, Train Loss: 0.0548, Val Loss: 0.0363\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0549, Val Loss: 0.0363\n",
            "Epoch 66/100, Train Loss: 0.0524, Val Loss: 0.0363\n",
            "Epoch 67/100, Train Loss: 0.0521, Val Loss: 0.0363\n",
            "Epoch 68/100, Train Loss: 0.0549, Val Loss: 0.0363\n",
            "Epoch 69/100, Train Loss: 0.0522, Val Loss: 0.0363\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0545, Val Loss: 0.0363\n",
            "Epoch 71/100, Train Loss: 0.0531, Val Loss: 0.0363\n",
            "Epoch 72/100, Train Loss: 0.0510, Val Loss: 0.0363\n",
            "Epoch 73/100, Train Loss: 0.0530, Val Loss: 0.0363\n",
            "Epoch 74/100, Train Loss: 0.0524, Val Loss: 0.0363\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0519, Val Loss: 0.0363\n",
            "Epoch 76/100, Train Loss: 0.0527, Val Loss: 0.0363\n",
            "Epoch 77/100, Train Loss: 0.0523, Val Loss: 0.0363\n",
            "Epoch 78/100, Train Loss: 0.0518, Val Loss: 0.0363\n",
            "Epoch 79/100, Train Loss: 0.0530, Val Loss: 0.0363\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0517, Val Loss: 0.0363\n",
            "Epoch 81/100, Train Loss: 0.0523, Val Loss: 0.0363\n",
            "Epoch 82/100, Train Loss: 0.0561, Val Loss: 0.0363\n",
            "Epoch 83/100, Train Loss: 0.0531, Val Loss: 0.0363\n",
            "Epoch 84/100, Train Loss: 0.0522, Val Loss: 0.0363\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0519, Val Loss: 0.0363\n",
            "Epoch 86/100, Train Loss: 0.0514, Val Loss: 0.0363\n",
            "Epoch 87/100, Train Loss: 0.0527, Val Loss: 0.0363\n",
            "Epoch 88/100, Train Loss: 0.0528, Val Loss: 0.0363\n",
            "Epoch 89/100, Train Loss: 0.0529, Val Loss: 0.0363\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0538, Val Loss: 0.0363\n",
            "Epoch 91/100, Train Loss: 0.0534, Val Loss: 0.0363\n",
            "Epoch 92/100, Train Loss: 0.0499, Val Loss: 0.0363\n",
            "Epoch 93/100, Train Loss: 0.0548, Val Loss: 0.0363\n",
            "Epoch 94/100, Train Loss: 0.0609, Val Loss: 0.0363\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0517, Val Loss: 0.0363\n",
            "Epoch 96/100, Train Loss: 0.0511, Val Loss: 0.0363\n",
            "Epoch 97/100, Train Loss: 0.0541, Val Loss: 0.0363\n",
            "Epoch 98/100, Train Loss: 0.0518, Val Loss: 0.0363\n",
            "Epoch 99/100, Train Loss: 0.0540, Val Loss: 0.0363\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0574, Val Loss: 0.0363\n",
            "\n",
            "Test iteration 10/28\n",
            "Current training set size: 106 samples\n",
            "Epoch 1/100, Train Loss: 0.1535, Val Loss: 0.1918\n",
            "Epoch 2/100, Train Loss: 0.1447, Val Loss: 0.1891\n",
            "Epoch 3/100, Train Loss: 0.1425, Val Loss: 0.1885\n",
            "Epoch 4/100, Train Loss: 0.1394, Val Loss: 0.1810\n",
            "Epoch 5/100, Train Loss: 0.1308, Val Loss: 0.1562\n",
            "Epoch 6/100, Train Loss: 0.1203, Val Loss: 0.1225\n",
            "Epoch 7/100, Train Loss: 0.0855, Val Loss: 0.0493\n",
            "Epoch 8/100, Train Loss: 0.0804, Val Loss: 0.0741\n",
            "Epoch 9/100, Train Loss: 0.0817, Val Loss: 0.0447\n",
            "Epoch 10/100, Train Loss: 0.0727, Val Loss: 0.0526\n",
            "Epoch 11/100, Train Loss: 0.0706, Val Loss: 0.0354\n",
            "Epoch 12/100, Train Loss: 0.0682, Val Loss: 0.0418\n",
            "Epoch 13/100, Train Loss: 0.0639, Val Loss: 0.0369\n",
            "Epoch 14/100, Train Loss: 0.0613, Val Loss: 0.0393\n",
            "Epoch 15/100, Train Loss: 0.0682, Val Loss: 0.0371\n",
            "Epoch 16/100, Train Loss: 0.0594, Val Loss: 0.0477\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0589, Val Loss: 0.0468\n",
            "Epoch 18/100, Train Loss: 0.0591, Val Loss: 0.0460\n",
            "Epoch 19/100, Train Loss: 0.0573, Val Loss: 0.0455\n",
            "Epoch 20/100, Train Loss: 0.0565, Val Loss: 0.0450\n",
            "Epoch 21/100, Train Loss: 0.0575, Val Loss: 0.0447\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0556, Val Loss: 0.0447\n",
            "Epoch 23/100, Train Loss: 0.0557, Val Loss: 0.0446\n",
            "Epoch 24/100, Train Loss: 0.0575, Val Loss: 0.0446\n",
            "Epoch 25/100, Train Loss: 0.0560, Val Loss: 0.0446\n",
            "Epoch 26/100, Train Loss: 0.0596, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0567, Val Loss: 0.0446\n",
            "Epoch 28/100, Train Loss: 0.0579, Val Loss: 0.0446\n",
            "Epoch 29/100, Train Loss: 0.0552, Val Loss: 0.0446\n",
            "Epoch 30/100, Train Loss: 0.0577, Val Loss: 0.0446\n",
            "Epoch 31/100, Train Loss: 0.0583, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0641, Val Loss: 0.0446\n",
            "Epoch 33/100, Train Loss: 0.0584, Val Loss: 0.0446\n",
            "Epoch 34/100, Train Loss: 0.0645, Val Loss: 0.0446\n",
            "Epoch 35/100, Train Loss: 0.0587, Val Loss: 0.0446\n",
            "Epoch 36/100, Train Loss: 0.0586, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0587, Val Loss: 0.0446\n",
            "Epoch 38/100, Train Loss: 0.0588, Val Loss: 0.0446\n",
            "Epoch 39/100, Train Loss: 0.0610, Val Loss: 0.0446\n",
            "Epoch 40/100, Train Loss: 0.0561, Val Loss: 0.0446\n",
            "Epoch 41/100, Train Loss: 0.0565, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0574, Val Loss: 0.0446\n",
            "Epoch 43/100, Train Loss: 0.0572, Val Loss: 0.0446\n",
            "Epoch 44/100, Train Loss: 0.0583, Val Loss: 0.0446\n",
            "Epoch 45/100, Train Loss: 0.0577, Val Loss: 0.0446\n",
            "Epoch 46/100, Train Loss: 0.0584, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0602, Val Loss: 0.0446\n",
            "Epoch 48/100, Train Loss: 0.0565, Val Loss: 0.0446\n",
            "Epoch 49/100, Train Loss: 0.0611, Val Loss: 0.0446\n",
            "Epoch 50/100, Train Loss: 0.0574, Val Loss: 0.0446\n",
            "Epoch 51/100, Train Loss: 0.0571, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0586, Val Loss: 0.0446\n",
            "Epoch 53/100, Train Loss: 0.0571, Val Loss: 0.0446\n",
            "Epoch 54/100, Train Loss: 0.0593, Val Loss: 0.0446\n",
            "Epoch 55/100, Train Loss: 0.0570, Val Loss: 0.0446\n",
            "Epoch 56/100, Train Loss: 0.0622, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0573, Val Loss: 0.0446\n",
            "Epoch 58/100, Train Loss: 0.0600, Val Loss: 0.0446\n",
            "Epoch 59/100, Train Loss: 0.0573, Val Loss: 0.0446\n",
            "Epoch 60/100, Train Loss: 0.0562, Val Loss: 0.0446\n",
            "Epoch 61/100, Train Loss: 0.0586, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0563, Val Loss: 0.0446\n",
            "Epoch 63/100, Train Loss: 0.0587, Val Loss: 0.0446\n",
            "Epoch 64/100, Train Loss: 0.0568, Val Loss: 0.0446\n",
            "Epoch 65/100, Train Loss: 0.0598, Val Loss: 0.0446\n",
            "Epoch 66/100, Train Loss: 0.0595, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0606, Val Loss: 0.0446\n",
            "Epoch 68/100, Train Loss: 0.0593, Val Loss: 0.0446\n",
            "Epoch 69/100, Train Loss: 0.0581, Val Loss: 0.0446\n",
            "Epoch 70/100, Train Loss: 0.0569, Val Loss: 0.0446\n",
            "Epoch 71/100, Train Loss: 0.0577, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0572, Val Loss: 0.0446\n",
            "Epoch 73/100, Train Loss: 0.0569, Val Loss: 0.0446\n",
            "Epoch 74/100, Train Loss: 0.0585, Val Loss: 0.0446\n",
            "Epoch 75/100, Train Loss: 0.0613, Val Loss: 0.0446\n",
            "Epoch 76/100, Train Loss: 0.0564, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0591, Val Loss: 0.0446\n",
            "Epoch 78/100, Train Loss: 0.0569, Val Loss: 0.0446\n",
            "Epoch 79/100, Train Loss: 0.0587, Val Loss: 0.0446\n",
            "Epoch 80/100, Train Loss: 0.0603, Val Loss: 0.0446\n",
            "Epoch 81/100, Train Loss: 0.0554, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0594, Val Loss: 0.0446\n",
            "Epoch 83/100, Train Loss: 0.0583, Val Loss: 0.0446\n",
            "Epoch 84/100, Train Loss: 0.0585, Val Loss: 0.0446\n",
            "Epoch 85/100, Train Loss: 0.0622, Val Loss: 0.0446\n",
            "Epoch 86/100, Train Loss: 0.0561, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0597, Val Loss: 0.0446\n",
            "Epoch 88/100, Train Loss: 0.0568, Val Loss: 0.0446\n",
            "Epoch 89/100, Train Loss: 0.0596, Val Loss: 0.0446\n",
            "Epoch 90/100, Train Loss: 0.0573, Val Loss: 0.0446\n",
            "Epoch 91/100, Train Loss: 0.0609, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0568, Val Loss: 0.0446\n",
            "Epoch 93/100, Train Loss: 0.0596, Val Loss: 0.0446\n",
            "Epoch 94/100, Train Loss: 0.0598, Val Loss: 0.0446\n",
            "Epoch 95/100, Train Loss: 0.0589, Val Loss: 0.0446\n",
            "Epoch 96/100, Train Loss: 0.0556, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0559, Val Loss: 0.0446\n",
            "Epoch 98/100, Train Loss: 0.0563, Val Loss: 0.0446\n",
            "Epoch 99/100, Train Loss: 0.0562, Val Loss: 0.0446\n",
            "Epoch 100/100, Train Loss: 0.0579, Val Loss: 0.0446\n",
            "\n",
            "Test iteration 11/28\n",
            "Current training set size: 107 samples\n",
            "Epoch 1/100, Train Loss: 0.1475, Val Loss: 0.1940\n",
            "Epoch 2/100, Train Loss: 0.1442, Val Loss: 0.1890\n",
            "Epoch 3/100, Train Loss: 0.1409, Val Loss: 0.1878\n",
            "Epoch 4/100, Train Loss: 0.1395, Val Loss: 0.1838\n",
            "Epoch 5/100, Train Loss: 0.1407, Val Loss: 0.1776\n",
            "Epoch 6/100, Train Loss: 0.1228, Val Loss: 0.1501\n",
            "Epoch 7/100, Train Loss: 0.1016, Val Loss: 0.0655\n",
            "Epoch 8/100, Train Loss: 0.0741, Val Loss: 0.0424\n",
            "Epoch 9/100, Train Loss: 0.0695, Val Loss: 0.0515\n",
            "Epoch 10/100, Train Loss: 0.0698, Val Loss: 0.0348\n",
            "Epoch 11/100, Train Loss: 0.0677, Val Loss: 0.0426\n",
            "Epoch 12/100, Train Loss: 0.0634, Val Loss: 0.0404\n",
            "Epoch 13/100, Train Loss: 0.0654, Val Loss: 0.0393\n",
            "Epoch 14/100, Train Loss: 0.0596, Val Loss: 0.0375\n",
            "Epoch 15/100, Train Loss: 0.0633, Val Loss: 0.0425\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0619, Val Loss: 0.0424\n",
            "Epoch 17/100, Train Loss: 0.0600, Val Loss: 0.0424\n",
            "Epoch 18/100, Train Loss: 0.0585, Val Loss: 0.0424\n",
            "Epoch 19/100, Train Loss: 0.0622, Val Loss: 0.0422\n",
            "Epoch 20/100, Train Loss: 0.0578, Val Loss: 0.0421\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0574, Val Loss: 0.0421\n",
            "Epoch 22/100, Train Loss: 0.0595, Val Loss: 0.0421\n",
            "Epoch 23/100, Train Loss: 0.0556, Val Loss: 0.0421\n",
            "Epoch 24/100, Train Loss: 0.0571, Val Loss: 0.0421\n",
            "Epoch 25/100, Train Loss: 0.0564, Val Loss: 0.0421\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0591, Val Loss: 0.0421\n",
            "Epoch 27/100, Train Loss: 0.0600, Val Loss: 0.0421\n",
            "Epoch 28/100, Train Loss: 0.0568, Val Loss: 0.0421\n",
            "Epoch 29/100, Train Loss: 0.0579, Val Loss: 0.0421\n",
            "Epoch 30/100, Train Loss: 0.0586, Val Loss: 0.0421\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0582, Val Loss: 0.0421\n",
            "Epoch 32/100, Train Loss: 0.0581, Val Loss: 0.0421\n",
            "Epoch 33/100, Train Loss: 0.0572, Val Loss: 0.0421\n",
            "Epoch 34/100, Train Loss: 0.0569, Val Loss: 0.0421\n",
            "Epoch 35/100, Train Loss: 0.0595, Val Loss: 0.0421\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0565, Val Loss: 0.0421\n",
            "Epoch 37/100, Train Loss: 0.0604, Val Loss: 0.0421\n",
            "Epoch 38/100, Train Loss: 0.0578, Val Loss: 0.0421\n",
            "Epoch 39/100, Train Loss: 0.0573, Val Loss: 0.0421\n",
            "Epoch 40/100, Train Loss: 0.0617, Val Loss: 0.0421\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0606, Val Loss: 0.0421\n",
            "Epoch 42/100, Train Loss: 0.0569, Val Loss: 0.0421\n",
            "Epoch 43/100, Train Loss: 0.0573, Val Loss: 0.0421\n",
            "Epoch 44/100, Train Loss: 0.0569, Val Loss: 0.0421\n",
            "Epoch 45/100, Train Loss: 0.0563, Val Loss: 0.0421\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0612, Val Loss: 0.0421\n",
            "Epoch 47/100, Train Loss: 0.0587, Val Loss: 0.0421\n",
            "Epoch 48/100, Train Loss: 0.0584, Val Loss: 0.0421\n",
            "Epoch 49/100, Train Loss: 0.0548, Val Loss: 0.0421\n",
            "Epoch 50/100, Train Loss: 0.0576, Val Loss: 0.0421\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0608, Val Loss: 0.0421\n",
            "Epoch 52/100, Train Loss: 0.0561, Val Loss: 0.0421\n",
            "Epoch 53/100, Train Loss: 0.0595, Val Loss: 0.0421\n",
            "Epoch 54/100, Train Loss: 0.0596, Val Loss: 0.0421\n",
            "Epoch 55/100, Train Loss: 0.0556, Val Loss: 0.0421\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0570, Val Loss: 0.0421\n",
            "Epoch 57/100, Train Loss: 0.0572, Val Loss: 0.0421\n",
            "Epoch 58/100, Train Loss: 0.0565, Val Loss: 0.0421\n",
            "Epoch 59/100, Train Loss: 0.0588, Val Loss: 0.0421\n",
            "Epoch 60/100, Train Loss: 0.0578, Val Loss: 0.0421\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0590, Val Loss: 0.0421\n",
            "Epoch 62/100, Train Loss: 0.0579, Val Loss: 0.0421\n",
            "Epoch 63/100, Train Loss: 0.0571, Val Loss: 0.0421\n",
            "Epoch 64/100, Train Loss: 0.0574, Val Loss: 0.0421\n",
            "Epoch 65/100, Train Loss: 0.0568, Val Loss: 0.0421\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0577, Val Loss: 0.0421\n",
            "Epoch 67/100, Train Loss: 0.0577, Val Loss: 0.0421\n",
            "Epoch 68/100, Train Loss: 0.0582, Val Loss: 0.0421\n",
            "Epoch 69/100, Train Loss: 0.0584, Val Loss: 0.0421\n",
            "Epoch 70/100, Train Loss: 0.0574, Val Loss: 0.0421\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0579, Val Loss: 0.0421\n",
            "Epoch 72/100, Train Loss: 0.0560, Val Loss: 0.0421\n",
            "Epoch 73/100, Train Loss: 0.0572, Val Loss: 0.0421\n",
            "Epoch 74/100, Train Loss: 0.0582, Val Loss: 0.0421\n",
            "Epoch 75/100, Train Loss: 0.0570, Val Loss: 0.0421\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0594, Val Loss: 0.0421\n",
            "Epoch 77/100, Train Loss: 0.0579, Val Loss: 0.0421\n",
            "Epoch 78/100, Train Loss: 0.0584, Val Loss: 0.0421\n",
            "Epoch 79/100, Train Loss: 0.0600, Val Loss: 0.0421\n",
            "Epoch 80/100, Train Loss: 0.0586, Val Loss: 0.0421\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0570, Val Loss: 0.0421\n",
            "Epoch 82/100, Train Loss: 0.0597, Val Loss: 0.0421\n",
            "Epoch 83/100, Train Loss: 0.0556, Val Loss: 0.0421\n",
            "Epoch 84/100, Train Loss: 0.0588, Val Loss: 0.0421\n",
            "Epoch 85/100, Train Loss: 0.0573, Val Loss: 0.0421\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0570, Val Loss: 0.0421\n",
            "Epoch 87/100, Train Loss: 0.0569, Val Loss: 0.0421\n",
            "Epoch 88/100, Train Loss: 0.0580, Val Loss: 0.0421\n",
            "Epoch 89/100, Train Loss: 0.0571, Val Loss: 0.0421\n",
            "Epoch 90/100, Train Loss: 0.0561, Val Loss: 0.0421\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0615, Val Loss: 0.0421\n",
            "Epoch 92/100, Train Loss: 0.0586, Val Loss: 0.0421\n",
            "Epoch 93/100, Train Loss: 0.0581, Val Loss: 0.0421\n",
            "Epoch 94/100, Train Loss: 0.0595, Val Loss: 0.0421\n",
            "Epoch 95/100, Train Loss: 0.0556, Val Loss: 0.0421\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0585, Val Loss: 0.0421\n",
            "Epoch 97/100, Train Loss: 0.0593, Val Loss: 0.0421\n",
            "Epoch 98/100, Train Loss: 0.0581, Val Loss: 0.0421\n",
            "Epoch 99/100, Train Loss: 0.0571, Val Loss: 0.0421\n",
            "Epoch 100/100, Train Loss: 0.0568, Val Loss: 0.0421\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 12/28\n",
            "Current training set size: 108 samples\n",
            "Epoch 1/100, Train Loss: 0.1586, Val Loss: 0.1921\n",
            "Epoch 2/100, Train Loss: 0.1453, Val Loss: 0.1912\n",
            "Epoch 3/100, Train Loss: 0.1432, Val Loss: 0.1902\n",
            "Epoch 4/100, Train Loss: 0.1395, Val Loss: 0.1841\n",
            "Epoch 5/100, Train Loss: 0.1321, Val Loss: 0.1805\n",
            "Epoch 6/100, Train Loss: 0.1266, Val Loss: 0.1328\n",
            "Epoch 7/100, Train Loss: 0.0952, Val Loss: 0.0622\n",
            "Epoch 8/100, Train Loss: 0.0779, Val Loss: 0.0521\n",
            "Epoch 9/100, Train Loss: 0.0709, Val Loss: 0.0468\n",
            "Epoch 10/100, Train Loss: 0.0702, Val Loss: 0.0347\n",
            "Epoch 11/100, Train Loss: 0.0692, Val Loss: 0.0456\n",
            "Epoch 12/100, Train Loss: 0.0641, Val Loss: 0.0426\n",
            "Epoch 13/100, Train Loss: 0.0679, Val Loss: 0.0434\n",
            "Epoch 14/100, Train Loss: 0.0604, Val Loss: 0.0469\n",
            "Epoch 15/100, Train Loss: 0.0722, Val Loss: 0.0421\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0615, Val Loss: 0.0403\n",
            "Epoch 17/100, Train Loss: 0.0612, Val Loss: 0.0387\n",
            "Epoch 18/100, Train Loss: 0.0614, Val Loss: 0.0382\n",
            "Epoch 19/100, Train Loss: 0.0583, Val Loss: 0.0377\n",
            "Epoch 20/100, Train Loss: 0.0594, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0563, Val Loss: 0.0376\n",
            "Epoch 22/100, Train Loss: 0.0583, Val Loss: 0.0376\n",
            "Epoch 23/100, Train Loss: 0.0572, Val Loss: 0.0376\n",
            "Epoch 24/100, Train Loss: 0.0585, Val Loss: 0.0376\n",
            "Epoch 25/100, Train Loss: 0.0600, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0587, Val Loss: 0.0376\n",
            "Epoch 27/100, Train Loss: 0.0595, Val Loss: 0.0376\n",
            "Epoch 28/100, Train Loss: 0.0593, Val Loss: 0.0376\n",
            "Epoch 29/100, Train Loss: 0.0575, Val Loss: 0.0376\n",
            "Epoch 30/100, Train Loss: 0.0577, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0561, Val Loss: 0.0376\n",
            "Epoch 32/100, Train Loss: 0.0569, Val Loss: 0.0376\n",
            "Epoch 33/100, Train Loss: 0.0575, Val Loss: 0.0376\n",
            "Epoch 34/100, Train Loss: 0.0575, Val Loss: 0.0376\n",
            "Epoch 35/100, Train Loss: 0.0596, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0580, Val Loss: 0.0376\n",
            "Epoch 37/100, Train Loss: 0.0563, Val Loss: 0.0376\n",
            "Epoch 38/100, Train Loss: 0.0572, Val Loss: 0.0376\n",
            "Epoch 39/100, Train Loss: 0.0586, Val Loss: 0.0376\n",
            "Epoch 40/100, Train Loss: 0.0555, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0578, Val Loss: 0.0376\n",
            "Epoch 42/100, Train Loss: 0.0574, Val Loss: 0.0376\n",
            "Epoch 43/100, Train Loss: 0.0584, Val Loss: 0.0376\n",
            "Epoch 44/100, Train Loss: 0.0574, Val Loss: 0.0376\n",
            "Epoch 45/100, Train Loss: 0.0607, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0579, Val Loss: 0.0376\n",
            "Epoch 47/100, Train Loss: 0.0606, Val Loss: 0.0376\n",
            "Epoch 48/100, Train Loss: 0.0555, Val Loss: 0.0376\n",
            "Epoch 49/100, Train Loss: 0.0584, Val Loss: 0.0376\n",
            "Epoch 50/100, Train Loss: 0.0601, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0585, Val Loss: 0.0376\n",
            "Epoch 52/100, Train Loss: 0.0598, Val Loss: 0.0376\n",
            "Epoch 53/100, Train Loss: 0.0593, Val Loss: 0.0376\n",
            "Epoch 54/100, Train Loss: 0.0592, Val Loss: 0.0376\n",
            "Epoch 55/100, Train Loss: 0.0601, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0583, Val Loss: 0.0376\n",
            "Epoch 57/100, Train Loss: 0.0574, Val Loss: 0.0376\n",
            "Epoch 58/100, Train Loss: 0.0591, Val Loss: 0.0376\n",
            "Epoch 59/100, Train Loss: 0.0567, Val Loss: 0.0376\n",
            "Epoch 60/100, Train Loss: 0.0574, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0586, Val Loss: 0.0376\n",
            "Epoch 62/100, Train Loss: 0.0561, Val Loss: 0.0376\n",
            "Epoch 63/100, Train Loss: 0.0602, Val Loss: 0.0376\n",
            "Epoch 64/100, Train Loss: 0.0586, Val Loss: 0.0376\n",
            "Epoch 65/100, Train Loss: 0.0585, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0581, Val Loss: 0.0376\n",
            "Epoch 67/100, Train Loss: 0.0582, Val Loss: 0.0376\n",
            "Epoch 68/100, Train Loss: 0.0590, Val Loss: 0.0376\n",
            "Epoch 69/100, Train Loss: 0.0574, Val Loss: 0.0376\n",
            "Epoch 70/100, Train Loss: 0.0583, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0576, Val Loss: 0.0376\n",
            "Epoch 72/100, Train Loss: 0.0563, Val Loss: 0.0376\n",
            "Epoch 73/100, Train Loss: 0.0557, Val Loss: 0.0376\n",
            "Epoch 74/100, Train Loss: 0.0583, Val Loss: 0.0376\n",
            "Epoch 75/100, Train Loss: 0.0585, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0566, Val Loss: 0.0376\n",
            "Epoch 77/100, Train Loss: 0.0574, Val Loss: 0.0376\n",
            "Epoch 78/100, Train Loss: 0.0596, Val Loss: 0.0376\n",
            "Epoch 79/100, Train Loss: 0.0579, Val Loss: 0.0376\n",
            "Epoch 80/100, Train Loss: 0.0592, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0581, Val Loss: 0.0376\n",
            "Epoch 82/100, Train Loss: 0.0585, Val Loss: 0.0376\n",
            "Epoch 83/100, Train Loss: 0.0560, Val Loss: 0.0376\n",
            "Epoch 84/100, Train Loss: 0.0593, Val Loss: 0.0376\n",
            "Epoch 85/100, Train Loss: 0.0577, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0570, Val Loss: 0.0376\n",
            "Epoch 87/100, Train Loss: 0.0602, Val Loss: 0.0376\n",
            "Epoch 88/100, Train Loss: 0.0581, Val Loss: 0.0376\n",
            "Epoch 89/100, Train Loss: 0.0583, Val Loss: 0.0376\n",
            "Epoch 90/100, Train Loss: 0.0568, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0574, Val Loss: 0.0376\n",
            "Epoch 92/100, Train Loss: 0.0547, Val Loss: 0.0376\n",
            "Epoch 93/100, Train Loss: 0.0602, Val Loss: 0.0376\n",
            "Epoch 94/100, Train Loss: 0.0567, Val Loss: 0.0376\n",
            "Epoch 95/100, Train Loss: 0.0571, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0568, Val Loss: 0.0376\n",
            "Epoch 97/100, Train Loss: 0.0597, Val Loss: 0.0376\n",
            "Epoch 98/100, Train Loss: 0.0576, Val Loss: 0.0376\n",
            "Epoch 99/100, Train Loss: 0.0588, Val Loss: 0.0376\n",
            "Epoch 100/100, Train Loss: 0.0585, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 13/28\n",
            "Current training set size: 109 samples\n",
            "Epoch 1/100, Train Loss: 0.1659, Val Loss: 0.1929\n",
            "Epoch 2/100, Train Loss: 0.1486, Val Loss: 0.1984\n",
            "Epoch 3/100, Train Loss: 0.1496, Val Loss: 0.1889\n",
            "Epoch 4/100, Train Loss: 0.1463, Val Loss: 0.1876\n",
            "Epoch 5/100, Train Loss: 0.1509, Val Loss: 0.1837\n",
            "Epoch 6/100, Train Loss: 0.1343, Val Loss: 0.1634\n",
            "Epoch 7/100, Train Loss: 0.1127, Val Loss: 0.1091\n",
            "Epoch 8/100, Train Loss: 0.0900, Val Loss: 0.0521\n",
            "Epoch 9/100, Train Loss: 0.0720, Val Loss: 0.0520\n",
            "Epoch 10/100, Train Loss: 0.0810, Val Loss: 0.0540\n",
            "Epoch 11/100, Train Loss: 0.0754, Val Loss: 0.0377\n",
            "Epoch 12/100, Train Loss: 0.0700, Val Loss: 0.0351\n",
            "Epoch 13/100, Train Loss: 0.0717, Val Loss: 0.0448\n",
            "Epoch 14/100, Train Loss: 0.0674, Val Loss: 0.0371\n",
            "Epoch 15/100, Train Loss: 0.0625, Val Loss: 0.0382\n",
            "Epoch 16/100, Train Loss: 0.0604, Val Loss: 0.0473\n",
            "Epoch 17/100, Train Loss: 0.0597, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0667, Val Loss: 0.0483\n",
            "Epoch 19/100, Train Loss: 0.0632, Val Loss: 0.0458\n",
            "Epoch 20/100, Train Loss: 0.0616, Val Loss: 0.0437\n",
            "Epoch 21/100, Train Loss: 0.0603, Val Loss: 0.0422\n",
            "Epoch 22/100, Train Loss: 0.0578, Val Loss: 0.0409\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0574, Val Loss: 0.0409\n",
            "Epoch 24/100, Train Loss: 0.0589, Val Loss: 0.0409\n",
            "Epoch 25/100, Train Loss: 0.0593, Val Loss: 0.0408\n",
            "Epoch 26/100, Train Loss: 0.0588, Val Loss: 0.0408\n",
            "Epoch 27/100, Train Loss: 0.0596, Val Loss: 0.0408\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0634, Val Loss: 0.0408\n",
            "Epoch 29/100, Train Loss: 0.0572, Val Loss: 0.0408\n",
            "Epoch 30/100, Train Loss: 0.0580, Val Loss: 0.0408\n",
            "Epoch 31/100, Train Loss: 0.0596, Val Loss: 0.0408\n",
            "Epoch 32/100, Train Loss: 0.0573, Val Loss: 0.0408\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0578, Val Loss: 0.0408\n",
            "Epoch 34/100, Train Loss: 0.0567, Val Loss: 0.0408\n",
            "Epoch 35/100, Train Loss: 0.0605, Val Loss: 0.0408\n",
            "Epoch 36/100, Train Loss: 0.0609, Val Loss: 0.0408\n",
            "Epoch 37/100, Train Loss: 0.0589, Val Loss: 0.0408\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0619, Val Loss: 0.0408\n",
            "Epoch 39/100, Train Loss: 0.0591, Val Loss: 0.0408\n",
            "Epoch 40/100, Train Loss: 0.0596, Val Loss: 0.0408\n",
            "Epoch 41/100, Train Loss: 0.0586, Val Loss: 0.0408\n",
            "Epoch 42/100, Train Loss: 0.0573, Val Loss: 0.0408\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0564, Val Loss: 0.0408\n",
            "Epoch 44/100, Train Loss: 0.0583, Val Loss: 0.0408\n",
            "Epoch 45/100, Train Loss: 0.0593, Val Loss: 0.0408\n",
            "Epoch 46/100, Train Loss: 0.0564, Val Loss: 0.0408\n",
            "Epoch 47/100, Train Loss: 0.0572, Val Loss: 0.0408\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0591, Val Loss: 0.0408\n",
            "Epoch 49/100, Train Loss: 0.0594, Val Loss: 0.0408\n",
            "Epoch 50/100, Train Loss: 0.0593, Val Loss: 0.0408\n",
            "Epoch 51/100, Train Loss: 0.0575, Val Loss: 0.0408\n",
            "Epoch 52/100, Train Loss: 0.0564, Val Loss: 0.0408\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0573, Val Loss: 0.0408\n",
            "Epoch 54/100, Train Loss: 0.0600, Val Loss: 0.0408\n",
            "Epoch 55/100, Train Loss: 0.0730, Val Loss: 0.0408\n",
            "Epoch 56/100, Train Loss: 0.0584, Val Loss: 0.0408\n",
            "Epoch 57/100, Train Loss: 0.0592, Val Loss: 0.0408\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0594, Val Loss: 0.0408\n",
            "Epoch 59/100, Train Loss: 0.0592, Val Loss: 0.0408\n",
            "Epoch 60/100, Train Loss: 0.0580, Val Loss: 0.0408\n",
            "Epoch 61/100, Train Loss: 0.0659, Val Loss: 0.0408\n",
            "Epoch 62/100, Train Loss: 0.0577, Val Loss: 0.0408\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0571, Val Loss: 0.0408\n",
            "Epoch 64/100, Train Loss: 0.0590, Val Loss: 0.0408\n",
            "Epoch 65/100, Train Loss: 0.0599, Val Loss: 0.0408\n",
            "Epoch 66/100, Train Loss: 0.0584, Val Loss: 0.0408\n",
            "Epoch 67/100, Train Loss: 0.0573, Val Loss: 0.0408\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0607, Val Loss: 0.0408\n",
            "Epoch 69/100, Train Loss: 0.0591, Val Loss: 0.0408\n",
            "Epoch 70/100, Train Loss: 0.0610, Val Loss: 0.0408\n",
            "Epoch 71/100, Train Loss: 0.0569, Val Loss: 0.0408\n",
            "Epoch 72/100, Train Loss: 0.0562, Val Loss: 0.0408\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0593, Val Loss: 0.0408\n",
            "Epoch 74/100, Train Loss: 0.0601, Val Loss: 0.0408\n",
            "Epoch 75/100, Train Loss: 0.0611, Val Loss: 0.0408\n",
            "Epoch 76/100, Train Loss: 0.0607, Val Loss: 0.0408\n",
            "Epoch 77/100, Train Loss: 0.0618, Val Loss: 0.0408\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0608, Val Loss: 0.0408\n",
            "Epoch 79/100, Train Loss: 0.0594, Val Loss: 0.0408\n",
            "Epoch 80/100, Train Loss: 0.0571, Val Loss: 0.0408\n",
            "Epoch 81/100, Train Loss: 0.0605, Val Loss: 0.0408\n",
            "Epoch 82/100, Train Loss: 0.0551, Val Loss: 0.0408\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0566, Val Loss: 0.0408\n",
            "Epoch 84/100, Train Loss: 0.0599, Val Loss: 0.0408\n",
            "Epoch 85/100, Train Loss: 0.0603, Val Loss: 0.0408\n",
            "Epoch 86/100, Train Loss: 0.0565, Val Loss: 0.0408\n",
            "Epoch 87/100, Train Loss: 0.0563, Val Loss: 0.0408\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0604, Val Loss: 0.0408\n",
            "Epoch 89/100, Train Loss: 0.0592, Val Loss: 0.0408\n",
            "Epoch 90/100, Train Loss: 0.0589, Val Loss: 0.0408\n",
            "Epoch 91/100, Train Loss: 0.0593, Val Loss: 0.0408\n",
            "Epoch 92/100, Train Loss: 0.0593, Val Loss: 0.0408\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0608, Val Loss: 0.0408\n",
            "Epoch 94/100, Train Loss: 0.0584, Val Loss: 0.0408\n",
            "Epoch 95/100, Train Loss: 0.0598, Val Loss: 0.0408\n",
            "Epoch 96/100, Train Loss: 0.0584, Val Loss: 0.0408\n",
            "Epoch 97/100, Train Loss: 0.0567, Val Loss: 0.0408\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0600, Val Loss: 0.0408\n",
            "Epoch 99/100, Train Loss: 0.0599, Val Loss: 0.0408\n",
            "Epoch 100/100, Train Loss: 0.0582, Val Loss: 0.0408\n",
            "\n",
            "Test iteration 14/28\n",
            "Current training set size: 110 samples\n",
            "Epoch 1/100, Train Loss: 0.1471, Val Loss: 0.2110\n",
            "Epoch 2/100, Train Loss: 0.1494, Val Loss: 0.1909\n",
            "Epoch 3/100, Train Loss: 0.1430, Val Loss: 0.1998\n",
            "Epoch 4/100, Train Loss: 0.1439, Val Loss: 0.1767\n",
            "Epoch 5/100, Train Loss: 0.1396, Val Loss: 0.1678\n",
            "Epoch 6/100, Train Loss: 0.1277, Val Loss: 0.1398\n",
            "Epoch 7/100, Train Loss: 0.0992, Val Loss: 0.1007\n",
            "Epoch 8/100, Train Loss: 0.0820, Val Loss: 0.0503\n",
            "Epoch 9/100, Train Loss: 0.0742, Val Loss: 0.0370\n",
            "Epoch 10/100, Train Loss: 0.0706, Val Loss: 0.0408\n",
            "Epoch 11/100, Train Loss: 0.0701, Val Loss: 0.0496\n",
            "Epoch 12/100, Train Loss: 0.0662, Val Loss: 0.0347\n",
            "Epoch 13/100, Train Loss: 0.0680, Val Loss: 0.0361\n",
            "Epoch 14/100, Train Loss: 0.0623, Val Loss: 0.0580\n",
            "Epoch 15/100, Train Loss: 0.0641, Val Loss: 0.0415\n",
            "Epoch 16/100, Train Loss: 0.0581, Val Loss: 0.0359\n",
            "Epoch 17/100, Train Loss: 0.0595, Val Loss: 0.0340\n",
            "Epoch 18/100, Train Loss: 0.0614, Val Loss: 0.0418\n",
            "Epoch 19/100, Train Loss: 0.0606, Val Loss: 0.0406\n",
            "Epoch 20/100, Train Loss: 0.0584, Val Loss: 0.0470\n",
            "Epoch 21/100, Train Loss: 0.0603, Val Loss: 0.0406\n",
            "Epoch 22/100, Train Loss: 0.0600, Val Loss: 0.0366\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0517, Val Loss: 0.0362\n",
            "Epoch 24/100, Train Loss: 0.0527, Val Loss: 0.0358\n",
            "Epoch 25/100, Train Loss: 0.0544, Val Loss: 0.0356\n",
            "Epoch 26/100, Train Loss: 0.0510, Val Loss: 0.0356\n",
            "Epoch 27/100, Train Loss: 0.0480, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0496, Val Loss: 0.0356\n",
            "Epoch 29/100, Train Loss: 0.0539, Val Loss: 0.0356\n",
            "Epoch 30/100, Train Loss: 0.0502, Val Loss: 0.0356\n",
            "Epoch 31/100, Train Loss: 0.0493, Val Loss: 0.0356\n",
            "Epoch 32/100, Train Loss: 0.0504, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0499, Val Loss: 0.0356\n",
            "Epoch 34/100, Train Loss: 0.0503, Val Loss: 0.0356\n",
            "Epoch 35/100, Train Loss: 0.0504, Val Loss: 0.0356\n",
            "Epoch 36/100, Train Loss: 0.0498, Val Loss: 0.0356\n",
            "Epoch 37/100, Train Loss: 0.0485, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0487, Val Loss: 0.0356\n",
            "Epoch 39/100, Train Loss: 0.0491, Val Loss: 0.0356\n",
            "Epoch 40/100, Train Loss: 0.0508, Val Loss: 0.0356\n",
            "Epoch 41/100, Train Loss: 0.0496, Val Loss: 0.0356\n",
            "Epoch 42/100, Train Loss: 0.0505, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0497, Val Loss: 0.0356\n",
            "Epoch 44/100, Train Loss: 0.0486, Val Loss: 0.0356\n",
            "Epoch 45/100, Train Loss: 0.0513, Val Loss: 0.0356\n",
            "Epoch 46/100, Train Loss: 0.0466, Val Loss: 0.0356\n",
            "Epoch 47/100, Train Loss: 0.0510, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0517, Val Loss: 0.0356\n",
            "Epoch 49/100, Train Loss: 0.0482, Val Loss: 0.0356\n",
            "Epoch 50/100, Train Loss: 0.0482, Val Loss: 0.0356\n",
            "Epoch 51/100, Train Loss: 0.0493, Val Loss: 0.0356\n",
            "Epoch 52/100, Train Loss: 0.0489, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0561, Val Loss: 0.0356\n",
            "Epoch 54/100, Train Loss: 0.0557, Val Loss: 0.0356\n",
            "Epoch 55/100, Train Loss: 0.0491, Val Loss: 0.0356\n",
            "Epoch 56/100, Train Loss: 0.0513, Val Loss: 0.0356\n",
            "Epoch 57/100, Train Loss: 0.0489, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0510, Val Loss: 0.0356\n",
            "Epoch 59/100, Train Loss: 0.0522, Val Loss: 0.0356\n",
            "Epoch 60/100, Train Loss: 0.0517, Val Loss: 0.0356\n",
            "Epoch 61/100, Train Loss: 0.0525, Val Loss: 0.0356\n",
            "Epoch 62/100, Train Loss: 0.0496, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0495, Val Loss: 0.0356\n",
            "Epoch 64/100, Train Loss: 0.0488, Val Loss: 0.0356\n",
            "Epoch 65/100, Train Loss: 0.0508, Val Loss: 0.0356\n",
            "Epoch 66/100, Train Loss: 0.0491, Val Loss: 0.0356\n",
            "Epoch 67/100, Train Loss: 0.0475, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0494, Val Loss: 0.0356\n",
            "Epoch 69/100, Train Loss: 0.0501, Val Loss: 0.0356\n",
            "Epoch 70/100, Train Loss: 0.0490, Val Loss: 0.0356\n",
            "Epoch 71/100, Train Loss: 0.0509, Val Loss: 0.0356\n",
            "Epoch 72/100, Train Loss: 0.0553, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0508, Val Loss: 0.0356\n",
            "Epoch 74/100, Train Loss: 0.0492, Val Loss: 0.0356\n",
            "Epoch 75/100, Train Loss: 0.0482, Val Loss: 0.0356\n",
            "Epoch 76/100, Train Loss: 0.0500, Val Loss: 0.0356\n",
            "Epoch 77/100, Train Loss: 0.0529, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0500, Val Loss: 0.0356\n",
            "Epoch 79/100, Train Loss: 0.0482, Val Loss: 0.0356\n",
            "Epoch 80/100, Train Loss: 0.0507, Val Loss: 0.0356\n",
            "Epoch 81/100, Train Loss: 0.0509, Val Loss: 0.0356\n",
            "Epoch 82/100, Train Loss: 0.0504, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0491, Val Loss: 0.0356\n",
            "Epoch 84/100, Train Loss: 0.0504, Val Loss: 0.0356\n",
            "Epoch 85/100, Train Loss: 0.0499, Val Loss: 0.0356\n",
            "Epoch 86/100, Train Loss: 0.0483, Val Loss: 0.0356\n",
            "Epoch 87/100, Train Loss: 0.0484, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0477, Val Loss: 0.0356\n",
            "Epoch 89/100, Train Loss: 0.0530, Val Loss: 0.0356\n",
            "Epoch 90/100, Train Loss: 0.0478, Val Loss: 0.0356\n",
            "Epoch 91/100, Train Loss: 0.0498, Val Loss: 0.0356\n",
            "Epoch 92/100, Train Loss: 0.0505, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0472, Val Loss: 0.0356\n",
            "Epoch 94/100, Train Loss: 0.0517, Val Loss: 0.0356\n",
            "Epoch 95/100, Train Loss: 0.0498, Val Loss: 0.0356\n",
            "Epoch 96/100, Train Loss: 0.0512, Val Loss: 0.0356\n",
            "Epoch 97/100, Train Loss: 0.0512, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0488, Val Loss: 0.0356\n",
            "Epoch 99/100, Train Loss: 0.0492, Val Loss: 0.0356\n",
            "Epoch 100/100, Train Loss: 0.0500, Val Loss: 0.0356\n",
            "\n",
            "Test iteration 15/28\n",
            "Current training set size: 111 samples\n",
            "Epoch 1/100, Train Loss: 0.1512, Val Loss: 0.1960\n",
            "Epoch 2/100, Train Loss: 0.1475, Val Loss: 0.1911\n",
            "Epoch 3/100, Train Loss: 0.1446, Val Loss: 0.1860\n",
            "Epoch 4/100, Train Loss: 0.1438, Val Loss: 0.1992\n",
            "Epoch 5/100, Train Loss: 0.1405, Val Loss: 0.1664\n",
            "Epoch 6/100, Train Loss: 0.1359, Val Loss: 0.1380\n",
            "Epoch 7/100, Train Loss: 0.1086, Val Loss: 0.0681\n",
            "Epoch 8/100, Train Loss: 0.0720, Val Loss: 0.0423\n",
            "Epoch 9/100, Train Loss: 0.0695, Val Loss: 0.0454\n",
            "Epoch 10/100, Train Loss: 0.0725, Val Loss: 0.0348\n",
            "Epoch 11/100, Train Loss: 0.0655, Val Loss: 0.0414\n",
            "Epoch 12/100, Train Loss: 0.0667, Val Loss: 0.0601\n",
            "Epoch 13/100, Train Loss: 0.0651, Val Loss: 0.0407\n",
            "Epoch 14/100, Train Loss: 0.0624, Val Loss: 0.0379\n",
            "Epoch 15/100, Train Loss: 0.0664, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0606, Val Loss: 0.0388\n",
            "Epoch 17/100, Train Loss: 0.0591, Val Loss: 0.0380\n",
            "Epoch 18/100, Train Loss: 0.0608, Val Loss: 0.0376\n",
            "Epoch 19/100, Train Loss: 0.0585, Val Loss: 0.0373\n",
            "Epoch 20/100, Train Loss: 0.0591, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0579, Val Loss: 0.0369\n",
            "Epoch 22/100, Train Loss: 0.0620, Val Loss: 0.0369\n",
            "Epoch 23/100, Train Loss: 0.0571, Val Loss: 0.0369\n",
            "Epoch 24/100, Train Loss: 0.0566, Val Loss: 0.0369\n",
            "Epoch 25/100, Train Loss: 0.0531, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0566, Val Loss: 0.0369\n",
            "Epoch 27/100, Train Loss: 0.0603, Val Loss: 0.0369\n",
            "Epoch 28/100, Train Loss: 0.0565, Val Loss: 0.0369\n",
            "Epoch 29/100, Train Loss: 0.0566, Val Loss: 0.0369\n",
            "Epoch 30/100, Train Loss: 0.0555, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0584, Val Loss: 0.0369\n",
            "Epoch 32/100, Train Loss: 0.0584, Val Loss: 0.0369\n",
            "Epoch 33/100, Train Loss: 0.0612, Val Loss: 0.0369\n",
            "Epoch 34/100, Train Loss: 0.0568, Val Loss: 0.0369\n",
            "Epoch 35/100, Train Loss: 0.0558, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0582, Val Loss: 0.0369\n",
            "Epoch 37/100, Train Loss: 0.0569, Val Loss: 0.0369\n",
            "Epoch 38/100, Train Loss: 0.0595, Val Loss: 0.0369\n",
            "Epoch 39/100, Train Loss: 0.0579, Val Loss: 0.0369\n",
            "Epoch 40/100, Train Loss: 0.0551, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0550, Val Loss: 0.0369\n",
            "Epoch 42/100, Train Loss: 0.0579, Val Loss: 0.0369\n",
            "Epoch 43/100, Train Loss: 0.0567, Val Loss: 0.0369\n",
            "Epoch 44/100, Train Loss: 0.0585, Val Loss: 0.0369\n",
            "Epoch 45/100, Train Loss: 0.0572, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0569, Val Loss: 0.0369\n",
            "Epoch 47/100, Train Loss: 0.0587, Val Loss: 0.0369\n",
            "Epoch 48/100, Train Loss: 0.0589, Val Loss: 0.0369\n",
            "Epoch 49/100, Train Loss: 0.0574, Val Loss: 0.0369\n",
            "Epoch 50/100, Train Loss: 0.0561, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0542, Val Loss: 0.0369\n",
            "Epoch 52/100, Train Loss: 0.0586, Val Loss: 0.0369\n",
            "Epoch 53/100, Train Loss: 0.0585, Val Loss: 0.0369\n",
            "Epoch 54/100, Train Loss: 0.0554, Val Loss: 0.0369\n",
            "Epoch 55/100, Train Loss: 0.0586, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0558, Val Loss: 0.0369\n",
            "Epoch 57/100, Train Loss: 0.0579, Val Loss: 0.0369\n",
            "Epoch 58/100, Train Loss: 0.0575, Val Loss: 0.0369\n",
            "Epoch 59/100, Train Loss: 0.0550, Val Loss: 0.0369\n",
            "Epoch 60/100, Train Loss: 0.0587, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0572, Val Loss: 0.0369\n",
            "Epoch 62/100, Train Loss: 0.0599, Val Loss: 0.0369\n",
            "Epoch 63/100, Train Loss: 0.0572, Val Loss: 0.0369\n",
            "Epoch 64/100, Train Loss: 0.0572, Val Loss: 0.0369\n",
            "Epoch 65/100, Train Loss: 0.0558, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0572, Val Loss: 0.0369\n",
            "Epoch 67/100, Train Loss: 0.0599, Val Loss: 0.0369\n",
            "Epoch 68/100, Train Loss: 0.0557, Val Loss: 0.0369\n",
            "Epoch 69/100, Train Loss: 0.0565, Val Loss: 0.0369\n",
            "Epoch 70/100, Train Loss: 0.0580, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0580, Val Loss: 0.0369\n",
            "Epoch 72/100, Train Loss: 0.0571, Val Loss: 0.0369\n",
            "Epoch 73/100, Train Loss: 0.0589, Val Loss: 0.0369\n",
            "Epoch 74/100, Train Loss: 0.0575, Val Loss: 0.0369\n",
            "Epoch 75/100, Train Loss: 0.0572, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0570, Val Loss: 0.0369\n",
            "Epoch 77/100, Train Loss: 0.0592, Val Loss: 0.0369\n",
            "Epoch 78/100, Train Loss: 0.0580, Val Loss: 0.0369\n",
            "Epoch 79/100, Train Loss: 0.0575, Val Loss: 0.0369\n",
            "Epoch 80/100, Train Loss: 0.0577, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0605, Val Loss: 0.0369\n",
            "Epoch 82/100, Train Loss: 0.0599, Val Loss: 0.0369\n",
            "Epoch 83/100, Train Loss: 0.0578, Val Loss: 0.0369\n",
            "Epoch 84/100, Train Loss: 0.0567, Val Loss: 0.0369\n",
            "Epoch 85/100, Train Loss: 0.0597, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0553, Val Loss: 0.0369\n",
            "Epoch 87/100, Train Loss: 0.0585, Val Loss: 0.0369\n",
            "Epoch 88/100, Train Loss: 0.0591, Val Loss: 0.0369\n",
            "Epoch 89/100, Train Loss: 0.0586, Val Loss: 0.0369\n",
            "Epoch 90/100, Train Loss: 0.0552, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0580, Val Loss: 0.0369\n",
            "Epoch 92/100, Train Loss: 0.0585, Val Loss: 0.0369\n",
            "Epoch 93/100, Train Loss: 0.0596, Val Loss: 0.0369\n",
            "Epoch 94/100, Train Loss: 0.0570, Val Loss: 0.0369\n",
            "Epoch 95/100, Train Loss: 0.0561, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0590, Val Loss: 0.0369\n",
            "Epoch 97/100, Train Loss: 0.0584, Val Loss: 0.0369\n",
            "Epoch 98/100, Train Loss: 0.0599, Val Loss: 0.0369\n",
            "Epoch 99/100, Train Loss: 0.0564, Val Loss: 0.0369\n",
            "Epoch 100/100, Train Loss: 0.0566, Val Loss: 0.0369\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 16/28\n",
            "Current training set size: 112 samples\n",
            "Epoch 1/100, Train Loss: 0.1519, Val Loss: 0.1971\n",
            "Epoch 2/100, Train Loss: 0.1495, Val Loss: 0.1880\n",
            "Epoch 3/100, Train Loss: 0.1455, Val Loss: 0.1835\n",
            "Epoch 4/100, Train Loss: 0.1389, Val Loss: 0.1804\n",
            "Epoch 5/100, Train Loss: 0.1337, Val Loss: 0.1726\n",
            "Epoch 6/100, Train Loss: 0.1127, Val Loss: 0.0882\n",
            "Epoch 7/100, Train Loss: 0.0737, Val Loss: 0.0366\n",
            "Epoch 8/100, Train Loss: 0.0799, Val Loss: 0.0418\n",
            "Epoch 9/100, Train Loss: 0.0723, Val Loss: 0.0382\n",
            "Epoch 10/100, Train Loss: 0.0663, Val Loss: 0.0473\n",
            "Epoch 11/100, Train Loss: 0.0714, Val Loss: 0.0381\n",
            "Epoch 12/100, Train Loss: 0.0647, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 13/100, Train Loss: 0.0604, Val Loss: 0.0402\n",
            "Epoch 14/100, Train Loss: 0.0634, Val Loss: 0.0401\n",
            "Epoch 15/100, Train Loss: 0.0597, Val Loss: 0.0402\n",
            "Epoch 16/100, Train Loss: 0.0606, Val Loss: 0.0399\n",
            "Epoch 17/100, Train Loss: 0.0580, Val Loss: 0.0399\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0608, Val Loss: 0.0399\n",
            "Epoch 19/100, Train Loss: 0.0609, Val Loss: 0.0399\n",
            "Epoch 20/100, Train Loss: 0.0634, Val Loss: 0.0399\n",
            "Epoch 21/100, Train Loss: 0.0612, Val Loss: 0.0399\n",
            "Epoch 22/100, Train Loss: 0.0623, Val Loss: 0.0399\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0619, Val Loss: 0.0399\n",
            "Epoch 24/100, Train Loss: 0.0605, Val Loss: 0.0399\n",
            "Epoch 25/100, Train Loss: 0.0624, Val Loss: 0.0399\n",
            "Epoch 26/100, Train Loss: 0.0608, Val Loss: 0.0399\n",
            "Epoch 27/100, Train Loss: 0.0620, Val Loss: 0.0399\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0630, Val Loss: 0.0399\n",
            "Epoch 29/100, Train Loss: 0.0611, Val Loss: 0.0399\n",
            "Epoch 30/100, Train Loss: 0.0604, Val Loss: 0.0399\n",
            "Epoch 31/100, Train Loss: 0.0590, Val Loss: 0.0399\n",
            "Epoch 32/100, Train Loss: 0.0631, Val Loss: 0.0399\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0624, Val Loss: 0.0399\n",
            "Epoch 34/100, Train Loss: 0.0626, Val Loss: 0.0399\n",
            "Epoch 35/100, Train Loss: 0.0637, Val Loss: 0.0399\n",
            "Epoch 36/100, Train Loss: 0.0620, Val Loss: 0.0399\n",
            "Epoch 37/100, Train Loss: 0.0619, Val Loss: 0.0399\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0607, Val Loss: 0.0399\n",
            "Epoch 39/100, Train Loss: 0.0599, Val Loss: 0.0399\n",
            "Epoch 40/100, Train Loss: 0.0625, Val Loss: 0.0399\n",
            "Epoch 41/100, Train Loss: 0.0632, Val Loss: 0.0399\n",
            "Epoch 42/100, Train Loss: 0.0615, Val Loss: 0.0399\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0605, Val Loss: 0.0399\n",
            "Epoch 44/100, Train Loss: 0.0615, Val Loss: 0.0399\n",
            "Epoch 45/100, Train Loss: 0.0596, Val Loss: 0.0399\n",
            "Epoch 46/100, Train Loss: 0.0614, Val Loss: 0.0399\n",
            "Epoch 47/100, Train Loss: 0.0588, Val Loss: 0.0399\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0636, Val Loss: 0.0399\n",
            "Epoch 49/100, Train Loss: 0.0606, Val Loss: 0.0399\n",
            "Epoch 50/100, Train Loss: 0.0598, Val Loss: 0.0399\n",
            "Epoch 51/100, Train Loss: 0.0637, Val Loss: 0.0399\n",
            "Epoch 52/100, Train Loss: 0.0609, Val Loss: 0.0399\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0619, Val Loss: 0.0399\n",
            "Epoch 54/100, Train Loss: 0.0594, Val Loss: 0.0399\n",
            "Epoch 55/100, Train Loss: 0.0622, Val Loss: 0.0399\n",
            "Epoch 56/100, Train Loss: 0.0605, Val Loss: 0.0399\n",
            "Epoch 57/100, Train Loss: 0.0615, Val Loss: 0.0399\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0596, Val Loss: 0.0399\n",
            "Epoch 59/100, Train Loss: 0.0614, Val Loss: 0.0399\n",
            "Epoch 60/100, Train Loss: 0.0617, Val Loss: 0.0399\n",
            "Epoch 61/100, Train Loss: 0.0617, Val Loss: 0.0399\n",
            "Epoch 62/100, Train Loss: 0.0616, Val Loss: 0.0399\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0626, Val Loss: 0.0399\n",
            "Epoch 64/100, Train Loss: 0.0608, Val Loss: 0.0399\n",
            "Epoch 65/100, Train Loss: 0.0621, Val Loss: 0.0399\n",
            "Epoch 66/100, Train Loss: 0.0617, Val Loss: 0.0399\n",
            "Epoch 67/100, Train Loss: 0.0613, Val Loss: 0.0399\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0628, Val Loss: 0.0399\n",
            "Epoch 69/100, Train Loss: 0.0628, Val Loss: 0.0399\n",
            "Epoch 70/100, Train Loss: 0.0590, Val Loss: 0.0399\n",
            "Epoch 71/100, Train Loss: 0.0600, Val Loss: 0.0399\n",
            "Epoch 72/100, Train Loss: 0.0614, Val Loss: 0.0399\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0628, Val Loss: 0.0399\n",
            "Epoch 74/100, Train Loss: 0.0605, Val Loss: 0.0399\n",
            "Epoch 75/100, Train Loss: 0.0602, Val Loss: 0.0399\n",
            "Epoch 76/100, Train Loss: 0.0631, Val Loss: 0.0399\n",
            "Epoch 77/100, Train Loss: 0.0616, Val Loss: 0.0399\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0617, Val Loss: 0.0399\n",
            "Epoch 79/100, Train Loss: 0.0639, Val Loss: 0.0399\n",
            "Epoch 80/100, Train Loss: 0.0618, Val Loss: 0.0399\n",
            "Epoch 81/100, Train Loss: 0.0616, Val Loss: 0.0399\n",
            "Epoch 82/100, Train Loss: 0.0601, Val Loss: 0.0399\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0613, Val Loss: 0.0399\n",
            "Epoch 84/100, Train Loss: 0.0613, Val Loss: 0.0399\n",
            "Epoch 85/100, Train Loss: 0.0622, Val Loss: 0.0399\n",
            "Epoch 86/100, Train Loss: 0.0613, Val Loss: 0.0399\n",
            "Epoch 87/100, Train Loss: 0.0601, Val Loss: 0.0399\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0593, Val Loss: 0.0399\n",
            "Epoch 89/100, Train Loss: 0.0615, Val Loss: 0.0399\n",
            "Epoch 90/100, Train Loss: 0.0598, Val Loss: 0.0399\n",
            "Epoch 91/100, Train Loss: 0.0636, Val Loss: 0.0399\n",
            "Epoch 92/100, Train Loss: 0.0626, Val Loss: 0.0399\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0589, Val Loss: 0.0399\n",
            "Epoch 94/100, Train Loss: 0.0591, Val Loss: 0.0399\n",
            "Epoch 95/100, Train Loss: 0.0630, Val Loss: 0.0399\n",
            "Epoch 96/100, Train Loss: 0.0621, Val Loss: 0.0399\n",
            "Epoch 97/100, Train Loss: 0.0611, Val Loss: 0.0399\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0608, Val Loss: 0.0399\n",
            "Epoch 99/100, Train Loss: 0.0602, Val Loss: 0.0399\n",
            "Epoch 100/100, Train Loss: 0.0607, Val Loss: 0.0399\n",
            "\n",
            "Test iteration 17/28\n",
            "Current training set size: 113 samples\n",
            "Epoch 1/100, Train Loss: 0.1523, Val Loss: 0.2032\n",
            "Epoch 2/100, Train Loss: 0.1451, Val Loss: 0.1909\n",
            "Epoch 3/100, Train Loss: 0.1467, Val Loss: 0.1847\n",
            "Epoch 4/100, Train Loss: 0.1366, Val Loss: 0.1791\n",
            "Epoch 5/100, Train Loss: 0.1302, Val Loss: 0.1586\n",
            "Epoch 6/100, Train Loss: 0.1158, Val Loss: 0.1167\n",
            "Epoch 7/100, Train Loss: 0.0847, Val Loss: 0.0473\n",
            "Epoch 8/100, Train Loss: 0.0738, Val Loss: 0.0614\n",
            "Epoch 9/100, Train Loss: 0.0645, Val Loss: 0.0441\n",
            "Epoch 10/100, Train Loss: 0.0689, Val Loss: 0.0380\n",
            "Epoch 11/100, Train Loss: 0.0687, Val Loss: 0.0688\n",
            "Epoch 12/100, Train Loss: 0.0651, Val Loss: 0.0464\n",
            "Epoch 13/100, Train Loss: 0.0637, Val Loss: 0.0423\n",
            "Epoch 14/100, Train Loss: 0.0643, Val Loss: 0.0490\n",
            "Epoch 15/100, Train Loss: 0.0614, Val Loss: 0.0376\n",
            "Epoch 16/100, Train Loss: 0.0657, Val Loss: 0.0356\n",
            "Epoch 17/100, Train Loss: 0.0655, Val Loss: 0.0329\n",
            "Epoch 18/100, Train Loss: 0.0565, Val Loss: 0.0463\n",
            "Epoch 19/100, Train Loss: 0.0616, Val Loss: 0.0416\n",
            "Epoch 20/100, Train Loss: 0.0545, Val Loss: 0.0371\n",
            "Epoch 21/100, Train Loss: 0.0528, Val Loss: 0.0327\n",
            "Epoch 22/100, Train Loss: 0.0480, Val Loss: 0.0366\n",
            "Epoch 23/100, Train Loss: 0.0482, Val Loss: 0.0414\n",
            "Epoch 24/100, Train Loss: 0.0509, Val Loss: 0.0413\n",
            "Epoch 25/100, Train Loss: 0.0464, Val Loss: 0.0424\n",
            "Epoch 26/100, Train Loss: 0.0444, Val Loss: 0.0340\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0440, Val Loss: 0.0341\n",
            "Epoch 28/100, Train Loss: 0.0429, Val Loss: 0.0343\n",
            "Epoch 29/100, Train Loss: 0.0470, Val Loss: 0.0346\n",
            "Epoch 30/100, Train Loss: 0.0433, Val Loss: 0.0348\n",
            "Epoch 31/100, Train Loss: 0.0449, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0463, Val Loss: 0.0351\n",
            "Epoch 33/100, Train Loss: 0.0447, Val Loss: 0.0351\n",
            "Epoch 34/100, Train Loss: 0.0428, Val Loss: 0.0351\n",
            "Epoch 35/100, Train Loss: 0.0407, Val Loss: 0.0351\n",
            "Epoch 36/100, Train Loss: 0.0430, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0467, Val Loss: 0.0351\n",
            "Epoch 38/100, Train Loss: 0.0447, Val Loss: 0.0351\n",
            "Epoch 39/100, Train Loss: 0.0431, Val Loss: 0.0351\n",
            "Epoch 40/100, Train Loss: 0.0465, Val Loss: 0.0351\n",
            "Epoch 41/100, Train Loss: 0.0446, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0415, Val Loss: 0.0351\n",
            "Epoch 43/100, Train Loss: 0.0398, Val Loss: 0.0351\n",
            "Epoch 44/100, Train Loss: 0.0459, Val Loss: 0.0351\n",
            "Epoch 45/100, Train Loss: 0.0427, Val Loss: 0.0351\n",
            "Epoch 46/100, Train Loss: 0.0431, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0469, Val Loss: 0.0351\n",
            "Epoch 48/100, Train Loss: 0.0430, Val Loss: 0.0351\n",
            "Epoch 49/100, Train Loss: 0.0436, Val Loss: 0.0351\n",
            "Epoch 50/100, Train Loss: 0.0450, Val Loss: 0.0351\n",
            "Epoch 51/100, Train Loss: 0.0461, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0423, Val Loss: 0.0351\n",
            "Epoch 53/100, Train Loss: 0.0435, Val Loss: 0.0351\n",
            "Epoch 54/100, Train Loss: 0.0445, Val Loss: 0.0351\n",
            "Epoch 55/100, Train Loss: 0.0412, Val Loss: 0.0351\n",
            "Epoch 56/100, Train Loss: 0.0445, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0469, Val Loss: 0.0351\n",
            "Epoch 58/100, Train Loss: 0.0447, Val Loss: 0.0351\n",
            "Epoch 59/100, Train Loss: 0.0449, Val Loss: 0.0351\n",
            "Epoch 60/100, Train Loss: 0.0424, Val Loss: 0.0351\n",
            "Epoch 61/100, Train Loss: 0.0450, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0432, Val Loss: 0.0351\n",
            "Epoch 63/100, Train Loss: 0.0460, Val Loss: 0.0351\n",
            "Epoch 64/100, Train Loss: 0.0430, Val Loss: 0.0351\n",
            "Epoch 65/100, Train Loss: 0.0431, Val Loss: 0.0351\n",
            "Epoch 66/100, Train Loss: 0.0433, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0413, Val Loss: 0.0351\n",
            "Epoch 68/100, Train Loss: 0.0410, Val Loss: 0.0351\n",
            "Epoch 69/100, Train Loss: 0.0432, Val Loss: 0.0351\n",
            "Epoch 70/100, Train Loss: 0.0427, Val Loss: 0.0351\n",
            "Epoch 71/100, Train Loss: 0.0403, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0453, Val Loss: 0.0351\n",
            "Epoch 73/100, Train Loss: 0.0455, Val Loss: 0.0351\n",
            "Epoch 74/100, Train Loss: 0.0425, Val Loss: 0.0351\n",
            "Epoch 75/100, Train Loss: 0.0445, Val Loss: 0.0351\n",
            "Epoch 76/100, Train Loss: 0.0434, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0416, Val Loss: 0.0351\n",
            "Epoch 78/100, Train Loss: 0.0428, Val Loss: 0.0351\n",
            "Epoch 79/100, Train Loss: 0.0438, Val Loss: 0.0351\n",
            "Epoch 80/100, Train Loss: 0.0432, Val Loss: 0.0351\n",
            "Epoch 81/100, Train Loss: 0.0456, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0464, Val Loss: 0.0351\n",
            "Epoch 83/100, Train Loss: 0.0445, Val Loss: 0.0351\n",
            "Epoch 84/100, Train Loss: 0.0454, Val Loss: 0.0351\n",
            "Epoch 85/100, Train Loss: 0.0442, Val Loss: 0.0351\n",
            "Epoch 86/100, Train Loss: 0.0440, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0423, Val Loss: 0.0351\n",
            "Epoch 88/100, Train Loss: 0.0424, Val Loss: 0.0351\n",
            "Epoch 89/100, Train Loss: 0.0417, Val Loss: 0.0351\n",
            "Epoch 90/100, Train Loss: 0.0419, Val Loss: 0.0351\n",
            "Epoch 91/100, Train Loss: 0.0430, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0433, Val Loss: 0.0351\n",
            "Epoch 93/100, Train Loss: 0.0483, Val Loss: 0.0351\n",
            "Epoch 94/100, Train Loss: 0.0429, Val Loss: 0.0351\n",
            "Epoch 95/100, Train Loss: 0.0446, Val Loss: 0.0351\n",
            "Epoch 96/100, Train Loss: 0.0452, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0443, Val Loss: 0.0351\n",
            "Epoch 98/100, Train Loss: 0.0419, Val Loss: 0.0351\n",
            "Epoch 99/100, Train Loss: 0.0428, Val Loss: 0.0351\n",
            "Epoch 100/100, Train Loss: 0.0407, Val Loss: 0.0351\n",
            "\n",
            "Test iteration 18/28\n",
            "Current training set size: 114 samples\n",
            "Epoch 1/100, Train Loss: 0.1548, Val Loss: 0.1972\n",
            "Epoch 2/100, Train Loss: 0.1434, Val Loss: 0.1924\n",
            "Epoch 3/100, Train Loss: 0.1423, Val Loss: 0.1844\n",
            "Epoch 4/100, Train Loss: 0.1370, Val Loss: 0.1798\n",
            "Epoch 5/100, Train Loss: 0.1282, Val Loss: 0.1428\n",
            "Epoch 6/100, Train Loss: 0.1101, Val Loss: 0.0832\n",
            "Epoch 7/100, Train Loss: 0.0852, Val Loss: 0.0468\n",
            "Epoch 8/100, Train Loss: 0.0692, Val Loss: 0.0494\n",
            "Epoch 9/100, Train Loss: 0.0701, Val Loss: 0.0364\n",
            "Epoch 10/100, Train Loss: 0.0657, Val Loss: 0.0410\n",
            "Epoch 11/100, Train Loss: 0.0644, Val Loss: 0.0337\n",
            "Epoch 12/100, Train Loss: 0.0718, Val Loss: 0.0425\n",
            "Epoch 13/100, Train Loss: 0.0603, Val Loss: 0.0430\n",
            "Epoch 14/100, Train Loss: 0.0710, Val Loss: 0.0363\n",
            "Epoch 15/100, Train Loss: 0.0603, Val Loss: 0.0445\n",
            "Epoch 16/100, Train Loss: 0.0601, Val Loss: 0.0421\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0530, Val Loss: 0.0419\n",
            "Epoch 18/100, Train Loss: 0.0545, Val Loss: 0.0418\n",
            "Epoch 19/100, Train Loss: 0.0538, Val Loss: 0.0417\n",
            "Epoch 20/100, Train Loss: 0.0586, Val Loss: 0.0416\n",
            "Epoch 21/100, Train Loss: 0.0556, Val Loss: 0.0415\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0535, Val Loss: 0.0415\n",
            "Epoch 23/100, Train Loss: 0.0534, Val Loss: 0.0415\n",
            "Epoch 24/100, Train Loss: 0.0544, Val Loss: 0.0415\n",
            "Epoch 25/100, Train Loss: 0.0558, Val Loss: 0.0415\n",
            "Epoch 26/100, Train Loss: 0.0530, Val Loss: 0.0415\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0546, Val Loss: 0.0415\n",
            "Epoch 28/100, Train Loss: 0.0536, Val Loss: 0.0415\n",
            "Epoch 29/100, Train Loss: 0.0563, Val Loss: 0.0415\n",
            "Epoch 30/100, Train Loss: 0.0530, Val Loss: 0.0415\n",
            "Epoch 31/100, Train Loss: 0.0542, Val Loss: 0.0415\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0525, Val Loss: 0.0415\n",
            "Epoch 33/100, Train Loss: 0.0538, Val Loss: 0.0415\n",
            "Epoch 34/100, Train Loss: 0.0553, Val Loss: 0.0415\n",
            "Epoch 35/100, Train Loss: 0.0536, Val Loss: 0.0415\n",
            "Epoch 36/100, Train Loss: 0.0517, Val Loss: 0.0415\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0535, Val Loss: 0.0415\n",
            "Epoch 38/100, Train Loss: 0.0549, Val Loss: 0.0415\n",
            "Epoch 39/100, Train Loss: 0.0546, Val Loss: 0.0415\n",
            "Epoch 40/100, Train Loss: 0.0533, Val Loss: 0.0415\n",
            "Epoch 41/100, Train Loss: 0.0540, Val Loss: 0.0415\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0531, Val Loss: 0.0415\n",
            "Epoch 43/100, Train Loss: 0.0526, Val Loss: 0.0415\n",
            "Epoch 44/100, Train Loss: 0.0504, Val Loss: 0.0415\n",
            "Epoch 45/100, Train Loss: 0.0531, Val Loss: 0.0415\n",
            "Epoch 46/100, Train Loss: 0.0529, Val Loss: 0.0415\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0531, Val Loss: 0.0415\n",
            "Epoch 48/100, Train Loss: 0.0524, Val Loss: 0.0415\n",
            "Epoch 49/100, Train Loss: 0.0548, Val Loss: 0.0415\n",
            "Epoch 50/100, Train Loss: 0.0568, Val Loss: 0.0415\n",
            "Epoch 51/100, Train Loss: 0.0538, Val Loss: 0.0415\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0545, Val Loss: 0.0415\n",
            "Epoch 53/100, Train Loss: 0.0573, Val Loss: 0.0415\n",
            "Epoch 54/100, Train Loss: 0.0527, Val Loss: 0.0415\n",
            "Epoch 55/100, Train Loss: 0.0541, Val Loss: 0.0415\n",
            "Epoch 56/100, Train Loss: 0.0521, Val Loss: 0.0415\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0523, Val Loss: 0.0415\n",
            "Epoch 58/100, Train Loss: 0.0521, Val Loss: 0.0415\n",
            "Epoch 59/100, Train Loss: 0.0538, Val Loss: 0.0415\n",
            "Epoch 60/100, Train Loss: 0.0544, Val Loss: 0.0415\n",
            "Epoch 61/100, Train Loss: 0.0539, Val Loss: 0.0415\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0561, Val Loss: 0.0415\n",
            "Epoch 63/100, Train Loss: 0.0524, Val Loss: 0.0415\n",
            "Epoch 64/100, Train Loss: 0.0558, Val Loss: 0.0415\n",
            "Epoch 65/100, Train Loss: 0.0532, Val Loss: 0.0415\n",
            "Epoch 66/100, Train Loss: 0.0542, Val Loss: 0.0415\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0532, Val Loss: 0.0415\n",
            "Epoch 68/100, Train Loss: 0.0556, Val Loss: 0.0415\n",
            "Epoch 69/100, Train Loss: 0.0559, Val Loss: 0.0415\n",
            "Epoch 70/100, Train Loss: 0.0532, Val Loss: 0.0415\n",
            "Epoch 71/100, Train Loss: 0.0544, Val Loss: 0.0415\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0552, Val Loss: 0.0415\n",
            "Epoch 73/100, Train Loss: 0.0548, Val Loss: 0.0415\n",
            "Epoch 74/100, Train Loss: 0.0548, Val Loss: 0.0415\n",
            "Epoch 75/100, Train Loss: 0.0542, Val Loss: 0.0415\n",
            "Epoch 76/100, Train Loss: 0.0577, Val Loss: 0.0415\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0520, Val Loss: 0.0415\n",
            "Epoch 78/100, Train Loss: 0.0527, Val Loss: 0.0415\n",
            "Epoch 79/100, Train Loss: 0.0534, Val Loss: 0.0415\n",
            "Epoch 80/100, Train Loss: 0.0534, Val Loss: 0.0415\n",
            "Epoch 81/100, Train Loss: 0.0528, Val Loss: 0.0415\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0526, Val Loss: 0.0415\n",
            "Epoch 83/100, Train Loss: 0.0527, Val Loss: 0.0415\n",
            "Epoch 84/100, Train Loss: 0.0567, Val Loss: 0.0415\n",
            "Epoch 85/100, Train Loss: 0.0525, Val Loss: 0.0415\n",
            "Epoch 86/100, Train Loss: 0.0561, Val Loss: 0.0415\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0532, Val Loss: 0.0415\n",
            "Epoch 88/100, Train Loss: 0.0557, Val Loss: 0.0415\n",
            "Epoch 89/100, Train Loss: 0.0595, Val Loss: 0.0415\n",
            "Epoch 90/100, Train Loss: 0.0534, Val Loss: 0.0415\n",
            "Epoch 91/100, Train Loss: 0.0553, Val Loss: 0.0415\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0521, Val Loss: 0.0415\n",
            "Epoch 93/100, Train Loss: 0.0547, Val Loss: 0.0415\n",
            "Epoch 94/100, Train Loss: 0.0545, Val Loss: 0.0415\n",
            "Epoch 95/100, Train Loss: 0.0536, Val Loss: 0.0415\n",
            "Epoch 96/100, Train Loss: 0.0564, Val Loss: 0.0415\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0518, Val Loss: 0.0415\n",
            "Epoch 98/100, Train Loss: 0.0535, Val Loss: 0.0415\n",
            "Epoch 99/100, Train Loss: 0.0543, Val Loss: 0.0415\n",
            "Epoch 100/100, Train Loss: 0.0557, Val Loss: 0.0415\n",
            "\n",
            "Test iteration 19/28\n",
            "Current training set size: 115 samples\n",
            "Epoch 1/100, Train Loss: 0.1526, Val Loss: 0.1917\n",
            "Epoch 2/100, Train Loss: 0.1468, Val Loss: 0.1931\n",
            "Epoch 3/100, Train Loss: 0.1407, Val Loss: 0.1847\n",
            "Epoch 4/100, Train Loss: 0.1384, Val Loss: 0.1805\n",
            "Epoch 5/100, Train Loss: 0.1310, Val Loss: 0.1519\n",
            "Epoch 6/100, Train Loss: 0.1082, Val Loss: 0.1060\n",
            "Epoch 7/100, Train Loss: 0.1004, Val Loss: 0.0419\n",
            "Epoch 8/100, Train Loss: 0.0869, Val Loss: 0.0457\n",
            "Epoch 9/100, Train Loss: 0.0702, Val Loss: 0.0560\n",
            "Epoch 10/100, Train Loss: 0.0695, Val Loss: 0.0358\n",
            "Epoch 11/100, Train Loss: 0.0652, Val Loss: 0.0374\n",
            "Epoch 12/100, Train Loss: 0.0655, Val Loss: 0.0343\n",
            "Epoch 13/100, Train Loss: 0.0611, Val Loss: 0.0501\n",
            "Epoch 14/100, Train Loss: 0.0662, Val Loss: 0.0348\n",
            "Epoch 15/100, Train Loss: 0.0604, Val Loss: 0.0455\n",
            "Epoch 16/100, Train Loss: 0.0633, Val Loss: 0.0321\n",
            "Epoch 17/100, Train Loss: 0.0590, Val Loss: 0.0390\n",
            "Epoch 18/100, Train Loss: 0.0637, Val Loss: 0.0435\n",
            "Epoch 19/100, Train Loss: 0.0542, Val Loss: 0.0385\n",
            "Epoch 20/100, Train Loss: 0.0594, Val Loss: 0.0371\n",
            "Epoch 21/100, Train Loss: 0.0538, Val Loss: 0.0347\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0488, Val Loss: 0.0349\n",
            "Epoch 23/100, Train Loss: 0.0496, Val Loss: 0.0350\n",
            "Epoch 24/100, Train Loss: 0.0530, Val Loss: 0.0352\n",
            "Epoch 25/100, Train Loss: 0.0500, Val Loss: 0.0356\n",
            "Epoch 26/100, Train Loss: 0.0504, Val Loss: 0.0357\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0506, Val Loss: 0.0357\n",
            "Epoch 28/100, Train Loss: 0.0476, Val Loss: 0.0357\n",
            "Epoch 29/100, Train Loss: 0.0498, Val Loss: 0.0357\n",
            "Epoch 30/100, Train Loss: 0.0515, Val Loss: 0.0357\n",
            "Epoch 31/100, Train Loss: 0.0501, Val Loss: 0.0357\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0506, Val Loss: 0.0357\n",
            "Epoch 33/100, Train Loss: 0.0539, Val Loss: 0.0357\n",
            "Epoch 34/100, Train Loss: 0.0500, Val Loss: 0.0357\n",
            "Epoch 35/100, Train Loss: 0.0476, Val Loss: 0.0357\n",
            "Epoch 36/100, Train Loss: 0.0487, Val Loss: 0.0357\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0501, Val Loss: 0.0357\n",
            "Epoch 38/100, Train Loss: 0.0478, Val Loss: 0.0357\n",
            "Epoch 39/100, Train Loss: 0.0469, Val Loss: 0.0357\n",
            "Epoch 40/100, Train Loss: 0.0497, Val Loss: 0.0357\n",
            "Epoch 41/100, Train Loss: 0.0499, Val Loss: 0.0357\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0501, Val Loss: 0.0357\n",
            "Epoch 43/100, Train Loss: 0.0511, Val Loss: 0.0357\n",
            "Epoch 44/100, Train Loss: 0.0511, Val Loss: 0.0357\n",
            "Epoch 45/100, Train Loss: 0.0523, Val Loss: 0.0357\n",
            "Epoch 46/100, Train Loss: 0.0501, Val Loss: 0.0357\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0495, Val Loss: 0.0357\n",
            "Epoch 48/100, Train Loss: 0.0520, Val Loss: 0.0357\n",
            "Epoch 49/100, Train Loss: 0.0505, Val Loss: 0.0357\n",
            "Epoch 50/100, Train Loss: 0.0526, Val Loss: 0.0357\n",
            "Epoch 51/100, Train Loss: 0.0496, Val Loss: 0.0357\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0506, Val Loss: 0.0357\n",
            "Epoch 53/100, Train Loss: 0.0498, Val Loss: 0.0357\n",
            "Epoch 54/100, Train Loss: 0.0516, Val Loss: 0.0357\n",
            "Epoch 55/100, Train Loss: 0.0504, Val Loss: 0.0357\n",
            "Epoch 56/100, Train Loss: 0.0517, Val Loss: 0.0357\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0512, Val Loss: 0.0357\n",
            "Epoch 58/100, Train Loss: 0.0486, Val Loss: 0.0357\n",
            "Epoch 59/100, Train Loss: 0.0498, Val Loss: 0.0357\n",
            "Epoch 60/100, Train Loss: 0.0485, Val Loss: 0.0357\n",
            "Epoch 61/100, Train Loss: 0.0472, Val Loss: 0.0357\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0491, Val Loss: 0.0357\n",
            "Epoch 63/100, Train Loss: 0.0521, Val Loss: 0.0357\n",
            "Epoch 64/100, Train Loss: 0.0488, Val Loss: 0.0357\n",
            "Epoch 65/100, Train Loss: 0.0502, Val Loss: 0.0357\n",
            "Epoch 66/100, Train Loss: 0.0499, Val Loss: 0.0357\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0493, Val Loss: 0.0357\n",
            "Epoch 68/100, Train Loss: 0.0499, Val Loss: 0.0357\n",
            "Epoch 69/100, Train Loss: 0.0484, Val Loss: 0.0357\n",
            "Epoch 70/100, Train Loss: 0.0491, Val Loss: 0.0357\n",
            "Epoch 71/100, Train Loss: 0.0499, Val Loss: 0.0357\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0518, Val Loss: 0.0357\n",
            "Epoch 73/100, Train Loss: 0.0511, Val Loss: 0.0357\n",
            "Epoch 74/100, Train Loss: 0.0511, Val Loss: 0.0357\n",
            "Epoch 75/100, Train Loss: 0.0521, Val Loss: 0.0357\n",
            "Epoch 76/100, Train Loss: 0.0514, Val Loss: 0.0357\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0493, Val Loss: 0.0357\n",
            "Epoch 78/100, Train Loss: 0.0500, Val Loss: 0.0357\n",
            "Epoch 79/100, Train Loss: 0.0508, Val Loss: 0.0357\n",
            "Epoch 80/100, Train Loss: 0.0511, Val Loss: 0.0357\n",
            "Epoch 81/100, Train Loss: 0.0469, Val Loss: 0.0357\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0522, Val Loss: 0.0357\n",
            "Epoch 83/100, Train Loss: 0.0506, Val Loss: 0.0357\n",
            "Epoch 84/100, Train Loss: 0.0512, Val Loss: 0.0357\n",
            "Epoch 85/100, Train Loss: 0.0489, Val Loss: 0.0357\n",
            "Epoch 86/100, Train Loss: 0.0500, Val Loss: 0.0357\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0507, Val Loss: 0.0357\n",
            "Epoch 88/100, Train Loss: 0.0505, Val Loss: 0.0357\n",
            "Epoch 89/100, Train Loss: 0.0506, Val Loss: 0.0357\n",
            "Epoch 90/100, Train Loss: 0.0494, Val Loss: 0.0357\n",
            "Epoch 91/100, Train Loss: 0.0494, Val Loss: 0.0357\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0482, Val Loss: 0.0357\n",
            "Epoch 93/100, Train Loss: 0.0522, Val Loss: 0.0357\n",
            "Epoch 94/100, Train Loss: 0.0508, Val Loss: 0.0357\n",
            "Epoch 95/100, Train Loss: 0.0517, Val Loss: 0.0357\n",
            "Epoch 96/100, Train Loss: 0.0515, Val Loss: 0.0357\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0518, Val Loss: 0.0357\n",
            "Epoch 98/100, Train Loss: 0.0495, Val Loss: 0.0357\n",
            "Epoch 99/100, Train Loss: 0.0505, Val Loss: 0.0357\n",
            "Epoch 100/100, Train Loss: 0.0493, Val Loss: 0.0357\n",
            "\n",
            "Test iteration 20/28\n",
            "Current training set size: 116 samples\n",
            "Epoch 1/100, Train Loss: 0.1498, Val Loss: 0.1950\n",
            "Epoch 2/100, Train Loss: 0.1436, Val Loss: 0.1880\n",
            "Epoch 3/100, Train Loss: 0.1459, Val Loss: 0.1913\n",
            "Epoch 4/100, Train Loss: 0.1368, Val Loss: 0.1792\n",
            "Epoch 5/100, Train Loss: 0.1283, Val Loss: 0.1480\n",
            "Epoch 6/100, Train Loss: 0.1083, Val Loss: 0.0806\n",
            "Epoch 7/100, Train Loss: 0.0791, Val Loss: 0.0390\n",
            "Epoch 8/100, Train Loss: 0.0833, Val Loss: 0.0907\n",
            "Epoch 9/100, Train Loss: 0.0850, Val Loss: 0.0468\n",
            "Epoch 10/100, Train Loss: 0.0751, Val Loss: 0.0456\n",
            "Epoch 11/100, Train Loss: 0.0747, Val Loss: 0.0527\n",
            "Epoch 12/100, Train Loss: 0.0681, Val Loss: 0.0547\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 13/100, Train Loss: 0.0723, Val Loss: 0.0524\n",
            "Epoch 14/100, Train Loss: 0.0708, Val Loss: 0.0495\n",
            "Epoch 15/100, Train Loss: 0.0694, Val Loss: 0.0470\n",
            "Epoch 16/100, Train Loss: 0.0661, Val Loss: 0.0453\n",
            "Epoch 17/100, Train Loss: 0.0657, Val Loss: 0.0443\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0655, Val Loss: 0.0443\n",
            "Epoch 19/100, Train Loss: 0.0649, Val Loss: 0.0443\n",
            "Epoch 20/100, Train Loss: 0.0672, Val Loss: 0.0443\n",
            "Epoch 21/100, Train Loss: 0.0662, Val Loss: 0.0443\n",
            "Epoch 22/100, Train Loss: 0.0657, Val Loss: 0.0443\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0657, Val Loss: 0.0443\n",
            "Epoch 24/100, Train Loss: 0.0678, Val Loss: 0.0443\n",
            "Epoch 25/100, Train Loss: 0.0658, Val Loss: 0.0443\n",
            "Epoch 26/100, Train Loss: 0.0650, Val Loss: 0.0443\n",
            "Epoch 27/100, Train Loss: 0.0636, Val Loss: 0.0443\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0669, Val Loss: 0.0443\n",
            "Epoch 29/100, Train Loss: 0.0664, Val Loss: 0.0443\n",
            "Epoch 30/100, Train Loss: 0.0671, Val Loss: 0.0443\n",
            "Epoch 31/100, Train Loss: 0.0666, Val Loss: 0.0443\n",
            "Epoch 32/100, Train Loss: 0.0644, Val Loss: 0.0443\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0675, Val Loss: 0.0443\n",
            "Epoch 34/100, Train Loss: 0.0676, Val Loss: 0.0443\n",
            "Epoch 35/100, Train Loss: 0.0637, Val Loss: 0.0443\n",
            "Epoch 36/100, Train Loss: 0.0637, Val Loss: 0.0443\n",
            "Epoch 37/100, Train Loss: 0.0666, Val Loss: 0.0443\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0660, Val Loss: 0.0443\n",
            "Epoch 39/100, Train Loss: 0.0675, Val Loss: 0.0443\n",
            "Epoch 40/100, Train Loss: 0.0669, Val Loss: 0.0443\n",
            "Epoch 41/100, Train Loss: 0.0634, Val Loss: 0.0443\n",
            "Epoch 42/100, Train Loss: 0.0669, Val Loss: 0.0443\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0624, Val Loss: 0.0443\n",
            "Epoch 44/100, Train Loss: 0.0653, Val Loss: 0.0443\n",
            "Epoch 45/100, Train Loss: 0.0660, Val Loss: 0.0443\n",
            "Epoch 46/100, Train Loss: 0.0673, Val Loss: 0.0443\n",
            "Epoch 47/100, Train Loss: 0.0641, Val Loss: 0.0443\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0650, Val Loss: 0.0443\n",
            "Epoch 49/100, Train Loss: 0.0655, Val Loss: 0.0443\n",
            "Epoch 50/100, Train Loss: 0.0690, Val Loss: 0.0443\n",
            "Epoch 51/100, Train Loss: 0.0646, Val Loss: 0.0443\n",
            "Epoch 52/100, Train Loss: 0.0655, Val Loss: 0.0443\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0665, Val Loss: 0.0443\n",
            "Epoch 54/100, Train Loss: 0.0645, Val Loss: 0.0443\n",
            "Epoch 55/100, Train Loss: 0.0683, Val Loss: 0.0443\n",
            "Epoch 56/100, Train Loss: 0.0658, Val Loss: 0.0443\n",
            "Epoch 57/100, Train Loss: 0.0641, Val Loss: 0.0443\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0657, Val Loss: 0.0443\n",
            "Epoch 59/100, Train Loss: 0.0656, Val Loss: 0.0443\n",
            "Epoch 60/100, Train Loss: 0.0658, Val Loss: 0.0443\n",
            "Epoch 61/100, Train Loss: 0.0646, Val Loss: 0.0443\n",
            "Epoch 62/100, Train Loss: 0.0667, Val Loss: 0.0443\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0663, Val Loss: 0.0443\n",
            "Epoch 64/100, Train Loss: 0.0645, Val Loss: 0.0443\n",
            "Epoch 65/100, Train Loss: 0.0660, Val Loss: 0.0443\n",
            "Epoch 66/100, Train Loss: 0.0653, Val Loss: 0.0443\n",
            "Epoch 67/100, Train Loss: 0.0657, Val Loss: 0.0443\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0660, Val Loss: 0.0443\n",
            "Epoch 69/100, Train Loss: 0.0662, Val Loss: 0.0443\n",
            "Epoch 70/100, Train Loss: 0.0634, Val Loss: 0.0443\n",
            "Epoch 71/100, Train Loss: 0.0661, Val Loss: 0.0443\n",
            "Epoch 72/100, Train Loss: 0.0662, Val Loss: 0.0443\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0666, Val Loss: 0.0443\n",
            "Epoch 74/100, Train Loss: 0.0666, Val Loss: 0.0443\n",
            "Epoch 75/100, Train Loss: 0.0633, Val Loss: 0.0443\n",
            "Epoch 76/100, Train Loss: 0.0680, Val Loss: 0.0443\n",
            "Epoch 77/100, Train Loss: 0.0663, Val Loss: 0.0443\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0646, Val Loss: 0.0443\n",
            "Epoch 79/100, Train Loss: 0.0660, Val Loss: 0.0443\n",
            "Epoch 80/100, Train Loss: 0.0635, Val Loss: 0.0443\n",
            "Epoch 81/100, Train Loss: 0.0643, Val Loss: 0.0443\n",
            "Epoch 82/100, Train Loss: 0.0651, Val Loss: 0.0443\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0671, Val Loss: 0.0443\n",
            "Epoch 84/100, Train Loss: 0.0679, Val Loss: 0.0443\n",
            "Epoch 85/100, Train Loss: 0.0665, Val Loss: 0.0443\n",
            "Epoch 86/100, Train Loss: 0.0669, Val Loss: 0.0443\n",
            "Epoch 87/100, Train Loss: 0.0647, Val Loss: 0.0443\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0652, Val Loss: 0.0443\n",
            "Epoch 89/100, Train Loss: 0.0653, Val Loss: 0.0443\n",
            "Epoch 90/100, Train Loss: 0.0636, Val Loss: 0.0443\n",
            "Epoch 91/100, Train Loss: 0.0654, Val Loss: 0.0443\n",
            "Epoch 92/100, Train Loss: 0.0644, Val Loss: 0.0443\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0646, Val Loss: 0.0443\n",
            "Epoch 94/100, Train Loss: 0.0665, Val Loss: 0.0443\n",
            "Epoch 95/100, Train Loss: 0.0642, Val Loss: 0.0443\n",
            "Epoch 96/100, Train Loss: 0.0656, Val Loss: 0.0443\n",
            "Epoch 97/100, Train Loss: 0.0680, Val Loss: 0.0443\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0632, Val Loss: 0.0443\n",
            "Epoch 99/100, Train Loss: 0.0660, Val Loss: 0.0443\n",
            "Epoch 100/100, Train Loss: 0.0662, Val Loss: 0.0443\n",
            "\n",
            "Test iteration 21/28\n",
            "Current training set size: 117 samples\n",
            "Epoch 1/100, Train Loss: 0.1554, Val Loss: 0.1918\n",
            "Epoch 2/100, Train Loss: 0.1502, Val Loss: 0.1942\n",
            "Epoch 3/100, Train Loss: 0.1497, Val Loss: 0.1920\n",
            "Epoch 4/100, Train Loss: 0.1458, Val Loss: 0.1782\n",
            "Epoch 5/100, Train Loss: 0.1359, Val Loss: 0.1718\n",
            "Epoch 6/100, Train Loss: 0.1280, Val Loss: 0.1568\n",
            "Epoch 7/100, Train Loss: 0.1022, Val Loss: 0.0568\n",
            "Epoch 8/100, Train Loss: 0.0774, Val Loss: 0.0452\n",
            "Epoch 9/100, Train Loss: 0.0666, Val Loss: 0.0432\n",
            "Epoch 10/100, Train Loss: 0.0707, Val Loss: 0.0440\n",
            "Epoch 11/100, Train Loss: 0.0653, Val Loss: 0.0405\n",
            "Epoch 12/100, Train Loss: 0.0657, Val Loss: 0.0430\n",
            "Epoch 13/100, Train Loss: 0.0672, Val Loss: 0.0479\n",
            "Epoch 14/100, Train Loss: 0.0627, Val Loss: 0.0482\n",
            "Epoch 15/100, Train Loss: 0.0877, Val Loss: 0.0369\n",
            "Epoch 16/100, Train Loss: 0.0651, Val Loss: 0.0436\n",
            "Epoch 17/100, Train Loss: 0.0603, Val Loss: 0.0367\n",
            "Epoch 18/100, Train Loss: 0.0604, Val Loss: 0.0390\n",
            "Epoch 19/100, Train Loss: 0.0628, Val Loss: 0.0443\n",
            "Epoch 20/100, Train Loss: 0.0597, Val Loss: 0.0447\n",
            "Epoch 21/100, Train Loss: 0.0513, Val Loss: 0.0350\n",
            "Epoch 22/100, Train Loss: 0.0557, Val Loss: 0.0387\n",
            "Epoch 23/100, Train Loss: 0.0491, Val Loss: 0.0447\n",
            "Epoch 24/100, Train Loss: 0.0546, Val Loss: 0.0406\n",
            "Epoch 25/100, Train Loss: 0.0528, Val Loss: 0.0322\n",
            "Epoch 26/100, Train Loss: 0.0482, Val Loss: 0.0405\n",
            "Epoch 27/100, Train Loss: 0.0505, Val Loss: 0.0353\n",
            "Epoch 28/100, Train Loss: 0.0465, Val Loss: 0.0404\n",
            "Epoch 29/100, Train Loss: 0.0487, Val Loss: 0.0484\n",
            "Epoch 30/100, Train Loss: 0.0441, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0444, Val Loss: 0.0378\n",
            "Epoch 32/100, Train Loss: 0.0444, Val Loss: 0.0378\n",
            "Epoch 33/100, Train Loss: 0.0430, Val Loss: 0.0376\n",
            "Epoch 34/100, Train Loss: 0.0392, Val Loss: 0.0374\n",
            "Epoch 35/100, Train Loss: 0.0427, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0436, Val Loss: 0.0374\n",
            "Epoch 37/100, Train Loss: 0.0416, Val Loss: 0.0374\n",
            "Epoch 38/100, Train Loss: 0.0400, Val Loss: 0.0374\n",
            "Epoch 39/100, Train Loss: 0.0426, Val Loss: 0.0374\n",
            "Epoch 40/100, Train Loss: 0.0414, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0428, Val Loss: 0.0374\n",
            "Epoch 42/100, Train Loss: 0.0444, Val Loss: 0.0374\n",
            "Epoch 43/100, Train Loss: 0.0448, Val Loss: 0.0374\n",
            "Epoch 44/100, Train Loss: 0.0436, Val Loss: 0.0374\n",
            "Epoch 45/100, Train Loss: 0.0418, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0501, Val Loss: 0.0374\n",
            "Epoch 47/100, Train Loss: 0.0455, Val Loss: 0.0374\n",
            "Epoch 48/100, Train Loss: 0.0432, Val Loss: 0.0374\n",
            "Epoch 49/100, Train Loss: 0.0417, Val Loss: 0.0374\n",
            "Epoch 50/100, Train Loss: 0.0473, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0433, Val Loss: 0.0374\n",
            "Epoch 52/100, Train Loss: 0.0435, Val Loss: 0.0374\n",
            "Epoch 53/100, Train Loss: 0.0499, Val Loss: 0.0374\n",
            "Epoch 54/100, Train Loss: 0.0434, Val Loss: 0.0374\n",
            "Epoch 55/100, Train Loss: 0.0430, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0478, Val Loss: 0.0374\n",
            "Epoch 57/100, Train Loss: 0.0424, Val Loss: 0.0374\n",
            "Epoch 58/100, Train Loss: 0.0406, Val Loss: 0.0374\n",
            "Epoch 59/100, Train Loss: 0.0429, Val Loss: 0.0374\n",
            "Epoch 60/100, Train Loss: 0.0401, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0390, Val Loss: 0.0374\n",
            "Epoch 62/100, Train Loss: 0.0428, Val Loss: 0.0374\n",
            "Epoch 63/100, Train Loss: 0.0416, Val Loss: 0.0374\n",
            "Epoch 64/100, Train Loss: 0.0437, Val Loss: 0.0374\n",
            "Epoch 65/100, Train Loss: 0.0399, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0405, Val Loss: 0.0374\n",
            "Epoch 67/100, Train Loss: 0.0432, Val Loss: 0.0374\n",
            "Epoch 68/100, Train Loss: 0.0433, Val Loss: 0.0374\n",
            "Epoch 69/100, Train Loss: 0.0467, Val Loss: 0.0374\n",
            "Epoch 70/100, Train Loss: 0.0431, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0469, Val Loss: 0.0374\n",
            "Epoch 72/100, Train Loss: 0.0443, Val Loss: 0.0374\n",
            "Epoch 73/100, Train Loss: 0.0408, Val Loss: 0.0374\n",
            "Epoch 74/100, Train Loss: 0.0426, Val Loss: 0.0374\n",
            "Epoch 75/100, Train Loss: 0.0453, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0423, Val Loss: 0.0374\n",
            "Epoch 77/100, Train Loss: 0.0409, Val Loss: 0.0374\n",
            "Epoch 78/100, Train Loss: 0.0500, Val Loss: 0.0374\n",
            "Epoch 79/100, Train Loss: 0.0441, Val Loss: 0.0374\n",
            "Epoch 80/100, Train Loss: 0.0412, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0397, Val Loss: 0.0374\n",
            "Epoch 82/100, Train Loss: 0.0430, Val Loss: 0.0374\n",
            "Epoch 83/100, Train Loss: 0.0415, Val Loss: 0.0374\n",
            "Epoch 84/100, Train Loss: 0.0464, Val Loss: 0.0374\n",
            "Epoch 85/100, Train Loss: 0.0420, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0447, Val Loss: 0.0374\n",
            "Epoch 87/100, Train Loss: 0.0441, Val Loss: 0.0374\n",
            "Epoch 88/100, Train Loss: 0.0458, Val Loss: 0.0374\n",
            "Epoch 89/100, Train Loss: 0.0418, Val Loss: 0.0374\n",
            "Epoch 90/100, Train Loss: 0.0427, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0418, Val Loss: 0.0374\n",
            "Epoch 92/100, Train Loss: 0.0435, Val Loss: 0.0374\n",
            "Epoch 93/100, Train Loss: 0.0409, Val Loss: 0.0374\n",
            "Epoch 94/100, Train Loss: 0.0406, Val Loss: 0.0374\n",
            "Epoch 95/100, Train Loss: 0.0416, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0412, Val Loss: 0.0374\n",
            "Epoch 97/100, Train Loss: 0.0437, Val Loss: 0.0374\n",
            "Epoch 98/100, Train Loss: 0.0439, Val Loss: 0.0374\n",
            "Epoch 99/100, Train Loss: 0.0434, Val Loss: 0.0374\n",
            "Epoch 100/100, Train Loss: 0.0419, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 22/28\n",
            "Current training set size: 118 samples\n",
            "Epoch 1/100, Train Loss: 0.1612, Val Loss: 0.1924\n",
            "Epoch 2/100, Train Loss: 0.1521, Val Loss: 0.1914\n",
            "Epoch 3/100, Train Loss: 0.1462, Val Loss: 0.2087\n",
            "Epoch 4/100, Train Loss: 0.1456, Val Loss: 0.1917\n",
            "Epoch 5/100, Train Loss: 0.1440, Val Loss: 0.1746\n",
            "Epoch 6/100, Train Loss: 0.1289, Val Loss: 0.1629\n",
            "Epoch 7/100, Train Loss: 0.1085, Val Loss: 0.0906\n",
            "Epoch 8/100, Train Loss: 0.0832, Val Loss: 0.0410\n",
            "Epoch 9/100, Train Loss: 0.0718, Val Loss: 0.0394\n",
            "Epoch 10/100, Train Loss: 0.0665, Val Loss: 0.0403\n",
            "Epoch 11/100, Train Loss: 0.0752, Val Loss: 0.0850\n",
            "Epoch 12/100, Train Loss: 0.0794, Val Loss: 0.0413\n",
            "Epoch 13/100, Train Loss: 0.0641, Val Loss: 0.0449\n",
            "Epoch 14/100, Train Loss: 0.0706, Val Loss: 0.0447\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 15/100, Train Loss: 0.0650, Val Loss: 0.0446\n",
            "Epoch 16/100, Train Loss: 0.0630, Val Loss: 0.0442\n",
            "Epoch 17/100, Train Loss: 0.0616, Val Loss: 0.0440\n",
            "Epoch 18/100, Train Loss: 0.0618, Val Loss: 0.0440\n",
            "Epoch 19/100, Train Loss: 0.0621, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0598, Val Loss: 0.0437\n",
            "Epoch 21/100, Train Loss: 0.0645, Val Loss: 0.0437\n",
            "Epoch 22/100, Train Loss: 0.0595, Val Loss: 0.0437\n",
            "Epoch 23/100, Train Loss: 0.0617, Val Loss: 0.0437\n",
            "Epoch 24/100, Train Loss: 0.0608, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0618, Val Loss: 0.0437\n",
            "Epoch 26/100, Train Loss: 0.0603, Val Loss: 0.0437\n",
            "Epoch 27/100, Train Loss: 0.0681, Val Loss: 0.0437\n",
            "Epoch 28/100, Train Loss: 0.0622, Val Loss: 0.0437\n",
            "Epoch 29/100, Train Loss: 0.0638, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0606, Val Loss: 0.0437\n",
            "Epoch 31/100, Train Loss: 0.0620, Val Loss: 0.0437\n",
            "Epoch 32/100, Train Loss: 0.0603, Val Loss: 0.0437\n",
            "Epoch 33/100, Train Loss: 0.0604, Val Loss: 0.0437\n",
            "Epoch 34/100, Train Loss: 0.0632, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0620, Val Loss: 0.0437\n",
            "Epoch 36/100, Train Loss: 0.0632, Val Loss: 0.0437\n",
            "Epoch 37/100, Train Loss: 0.0613, Val Loss: 0.0437\n",
            "Epoch 38/100, Train Loss: 0.0625, Val Loss: 0.0437\n",
            "Epoch 39/100, Train Loss: 0.0624, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0620, Val Loss: 0.0437\n",
            "Epoch 41/100, Train Loss: 0.0635, Val Loss: 0.0437\n",
            "Epoch 42/100, Train Loss: 0.0634, Val Loss: 0.0437\n",
            "Epoch 43/100, Train Loss: 0.0601, Val Loss: 0.0437\n",
            "Epoch 44/100, Train Loss: 0.0625, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0607, Val Loss: 0.0437\n",
            "Epoch 46/100, Train Loss: 0.0613, Val Loss: 0.0437\n",
            "Epoch 47/100, Train Loss: 0.0614, Val Loss: 0.0437\n",
            "Epoch 48/100, Train Loss: 0.0621, Val Loss: 0.0437\n",
            "Epoch 49/100, Train Loss: 0.0617, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0594, Val Loss: 0.0437\n",
            "Epoch 51/100, Train Loss: 0.0649, Val Loss: 0.0437\n",
            "Epoch 52/100, Train Loss: 0.0615, Val Loss: 0.0437\n",
            "Epoch 53/100, Train Loss: 0.0628, Val Loss: 0.0437\n",
            "Epoch 54/100, Train Loss: 0.0650, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0624, Val Loss: 0.0437\n",
            "Epoch 56/100, Train Loss: 0.0674, Val Loss: 0.0437\n",
            "Epoch 57/100, Train Loss: 0.0619, Val Loss: 0.0437\n",
            "Epoch 58/100, Train Loss: 0.0604, Val Loss: 0.0437\n",
            "Epoch 59/100, Train Loss: 0.0658, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0623, Val Loss: 0.0437\n",
            "Epoch 61/100, Train Loss: 0.0611, Val Loss: 0.0437\n",
            "Epoch 62/100, Train Loss: 0.0622, Val Loss: 0.0437\n",
            "Epoch 63/100, Train Loss: 0.0645, Val Loss: 0.0437\n",
            "Epoch 64/100, Train Loss: 0.0624, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0614, Val Loss: 0.0437\n",
            "Epoch 66/100, Train Loss: 0.0610, Val Loss: 0.0437\n",
            "Epoch 67/100, Train Loss: 0.0618, Val Loss: 0.0437\n",
            "Epoch 68/100, Train Loss: 0.0632, Val Loss: 0.0437\n",
            "Epoch 69/100, Train Loss: 0.0617, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0639, Val Loss: 0.0437\n",
            "Epoch 71/100, Train Loss: 0.0617, Val Loss: 0.0437\n",
            "Epoch 72/100, Train Loss: 0.0622, Val Loss: 0.0437\n",
            "Epoch 73/100, Train Loss: 0.0625, Val Loss: 0.0437\n",
            "Epoch 74/100, Train Loss: 0.0616, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0599, Val Loss: 0.0437\n",
            "Epoch 76/100, Train Loss: 0.0604, Val Loss: 0.0437\n",
            "Epoch 77/100, Train Loss: 0.0672, Val Loss: 0.0437\n",
            "Epoch 78/100, Train Loss: 0.0619, Val Loss: 0.0437\n",
            "Epoch 79/100, Train Loss: 0.0620, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0631, Val Loss: 0.0437\n",
            "Epoch 81/100, Train Loss: 0.0614, Val Loss: 0.0437\n",
            "Epoch 82/100, Train Loss: 0.0627, Val Loss: 0.0437\n",
            "Epoch 83/100, Train Loss: 0.0623, Val Loss: 0.0437\n",
            "Epoch 84/100, Train Loss: 0.0600, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0594, Val Loss: 0.0437\n",
            "Epoch 86/100, Train Loss: 0.0625, Val Loss: 0.0437\n",
            "Epoch 87/100, Train Loss: 0.0618, Val Loss: 0.0437\n",
            "Epoch 88/100, Train Loss: 0.0611, Val Loss: 0.0437\n",
            "Epoch 89/100, Train Loss: 0.0596, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0632, Val Loss: 0.0437\n",
            "Epoch 91/100, Train Loss: 0.0621, Val Loss: 0.0437\n",
            "Epoch 92/100, Train Loss: 0.0634, Val Loss: 0.0437\n",
            "Epoch 93/100, Train Loss: 0.0623, Val Loss: 0.0437\n",
            "Epoch 94/100, Train Loss: 0.0626, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0626, Val Loss: 0.0437\n",
            "Epoch 96/100, Train Loss: 0.0614, Val Loss: 0.0437\n",
            "Epoch 97/100, Train Loss: 0.0607, Val Loss: 0.0437\n",
            "Epoch 98/100, Train Loss: 0.0612, Val Loss: 0.0437\n",
            "Epoch 99/100, Train Loss: 0.0619, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0636, Val Loss: 0.0437\n",
            "\n",
            "Test iteration 23/28\n",
            "Current training set size: 119 samples\n",
            "Epoch 1/100, Train Loss: 0.1502, Val Loss: 0.2125\n",
            "Epoch 2/100, Train Loss: 0.1500, Val Loss: 0.1945\n",
            "Epoch 3/100, Train Loss: 0.1450, Val Loss: 0.1915\n",
            "Epoch 4/100, Train Loss: 0.1405, Val Loss: 0.1802\n",
            "Epoch 5/100, Train Loss: 0.1400, Val Loss: 0.1736\n",
            "Epoch 6/100, Train Loss: 0.1258, Val Loss: 0.1218\n",
            "Epoch 7/100, Train Loss: 0.0931, Val Loss: 0.0709\n",
            "Epoch 8/100, Train Loss: 0.0741, Val Loss: 0.0398\n",
            "Epoch 9/100, Train Loss: 0.0758, Val Loss: 0.0417\n",
            "Epoch 10/100, Train Loss: 0.0711, Val Loss: 0.0326\n",
            "Epoch 11/100, Train Loss: 0.0689, Val Loss: 0.0497\n",
            "Epoch 12/100, Train Loss: 0.0667, Val Loss: 0.0472\n",
            "Epoch 13/100, Train Loss: 0.0723, Val Loss: 0.0487\n",
            "Epoch 14/100, Train Loss: 0.0643, Val Loss: 0.0404\n",
            "Epoch 15/100, Train Loss: 0.0645, Val Loss: 0.0419\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0655, Val Loss: 0.0409\n",
            "Epoch 17/100, Train Loss: 0.0632, Val Loss: 0.0394\n",
            "Epoch 18/100, Train Loss: 0.0597, Val Loss: 0.0389\n",
            "Epoch 19/100, Train Loss: 0.0606, Val Loss: 0.0389\n",
            "Epoch 20/100, Train Loss: 0.0592, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0627, Val Loss: 0.0389\n",
            "Epoch 22/100, Train Loss: 0.0601, Val Loss: 0.0389\n",
            "Epoch 23/100, Train Loss: 0.0627, Val Loss: 0.0389\n",
            "Epoch 24/100, Train Loss: 0.0571, Val Loss: 0.0389\n",
            "Epoch 25/100, Train Loss: 0.0602, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0597, Val Loss: 0.0389\n",
            "Epoch 27/100, Train Loss: 0.0605, Val Loss: 0.0389\n",
            "Epoch 28/100, Train Loss: 0.0593, Val Loss: 0.0389\n",
            "Epoch 29/100, Train Loss: 0.0634, Val Loss: 0.0389\n",
            "Epoch 30/100, Train Loss: 0.0598, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0599, Val Loss: 0.0389\n",
            "Epoch 32/100, Train Loss: 0.0620, Val Loss: 0.0389\n",
            "Epoch 33/100, Train Loss: 0.0578, Val Loss: 0.0389\n",
            "Epoch 34/100, Train Loss: 0.0613, Val Loss: 0.0389\n",
            "Epoch 35/100, Train Loss: 0.0590, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0616, Val Loss: 0.0389\n",
            "Epoch 37/100, Train Loss: 0.0632, Val Loss: 0.0389\n",
            "Epoch 38/100, Train Loss: 0.0616, Val Loss: 0.0389\n",
            "Epoch 39/100, Train Loss: 0.0611, Val Loss: 0.0389\n",
            "Epoch 40/100, Train Loss: 0.0628, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0619, Val Loss: 0.0389\n",
            "Epoch 42/100, Train Loss: 0.0598, Val Loss: 0.0389\n",
            "Epoch 43/100, Train Loss: 0.0614, Val Loss: 0.0389\n",
            "Epoch 44/100, Train Loss: 0.0619, Val Loss: 0.0389\n",
            "Epoch 45/100, Train Loss: 0.0622, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0615, Val Loss: 0.0389\n",
            "Epoch 47/100, Train Loss: 0.0597, Val Loss: 0.0389\n",
            "Epoch 48/100, Train Loss: 0.0604, Val Loss: 0.0389\n",
            "Epoch 49/100, Train Loss: 0.0621, Val Loss: 0.0389\n",
            "Epoch 50/100, Train Loss: 0.0600, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0633, Val Loss: 0.0389\n",
            "Epoch 52/100, Train Loss: 0.0618, Val Loss: 0.0389\n",
            "Epoch 53/100, Train Loss: 0.0603, Val Loss: 0.0389\n",
            "Epoch 54/100, Train Loss: 0.0597, Val Loss: 0.0389\n",
            "Epoch 55/100, Train Loss: 0.0612, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0614, Val Loss: 0.0389\n",
            "Epoch 57/100, Train Loss: 0.0610, Val Loss: 0.0389\n",
            "Epoch 58/100, Train Loss: 0.0610, Val Loss: 0.0389\n",
            "Epoch 59/100, Train Loss: 0.0628, Val Loss: 0.0389\n",
            "Epoch 60/100, Train Loss: 0.0621, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0606, Val Loss: 0.0389\n",
            "Epoch 62/100, Train Loss: 0.0612, Val Loss: 0.0389\n",
            "Epoch 63/100, Train Loss: 0.0622, Val Loss: 0.0389\n",
            "Epoch 64/100, Train Loss: 0.0640, Val Loss: 0.0389\n",
            "Epoch 65/100, Train Loss: 0.0592, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0624, Val Loss: 0.0389\n",
            "Epoch 67/100, Train Loss: 0.0618, Val Loss: 0.0389\n",
            "Epoch 68/100, Train Loss: 0.0586, Val Loss: 0.0389\n",
            "Epoch 69/100, Train Loss: 0.0607, Val Loss: 0.0389\n",
            "Epoch 70/100, Train Loss: 0.0643, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0602, Val Loss: 0.0389\n",
            "Epoch 72/100, Train Loss: 0.0614, Val Loss: 0.0389\n",
            "Epoch 73/100, Train Loss: 0.0637, Val Loss: 0.0389\n",
            "Epoch 74/100, Train Loss: 0.0612, Val Loss: 0.0389\n",
            "Epoch 75/100, Train Loss: 0.0595, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0627, Val Loss: 0.0389\n",
            "Epoch 77/100, Train Loss: 0.0604, Val Loss: 0.0389\n",
            "Epoch 78/100, Train Loss: 0.0612, Val Loss: 0.0389\n",
            "Epoch 79/100, Train Loss: 0.0623, Val Loss: 0.0389\n",
            "Epoch 80/100, Train Loss: 0.0616, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0615, Val Loss: 0.0389\n",
            "Epoch 82/100, Train Loss: 0.0595, Val Loss: 0.0389\n",
            "Epoch 83/100, Train Loss: 0.0632, Val Loss: 0.0389\n",
            "Epoch 84/100, Train Loss: 0.0588, Val Loss: 0.0389\n",
            "Epoch 85/100, Train Loss: 0.0621, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0608, Val Loss: 0.0389\n",
            "Epoch 87/100, Train Loss: 0.0616, Val Loss: 0.0389\n",
            "Epoch 88/100, Train Loss: 0.0593, Val Loss: 0.0389\n",
            "Epoch 89/100, Train Loss: 0.0607, Val Loss: 0.0389\n",
            "Epoch 90/100, Train Loss: 0.0619, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0635, Val Loss: 0.0389\n",
            "Epoch 92/100, Train Loss: 0.0617, Val Loss: 0.0389\n",
            "Epoch 93/100, Train Loss: 0.0623, Val Loss: 0.0389\n",
            "Epoch 94/100, Train Loss: 0.0608, Val Loss: 0.0389\n",
            "Epoch 95/100, Train Loss: 0.0598, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0648, Val Loss: 0.0389\n",
            "Epoch 97/100, Train Loss: 0.0597, Val Loss: 0.0389\n",
            "Epoch 98/100, Train Loss: 0.0617, Val Loss: 0.0389\n",
            "Epoch 99/100, Train Loss: 0.0624, Val Loss: 0.0389\n",
            "Epoch 100/100, Train Loss: 0.0621, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 24/28\n",
            "Current training set size: 120 samples\n",
            "Epoch 1/100, Train Loss: 0.1511, Val Loss: 0.1927\n",
            "Epoch 2/100, Train Loss: 0.1467, Val Loss: 0.1904\n",
            "Epoch 3/100, Train Loss: 0.1488, Val Loss: 0.1887\n",
            "Epoch 4/100, Train Loss: 0.1418, Val Loss: 0.1835\n",
            "Epoch 5/100, Train Loss: 0.1380, Val Loss: 0.1721\n",
            "Epoch 6/100, Train Loss: 0.1183, Val Loss: 0.1267\n",
            "Epoch 7/100, Train Loss: 0.1172, Val Loss: 0.0735\n",
            "Epoch 8/100, Train Loss: 0.0786, Val Loss: 0.0431\n",
            "Epoch 9/100, Train Loss: 0.0755, Val Loss: 0.0433\n",
            "Epoch 10/100, Train Loss: 0.0715, Val Loss: 0.0440\n",
            "Epoch 11/100, Train Loss: 0.0753, Val Loss: 0.0505\n",
            "Epoch 12/100, Train Loss: 0.0722, Val Loss: 0.0335\n",
            "Epoch 13/100, Train Loss: 0.0641, Val Loss: 0.0473\n",
            "Epoch 14/100, Train Loss: 0.0649, Val Loss: 0.0416\n",
            "Epoch 15/100, Train Loss: 0.0650, Val Loss: 0.0386\n",
            "Epoch 16/100, Train Loss: 0.0595, Val Loss: 0.0420\n",
            "Epoch 17/100, Train Loss: 0.0636, Val Loss: 0.0450\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0605, Val Loss: 0.0447\n",
            "Epoch 19/100, Train Loss: 0.0602, Val Loss: 0.0444\n",
            "Epoch 20/100, Train Loss: 0.0606, Val Loss: 0.0441\n",
            "Epoch 21/100, Train Loss: 0.0573, Val Loss: 0.0437\n",
            "Epoch 22/100, Train Loss: 0.0586, Val Loss: 0.0436\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0561, Val Loss: 0.0436\n",
            "Epoch 24/100, Train Loss: 0.0581, Val Loss: 0.0436\n",
            "Epoch 25/100, Train Loss: 0.0571, Val Loss: 0.0436\n",
            "Epoch 26/100, Train Loss: 0.0566, Val Loss: 0.0436\n",
            "Epoch 27/100, Train Loss: 0.0578, Val Loss: 0.0436\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0597, Val Loss: 0.0436\n",
            "Epoch 29/100, Train Loss: 0.0596, Val Loss: 0.0436\n",
            "Epoch 30/100, Train Loss: 0.0592, Val Loss: 0.0436\n",
            "Epoch 31/100, Train Loss: 0.0572, Val Loss: 0.0436\n",
            "Epoch 32/100, Train Loss: 0.0590, Val Loss: 0.0436\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0578, Val Loss: 0.0436\n",
            "Epoch 34/100, Train Loss: 0.0588, Val Loss: 0.0436\n",
            "Epoch 35/100, Train Loss: 0.0585, Val Loss: 0.0436\n",
            "Epoch 36/100, Train Loss: 0.0598, Val Loss: 0.0436\n",
            "Epoch 37/100, Train Loss: 0.0602, Val Loss: 0.0436\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0609, Val Loss: 0.0436\n",
            "Epoch 39/100, Train Loss: 0.0595, Val Loss: 0.0436\n",
            "Epoch 40/100, Train Loss: 0.0591, Val Loss: 0.0436\n",
            "Epoch 41/100, Train Loss: 0.0586, Val Loss: 0.0436\n",
            "Epoch 42/100, Train Loss: 0.0582, Val Loss: 0.0436\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0555, Val Loss: 0.0436\n",
            "Epoch 44/100, Train Loss: 0.0614, Val Loss: 0.0436\n",
            "Epoch 45/100, Train Loss: 0.0587, Val Loss: 0.0436\n",
            "Epoch 46/100, Train Loss: 0.0596, Val Loss: 0.0436\n",
            "Epoch 47/100, Train Loss: 0.0574, Val Loss: 0.0436\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0585, Val Loss: 0.0436\n",
            "Epoch 49/100, Train Loss: 0.0569, Val Loss: 0.0436\n",
            "Epoch 50/100, Train Loss: 0.0588, Val Loss: 0.0436\n",
            "Epoch 51/100, Train Loss: 0.0581, Val Loss: 0.0436\n",
            "Epoch 52/100, Train Loss: 0.0581, Val Loss: 0.0436\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0577, Val Loss: 0.0436\n",
            "Epoch 54/100, Train Loss: 0.0566, Val Loss: 0.0436\n",
            "Epoch 55/100, Train Loss: 0.0554, Val Loss: 0.0436\n",
            "Epoch 56/100, Train Loss: 0.0590, Val Loss: 0.0436\n",
            "Epoch 57/100, Train Loss: 0.0590, Val Loss: 0.0436\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0593, Val Loss: 0.0436\n",
            "Epoch 59/100, Train Loss: 0.0578, Val Loss: 0.0436\n",
            "Epoch 60/100, Train Loss: 0.0576, Val Loss: 0.0436\n",
            "Epoch 61/100, Train Loss: 0.0580, Val Loss: 0.0436\n",
            "Epoch 62/100, Train Loss: 0.0580, Val Loss: 0.0436\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0594, Val Loss: 0.0436\n",
            "Epoch 64/100, Train Loss: 0.0572, Val Loss: 0.0436\n",
            "Epoch 65/100, Train Loss: 0.0581, Val Loss: 0.0436\n",
            "Epoch 66/100, Train Loss: 0.0591, Val Loss: 0.0436\n",
            "Epoch 67/100, Train Loss: 0.0572, Val Loss: 0.0436\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0585, Val Loss: 0.0436\n",
            "Epoch 69/100, Train Loss: 0.0576, Val Loss: 0.0436\n",
            "Epoch 70/100, Train Loss: 0.0572, Val Loss: 0.0436\n",
            "Epoch 71/100, Train Loss: 0.0572, Val Loss: 0.0436\n",
            "Epoch 72/100, Train Loss: 0.0576, Val Loss: 0.0436\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0571, Val Loss: 0.0436\n",
            "Epoch 74/100, Train Loss: 0.0575, Val Loss: 0.0436\n",
            "Epoch 75/100, Train Loss: 0.0579, Val Loss: 0.0436\n",
            "Epoch 76/100, Train Loss: 0.0581, Val Loss: 0.0436\n",
            "Epoch 77/100, Train Loss: 0.0576, Val Loss: 0.0436\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0575, Val Loss: 0.0436\n",
            "Epoch 79/100, Train Loss: 0.0576, Val Loss: 0.0436\n",
            "Epoch 80/100, Train Loss: 0.0596, Val Loss: 0.0436\n",
            "Epoch 81/100, Train Loss: 0.0601, Val Loss: 0.0436\n",
            "Epoch 82/100, Train Loss: 0.0588, Val Loss: 0.0436\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0604, Val Loss: 0.0436\n",
            "Epoch 84/100, Train Loss: 0.0581, Val Loss: 0.0436\n",
            "Epoch 85/100, Train Loss: 0.0599, Val Loss: 0.0436\n",
            "Epoch 86/100, Train Loss: 0.0594, Val Loss: 0.0436\n",
            "Epoch 87/100, Train Loss: 0.0576, Val Loss: 0.0436\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0599, Val Loss: 0.0436\n",
            "Epoch 89/100, Train Loss: 0.0591, Val Loss: 0.0436\n",
            "Epoch 90/100, Train Loss: 0.0587, Val Loss: 0.0436\n",
            "Epoch 91/100, Train Loss: 0.0581, Val Loss: 0.0436\n",
            "Epoch 92/100, Train Loss: 0.0585, Val Loss: 0.0436\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0588, Val Loss: 0.0436\n",
            "Epoch 94/100, Train Loss: 0.0582, Val Loss: 0.0436\n",
            "Epoch 95/100, Train Loss: 0.0581, Val Loss: 0.0436\n",
            "Epoch 96/100, Train Loss: 0.0578, Val Loss: 0.0436\n",
            "Epoch 97/100, Train Loss: 0.0598, Val Loss: 0.0436\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0593, Val Loss: 0.0436\n",
            "Epoch 99/100, Train Loss: 0.0559, Val Loss: 0.0436\n",
            "Epoch 100/100, Train Loss: 0.0593, Val Loss: 0.0436\n",
            "\n",
            "Test iteration 25/28\n",
            "Current training set size: 121 samples\n",
            "Epoch 1/100, Train Loss: 0.1563, Val Loss: 0.1979\n",
            "Epoch 2/100, Train Loss: 0.1506, Val Loss: 0.1888\n",
            "Epoch 3/100, Train Loss: 0.1426, Val Loss: 0.1872\n",
            "Epoch 4/100, Train Loss: 0.1401, Val Loss: 0.1753\n",
            "Epoch 5/100, Train Loss: 0.1348, Val Loss: 0.1592\n",
            "Epoch 6/100, Train Loss: 0.1125, Val Loss: 0.1068\n",
            "Epoch 7/100, Train Loss: 0.0809, Val Loss: 0.0479\n",
            "Epoch 8/100, Train Loss: 0.0749, Val Loss: 0.0383\n",
            "Epoch 9/100, Train Loss: 0.0683, Val Loss: 0.0445\n",
            "Epoch 10/100, Train Loss: 0.0679, Val Loss: 0.0371\n",
            "Epoch 11/100, Train Loss: 0.0730, Val Loss: 0.0415\n",
            "Epoch 12/100, Train Loss: 0.0709, Val Loss: 0.0491\n",
            "Epoch 13/100, Train Loss: 0.0757, Val Loss: 0.0497\n",
            "Epoch 14/100, Train Loss: 0.0650, Val Loss: 0.0344\n",
            "Epoch 15/100, Train Loss: 0.0627, Val Loss: 0.0432\n",
            "Epoch 16/100, Train Loss: 0.0632, Val Loss: 0.0395\n",
            "Epoch 17/100, Train Loss: 0.0643, Val Loss: 0.0393\n",
            "Epoch 18/100, Train Loss: 0.0576, Val Loss: 0.0355\n",
            "Epoch 19/100, Train Loss: 0.0582, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0576, Val Loss: 0.0380\n",
            "Epoch 21/100, Train Loss: 0.0527, Val Loss: 0.0370\n",
            "Epoch 22/100, Train Loss: 0.0550, Val Loss: 0.0366\n",
            "Epoch 23/100, Train Loss: 0.0527, Val Loss: 0.0365\n",
            "Epoch 24/100, Train Loss: 0.0522, Val Loss: 0.0362\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0532, Val Loss: 0.0362\n",
            "Epoch 26/100, Train Loss: 0.0533, Val Loss: 0.0362\n",
            "Epoch 27/100, Train Loss: 0.0544, Val Loss: 0.0362\n",
            "Epoch 28/100, Train Loss: 0.0531, Val Loss: 0.0362\n",
            "Epoch 29/100, Train Loss: 0.0527, Val Loss: 0.0362\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0531, Val Loss: 0.0362\n",
            "Epoch 31/100, Train Loss: 0.0505, Val Loss: 0.0362\n",
            "Epoch 32/100, Train Loss: 0.0550, Val Loss: 0.0362\n",
            "Epoch 33/100, Train Loss: 0.0632, Val Loss: 0.0362\n",
            "Epoch 34/100, Train Loss: 0.0543, Val Loss: 0.0362\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0533, Val Loss: 0.0362\n",
            "Epoch 36/100, Train Loss: 0.0528, Val Loss: 0.0362\n",
            "Epoch 37/100, Train Loss: 0.0623, Val Loss: 0.0362\n",
            "Epoch 38/100, Train Loss: 0.0523, Val Loss: 0.0362\n",
            "Epoch 39/100, Train Loss: 0.0521, Val Loss: 0.0362\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0534, Val Loss: 0.0362\n",
            "Epoch 41/100, Train Loss: 0.0510, Val Loss: 0.0362\n",
            "Epoch 42/100, Train Loss: 0.0702, Val Loss: 0.0362\n",
            "Epoch 43/100, Train Loss: 0.0535, Val Loss: 0.0362\n",
            "Epoch 44/100, Train Loss: 0.0552, Val Loss: 0.0362\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0535, Val Loss: 0.0362\n",
            "Epoch 46/100, Train Loss: 0.0528, Val Loss: 0.0362\n",
            "Epoch 47/100, Train Loss: 0.0542, Val Loss: 0.0362\n",
            "Epoch 48/100, Train Loss: 0.0536, Val Loss: 0.0362\n",
            "Epoch 49/100, Train Loss: 0.0525, Val Loss: 0.0362\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0712, Val Loss: 0.0362\n",
            "Epoch 51/100, Train Loss: 0.0520, Val Loss: 0.0362\n",
            "Epoch 52/100, Train Loss: 0.0529, Val Loss: 0.0362\n",
            "Epoch 53/100, Train Loss: 0.0539, Val Loss: 0.0362\n",
            "Epoch 54/100, Train Loss: 0.0522, Val Loss: 0.0362\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0517, Val Loss: 0.0362\n",
            "Epoch 56/100, Train Loss: 0.0549, Val Loss: 0.0362\n",
            "Epoch 57/100, Train Loss: 0.0662, Val Loss: 0.0362\n",
            "Epoch 58/100, Train Loss: 0.0531, Val Loss: 0.0362\n",
            "Epoch 59/100, Train Loss: 0.0539, Val Loss: 0.0362\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0534, Val Loss: 0.0362\n",
            "Epoch 61/100, Train Loss: 0.0512, Val Loss: 0.0362\n",
            "Epoch 62/100, Train Loss: 0.0535, Val Loss: 0.0362\n",
            "Epoch 63/100, Train Loss: 0.0508, Val Loss: 0.0362\n",
            "Epoch 64/100, Train Loss: 0.0528, Val Loss: 0.0362\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0508, Val Loss: 0.0362\n",
            "Epoch 66/100, Train Loss: 0.0561, Val Loss: 0.0362\n",
            "Epoch 67/100, Train Loss: 0.0517, Val Loss: 0.0362\n",
            "Epoch 68/100, Train Loss: 0.0634, Val Loss: 0.0362\n",
            "Epoch 69/100, Train Loss: 0.0547, Val Loss: 0.0362\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0546, Val Loss: 0.0362\n",
            "Epoch 71/100, Train Loss: 0.0566, Val Loss: 0.0362\n",
            "Epoch 72/100, Train Loss: 0.0550, Val Loss: 0.0362\n",
            "Epoch 73/100, Train Loss: 0.0536, Val Loss: 0.0362\n",
            "Epoch 74/100, Train Loss: 0.0511, Val Loss: 0.0362\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0534, Val Loss: 0.0362\n",
            "Epoch 76/100, Train Loss: 0.0556, Val Loss: 0.0362\n",
            "Epoch 77/100, Train Loss: 0.0511, Val Loss: 0.0362\n",
            "Epoch 78/100, Train Loss: 0.0660, Val Loss: 0.0362\n",
            "Epoch 79/100, Train Loss: 0.0527, Val Loss: 0.0362\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0532, Val Loss: 0.0362\n",
            "Epoch 81/100, Train Loss: 0.0538, Val Loss: 0.0362\n",
            "Epoch 82/100, Train Loss: 0.0529, Val Loss: 0.0362\n",
            "Epoch 83/100, Train Loss: 0.0560, Val Loss: 0.0362\n",
            "Epoch 84/100, Train Loss: 0.0525, Val Loss: 0.0362\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0526, Val Loss: 0.0362\n",
            "Epoch 86/100, Train Loss: 0.0545, Val Loss: 0.0362\n",
            "Epoch 87/100, Train Loss: 0.0526, Val Loss: 0.0362\n",
            "Epoch 88/100, Train Loss: 0.0530, Val Loss: 0.0362\n",
            "Epoch 89/100, Train Loss: 0.0541, Val Loss: 0.0362\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0521, Val Loss: 0.0362\n",
            "Epoch 91/100, Train Loss: 0.0593, Val Loss: 0.0362\n",
            "Epoch 92/100, Train Loss: 0.0536, Val Loss: 0.0362\n",
            "Epoch 93/100, Train Loss: 0.0509, Val Loss: 0.0362\n",
            "Epoch 94/100, Train Loss: 0.0544, Val Loss: 0.0362\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0540, Val Loss: 0.0362\n",
            "Epoch 96/100, Train Loss: 0.0544, Val Loss: 0.0362\n",
            "Epoch 97/100, Train Loss: 0.0533, Val Loss: 0.0362\n",
            "Epoch 98/100, Train Loss: 0.0539, Val Loss: 0.0362\n",
            "Epoch 99/100, Train Loss: 0.0560, Val Loss: 0.0362\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0529, Val Loss: 0.0362\n",
            "\n",
            "Test iteration 26/28\n",
            "Current training set size: 122 samples\n",
            "Epoch 1/100, Train Loss: 0.1509, Val Loss: 0.2017\n",
            "Epoch 2/100, Train Loss: 0.1464, Val Loss: 0.1890\n",
            "Epoch 3/100, Train Loss: 0.1444, Val Loss: 0.1845\n",
            "Epoch 4/100, Train Loss: 0.1422, Val Loss: 0.1882\n",
            "Epoch 5/100, Train Loss: 0.1379, Val Loss: 0.1627\n",
            "Epoch 6/100, Train Loss: 0.1252, Val Loss: 0.1241\n",
            "Epoch 7/100, Train Loss: 0.1021, Val Loss: 0.0396\n",
            "Epoch 8/100, Train Loss: 0.0749, Val Loss: 0.0494\n",
            "Epoch 9/100, Train Loss: 0.0737, Val Loss: 0.0376\n",
            "Epoch 10/100, Train Loss: 0.0740, Val Loss: 0.0461\n",
            "Epoch 11/100, Train Loss: 0.0680, Val Loss: 0.0395\n",
            "Epoch 12/100, Train Loss: 0.0719, Val Loss: 0.0583\n",
            "Epoch 13/100, Train Loss: 0.0692, Val Loss: 0.0460\n",
            "Epoch 14/100, Train Loss: 0.0744, Val Loss: 0.0470\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 15/100, Train Loss: 0.0760, Val Loss: 0.0458\n",
            "Epoch 16/100, Train Loss: 0.0709, Val Loss: 0.0448\n",
            "Epoch 17/100, Train Loss: 0.0671, Val Loss: 0.0444\n",
            "Epoch 18/100, Train Loss: 0.0621, Val Loss: 0.0442\n",
            "Epoch 19/100, Train Loss: 0.0626, Val Loss: 0.0441\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0628, Val Loss: 0.0441\n",
            "Epoch 21/100, Train Loss: 0.0607, Val Loss: 0.0441\n",
            "Epoch 22/100, Train Loss: 0.0628, Val Loss: 0.0441\n",
            "Epoch 23/100, Train Loss: 0.0629, Val Loss: 0.0441\n",
            "Epoch 24/100, Train Loss: 0.0635, Val Loss: 0.0441\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0614, Val Loss: 0.0441\n",
            "Epoch 26/100, Train Loss: 0.0686, Val Loss: 0.0441\n",
            "Epoch 27/100, Train Loss: 0.0609, Val Loss: 0.0441\n",
            "Epoch 28/100, Train Loss: 0.0625, Val Loss: 0.0441\n",
            "Epoch 29/100, Train Loss: 0.0622, Val Loss: 0.0441\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0631, Val Loss: 0.0441\n",
            "Epoch 31/100, Train Loss: 0.0654, Val Loss: 0.0441\n",
            "Epoch 32/100, Train Loss: 0.0632, Val Loss: 0.0441\n",
            "Epoch 33/100, Train Loss: 0.0628, Val Loss: 0.0441\n",
            "Epoch 34/100, Train Loss: 0.0627, Val Loss: 0.0441\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0635, Val Loss: 0.0441\n",
            "Epoch 36/100, Train Loss: 0.0701, Val Loss: 0.0441\n",
            "Epoch 37/100, Train Loss: 0.0623, Val Loss: 0.0441\n",
            "Epoch 38/100, Train Loss: 0.0626, Val Loss: 0.0441\n",
            "Epoch 39/100, Train Loss: 0.0647, Val Loss: 0.0441\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0617, Val Loss: 0.0441\n",
            "Epoch 41/100, Train Loss: 0.0620, Val Loss: 0.0441\n",
            "Epoch 42/100, Train Loss: 0.0612, Val Loss: 0.0441\n",
            "Epoch 43/100, Train Loss: 0.0635, Val Loss: 0.0441\n",
            "Epoch 44/100, Train Loss: 0.0607, Val Loss: 0.0441\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0612, Val Loss: 0.0441\n",
            "Epoch 46/100, Train Loss: 0.0622, Val Loss: 0.0441\n",
            "Epoch 47/100, Train Loss: 0.0611, Val Loss: 0.0441\n",
            "Epoch 48/100, Train Loss: 0.0625, Val Loss: 0.0441\n",
            "Epoch 49/100, Train Loss: 0.0617, Val Loss: 0.0441\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0633, Val Loss: 0.0441\n",
            "Epoch 51/100, Train Loss: 0.0680, Val Loss: 0.0441\n",
            "Epoch 52/100, Train Loss: 0.0643, Val Loss: 0.0441\n",
            "Epoch 53/100, Train Loss: 0.0627, Val Loss: 0.0441\n",
            "Epoch 54/100, Train Loss: 0.0637, Val Loss: 0.0441\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0624, Val Loss: 0.0441\n",
            "Epoch 56/100, Train Loss: 0.0620, Val Loss: 0.0441\n",
            "Epoch 57/100, Train Loss: 0.0643, Val Loss: 0.0441\n",
            "Epoch 58/100, Train Loss: 0.0631, Val Loss: 0.0441\n",
            "Epoch 59/100, Train Loss: 0.0628, Val Loss: 0.0441\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0667, Val Loss: 0.0441\n",
            "Epoch 61/100, Train Loss: 0.0614, Val Loss: 0.0441\n",
            "Epoch 62/100, Train Loss: 0.0601, Val Loss: 0.0441\n",
            "Epoch 63/100, Train Loss: 0.0634, Val Loss: 0.0441\n",
            "Epoch 64/100, Train Loss: 0.0639, Val Loss: 0.0441\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0622, Val Loss: 0.0441\n",
            "Epoch 66/100, Train Loss: 0.0613, Val Loss: 0.0441\n",
            "Epoch 67/100, Train Loss: 0.0635, Val Loss: 0.0441\n",
            "Epoch 68/100, Train Loss: 0.0625, Val Loss: 0.0441\n",
            "Epoch 69/100, Train Loss: 0.0617, Val Loss: 0.0441\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0617, Val Loss: 0.0441\n",
            "Epoch 71/100, Train Loss: 0.0629, Val Loss: 0.0441\n",
            "Epoch 72/100, Train Loss: 0.0616, Val Loss: 0.0441\n",
            "Epoch 73/100, Train Loss: 0.0616, Val Loss: 0.0441\n",
            "Epoch 74/100, Train Loss: 0.0614, Val Loss: 0.0441\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0630, Val Loss: 0.0441\n",
            "Epoch 76/100, Train Loss: 0.0635, Val Loss: 0.0441\n",
            "Epoch 77/100, Train Loss: 0.0627, Val Loss: 0.0441\n",
            "Epoch 78/100, Train Loss: 0.0636, Val Loss: 0.0441\n",
            "Epoch 79/100, Train Loss: 0.0634, Val Loss: 0.0441\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0649, Val Loss: 0.0441\n",
            "Epoch 81/100, Train Loss: 0.0626, Val Loss: 0.0441\n",
            "Epoch 82/100, Train Loss: 0.0626, Val Loss: 0.0441\n",
            "Epoch 83/100, Train Loss: 0.0631, Val Loss: 0.0441\n",
            "Epoch 84/100, Train Loss: 0.0641, Val Loss: 0.0441\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0615, Val Loss: 0.0441\n",
            "Epoch 86/100, Train Loss: 0.0615, Val Loss: 0.0441\n",
            "Epoch 87/100, Train Loss: 0.0669, Val Loss: 0.0441\n",
            "Epoch 88/100, Train Loss: 0.0631, Val Loss: 0.0441\n",
            "Epoch 89/100, Train Loss: 0.0626, Val Loss: 0.0441\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0609, Val Loss: 0.0441\n",
            "Epoch 91/100, Train Loss: 0.0608, Val Loss: 0.0441\n",
            "Epoch 92/100, Train Loss: 0.0634, Val Loss: 0.0441\n",
            "Epoch 93/100, Train Loss: 0.0624, Val Loss: 0.0441\n",
            "Epoch 94/100, Train Loss: 0.0633, Val Loss: 0.0441\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0614, Val Loss: 0.0441\n",
            "Epoch 96/100, Train Loss: 0.0662, Val Loss: 0.0441\n",
            "Epoch 97/100, Train Loss: 0.0633, Val Loss: 0.0441\n",
            "Epoch 98/100, Train Loss: 0.0645, Val Loss: 0.0441\n",
            "Epoch 99/100, Train Loss: 0.0596, Val Loss: 0.0441\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0609, Val Loss: 0.0441\n",
            "\n",
            "Test iteration 27/28\n",
            "Current training set size: 123 samples\n",
            "Epoch 1/100, Train Loss: 0.1479, Val Loss: 0.2124\n",
            "Epoch 2/100, Train Loss: 0.1464, Val Loss: 0.1909\n",
            "Epoch 3/100, Train Loss: 0.1431, Val Loss: 0.1932\n",
            "Epoch 4/100, Train Loss: 0.1393, Val Loss: 0.1987\n",
            "Epoch 5/100, Train Loss: 0.1345, Val Loss: 0.1740\n",
            "Epoch 6/100, Train Loss: 0.1114, Val Loss: 0.0964\n",
            "Epoch 7/100, Train Loss: 0.0760, Val Loss: 0.0314\n",
            "Epoch 8/100, Train Loss: 0.0753, Val Loss: 0.0406\n",
            "Epoch 9/100, Train Loss: 0.0751, Val Loss: 0.0341\n",
            "Epoch 10/100, Train Loss: 0.0703, Val Loss: 0.0438\n",
            "Epoch 11/100, Train Loss: 0.0650, Val Loss: 0.0388\n",
            "Epoch 12/100, Train Loss: 0.0691, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 13/100, Train Loss: 0.0692, Val Loss: 0.0389\n",
            "Epoch 14/100, Train Loss: 0.0657, Val Loss: 0.0384\n",
            "Epoch 15/100, Train Loss: 0.0631, Val Loss: 0.0381\n",
            "Epoch 16/100, Train Loss: 0.0637, Val Loss: 0.0381\n",
            "Epoch 17/100, Train Loss: 0.0647, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0621, Val Loss: 0.0381\n",
            "Epoch 19/100, Train Loss: 0.0647, Val Loss: 0.0381\n",
            "Epoch 20/100, Train Loss: 0.0631, Val Loss: 0.0381\n",
            "Epoch 21/100, Train Loss: 0.0632, Val Loss: 0.0381\n",
            "Epoch 22/100, Train Loss: 0.0619, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0620, Val Loss: 0.0381\n",
            "Epoch 24/100, Train Loss: 0.0630, Val Loss: 0.0381\n",
            "Epoch 25/100, Train Loss: 0.0606, Val Loss: 0.0381\n",
            "Epoch 26/100, Train Loss: 0.0639, Val Loss: 0.0381\n",
            "Epoch 27/100, Train Loss: 0.0657, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0630, Val Loss: 0.0381\n",
            "Epoch 29/100, Train Loss: 0.0634, Val Loss: 0.0381\n",
            "Epoch 30/100, Train Loss: 0.0637, Val Loss: 0.0381\n",
            "Epoch 31/100, Train Loss: 0.0630, Val Loss: 0.0381\n",
            "Epoch 32/100, Train Loss: 0.0648, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0620, Val Loss: 0.0381\n",
            "Epoch 34/100, Train Loss: 0.0629, Val Loss: 0.0381\n",
            "Epoch 35/100, Train Loss: 0.0644, Val Loss: 0.0381\n",
            "Epoch 36/100, Train Loss: 0.0632, Val Loss: 0.0381\n",
            "Epoch 37/100, Train Loss: 0.0626, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0640, Val Loss: 0.0381\n",
            "Epoch 39/100, Train Loss: 0.0628, Val Loss: 0.0381\n",
            "Epoch 40/100, Train Loss: 0.0614, Val Loss: 0.0381\n",
            "Epoch 41/100, Train Loss: 0.0626, Val Loss: 0.0381\n",
            "Epoch 42/100, Train Loss: 0.0634, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0617, Val Loss: 0.0381\n",
            "Epoch 44/100, Train Loss: 0.0599, Val Loss: 0.0381\n",
            "Epoch 45/100, Train Loss: 0.0644, Val Loss: 0.0381\n",
            "Epoch 46/100, Train Loss: 0.0627, Val Loss: 0.0381\n",
            "Epoch 47/100, Train Loss: 0.0614, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0628, Val Loss: 0.0381\n",
            "Epoch 49/100, Train Loss: 0.0624, Val Loss: 0.0381\n",
            "Epoch 50/100, Train Loss: 0.0612, Val Loss: 0.0381\n",
            "Epoch 51/100, Train Loss: 0.0623, Val Loss: 0.0381\n",
            "Epoch 52/100, Train Loss: 0.0635, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0633, Val Loss: 0.0381\n",
            "Epoch 54/100, Train Loss: 0.0628, Val Loss: 0.0381\n",
            "Epoch 55/100, Train Loss: 0.0643, Val Loss: 0.0381\n",
            "Epoch 56/100, Train Loss: 0.0615, Val Loss: 0.0381\n",
            "Epoch 57/100, Train Loss: 0.0630, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0649, Val Loss: 0.0381\n",
            "Epoch 59/100, Train Loss: 0.0624, Val Loss: 0.0381\n",
            "Epoch 60/100, Train Loss: 0.0631, Val Loss: 0.0381\n",
            "Epoch 61/100, Train Loss: 0.0632, Val Loss: 0.0381\n",
            "Epoch 62/100, Train Loss: 0.0638, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0635, Val Loss: 0.0381\n",
            "Epoch 64/100, Train Loss: 0.0628, Val Loss: 0.0381\n",
            "Epoch 65/100, Train Loss: 0.0638, Val Loss: 0.0381\n",
            "Epoch 66/100, Train Loss: 0.0628, Val Loss: 0.0381\n",
            "Epoch 67/100, Train Loss: 0.0625, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0622, Val Loss: 0.0381\n",
            "Epoch 69/100, Train Loss: 0.0630, Val Loss: 0.0381\n",
            "Epoch 70/100, Train Loss: 0.0616, Val Loss: 0.0381\n",
            "Epoch 71/100, Train Loss: 0.0626, Val Loss: 0.0381\n",
            "Epoch 72/100, Train Loss: 0.0616, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0641, Val Loss: 0.0381\n",
            "Epoch 74/100, Train Loss: 0.0639, Val Loss: 0.0381\n",
            "Epoch 75/100, Train Loss: 0.0623, Val Loss: 0.0381\n",
            "Epoch 76/100, Train Loss: 0.0626, Val Loss: 0.0381\n",
            "Epoch 77/100, Train Loss: 0.0652, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0631, Val Loss: 0.0381\n",
            "Epoch 79/100, Train Loss: 0.0635, Val Loss: 0.0381\n",
            "Epoch 80/100, Train Loss: 0.0618, Val Loss: 0.0381\n",
            "Epoch 81/100, Train Loss: 0.0618, Val Loss: 0.0381\n",
            "Epoch 82/100, Train Loss: 0.0636, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0633, Val Loss: 0.0381\n",
            "Epoch 84/100, Train Loss: 0.0608, Val Loss: 0.0381\n",
            "Epoch 85/100, Train Loss: 0.0656, Val Loss: 0.0381\n",
            "Epoch 86/100, Train Loss: 0.0628, Val Loss: 0.0381\n",
            "Epoch 87/100, Train Loss: 0.0614, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0627, Val Loss: 0.0381\n",
            "Epoch 89/100, Train Loss: 0.0635, Val Loss: 0.0381\n",
            "Epoch 90/100, Train Loss: 0.0621, Val Loss: 0.0381\n",
            "Epoch 91/100, Train Loss: 0.0623, Val Loss: 0.0381\n",
            "Epoch 92/100, Train Loss: 0.0634, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0626, Val Loss: 0.0381\n",
            "Epoch 94/100, Train Loss: 0.0621, Val Loss: 0.0381\n",
            "Epoch 95/100, Train Loss: 0.0638, Val Loss: 0.0381\n",
            "Epoch 96/100, Train Loss: 0.0638, Val Loss: 0.0381\n",
            "Epoch 97/100, Train Loss: 0.0623, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0629, Val Loss: 0.0381\n",
            "Epoch 99/100, Train Loss: 0.0621, Val Loss: 0.0381\n",
            "Epoch 100/100, Train Loss: 0.0622, Val Loss: 0.0381\n",
            "\n",
            "Test iteration 28/28\n",
            "Current training set size: 124 samples\n",
            "Epoch 1/100, Train Loss: 0.1583, Val Loss: 0.1926\n",
            "Epoch 2/100, Train Loss: 0.1486, Val Loss: 0.1906\n",
            "Epoch 3/100, Train Loss: 0.1464, Val Loss: 0.1826\n",
            "Epoch 4/100, Train Loss: 0.1432, Val Loss: 0.1803\n",
            "Epoch 5/100, Train Loss: 0.1321, Val Loss: 0.1668\n",
            "Epoch 6/100, Train Loss: 0.1129, Val Loss: 0.0874\n",
            "Epoch 7/100, Train Loss: 0.0787, Val Loss: 0.0369\n",
            "Epoch 8/100, Train Loss: 0.0714, Val Loss: 0.0598\n",
            "Epoch 9/100, Train Loss: 0.0720, Val Loss: 0.0416\n",
            "Epoch 10/100, Train Loss: 0.0680, Val Loss: 0.0364\n",
            "Epoch 11/100, Train Loss: 0.0718, Val Loss: 0.0381\n",
            "Epoch 12/100, Train Loss: 0.0685, Val Loss: 0.0414\n",
            "Epoch 13/100, Train Loss: 0.0655, Val Loss: 0.0378\n",
            "Epoch 14/100, Train Loss: 0.0624, Val Loss: 0.0328\n",
            "Epoch 15/100, Train Loss: 0.0647, Val Loss: 0.0388\n",
            "Epoch 16/100, Train Loss: 0.0656, Val Loss: 0.0365\n",
            "Epoch 17/100, Train Loss: 0.0588, Val Loss: 0.0484\n",
            "Epoch 18/100, Train Loss: 0.0587, Val Loss: 0.0355\n",
            "Epoch 19/100, Train Loss: 0.0619, Val Loss: 0.0396\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0595, Val Loss: 0.0375\n",
            "Epoch 21/100, Train Loss: 0.0582, Val Loss: 0.0366\n",
            "Epoch 22/100, Train Loss: 0.0557, Val Loss: 0.0362\n",
            "Epoch 23/100, Train Loss: 0.0525, Val Loss: 0.0363\n",
            "Epoch 24/100, Train Loss: 0.0554, Val Loss: 0.0365\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0539, Val Loss: 0.0365\n",
            "Epoch 26/100, Train Loss: 0.0529, Val Loss: 0.0365\n",
            "Epoch 27/100, Train Loss: 0.0543, Val Loss: 0.0365\n",
            "Epoch 28/100, Train Loss: 0.0558, Val Loss: 0.0365\n",
            "Epoch 29/100, Train Loss: 0.0550, Val Loss: 0.0365\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0544, Val Loss: 0.0365\n",
            "Epoch 31/100, Train Loss: 0.0549, Val Loss: 0.0365\n",
            "Epoch 32/100, Train Loss: 0.0544, Val Loss: 0.0365\n",
            "Epoch 33/100, Train Loss: 0.0522, Val Loss: 0.0365\n",
            "Epoch 34/100, Train Loss: 0.0542, Val Loss: 0.0365\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0520, Val Loss: 0.0365\n",
            "Epoch 36/100, Train Loss: 0.0561, Val Loss: 0.0365\n",
            "Epoch 37/100, Train Loss: 0.0529, Val Loss: 0.0365\n",
            "Epoch 38/100, Train Loss: 0.0548, Val Loss: 0.0365\n",
            "Epoch 39/100, Train Loss: 0.0556, Val Loss: 0.0365\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0557, Val Loss: 0.0365\n",
            "Epoch 41/100, Train Loss: 0.0546, Val Loss: 0.0365\n",
            "Epoch 42/100, Train Loss: 0.0526, Val Loss: 0.0365\n",
            "Epoch 43/100, Train Loss: 0.0559, Val Loss: 0.0365\n",
            "Epoch 44/100, Train Loss: 0.0532, Val Loss: 0.0365\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0540, Val Loss: 0.0365\n",
            "Epoch 46/100, Train Loss: 0.0528, Val Loss: 0.0365\n",
            "Epoch 47/100, Train Loss: 0.0550, Val Loss: 0.0365\n",
            "Epoch 48/100, Train Loss: 0.0536, Val Loss: 0.0365\n",
            "Epoch 49/100, Train Loss: 0.0524, Val Loss: 0.0365\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0554, Val Loss: 0.0365\n",
            "Epoch 51/100, Train Loss: 0.0555, Val Loss: 0.0365\n",
            "Epoch 52/100, Train Loss: 0.0535, Val Loss: 0.0365\n",
            "Epoch 53/100, Train Loss: 0.0549, Val Loss: 0.0365\n",
            "Epoch 54/100, Train Loss: 0.0546, Val Loss: 0.0365\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0546, Val Loss: 0.0365\n",
            "Epoch 56/100, Train Loss: 0.0534, Val Loss: 0.0365\n",
            "Epoch 57/100, Train Loss: 0.0519, Val Loss: 0.0365\n",
            "Epoch 58/100, Train Loss: 0.0530, Val Loss: 0.0365\n",
            "Epoch 59/100, Train Loss: 0.0541, Val Loss: 0.0365\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0529, Val Loss: 0.0365\n",
            "Epoch 61/100, Train Loss: 0.0543, Val Loss: 0.0365\n",
            "Epoch 62/100, Train Loss: 0.0552, Val Loss: 0.0365\n",
            "Epoch 63/100, Train Loss: 0.0553, Val Loss: 0.0365\n",
            "Epoch 64/100, Train Loss: 0.0519, Val Loss: 0.0365\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0549, Val Loss: 0.0365\n",
            "Epoch 66/100, Train Loss: 0.0556, Val Loss: 0.0365\n",
            "Epoch 67/100, Train Loss: 0.0543, Val Loss: 0.0365\n",
            "Epoch 68/100, Train Loss: 0.0539, Val Loss: 0.0365\n",
            "Epoch 69/100, Train Loss: 0.0543, Val Loss: 0.0365\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0555, Val Loss: 0.0365\n",
            "Epoch 71/100, Train Loss: 0.0556, Val Loss: 0.0365\n",
            "Epoch 72/100, Train Loss: 0.0532, Val Loss: 0.0365\n",
            "Epoch 73/100, Train Loss: 0.0531, Val Loss: 0.0365\n",
            "Epoch 74/100, Train Loss: 0.0550, Val Loss: 0.0365\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0543, Val Loss: 0.0365\n",
            "Epoch 76/100, Train Loss: 0.0556, Val Loss: 0.0365\n",
            "Epoch 77/100, Train Loss: 0.0541, Val Loss: 0.0365\n",
            "Epoch 78/100, Train Loss: 0.0540, Val Loss: 0.0365\n",
            "Epoch 79/100, Train Loss: 0.0537, Val Loss: 0.0365\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0544, Val Loss: 0.0365\n",
            "Epoch 81/100, Train Loss: 0.0547, Val Loss: 0.0365\n",
            "Epoch 82/100, Train Loss: 0.0520, Val Loss: 0.0365\n",
            "Epoch 83/100, Train Loss: 0.0541, Val Loss: 0.0365\n",
            "Epoch 84/100, Train Loss: 0.0539, Val Loss: 0.0365\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0522, Val Loss: 0.0365\n",
            "Epoch 86/100, Train Loss: 0.0539, Val Loss: 0.0365\n",
            "Epoch 87/100, Train Loss: 0.0552, Val Loss: 0.0365\n",
            "Epoch 88/100, Train Loss: 0.0547, Val Loss: 0.0365\n",
            "Epoch 89/100, Train Loss: 0.0559, Val Loss: 0.0365\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0548, Val Loss: 0.0365\n",
            "Epoch 91/100, Train Loss: 0.0558, Val Loss: 0.0365\n",
            "Epoch 92/100, Train Loss: 0.0528, Val Loss: 0.0365\n",
            "Epoch 93/100, Train Loss: 0.0539, Val Loss: 0.0365\n",
            "Epoch 94/100, Train Loss: 0.0516, Val Loss: 0.0365\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0559, Val Loss: 0.0365\n",
            "Epoch 96/100, Train Loss: 0.0529, Val Loss: 0.0365\n",
            "Epoch 97/100, Train Loss: 0.0545, Val Loss: 0.0365\n",
            "Epoch 98/100, Train Loss: 0.0531, Val Loss: 0.0365\n",
            "Epoch 99/100, Train Loss: 0.0547, Val Loss: 0.0365\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0551, Val Loss: 0.0365\n",
            "Results:\n",
            "            RMSE            MAE       MAPE model_type  units  drop_rate  \\\n",
            "0  246224.854561  131556.025670  14.706872        gru      8        0.1   \n",
            "1  251255.446773  133765.324777   4.758086        gru      8        0.1   \n",
            "2  242925.981965  134619.986607  22.945423        gru      8        0.2   \n",
            "3  251446.799289  137708.970982  19.582800        gru      8        0.2   \n",
            "4  243101.926192  129849.833705  23.511106        gru     16        0.1   \n",
            "5  244154.166413  125539.046875  18.763624        gru     16        0.1   \n",
            "6  247750.635084  129658.975446  21.587090        gru     16        0.2   \n",
            "7  248808.993662  121571.821429   9.699997        gru     16        0.2   \n",
            "\n",
            "   dense_unit  batch_size  epochs  \n",
            "0          32           4     100  \n",
            "1          64           4     100  \n",
            "2          32           4     100  \n",
            "3          64           4     100  \n",
            "4          32           4     100  \n",
            "5          64           4     100  \n",
            "6          32           4     100  \n",
            "7          64           4     100  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adjusted_valuelist.to_csv( '/content/drive/MyDrive/GRU_random.csv', index=False)\n",
        "pd.DataFrame(all_adjusted_predictions).to_csv(\"/content/drive/MyDrive/all_adjusted_predictions_GRU_random.csv\", index=False)\n",
        "pd.DataFrame(all_ground_truths).to_csv(\"/content/drive/MyDrive/all_ground_truths_GRU_random.csv\", index=False)"
      ],
      "metadata": {
        "id": "lC2ILwADeIfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adjusted_valuelist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "GUoVQVRieZje",
        "outputId": "8a39a888-971b-411a-b600-a87fd4e54b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            RMSE            MAE       MAPE model_type  units  drop_rate  \\\n",
              "0  246224.854561  131556.025670  14.706872        gru      8        0.1   \n",
              "1  251255.446773  133765.324777   4.758086        gru      8        0.1   \n",
              "2  242925.981965  134619.986607  22.945423        gru      8        0.2   \n",
              "3  251446.799289  137708.970982  19.582800        gru      8        0.2   \n",
              "4  243101.926192  129849.833705  23.511106        gru     16        0.1   \n",
              "5  244154.166413  125539.046875  18.763624        gru     16        0.1   \n",
              "6  247750.635084  129658.975446  21.587090        gru     16        0.2   \n",
              "7  248808.993662  121571.821429   9.699997        gru     16        0.2   \n",
              "\n",
              "   dense_unit  batch_size  epochs  \n",
              "0          32           4     100  \n",
              "1          64           4     100  \n",
              "2          32           4     100  \n",
              "3          64           4     100  \n",
              "4          32           4     100  \n",
              "5          64           4     100  \n",
              "6          32           4     100  \n",
              "7          64           4     100  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-63b6e5f0-d1e1-4c4e-a85d-7e0faa437f2b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAE</th>\n",
              "      <th>MAPE</th>\n",
              "      <th>model_type</th>\n",
              "      <th>units</th>\n",
              "      <th>drop_rate</th>\n",
              "      <th>dense_unit</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>epochs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>246224.854561</td>\n",
              "      <td>131556.025670</td>\n",
              "      <td>14.706872</td>\n",
              "      <td>gru</td>\n",
              "      <td>8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>251255.446773</td>\n",
              "      <td>133765.324777</td>\n",
              "      <td>4.758086</td>\n",
              "      <td>gru</td>\n",
              "      <td>8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>242925.981965</td>\n",
              "      <td>134619.986607</td>\n",
              "      <td>22.945423</td>\n",
              "      <td>gru</td>\n",
              "      <td>8</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>251446.799289</td>\n",
              "      <td>137708.970982</td>\n",
              "      <td>19.582800</td>\n",
              "      <td>gru</td>\n",
              "      <td>8</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>243101.926192</td>\n",
              "      <td>129849.833705</td>\n",
              "      <td>23.511106</td>\n",
              "      <td>gru</td>\n",
              "      <td>16</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>244154.166413</td>\n",
              "      <td>125539.046875</td>\n",
              "      <td>18.763624</td>\n",
              "      <td>gru</td>\n",
              "      <td>16</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>247750.635084</td>\n",
              "      <td>129658.975446</td>\n",
              "      <td>21.587090</td>\n",
              "      <td>gru</td>\n",
              "      <td>16</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>248808.993662</td>\n",
              "      <td>121571.821429</td>\n",
              "      <td>9.699997</td>\n",
              "      <td>gru</td>\n",
              "      <td>16</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-63b6e5f0-d1e1-4c4e-a85d-7e0faa437f2b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-63b6e5f0-d1e1-4c4e-a85d-7e0faa437f2b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-63b6e5f0-d1e1-4c4e-a85d-7e0faa437f2b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-03816b80-cb34-4657-89aa-0dea4f730940\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-03816b80-cb34-4657-89aa-0dea4f730940')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-03816b80-cb34-4657-89aa-0dea4f730940 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_136f32ae-a03f-4326-8f0d-845d3f7f41ee\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('adjusted_valuelist')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_136f32ae-a03f-4326-8f0d-845d3f7f41ee button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('adjusted_valuelist');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "adjusted_valuelist",
              "summary": "{\n  \"name\": \"adjusted_valuelist\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3426.7283966397417,\n        \"min\": 242925.98196453342,\n        \"max\": 251446.7992894524,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          251255.44677294063,\n          244154.16641264525,\n          246224.8545605374\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5157.040179332374,\n        \"min\": 121571.82142857143,\n        \"max\": 137708.97098214287,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          133765.3247767857,\n          125539.046875,\n          131556.02566964287\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          4.758086204528809,\n          18.76362419128418,\n          14.70687198638916\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"gru\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"units\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 8,\n        \"max\": 16,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          16\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"drop_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05345224838248488,\n        \"min\": 0.1,\n        \"max\": 0.2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dense_unit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17,\n        \"min\": 32,\n        \"max\": 64,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 4,\n        \"max\": 4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"epochs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 100,\n        \"max\": 100,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          100\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_adjusted_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTWXowVZeaAp",
        "outputId": "0cc69d8d-45b0-42a0-9098-0f865a2a89ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gru_unit8_drop0.1_dense32_batch4_epochs100': array([      0.  ,  647791.  ,  394569.38, 1314448.4 ,  550984.3 ,\n",
              "         410702.03,  572928.4 ,  666221.9 , 1368073.  ,  640821.56,\n",
              "        1207034.9 , 1442079.  ,  664889.9 ,  436922.8 ,  730164.3 ,\n",
              "         648040.  ,  414720.7 ,  414929.34, 1490724.2 , 1271937.4 ,\n",
              "         442434.12, 1268360.1 ,  623061.5 ,  412776.8 ,  627524.44,\n",
              "         422915.4 , 1447373.6 ,  703705.7 ], dtype=float32),\n",
              " 'gru_unit8_drop0.1_dense64_batch4_epochs100': array([      0.  ,  591606.6 ,  418470.2 , 1287424.6 ,  569016.5 ,\n",
              "         388727.2 ,  561687.8 ,  572062.1 , 1347292.2 ,  614513.56,\n",
              "        1226774.8 , 1416928.9 ,  690049.9 ,  448817.03,  751976.4 ,\n",
              "         605348.1 ,  436730.6 ,  414276.12, 1526023.6 , 1223477.5 ,\n",
              "         478004.94, 1287703.2 ,  678106.94,  407717.62,  608032.94,\n",
              "         432986.97, 1472487.  ,  705885.75], dtype=float32),\n",
              " 'gru_unit8_drop0.2_dense32_batch4_epochs100': array([      0.  ,  694317.06,  426255.9 , 1339773.  ,  563487.44,\n",
              "         409407.34,  555719.25,  637621.25, 1352710.2 ,  623392.4 ,\n",
              "        1181925.1 , 1451972.  ,  698000.44,  409203.78,  755555.6 ,\n",
              "         661139.25,  436336.84,  397935.53, 1455627.1 , 1242214.2 ,\n",
              "         471087.06, 1289458.4 ,  646587.5 ,  410965.12,  638350.2 ,\n",
              "         447201.34, 1425426.4 ,  692342.  ], dtype=float32),\n",
              " 'gru_unit8_drop0.2_dense64_batch4_epochs100': array([      0.  ,  675327.1 ,  395031.66, 1302683.8 ,  556391.75,\n",
              "         410090.1 ,  600205.4 ,  711565.5 , 1348424.9 ,  640945.06,\n",
              "        1292549.  , 1448039.4 ,  683039.9 ,  450794.75,  761446.5 ,\n",
              "         652956.56,  453634.62,  448628.5 , 1508832.6 , 1255121.6 ,\n",
              "         481596.1 , 1278632.6 ,  698197.4 ,  409886.2 ,  620348.4 ,\n",
              "         444001.66, 1438360.  ,  714855.  ], dtype=float32),\n",
              " 'gru_unit16_drop0.1_dense32_batch4_epochs100': array([      0.  ,  697511.7 ,  436906.72, 1325525.2 ,  553796.1 ,\n",
              "         430852.62,  582555.1 ,  666165.25, 1364969.8 ,  625039.  ,\n",
              "        1271912.8 , 1453364.  ,  640449.8 ,  461850.97,  761394.4 ,\n",
              "         600715.6 ,  411134.97,  434472.06, 1512592.5 , 1223182.8 ,\n",
              "         472104.22, 1284939.  ,  614302.94,  427935.72,  641954.5 ,\n",
              "         434958.06, 1537022.2 ,  685012.44], dtype=float32),\n",
              " 'gru_unit16_drop0.1_dense64_batch4_epochs100': array([      0.  ,  670700.94,  390028.66, 1300339.5 ,  585371.56,\n",
              "         408291.28,  580188.7 ,  639089.25, 1360037.  ,  627477.8 ,\n",
              "        1246619.6 , 1500575.4 ,  659801.7 ,  433776.66,  790513.6 ,\n",
              "         642255.06,  430687.2 ,  443659.78, 1538331.6 , 1172847.2 ,\n",
              "         515446.66, 1267407.6 ,  625842.1 ,  433328.66,  639566.7 ,\n",
              "         457261.88, 1560673.2 ,  726707.1 ], dtype=float32),\n",
              " 'gru_unit16_drop0.2_dense32_batch4_epochs100': array([      0.  ,  686646.06,  396367.3 , 1338323.9 ,  580245.2 ,\n",
              "         425701.38,  588836.56,  648492.6 , 1353148.6 ,  630189.6 ,\n",
              "        1210776.4 , 1455291.  ,  608430.06,  436835.28,  757938.44,\n",
              "         641092.75,  447090.  ,  429576.66, 1521937.  , 1158905.  ,\n",
              "         519434.38, 1265332.2 ,  640318.44,  417757.4 ,  644183.9 ,\n",
              "         465511.6 , 1504637.4 ,  735697.94], dtype=float32),\n",
              " 'gru_unit16_drop0.2_dense64_batch4_epochs100': array([      0.  ,  619515.4 ,  442655.  , 1320602.4 ,  546834.5 ,\n",
              "         415703.66,  602483.  ,  633140.1 , 1386543.4 ,  629150.7 ,\n",
              "        1234056.4 , 1482534.8 ,  648674.75,  426837.06,  780306.75,\n",
              "         646135.4 ,  396086.16,  438601.6 , 1562310.4 , 1153695.4 ,\n",
              "         538150.94, 1275109.4 ,  626660.9 ,  433350.78,  638472.44,\n",
              "         476524.2 , 1533594.9 ,  715354.75], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_ground_truths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLyFlrb6ebjz",
        "outputId": "a2cab63a-c14f-4047-d584-7b461ffe0709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gru_unit8_drop0.1_dense32_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'gru_unit8_drop0.1_dense64_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'gru_unit8_drop0.2_dense32_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'gru_unit8_drop0.2_dense64_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'gru_unit16_drop0.1_dense32_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'gru_unit16_drop0.1_dense64_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'gru_unit16_drop0.2_dense32_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'gru_unit16_drop0.2_dense64_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config_key = 'gru_unit8_drop0.1_dense64_batch4_epochs100'\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(all_ground_truths[config_key], label='Ground Truth')\n",
        "plt.plot(all_adjusted_predictions[config_key], label='GRU Predictions')\n",
        "plt.title(f'GRU Performance: {config_key}')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "nGaBLUZQeqPe",
        "outputId": "6a3a2559-5ae9-4ebd-8bfd-1e5dbdfe4500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LSTM"
      ],
      "metadata": {
        "id": "1XfGFdWM4nw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Giảm số lượng/GRU unit, dense unit, epochs và sử dụng batch size nhỏ hơn để huấn luyện nhanh hơn.\n",
        "# model_types = ['multi-scale']\n",
        "# lstm_unit = [128, 256, 512]\n",
        "# gru_unit = [8, 16, 32]\n",
        "# drop_rate = [0.1, 0.2]\n",
        "# dense_unit = [16, 32, 64]\n",
        "# batch_size_num = [2, 4]\n",
        "# epochs = [100]\n",
        "\n",
        "model_types = ['lstm']\n",
        "lstm_unit = [256,512]\n",
        "gru_unit = [8,16]\n",
        "drop_rate = [0.1,0.2]\n",
        "dense_unit = [32,64]\n",
        "batch_size_num = [4]\n",
        "epochs = [100]\n",
        "\n",
        "# # Replace the current parameter definitions\n",
        "# model_types = ['hybrid', 'sequential', 'stacked', 'bidirectional', 'cnn-rnn', 'multi-scale', 'transformer-rnn', 'ensemble', 'lstm', 'gru']\n",
        "# lstm_unit = [128]\n",
        "# gru_unit = [8]\n",
        "# drop_rate = [0.1]\n",
        "# dense_unit = [64]\n",
        "# batch_size_num = [2]\n",
        "# epochs = [100]\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import concatenate\n",
        "import itertools\n",
        "from math import sqrt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "class AttentionGRU(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, gru_units, dropout_rate, dense_units):\n",
        "        super(AttentionGRU, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention1 = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # GRU layer\n",
        "        self.gru = nn.GRU(input_dim, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear(gru_units * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, time_steps, input_dim)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.attention1(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)  # (batch_size, time_steps, input_dim)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # GRU layer - output: (batch, seq, hidden_size)\n",
        "        gru_out, _ = self.gru(attention_mul)\n",
        "\n",
        "        # Flatten and Dense layers\n",
        "        flattened = self.flatten(gru_out)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class AttentionLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, dropout_rate, dense_units):\n",
        "        super(AttentionLSTM, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention1 = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear(lstm_units * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, time_steps, input_dim)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.attention1(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)  # (batch_size, time_steps, input_dim)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # LSTM layer - output: (batch, seq, hidden_size)\n",
        "        lstm_out, _ = self.lstm(attention_mul)\n",
        "\n",
        "        # Flatten and Dense layers\n",
        "        flattened = self.flatten(lstm_out)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class HybridLSTM_GRU(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(HybridLSTM_GRU, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention1 = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # LSTM and GRU layers\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "        self.gru = nn.GRU(input_dim, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        # Combined size from both LSTM and GRU\n",
        "        self.dense1 = nn.Linear((lstm_units + gru_units) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, time_steps, input_dim)\n",
        "\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.attention1(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)  # (batch_size, time_steps, input_dim)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # LSTM and GRU layers\n",
        "        lstm_out, _ = self.lstm(attention_mul)\n",
        "        gru_out, _ = self.gru(attention_mul)\n",
        "\n",
        "        # Concatenate LSTM and GRU outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Flatten and Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class SequentialHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(SequentialHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # LSTM followed by GRU\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "        self.gru = nn.GRU(lstm_units, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear(gru_units * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)\n",
        "        a = self.attention(a)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # Sequential processing: LSTM then GRU\n",
        "        lstm_out, _ = self.lstm(attention_mul)\n",
        "        gru_out, _ = self.gru(lstm_out)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(gru_out)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class StackedHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(StackedHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention\n",
        "        self.attention = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # Two stacked LSTM layers\n",
        "        self.lstm1 = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(lstm_units, lstm_units//2, batch_first=True)\n",
        "\n",
        "        # Two stacked GRU layers\n",
        "        self.gru1 = nn.GRU(input_dim, gru_units, batch_first=True)\n",
        "        self.gru2 = nn.GRU(gru_units, gru_units//2, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units//2 + gru_units//2) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)\n",
        "        a = self.attention(a)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # Stacked LSTM path\n",
        "        lstm_out1, _ = self.lstm1(attention_mul)\n",
        "        lstm_out2, _ = self.lstm2(lstm_out1)\n",
        "\n",
        "        # Stacked GRU path\n",
        "        gru_out1, _ = self.gru1(attention_mul)\n",
        "        gru_out2, _ = self.gru2(gru_out1)\n",
        "\n",
        "        # Concatenate final outputs\n",
        "        combined = torch.cat((lstm_out2, gru_out2), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class BidirectionalHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(BidirectionalHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # Bidirectional LSTM and GRU\n",
        "        self.bilstm = nn.LSTM(input_dim, lstm_units, batch_first=True, bidirectional=True)\n",
        "        self.bigru = nn.GRU(input_dim, gru_units, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units*2 + gru_units*2) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)\n",
        "        a = self.attention(a)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # Bidirectional networks\n",
        "        lstm_out, _ = self.bilstm(attention_mul)\n",
        "        gru_out, _ = self.bigru(attention_mul)\n",
        "\n",
        "        # Concatenate outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class CNNRNNHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(CNNRNNHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # 1D CNN for feature extraction\n",
        "        self.conv1 = nn.Conv1d(input_dim, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # RNN layers\n",
        "        self.lstm = nn.LSTM(64, lstm_units, batch_first=True)\n",
        "        self.gru = nn.GRU(64, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units + gru_units) * (time_steps-1), dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # CNN feature extraction\n",
        "        x = x.permute(0, 2, 1)  # (batch, features, seq_len)\n",
        "        cnn_out = self.relu(self.conv1(x))\n",
        "        cnn_out = self.maxpool(cnn_out)\n",
        "        cnn_out = self.relu(self.conv2(cnn_out))\n",
        "        cnn_out = cnn_out.permute(0, 2, 1)  # (batch, seq_len, features)\n",
        "\n",
        "        # RNN processing\n",
        "        lstm_out, _ = self.lstm(cnn_out)\n",
        "        gru_out, _ = self.gru(cnn_out)\n",
        "\n",
        "        # Combine outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class MultiScaleHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(MultiScaleHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # LSTM for long-term dependencies\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "\n",
        "        # GRU for shorter-term dependencies (operating on windows)\n",
        "        self.gru = nn.GRU(input_dim, gru_units, batch_first=True)\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units + gru_units) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Full sequence for LSTM (long-term)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Attention mechanism for GRU input\n",
        "        a = x.permute(0, 2, 1)\n",
        "        a = self.attention(a)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)\n",
        "\n",
        "        # Apply attention weights for GRU (short-term focus)\n",
        "        gru_input = torch.mul(x, a)\n",
        "        gru_out, _ = self.gru(gru_input)\n",
        "\n",
        "        # Combine outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class TransformerRNNHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units, nhead=4):\n",
        "        super(TransformerRNNHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Input projection for transformer\n",
        "        self.input_proj = nn.Linear(input_dim, 64)\n",
        "\n",
        "        # Transformer encoder layer\n",
        "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=64, nhead=nhead, dropout=dropout_rate, batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layer, num_layers=2)\n",
        "\n",
        "        # RNN layers\n",
        "        self.lstm = nn.LSTM(64, lstm_units, batch_first=True)\n",
        "        self.gru = nn.GRU(64, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units + gru_units) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Project input to transformer dimension\n",
        "        x_proj = self.input_proj(x)\n",
        "\n",
        "        # Apply transformer encoder\n",
        "        transformer_out = self.transformer_encoder(x_proj)\n",
        "\n",
        "        # Process with RNNs\n",
        "        lstm_out, _ = self.lstm(transformer_out)\n",
        "        gru_out, _ = self.gru(transformer_out)\n",
        "\n",
        "        # Combine outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class EnsembleHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(EnsembleHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Individual models\n",
        "        self.lstm_model = AttentionLSTM(input_dim, time_steps, lstm_units, dropout_rate, dense_units)\n",
        "        self.gru_model = AttentionGRU(input_dim, time_steps, gru_units, dropout_rate, dense_units)\n",
        "\n",
        "        # Combination layer\n",
        "        self.combine = nn.Linear(2, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get predictions from each model\n",
        "        lstm_pred = self.lstm_model(x)\n",
        "        gru_pred = self.gru_model(x)\n",
        "\n",
        "        # Combine predictions (learnable weights)\n",
        "        combined = torch.cat((lstm_pred, gru_pred), dim=1)\n",
        "        output = self.final_activation(self.combine(combined))\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "def build_model(train_X, train_Y, val_X, val_Y, model_type='gru', lstm_units=128, gru_units=128, drop_rate=0.3, dense_unit=64, batch_size=32, epochs=100):\n",
        "    # Print training parameters\n",
        "    train_X_tensor = torch.FloatTensor(train_X)\n",
        "    train_Y_tensor = torch.FloatTensor(train_Y.reshape(-1, 1))\n",
        "    val_X_tensor = torch.FloatTensor(val_X)\n",
        "    val_Y_tensor = torch.FloatTensor(val_Y.reshape(-1, 1))\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
        "    val_dataset = TensorDataset(val_X_tensor, val_Y_tensor)\n",
        "\n",
        "    # Create reproducible DataLoaders with fixed seeds\n",
        "    train_generator = torch.Generator()\n",
        "    train_generator.manual_seed(SEED)\n",
        "    val_generator = torch.Generator()\n",
        "    val_generator.manual_seed(SEED)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=train_generator)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, generator=val_generator)\n",
        "\n",
        "    # Initialize model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    time_steps = train_X.shape[1]\n",
        "    input_dim = train_X.shape[2]\n",
        "\n",
        "    # Initialize model with fixed initial weights\n",
        "    torch.manual_seed(SEED)\n",
        "\n",
        "    if model_type == 'gru':\n",
        "        model = AttentionGRU(input_dim, time_steps, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'lstm':\n",
        "        model = AttentionLSTM(input_dim, time_steps, lstm_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'hybrid':\n",
        "        model = HybridLSTM_GRU(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'sequential':\n",
        "        model = SequentialHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'stacked':\n",
        "        model = StackedHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'bidirectional':\n",
        "        model = BidirectionalHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'cnn-rnn':\n",
        "        model = CNNRNNHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'multi-scale':\n",
        "        model = MultiScaleHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'transformer-rnn':\n",
        "        model = TransformerRNNHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'ensemble':\n",
        "        model = EnsembleHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "    # Initialize optimizer and loss function\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    criterion = nn.L1Loss()  # MAE loss\n",
        "\n",
        "    # Training loop\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    patience = 5\n",
        "    lr_factor = 0.01\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        # Print progress\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
        "\n",
        "        # Learning rate schedule based on validation loss\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] *= lr_factor\n",
        "                patience_counter = 0\n",
        "                print(f'Reducing learning rate by factor of {lr_factor}')\n",
        "\n",
        "    return model\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    mask = y_true != 0\n",
        "    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "    return mape\n",
        "\n",
        "def walk_forward(train_X, train_Y, val_X, val_Y, test_X, test_Y, grid_search, scaler):\n",
        "    r, f, c = test_X.shape\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    all_predictions = {}\n",
        "    all_adjusted_predictions = {}\n",
        "    all_ground_truths = {}\n",
        "\n",
        "    # Create lists to store all evaluation results\n",
        "    original_valuelists = []\n",
        "    adjusted_valuelists = []\n",
        "\n",
        "    for x in grid_search:\n",
        "        history_x = np.array([x for x in train_X])\n",
        "        history_y = np.array([y for y in train_Y])\n",
        "        predictions = list()\n",
        "        adjusted_predictions = list()\n",
        "        groundtrue = list()\n",
        "\n",
        "        # Extract model type first to determine how to unpack the rest\n",
        "        model_type = x[0]\n",
        "\n",
        "        # Create the appropriate config_key and extract parameters based on model type\n",
        "        if model_type in ['hybrid', 'sequential', 'stacked', 'bidirectional', 'cnn-rnn', 'multi-scale', 'transformer-rnn', 'ensemble']:\n",
        "            # Hybrid model has 7 parameters\n",
        "            model_type, lstm_unit_val, gru_unit_val, drop, dense, batch, epoch = x\n",
        "            units = f\"L{lstm_unit_val}_G{gru_unit_val}\"  # For logging\n",
        "            config_key = f\"{model_type}_lstmUnit{lstm_unit_val}_gruUnit{gru_unit_val}_drop{drop}_dense{dense}_batch{batch}_epochs{epoch}\"\n",
        "        else:\n",
        "            # LSTM and GRU models have 6 parameters\n",
        "            model_type, units, drop, dense, batch, epoch = x\n",
        "            config_key = f\"{model_type}_unit{units}_drop{drop}_dense{dense}_batch{batch}_epochs{epoch}\"\n",
        "\n",
        "        print(\"\\n\" + \"*\"*50)\n",
        "        print(f\"Starting walk-forward validation with parameters:\")\n",
        "        print(f\"Model Type: {model_type}, Units: {units}, Dropout: {drop}, Dense Units: {dense}\")\n",
        "        print(f\"Batch Size: {batch}, Epochs: {epoch}\")\n",
        "        print(f\"Device: {device}\")\n",
        "        print(f\"Total test samples: {len(test_X)}\")\n",
        "        print(\"*\"*50 + \"\\n\")\n",
        "\n",
        "        for i in range(len(test_X)):\n",
        "            print(f\"\\nTest iteration {i+1}/{len(test_X)}\")\n",
        "            print(f\"Current training set size: {history_x.shape[0]} samples\")\n",
        "\n",
        "            if model_type in ['hybrid', 'sequential', 'stacked', 'bidirectional', 'cnn-rnn', 'multi-scale', 'transformer-rnn', 'ensemble']:\n",
        "                model = build_model(history_x, history_y, val_X, val_Y, model_type=model_type,\n",
        "                                lstm_units=lstm_unit_val, gru_units=gru_unit_val, drop_rate=drop,\n",
        "                                dense_unit=dense, batch_size=batch, epochs=epoch)\n",
        "            else:\n",
        "                model = build_model(history_x, history_y, val_X, val_Y, model_type=model_type,\n",
        "                                lstm_units=units if model_type == 'lstm' else 128,\n",
        "                                gru_units=units if model_type == 'gru' else 128,\n",
        "                                drop_rate=drop, dense_unit=dense, batch_size=batch, epochs=epoch)\n",
        "\n",
        "            # Rest of the function remains the same\n",
        "            model.eval()\n",
        "\n",
        "            # Convert test data to tensor\n",
        "            test_tensor = torch.FloatTensor(test_X[i].reshape(1, f, c)).to(device)\n",
        "\n",
        "            # Predict\n",
        "            with torch.no_grad():\n",
        "                yhat = model(test_tensor).cpu().numpy()\n",
        "\n",
        "            inv_yhat, inv_y = inverscale(yhat, test_X[i], test_Y[i], scaler)\n",
        "            prev_month_lockdown = test_X[i][11][5]\n",
        "            adjusted_inv_yhat = inv_yhat * (1 - prev_month_lockdown)\n",
        "            predictions.append(inv_yhat)\n",
        "            adjusted_predictions.append(adjusted_inv_yhat)\n",
        "            groundtrue.append(inv_y)\n",
        "\n",
        "            # Observation\n",
        "            obs_x = test_X[i]\n",
        "            obs_y = test_Y[i]\n",
        "\n",
        "            history_x = np.append(history_x, [obs_x], axis=0)\n",
        "            history_y = np.append(history_y, obs_y)\n",
        "\n",
        "        # Store predictions and ground truth for this configuration\n",
        "        all_predictions[config_key] = np.array(predictions).flatten()\n",
        "        all_adjusted_predictions[config_key] = np.array(adjusted_predictions).flatten()\n",
        "        all_ground_truths[config_key] = np.array(groundtrue).flatten()\n",
        "\n",
        "        original_valuelist = evalue(predictions, groundtrue)\n",
        "        original_valuelist['model_type'] = model_type\n",
        "        original_valuelist['units'] = units\n",
        "        original_valuelist['drop_rate'] = drop\n",
        "        original_valuelist['dense_unit'] = dense\n",
        "        original_valuelist['batch_size'] = batch\n",
        "        original_valuelist['epochs'] = epoch\n",
        "\n",
        "        # Evaluate with adjusted predictions\n",
        "        adjusted_valuelist = evalue(adjusted_predictions, groundtrue)\n",
        "        adjusted_valuelist['model_type'] = model_type\n",
        "        adjusted_valuelist['units'] = units\n",
        "        adjusted_valuelist['drop_rate'] = drop\n",
        "        adjusted_valuelist['dense_unit'] = dense\n",
        "        adjusted_valuelist['batch_size'] = batch\n",
        "        adjusted_valuelist['epochs'] = epoch\n",
        "\n",
        "        # Append to the lists of results\n",
        "        original_valuelists.append(original_valuelist)\n",
        "        adjusted_valuelists.append(adjusted_valuelist)\n",
        "\n",
        "    # Combine all results\n",
        "    all_original_valuelist = pd.concat(original_valuelists, ignore_index=True)\n",
        "    all_adjusted_valuelist = pd.concat(adjusted_valuelists, ignore_index=True)\n",
        "\n",
        "    return all_original_valuelist, all_adjusted_valuelist, all_predictions, all_ground_truths, all_adjusted_predictions\n",
        "\n",
        "def evalue(yhat, inv_y):\n",
        "    valuelist = {}\n",
        "    DLM_rmse = sqrt(mean_squared_error(inv_y, yhat))\n",
        "    valuelist.update({'RMSE': {'DLM': DLM_rmse}})\n",
        "    DLM_mae = mean_absolute_error(inv_y, yhat)\n",
        "    valuelist.update({'MAE': {'DLM': DLM_mae}})\n",
        "    DLM_mape = mean_absolute_percentage_error(inv_y, yhat)\n",
        "    valuelist.update({'MAPE': {'DLM': DLM_mape}})\n",
        "    return pd.DataFrame(valuelist)\n",
        "\n",
        "def inverscale(yhat, test_X, test_Y, scaler):\n",
        "    feature = len(scaler.scale_)\n",
        "    test_Y = np.array(test_Y)\n",
        "    test_X = test_X[1, 0:feature]\n",
        "    test_X = test_X.reshape(1, test_X.shape[0])\n",
        "\n",
        "    if len(yhat.shape) == 1:\n",
        "        yhat = yhat.reshape(len(yhat), 1)\n",
        "\n",
        "    inv_yhat = concatenate((yhat, test_X[:, :-1]), axis=1)\n",
        "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "    inv_yhat = inv_yhat[:, 0]\n",
        "\n",
        "    test_Y = test_Y.reshape(1, 1)\n",
        "    inv_y = concatenate((test_Y, test_X[:, :-1]), axis=1)\n",
        "    inv_y = scaler.inverse_transform(inv_y)\n",
        "    inv_y = inv_y[:, 0]\n",
        "    return inv_yhat, inv_y\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    values = reframed.values\n",
        "    reframed_with_dates_values = reframed_with_dates.values\n",
        "\n",
        "    # Import train_test_split for random splitting\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    # Drop the date column for the splitting but keep track of indices\n",
        "    reframed_without_dates = reframed.copy()\n",
        "\n",
        "    # First split: 80% train+val, 20% test\n",
        "    train_val_indices, test_indices = train_test_split(\n",
        "        np.arange(len(reframed_without_dates)),\n",
        "        test_size=0.2,\n",
        "        random_state=SEED\n",
        "    )\n",
        "\n",
        "    # Second split: From the 80%, use 7/8 for train (70% of total) and 1/8 for val (10% of total)\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        train_val_indices,\n",
        "        test_size=0.125,  # 0.125 * 0.8 = 0.1 (10% of total)\n",
        "        random_state=SEED\n",
        "    )\n",
        "\n",
        "    # Extract values for train, validation, and test sets\n",
        "    train_data = reframed.iloc[train_indices].values\n",
        "    val_data = reframed.iloc[val_indices].values\n",
        "    test_data = reframed.iloc[test_indices].values\n",
        "\n",
        "    # Store the corresponding dates for reference\n",
        "    train_dates = reframed_with_dates.iloc[train_indices]['date']\n",
        "    val_dates = reframed_with_dates.iloc[val_indices]['date']\n",
        "    test_dates = reframed_with_dates.iloc[test_indices]['date']\n",
        "\n",
        "    # Split into X and Y\n",
        "    train_X, train_Y = train_data[:, :-1], train_data[:, -1]\n",
        "    val_X, val_Y = val_data[:, :-1], val_data[:, -1]\n",
        "    test_X, test_Y = test_data[:, :-1], test_data[:, -1]\n",
        "\n",
        "    # Reshape input to be 3D [samples, timesteps, features]\n",
        "    train_X = train_X.reshape(train_X.shape[0], 12, int(train_X.shape[1]/12))\n",
        "    val_X = val_X.reshape(val_X.shape[0], 12, int(val_X.shape[1]/12))\n",
        "    test_X = test_X.reshape(test_X.shape[0], 12, int(test_X.shape[1]/12))\n",
        "\n",
        "    # Modified grid search creation for all model types\n",
        "    grid_search = []\n",
        "    for model_type in model_types:\n",
        "        if model_type == 'lstm':\n",
        "            grid_search.extend(\n",
        "                list(itertools.product([model_type], lstm_unit, drop_rate, dense_unit, batch_size_num, epochs))\n",
        "            )\n",
        "        elif model_type == 'gru':\n",
        "            grid_search.extend(\n",
        "                list(itertools.product([model_type], gru_unit, drop_rate, dense_unit, batch_size_num, epochs))\n",
        "            )\n",
        "        else:\n",
        "            # All other models (hybrid, sequential, stacked, etc.) need both LSTM and GRU units\n",
        "            grid_search.extend(\n",
        "                list(itertools.product([model_type], lstm_unit, gru_unit, drop_rate, dense_unit, batch_size_num, epochs))\n",
        "            )\n",
        "\n",
        "    original_valuelist, adjusted_valuelist, all_predictions, all_ground_truths, all_adjusted_predictions = walk_forward(train_X, train_Y, val_X, val_Y, test_X, test_Y, grid_search, scaler)\n",
        "\n",
        "    # Group results by model type\n",
        "    # gru_results = adjusted_valuelist[adjusted_valuelist['model_type'] == 'gru']\n",
        "    # lstm_results = adjusted_valuelist[adjusted_valuelist['model_type'] == 'lstm']\n",
        "\n",
        "    print(\"Results:\")\n",
        "    print(adjusted_valuelist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu9UVQHg4q9M",
        "outputId": "838e44c0-18dd-484a-d42f-4bc024002455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 73/100, Train Loss: 0.0364, Val Loss: 0.0440\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0363, Val Loss: 0.0440\n",
            "Epoch 75/100, Train Loss: 0.0367, Val Loss: 0.0440\n",
            "Epoch 76/100, Train Loss: 0.0370, Val Loss: 0.0440\n",
            "Epoch 77/100, Train Loss: 0.0365, Val Loss: 0.0440\n",
            "Epoch 78/100, Train Loss: 0.0362, Val Loss: 0.0440\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0361, Val Loss: 0.0440\n",
            "Epoch 80/100, Train Loss: 0.0357, Val Loss: 0.0440\n",
            "Epoch 81/100, Train Loss: 0.0371, Val Loss: 0.0440\n",
            "Epoch 82/100, Train Loss: 0.0379, Val Loss: 0.0440\n",
            "Epoch 83/100, Train Loss: 0.0361, Val Loss: 0.0440\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0370, Val Loss: 0.0440\n",
            "Epoch 85/100, Train Loss: 0.0363, Val Loss: 0.0440\n",
            "Epoch 86/100, Train Loss: 0.0364, Val Loss: 0.0440\n",
            "Epoch 87/100, Train Loss: 0.0370, Val Loss: 0.0440\n",
            "Epoch 88/100, Train Loss: 0.0365, Val Loss: 0.0440\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0365, Val Loss: 0.0440\n",
            "Epoch 90/100, Train Loss: 0.0374, Val Loss: 0.0440\n",
            "Epoch 91/100, Train Loss: 0.0371, Val Loss: 0.0440\n",
            "Epoch 92/100, Train Loss: 0.0370, Val Loss: 0.0440\n",
            "Epoch 93/100, Train Loss: 0.0365, Val Loss: 0.0440\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0369, Val Loss: 0.0440\n",
            "Epoch 95/100, Train Loss: 0.0357, Val Loss: 0.0440\n",
            "Epoch 96/100, Train Loss: 0.0367, Val Loss: 0.0440\n",
            "Epoch 97/100, Train Loss: 0.0383, Val Loss: 0.0440\n",
            "Epoch 98/100, Train Loss: 0.0366, Val Loss: 0.0440\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0371, Val Loss: 0.0440\n",
            "Epoch 100/100, Train Loss: 0.0356, Val Loss: 0.0440\n",
            "\n",
            "Test iteration 16/28\n",
            "Current training set size: 112 samples\n",
            "Epoch 1/100, Train Loss: 0.1561, Val Loss: 0.1867\n",
            "Epoch 2/100, Train Loss: 0.1416, Val Loss: 0.1966\n",
            "Epoch 3/100, Train Loss: 0.1390, Val Loss: 0.1281\n",
            "Epoch 4/100, Train Loss: 0.1210, Val Loss: 0.0543\n",
            "Epoch 5/100, Train Loss: 0.0763, Val Loss: 0.0641\n",
            "Epoch 6/100, Train Loss: 0.0819, Val Loss: 0.0397\n",
            "Epoch 7/100, Train Loss: 0.0727, Val Loss: 0.0337\n",
            "Epoch 8/100, Train Loss: 0.0680, Val Loss: 0.0595\n",
            "Epoch 9/100, Train Loss: 0.0609, Val Loss: 0.0426\n",
            "Epoch 10/100, Train Loss: 0.0518, Val Loss: 0.0358\n",
            "Epoch 11/100, Train Loss: 0.0559, Val Loss: 0.0476\n",
            "Epoch 12/100, Train Loss: 0.0488, Val Loss: 0.0512\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 13/100, Train Loss: 0.0484, Val Loss: 0.0467\n",
            "Epoch 14/100, Train Loss: 0.0435, Val Loss: 0.0455\n",
            "Epoch 15/100, Train Loss: 0.0402, Val Loss: 0.0436\n",
            "Epoch 16/100, Train Loss: 0.0389, Val Loss: 0.0416\n",
            "Epoch 17/100, Train Loss: 0.0384, Val Loss: 0.0408\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0383, Val Loss: 0.0407\n",
            "Epoch 19/100, Train Loss: 0.0386, Val Loss: 0.0407\n",
            "Epoch 20/100, Train Loss: 0.0379, Val Loss: 0.0407\n",
            "Epoch 21/100, Train Loss: 0.0387, Val Loss: 0.0407\n",
            "Epoch 22/100, Train Loss: 0.0385, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0390, Val Loss: 0.0407\n",
            "Epoch 24/100, Train Loss: 0.0387, Val Loss: 0.0407\n",
            "Epoch 25/100, Train Loss: 0.0384, Val Loss: 0.0407\n",
            "Epoch 26/100, Train Loss: 0.0380, Val Loss: 0.0407\n",
            "Epoch 27/100, Train Loss: 0.0383, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0393, Val Loss: 0.0407\n",
            "Epoch 29/100, Train Loss: 0.0388, Val Loss: 0.0407\n",
            "Epoch 30/100, Train Loss: 0.0386, Val Loss: 0.0407\n",
            "Epoch 31/100, Train Loss: 0.0383, Val Loss: 0.0407\n",
            "Epoch 32/100, Train Loss: 0.0383, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0388, Val Loss: 0.0407\n",
            "Epoch 34/100, Train Loss: 0.0389, Val Loss: 0.0407\n",
            "Epoch 35/100, Train Loss: 0.0378, Val Loss: 0.0407\n",
            "Epoch 36/100, Train Loss: 0.0388, Val Loss: 0.0407\n",
            "Epoch 37/100, Train Loss: 0.0386, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0387, Val Loss: 0.0407\n",
            "Epoch 39/100, Train Loss: 0.0379, Val Loss: 0.0407\n",
            "Epoch 40/100, Train Loss: 0.0388, Val Loss: 0.0407\n",
            "Epoch 41/100, Train Loss: 0.0391, Val Loss: 0.0407\n",
            "Epoch 42/100, Train Loss: 0.0380, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0385, Val Loss: 0.0407\n",
            "Epoch 44/100, Train Loss: 0.0386, Val Loss: 0.0407\n",
            "Epoch 45/100, Train Loss: 0.0380, Val Loss: 0.0407\n",
            "Epoch 46/100, Train Loss: 0.0386, Val Loss: 0.0407\n",
            "Epoch 47/100, Train Loss: 0.0381, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0388, Val Loss: 0.0407\n",
            "Epoch 49/100, Train Loss: 0.0386, Val Loss: 0.0407\n",
            "Epoch 50/100, Train Loss: 0.0381, Val Loss: 0.0407\n",
            "Epoch 51/100, Train Loss: 0.0389, Val Loss: 0.0407\n",
            "Epoch 52/100, Train Loss: 0.0383, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0386, Val Loss: 0.0407\n",
            "Epoch 54/100, Train Loss: 0.0393, Val Loss: 0.0407\n",
            "Epoch 55/100, Train Loss: 0.0385, Val Loss: 0.0407\n",
            "Epoch 56/100, Train Loss: 0.0389, Val Loss: 0.0407\n",
            "Epoch 57/100, Train Loss: 0.0385, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0389, Val Loss: 0.0407\n",
            "Epoch 59/100, Train Loss: 0.0385, Val Loss: 0.0407\n",
            "Epoch 60/100, Train Loss: 0.0386, Val Loss: 0.0407\n",
            "Epoch 61/100, Train Loss: 0.0385, Val Loss: 0.0407\n",
            "Epoch 62/100, Train Loss: 0.0387, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0390, Val Loss: 0.0407\n",
            "Epoch 64/100, Train Loss: 0.0390, Val Loss: 0.0407\n",
            "Epoch 65/100, Train Loss: 0.0396, Val Loss: 0.0407\n",
            "Epoch 66/100, Train Loss: 0.0391, Val Loss: 0.0407\n",
            "Epoch 67/100, Train Loss: 0.0388, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0381, Val Loss: 0.0407\n",
            "Epoch 69/100, Train Loss: 0.0387, Val Loss: 0.0407\n",
            "Epoch 70/100, Train Loss: 0.0388, Val Loss: 0.0407\n",
            "Epoch 71/100, Train Loss: 0.0387, Val Loss: 0.0407\n",
            "Epoch 72/100, Train Loss: 0.0387, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0382, Val Loss: 0.0407\n",
            "Epoch 74/100, Train Loss: 0.0387, Val Loss: 0.0407\n",
            "Epoch 75/100, Train Loss: 0.0389, Val Loss: 0.0407\n",
            "Epoch 76/100, Train Loss: 0.0381, Val Loss: 0.0407\n",
            "Epoch 77/100, Train Loss: 0.0381, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0384, Val Loss: 0.0407\n",
            "Epoch 79/100, Train Loss: 0.0387, Val Loss: 0.0407\n",
            "Epoch 80/100, Train Loss: 0.0385, Val Loss: 0.0407\n",
            "Epoch 81/100, Train Loss: 0.0382, Val Loss: 0.0407\n",
            "Epoch 82/100, Train Loss: 0.0387, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0387, Val Loss: 0.0407\n",
            "Epoch 84/100, Train Loss: 0.0390, Val Loss: 0.0407\n",
            "Epoch 85/100, Train Loss: 0.0388, Val Loss: 0.0407\n",
            "Epoch 86/100, Train Loss: 0.0387, Val Loss: 0.0407\n",
            "Epoch 87/100, Train Loss: 0.0389, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0381, Val Loss: 0.0407\n",
            "Epoch 89/100, Train Loss: 0.0384, Val Loss: 0.0407\n",
            "Epoch 90/100, Train Loss: 0.0387, Val Loss: 0.0407\n",
            "Epoch 91/100, Train Loss: 0.0383, Val Loss: 0.0407\n",
            "Epoch 92/100, Train Loss: 0.0383, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0384, Val Loss: 0.0407\n",
            "Epoch 94/100, Train Loss: 0.0383, Val Loss: 0.0407\n",
            "Epoch 95/100, Train Loss: 0.0390, Val Loss: 0.0407\n",
            "Epoch 96/100, Train Loss: 0.0377, Val Loss: 0.0407\n",
            "Epoch 97/100, Train Loss: 0.0391, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0384, Val Loss: 0.0407\n",
            "Epoch 99/100, Train Loss: 0.0388, Val Loss: 0.0407\n",
            "Epoch 100/100, Train Loss: 0.0386, Val Loss: 0.0407\n",
            "\n",
            "Test iteration 17/28\n",
            "Current training set size: 113 samples\n",
            "Epoch 1/100, Train Loss: 0.1600, Val Loss: 0.1898\n",
            "Epoch 2/100, Train Loss: 0.1416, Val Loss: 0.1942\n",
            "Epoch 3/100, Train Loss: 0.1295, Val Loss: 0.1793\n",
            "Epoch 4/100, Train Loss: 0.1323, Val Loss: 0.1504\n",
            "Epoch 5/100, Train Loss: 0.1027, Val Loss: 0.1050\n",
            "Epoch 6/100, Train Loss: 0.0842, Val Loss: 0.0801\n",
            "Epoch 7/100, Train Loss: 0.0763, Val Loss: 0.0617\n",
            "Epoch 8/100, Train Loss: 0.0814, Val Loss: 0.0762\n",
            "Epoch 9/100, Train Loss: 0.0680, Val Loss: 0.0566\n",
            "Epoch 10/100, Train Loss: 0.0654, Val Loss: 0.0410\n",
            "Epoch 11/100, Train Loss: 0.0567, Val Loss: 0.0418\n",
            "Epoch 12/100, Train Loss: 0.0514, Val Loss: 0.0505\n",
            "Epoch 13/100, Train Loss: 0.0467, Val Loss: 0.0393\n",
            "Epoch 14/100, Train Loss: 0.0402, Val Loss: 0.0369\n",
            "Epoch 15/100, Train Loss: 0.0536, Val Loss: 0.0370\n",
            "Epoch 16/100, Train Loss: 0.0589, Val Loss: 0.0525\n",
            "Epoch 17/100, Train Loss: 0.0432, Val Loss: 0.0459\n",
            "Epoch 18/100, Train Loss: 0.0472, Val Loss: 0.0431\n",
            "Epoch 19/100, Train Loss: 0.0388, Val Loss: 0.0397\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0338, Val Loss: 0.0402\n",
            "Epoch 21/100, Train Loss: 0.0329, Val Loss: 0.0408\n",
            "Epoch 22/100, Train Loss: 0.0327, Val Loss: 0.0414\n",
            "Epoch 23/100, Train Loss: 0.0326, Val Loss: 0.0414\n",
            "Epoch 24/100, Train Loss: 0.0346, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0327, Val Loss: 0.0413\n",
            "Epoch 26/100, Train Loss: 0.0349, Val Loss: 0.0413\n",
            "Epoch 27/100, Train Loss: 0.0335, Val Loss: 0.0413\n",
            "Epoch 28/100, Train Loss: 0.0332, Val Loss: 0.0413\n",
            "Epoch 29/100, Train Loss: 0.0349, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0330, Val Loss: 0.0413\n",
            "Epoch 31/100, Train Loss: 0.0326, Val Loss: 0.0413\n",
            "Epoch 32/100, Train Loss: 0.0328, Val Loss: 0.0413\n",
            "Epoch 33/100, Train Loss: 0.0335, Val Loss: 0.0413\n",
            "Epoch 34/100, Train Loss: 0.0327, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0334, Val Loss: 0.0413\n",
            "Epoch 36/100, Train Loss: 0.0340, Val Loss: 0.0413\n",
            "Epoch 37/100, Train Loss: 0.0336, Val Loss: 0.0413\n",
            "Epoch 38/100, Train Loss: 0.0333, Val Loss: 0.0413\n",
            "Epoch 39/100, Train Loss: 0.0328, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0340, Val Loss: 0.0413\n",
            "Epoch 41/100, Train Loss: 0.0321, Val Loss: 0.0413\n",
            "Epoch 42/100, Train Loss: 0.0331, Val Loss: 0.0413\n",
            "Epoch 43/100, Train Loss: 0.0322, Val Loss: 0.0413\n",
            "Epoch 44/100, Train Loss: 0.0342, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0332, Val Loss: 0.0413\n",
            "Epoch 46/100, Train Loss: 0.0333, Val Loss: 0.0413\n",
            "Epoch 47/100, Train Loss: 0.0332, Val Loss: 0.0413\n",
            "Epoch 48/100, Train Loss: 0.0325, Val Loss: 0.0413\n",
            "Epoch 49/100, Train Loss: 0.0323, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0321, Val Loss: 0.0413\n",
            "Epoch 51/100, Train Loss: 0.0332, Val Loss: 0.0413\n",
            "Epoch 52/100, Train Loss: 0.0334, Val Loss: 0.0413\n",
            "Epoch 53/100, Train Loss: 0.0334, Val Loss: 0.0413\n",
            "Epoch 54/100, Train Loss: 0.0344, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0331, Val Loss: 0.0413\n",
            "Epoch 56/100, Train Loss: 0.0328, Val Loss: 0.0413\n",
            "Epoch 57/100, Train Loss: 0.0329, Val Loss: 0.0413\n",
            "Epoch 58/100, Train Loss: 0.0324, Val Loss: 0.0413\n",
            "Epoch 59/100, Train Loss: 0.0333, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0324, Val Loss: 0.0413\n",
            "Epoch 61/100, Train Loss: 0.0324, Val Loss: 0.0413\n",
            "Epoch 62/100, Train Loss: 0.0324, Val Loss: 0.0413\n",
            "Epoch 63/100, Train Loss: 0.0324, Val Loss: 0.0413\n",
            "Epoch 64/100, Train Loss: 0.0330, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0335, Val Loss: 0.0413\n",
            "Epoch 66/100, Train Loss: 0.0335, Val Loss: 0.0413\n",
            "Epoch 67/100, Train Loss: 0.0338, Val Loss: 0.0413\n",
            "Epoch 68/100, Train Loss: 0.0327, Val Loss: 0.0413\n",
            "Epoch 69/100, Train Loss: 0.0328, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0337, Val Loss: 0.0413\n",
            "Epoch 71/100, Train Loss: 0.0331, Val Loss: 0.0413\n",
            "Epoch 72/100, Train Loss: 0.0324, Val Loss: 0.0413\n",
            "Epoch 73/100, Train Loss: 0.0330, Val Loss: 0.0413\n",
            "Epoch 74/100, Train Loss: 0.0334, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0331, Val Loss: 0.0413\n",
            "Epoch 76/100, Train Loss: 0.0326, Val Loss: 0.0413\n",
            "Epoch 77/100, Train Loss: 0.0327, Val Loss: 0.0413\n",
            "Epoch 78/100, Train Loss: 0.0334, Val Loss: 0.0413\n",
            "Epoch 79/100, Train Loss: 0.0331, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0334, Val Loss: 0.0413\n",
            "Epoch 81/100, Train Loss: 0.0325, Val Loss: 0.0413\n",
            "Epoch 82/100, Train Loss: 0.0327, Val Loss: 0.0413\n",
            "Epoch 83/100, Train Loss: 0.0323, Val Loss: 0.0413\n",
            "Epoch 84/100, Train Loss: 0.0325, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0325, Val Loss: 0.0413\n",
            "Epoch 86/100, Train Loss: 0.0324, Val Loss: 0.0413\n",
            "Epoch 87/100, Train Loss: 0.0334, Val Loss: 0.0413\n",
            "Epoch 88/100, Train Loss: 0.0330, Val Loss: 0.0413\n",
            "Epoch 89/100, Train Loss: 0.0330, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0322, Val Loss: 0.0413\n",
            "Epoch 91/100, Train Loss: 0.0320, Val Loss: 0.0413\n",
            "Epoch 92/100, Train Loss: 0.0324, Val Loss: 0.0413\n",
            "Epoch 93/100, Train Loss: 0.0384, Val Loss: 0.0413\n",
            "Epoch 94/100, Train Loss: 0.0335, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0332, Val Loss: 0.0413\n",
            "Epoch 96/100, Train Loss: 0.0329, Val Loss: 0.0413\n",
            "Epoch 97/100, Train Loss: 0.0323, Val Loss: 0.0413\n",
            "Epoch 98/100, Train Loss: 0.0347, Val Loss: 0.0413\n",
            "Epoch 99/100, Train Loss: 0.0323, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0325, Val Loss: 0.0413\n",
            "\n",
            "Test iteration 18/28\n",
            "Current training set size: 114 samples\n",
            "Epoch 1/100, Train Loss: 0.1616, Val Loss: 0.2174\n",
            "Epoch 2/100, Train Loss: 0.1424, Val Loss: 0.1840\n",
            "Epoch 3/100, Train Loss: 0.1431, Val Loss: 0.1646\n",
            "Epoch 4/100, Train Loss: 0.1355, Val Loss: 0.1556\n",
            "Epoch 5/100, Train Loss: 0.1129, Val Loss: 0.0902\n",
            "Epoch 6/100, Train Loss: 0.0847, Val Loss: 0.0321\n",
            "Epoch 7/100, Train Loss: 0.0659, Val Loss: 0.0498\n",
            "Epoch 8/100, Train Loss: 0.0748, Val Loss: 0.0630\n",
            "Epoch 9/100, Train Loss: 0.0641, Val Loss: 0.0473\n",
            "Epoch 10/100, Train Loss: 0.0569, Val Loss: 0.0448\n",
            "Epoch 11/100, Train Loss: 0.0534, Val Loss: 0.0375\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 12/100, Train Loss: 0.0434, Val Loss: 0.0372\n",
            "Epoch 13/100, Train Loss: 0.0418, Val Loss: 0.0371\n",
            "Epoch 14/100, Train Loss: 0.0466, Val Loss: 0.0371\n",
            "Epoch 15/100, Train Loss: 0.0411, Val Loss: 0.0370\n",
            "Epoch 16/100, Train Loss: 0.0410, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0406, Val Loss: 0.0370\n",
            "Epoch 18/100, Train Loss: 0.0412, Val Loss: 0.0370\n",
            "Epoch 19/100, Train Loss: 0.0406, Val Loss: 0.0370\n",
            "Epoch 20/100, Train Loss: 0.0417, Val Loss: 0.0370\n",
            "Epoch 21/100, Train Loss: 0.0407, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0403, Val Loss: 0.0370\n",
            "Epoch 23/100, Train Loss: 0.0412, Val Loss: 0.0370\n",
            "Epoch 24/100, Train Loss: 0.0412, Val Loss: 0.0370\n",
            "Epoch 25/100, Train Loss: 0.0412, Val Loss: 0.0370\n",
            "Epoch 26/100, Train Loss: 0.0406, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0409, Val Loss: 0.0370\n",
            "Epoch 28/100, Train Loss: 0.0411, Val Loss: 0.0370\n",
            "Epoch 29/100, Train Loss: 0.0421, Val Loss: 0.0370\n",
            "Epoch 30/100, Train Loss: 0.0410, Val Loss: 0.0370\n",
            "Epoch 31/100, Train Loss: 0.0408, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0408, Val Loss: 0.0370\n",
            "Epoch 33/100, Train Loss: 0.0407, Val Loss: 0.0370\n",
            "Epoch 34/100, Train Loss: 0.0418, Val Loss: 0.0370\n",
            "Epoch 35/100, Train Loss: 0.0404, Val Loss: 0.0370\n",
            "Epoch 36/100, Train Loss: 0.0402, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0407, Val Loss: 0.0370\n",
            "Epoch 38/100, Train Loss: 0.0414, Val Loss: 0.0370\n",
            "Epoch 39/100, Train Loss: 0.0407, Val Loss: 0.0370\n",
            "Epoch 40/100, Train Loss: 0.0410, Val Loss: 0.0370\n",
            "Epoch 41/100, Train Loss: 0.0409, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0409, Val Loss: 0.0370\n",
            "Epoch 43/100, Train Loss: 0.0408, Val Loss: 0.0370\n",
            "Epoch 44/100, Train Loss: 0.0402, Val Loss: 0.0370\n",
            "Epoch 45/100, Train Loss: 0.0413, Val Loss: 0.0370\n",
            "Epoch 46/100, Train Loss: 0.0410, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0406, Val Loss: 0.0370\n",
            "Epoch 48/100, Train Loss: 0.0406, Val Loss: 0.0370\n",
            "Epoch 49/100, Train Loss: 0.0414, Val Loss: 0.0370\n",
            "Epoch 50/100, Train Loss: 0.0412, Val Loss: 0.0370\n",
            "Epoch 51/100, Train Loss: 0.0401, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0406, Val Loss: 0.0370\n",
            "Epoch 53/100, Train Loss: 0.0461, Val Loss: 0.0370\n",
            "Epoch 54/100, Train Loss: 0.0412, Val Loss: 0.0370\n",
            "Epoch 55/100, Train Loss: 0.0406, Val Loss: 0.0370\n",
            "Epoch 56/100, Train Loss: 0.0408, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0410, Val Loss: 0.0370\n",
            "Epoch 58/100, Train Loss: 0.0411, Val Loss: 0.0370\n",
            "Epoch 59/100, Train Loss: 0.0412, Val Loss: 0.0370\n",
            "Epoch 60/100, Train Loss: 0.0414, Val Loss: 0.0370\n",
            "Epoch 61/100, Train Loss: 0.0408, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0413, Val Loss: 0.0370\n",
            "Epoch 63/100, Train Loss: 0.0402, Val Loss: 0.0370\n",
            "Epoch 64/100, Train Loss: 0.0408, Val Loss: 0.0370\n",
            "Epoch 65/100, Train Loss: 0.0407, Val Loss: 0.0370\n",
            "Epoch 66/100, Train Loss: 0.0411, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0411, Val Loss: 0.0370\n",
            "Epoch 68/100, Train Loss: 0.0416, Val Loss: 0.0370\n",
            "Epoch 69/100, Train Loss: 0.0413, Val Loss: 0.0370\n",
            "Epoch 70/100, Train Loss: 0.0407, Val Loss: 0.0370\n",
            "Epoch 71/100, Train Loss: 0.0410, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0412, Val Loss: 0.0370\n",
            "Epoch 73/100, Train Loss: 0.0405, Val Loss: 0.0370\n",
            "Epoch 74/100, Train Loss: 0.0407, Val Loss: 0.0370\n",
            "Epoch 75/100, Train Loss: 0.0409, Val Loss: 0.0370\n",
            "Epoch 76/100, Train Loss: 0.0427, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0408, Val Loss: 0.0370\n",
            "Epoch 78/100, Train Loss: 0.0409, Val Loss: 0.0370\n",
            "Epoch 79/100, Train Loss: 0.0410, Val Loss: 0.0370\n",
            "Epoch 80/100, Train Loss: 0.0410, Val Loss: 0.0370\n",
            "Epoch 81/100, Train Loss: 0.0409, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0410, Val Loss: 0.0370\n",
            "Epoch 83/100, Train Loss: 0.0409, Val Loss: 0.0370\n",
            "Epoch 84/100, Train Loss: 0.0414, Val Loss: 0.0370\n",
            "Epoch 85/100, Train Loss: 0.0411, Val Loss: 0.0370\n",
            "Epoch 86/100, Train Loss: 0.0408, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0401, Val Loss: 0.0370\n",
            "Epoch 88/100, Train Loss: 0.0419, Val Loss: 0.0370\n",
            "Epoch 89/100, Train Loss: 0.0433, Val Loss: 0.0370\n",
            "Epoch 90/100, Train Loss: 0.0405, Val Loss: 0.0370\n",
            "Epoch 91/100, Train Loss: 0.0427, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0411, Val Loss: 0.0370\n",
            "Epoch 93/100, Train Loss: 0.0407, Val Loss: 0.0370\n",
            "Epoch 94/100, Train Loss: 0.0417, Val Loss: 0.0370\n",
            "Epoch 95/100, Train Loss: 0.0407, Val Loss: 0.0370\n",
            "Epoch 96/100, Train Loss: 0.0405, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0411, Val Loss: 0.0370\n",
            "Epoch 98/100, Train Loss: 0.0416, Val Loss: 0.0370\n",
            "Epoch 99/100, Train Loss: 0.0416, Val Loss: 0.0370\n",
            "Epoch 100/100, Train Loss: 0.0404, Val Loss: 0.0370\n",
            "\n",
            "Test iteration 19/28\n",
            "Current training set size: 115 samples\n",
            "Epoch 1/100, Train Loss: 0.1549, Val Loss: 0.1883\n",
            "Epoch 2/100, Train Loss: 0.1538, Val Loss: 0.1929\n",
            "Epoch 3/100, Train Loss: 0.1349, Val Loss: 0.1490\n",
            "Epoch 4/100, Train Loss: 0.1285, Val Loss: 0.1424\n",
            "Epoch 5/100, Train Loss: 0.0940, Val Loss: 0.1280\n",
            "Epoch 6/100, Train Loss: 0.0816, Val Loss: 0.0379\n",
            "Epoch 7/100, Train Loss: 0.0878, Val Loss: 0.0477\n",
            "Epoch 8/100, Train Loss: 0.0821, Val Loss: 0.0503\n",
            "Epoch 9/100, Train Loss: 0.0712, Val Loss: 0.0449\n",
            "Epoch 10/100, Train Loss: 0.0642, Val Loss: 0.0328\n",
            "Epoch 11/100, Train Loss: 0.0583, Val Loss: 0.0363\n",
            "Epoch 12/100, Train Loss: 0.0567, Val Loss: 0.0549\n",
            "Epoch 13/100, Train Loss: 0.0445, Val Loss: 0.0396\n",
            "Epoch 14/100, Train Loss: 0.0458, Val Loss: 0.0340\n",
            "Epoch 15/100, Train Loss: 0.0411, Val Loss: 0.0383\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0393, Val Loss: 0.0384\n",
            "Epoch 17/100, Train Loss: 0.0373, Val Loss: 0.0387\n",
            "Epoch 18/100, Train Loss: 0.0372, Val Loss: 0.0392\n",
            "Epoch 19/100, Train Loss: 0.0362, Val Loss: 0.0394\n",
            "Epoch 20/100, Train Loss: 0.0362, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0364, Val Loss: 0.0394\n",
            "Epoch 22/100, Train Loss: 0.0355, Val Loss: 0.0394\n",
            "Epoch 23/100, Train Loss: 0.0366, Val Loss: 0.0394\n",
            "Epoch 24/100, Train Loss: 0.0357, Val Loss: 0.0394\n",
            "Epoch 25/100, Train Loss: 0.0359, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0358, Val Loss: 0.0394\n",
            "Epoch 27/100, Train Loss: 0.0356, Val Loss: 0.0394\n",
            "Epoch 28/100, Train Loss: 0.0359, Val Loss: 0.0394\n",
            "Epoch 29/100, Train Loss: 0.0356, Val Loss: 0.0394\n",
            "Epoch 30/100, Train Loss: 0.0357, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0361, Val Loss: 0.0394\n",
            "Epoch 32/100, Train Loss: 0.0356, Val Loss: 0.0394\n",
            "Epoch 33/100, Train Loss: 0.0358, Val Loss: 0.0394\n",
            "Epoch 34/100, Train Loss: 0.0364, Val Loss: 0.0394\n",
            "Epoch 35/100, Train Loss: 0.0365, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0361, Val Loss: 0.0394\n",
            "Epoch 37/100, Train Loss: 0.0360, Val Loss: 0.0394\n",
            "Epoch 38/100, Train Loss: 0.0360, Val Loss: 0.0394\n",
            "Epoch 39/100, Train Loss: 0.0367, Val Loss: 0.0394\n",
            "Epoch 40/100, Train Loss: 0.0360, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0360, Val Loss: 0.0394\n",
            "Epoch 42/100, Train Loss: 0.0362, Val Loss: 0.0394\n",
            "Epoch 43/100, Train Loss: 0.0355, Val Loss: 0.0394\n",
            "Epoch 44/100, Train Loss: 0.0359, Val Loss: 0.0394\n",
            "Epoch 45/100, Train Loss: 0.0351, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0357, Val Loss: 0.0394\n",
            "Epoch 47/100, Train Loss: 0.0363, Val Loss: 0.0394\n",
            "Epoch 48/100, Train Loss: 0.0358, Val Loss: 0.0394\n",
            "Epoch 49/100, Train Loss: 0.0361, Val Loss: 0.0394\n",
            "Epoch 50/100, Train Loss: 0.0358, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0357, Val Loss: 0.0394\n",
            "Epoch 52/100, Train Loss: 0.0360, Val Loss: 0.0394\n",
            "Epoch 53/100, Train Loss: 0.0361, Val Loss: 0.0394\n",
            "Epoch 54/100, Train Loss: 0.0366, Val Loss: 0.0394\n",
            "Epoch 55/100, Train Loss: 0.0361, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0367, Val Loss: 0.0394\n",
            "Epoch 57/100, Train Loss: 0.0356, Val Loss: 0.0394\n",
            "Epoch 58/100, Train Loss: 0.0360, Val Loss: 0.0394\n",
            "Epoch 59/100, Train Loss: 0.0360, Val Loss: 0.0394\n",
            "Epoch 60/100, Train Loss: 0.0362, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0367, Val Loss: 0.0394\n",
            "Epoch 62/100, Train Loss: 0.0360, Val Loss: 0.0394\n",
            "Epoch 63/100, Train Loss: 0.0355, Val Loss: 0.0394\n",
            "Epoch 64/100, Train Loss: 0.0361, Val Loss: 0.0394\n",
            "Epoch 65/100, Train Loss: 0.0359, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0359, Val Loss: 0.0394\n",
            "Epoch 67/100, Train Loss: 0.0356, Val Loss: 0.0394\n",
            "Epoch 68/100, Train Loss: 0.0357, Val Loss: 0.0394\n",
            "Epoch 69/100, Train Loss: 0.0361, Val Loss: 0.0394\n",
            "Epoch 70/100, Train Loss: 0.0356, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0357, Val Loss: 0.0394\n",
            "Epoch 72/100, Train Loss: 0.0360, Val Loss: 0.0394\n",
            "Epoch 73/100, Train Loss: 0.0369, Val Loss: 0.0394\n",
            "Epoch 74/100, Train Loss: 0.0370, Val Loss: 0.0394\n",
            "Epoch 75/100, Train Loss: 0.0356, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0370, Val Loss: 0.0394\n",
            "Epoch 77/100, Train Loss: 0.0355, Val Loss: 0.0394\n",
            "Epoch 78/100, Train Loss: 0.0359, Val Loss: 0.0394\n",
            "Epoch 79/100, Train Loss: 0.0358, Val Loss: 0.0394\n",
            "Epoch 80/100, Train Loss: 0.0360, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0356, Val Loss: 0.0394\n",
            "Epoch 82/100, Train Loss: 0.0371, Val Loss: 0.0394\n",
            "Epoch 83/100, Train Loss: 0.0379, Val Loss: 0.0394\n",
            "Epoch 84/100, Train Loss: 0.0363, Val Loss: 0.0394\n",
            "Epoch 85/100, Train Loss: 0.0374, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0355, Val Loss: 0.0394\n",
            "Epoch 87/100, Train Loss: 0.0357, Val Loss: 0.0394\n",
            "Epoch 88/100, Train Loss: 0.0358, Val Loss: 0.0394\n",
            "Epoch 89/100, Train Loss: 0.0358, Val Loss: 0.0394\n",
            "Epoch 90/100, Train Loss: 0.0358, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0359, Val Loss: 0.0394\n",
            "Epoch 92/100, Train Loss: 0.0357, Val Loss: 0.0394\n",
            "Epoch 93/100, Train Loss: 0.0358, Val Loss: 0.0394\n",
            "Epoch 94/100, Train Loss: 0.0355, Val Loss: 0.0394\n",
            "Epoch 95/100, Train Loss: 0.0359, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0365, Val Loss: 0.0394\n",
            "Epoch 97/100, Train Loss: 0.0366, Val Loss: 0.0394\n",
            "Epoch 98/100, Train Loss: 0.0362, Val Loss: 0.0394\n",
            "Epoch 99/100, Train Loss: 0.0360, Val Loss: 0.0394\n",
            "Epoch 100/100, Train Loss: 0.0355, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 20/28\n",
            "Current training set size: 116 samples\n",
            "Epoch 1/100, Train Loss: 0.1551, Val Loss: 0.1891\n",
            "Epoch 2/100, Train Loss: 0.1414, Val Loss: 0.1739\n",
            "Epoch 3/100, Train Loss: 0.1273, Val Loss: 0.0994\n",
            "Epoch 4/100, Train Loss: 0.1000, Val Loss: 0.0656\n",
            "Epoch 5/100, Train Loss: 0.0923, Val Loss: 0.0690\n",
            "Epoch 6/100, Train Loss: 0.0817, Val Loss: 0.0710\n",
            "Epoch 7/100, Train Loss: 0.0841, Val Loss: 0.0435\n",
            "Epoch 8/100, Train Loss: 0.0817, Val Loss: 0.0636\n",
            "Epoch 9/100, Train Loss: 0.0705, Val Loss: 0.0482\n",
            "Epoch 10/100, Train Loss: 0.0786, Val Loss: 0.0438\n",
            "Epoch 11/100, Train Loss: 0.0614, Val Loss: 0.0461\n",
            "Epoch 12/100, Train Loss: 0.0534, Val Loss: 0.0608\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 13/100, Train Loss: 0.0603, Val Loss: 0.0534\n",
            "Epoch 14/100, Train Loss: 0.0535, Val Loss: 0.0500\n",
            "Epoch 15/100, Train Loss: 0.0500, Val Loss: 0.0488\n",
            "Epoch 16/100, Train Loss: 0.0479, Val Loss: 0.0473\n",
            "Epoch 17/100, Train Loss: 0.0472, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0464, Val Loss: 0.0458\n",
            "Epoch 19/100, Train Loss: 0.0467, Val Loss: 0.0458\n",
            "Epoch 20/100, Train Loss: 0.0469, Val Loss: 0.0458\n",
            "Epoch 21/100, Train Loss: 0.0464, Val Loss: 0.0458\n",
            "Epoch 22/100, Train Loss: 0.0464, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0467, Val Loss: 0.0458\n",
            "Epoch 24/100, Train Loss: 0.0462, Val Loss: 0.0458\n",
            "Epoch 25/100, Train Loss: 0.0464, Val Loss: 0.0458\n",
            "Epoch 26/100, Train Loss: 0.0468, Val Loss: 0.0458\n",
            "Epoch 27/100, Train Loss: 0.0464, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0466, Val Loss: 0.0458\n",
            "Epoch 29/100, Train Loss: 0.0462, Val Loss: 0.0458\n",
            "Epoch 30/100, Train Loss: 0.0460, Val Loss: 0.0458\n",
            "Epoch 31/100, Train Loss: 0.0463, Val Loss: 0.0458\n",
            "Epoch 32/100, Train Loss: 0.0463, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0467, Val Loss: 0.0458\n",
            "Epoch 34/100, Train Loss: 0.0466, Val Loss: 0.0458\n",
            "Epoch 35/100, Train Loss: 0.0462, Val Loss: 0.0458\n",
            "Epoch 36/100, Train Loss: 0.0467, Val Loss: 0.0458\n",
            "Epoch 37/100, Train Loss: 0.0465, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0466, Val Loss: 0.0458\n",
            "Epoch 39/100, Train Loss: 0.0467, Val Loss: 0.0458\n",
            "Epoch 40/100, Train Loss: 0.0466, Val Loss: 0.0458\n",
            "Epoch 41/100, Train Loss: 0.0466, Val Loss: 0.0458\n",
            "Epoch 42/100, Train Loss: 0.0463, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0468, Val Loss: 0.0458\n",
            "Epoch 44/100, Train Loss: 0.0466, Val Loss: 0.0458\n",
            "Epoch 45/100, Train Loss: 0.0467, Val Loss: 0.0458\n",
            "Epoch 46/100, Train Loss: 0.0459, Val Loss: 0.0458\n",
            "Epoch 47/100, Train Loss: 0.0464, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0464, Val Loss: 0.0458\n",
            "Epoch 49/100, Train Loss: 0.0464, Val Loss: 0.0458\n",
            "Epoch 50/100, Train Loss: 0.0468, Val Loss: 0.0458\n",
            "Epoch 51/100, Train Loss: 0.0465, Val Loss: 0.0458\n",
            "Epoch 52/100, Train Loss: 0.0466, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0465, Val Loss: 0.0458\n",
            "Epoch 54/100, Train Loss: 0.0464, Val Loss: 0.0458\n",
            "Epoch 55/100, Train Loss: 0.0464, Val Loss: 0.0458\n",
            "Epoch 56/100, Train Loss: 0.0464, Val Loss: 0.0458\n",
            "Epoch 57/100, Train Loss: 0.0466, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0465, Val Loss: 0.0458\n",
            "Epoch 59/100, Train Loss: 0.0464, Val Loss: 0.0458\n",
            "Epoch 60/100, Train Loss: 0.0460, Val Loss: 0.0458\n",
            "Epoch 61/100, Train Loss: 0.0465, Val Loss: 0.0458\n",
            "Epoch 62/100, Train Loss: 0.0470, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0468, Val Loss: 0.0458\n",
            "Epoch 64/100, Train Loss: 0.0463, Val Loss: 0.0458\n",
            "Epoch 65/100, Train Loss: 0.0462, Val Loss: 0.0458\n",
            "Epoch 66/100, Train Loss: 0.0459, Val Loss: 0.0458\n",
            "Epoch 67/100, Train Loss: 0.0466, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0463, Val Loss: 0.0458\n",
            "Epoch 69/100, Train Loss: 0.0464, Val Loss: 0.0458\n",
            "Epoch 70/100, Train Loss: 0.0475, Val Loss: 0.0458\n",
            "Epoch 71/100, Train Loss: 0.0465, Val Loss: 0.0458\n",
            "Epoch 72/100, Train Loss: 0.0466, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0468, Val Loss: 0.0458\n",
            "Epoch 74/100, Train Loss: 0.0464, Val Loss: 0.0458\n",
            "Epoch 75/100, Train Loss: 0.0463, Val Loss: 0.0458\n",
            "Epoch 76/100, Train Loss: 0.0473, Val Loss: 0.0458\n",
            "Epoch 77/100, Train Loss: 0.0468, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0463, Val Loss: 0.0458\n",
            "Epoch 79/100, Train Loss: 0.0467, Val Loss: 0.0458\n",
            "Epoch 80/100, Train Loss: 0.0462, Val Loss: 0.0458\n",
            "Epoch 81/100, Train Loss: 0.0465, Val Loss: 0.0458\n",
            "Epoch 82/100, Train Loss: 0.0466, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0464, Val Loss: 0.0458\n",
            "Epoch 84/100, Train Loss: 0.0463, Val Loss: 0.0458\n",
            "Epoch 85/100, Train Loss: 0.0463, Val Loss: 0.0458\n",
            "Epoch 86/100, Train Loss: 0.0466, Val Loss: 0.0458\n",
            "Epoch 87/100, Train Loss: 0.0462, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0463, Val Loss: 0.0458\n",
            "Epoch 89/100, Train Loss: 0.0464, Val Loss: 0.0458\n",
            "Epoch 90/100, Train Loss: 0.0463, Val Loss: 0.0458\n",
            "Epoch 91/100, Train Loss: 0.0464, Val Loss: 0.0458\n",
            "Epoch 92/100, Train Loss: 0.0466, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0464, Val Loss: 0.0458\n",
            "Epoch 94/100, Train Loss: 0.0466, Val Loss: 0.0458\n",
            "Epoch 95/100, Train Loss: 0.0466, Val Loss: 0.0458\n",
            "Epoch 96/100, Train Loss: 0.0468, Val Loss: 0.0458\n",
            "Epoch 97/100, Train Loss: 0.0466, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0471, Val Loss: 0.0458\n",
            "Epoch 99/100, Train Loss: 0.0463, Val Loss: 0.0458\n",
            "Epoch 100/100, Train Loss: 0.0464, Val Loss: 0.0458\n",
            "\n",
            "Test iteration 21/28\n",
            "Current training set size: 117 samples\n",
            "Epoch 1/100, Train Loss: 0.1581, Val Loss: 0.1881\n",
            "Epoch 2/100, Train Loss: 0.1467, Val Loss: 0.1745\n",
            "Epoch 3/100, Train Loss: 0.1583, Val Loss: 0.1797\n",
            "Epoch 4/100, Train Loss: 0.1357, Val Loss: 0.1742\n",
            "Epoch 5/100, Train Loss: 0.1222, Val Loss: 0.1181\n",
            "Epoch 6/100, Train Loss: 0.0817, Val Loss: 0.0739\n",
            "Epoch 7/100, Train Loss: 0.0767, Val Loss: 0.0817\n",
            "Epoch 8/100, Train Loss: 0.0730, Val Loss: 0.0536\n",
            "Epoch 9/100, Train Loss: 0.0618, Val Loss: 0.0388\n",
            "Epoch 10/100, Train Loss: 0.0569, Val Loss: 0.0394\n",
            "Epoch 11/100, Train Loss: 0.0532, Val Loss: 0.0464\n",
            "Epoch 12/100, Train Loss: 0.0495, Val Loss: 0.0419\n",
            "Epoch 13/100, Train Loss: 0.0501, Val Loss: 0.0423\n",
            "Epoch 14/100, Train Loss: 0.0516, Val Loss: 0.0483\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 15/100, Train Loss: 0.0495, Val Loss: 0.0458\n",
            "Epoch 16/100, Train Loss: 0.0426, Val Loss: 0.0436\n",
            "Epoch 17/100, Train Loss: 0.0423, Val Loss: 0.0423\n",
            "Epoch 18/100, Train Loss: 0.0413, Val Loss: 0.0419\n",
            "Epoch 19/100, Train Loss: 0.0408, Val Loss: 0.0414\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0413, Val Loss: 0.0414\n",
            "Epoch 21/100, Train Loss: 0.0413, Val Loss: 0.0414\n",
            "Epoch 22/100, Train Loss: 0.0409, Val Loss: 0.0414\n",
            "Epoch 23/100, Train Loss: 0.0412, Val Loss: 0.0413\n",
            "Epoch 24/100, Train Loss: 0.0406, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0405, Val Loss: 0.0413\n",
            "Epoch 26/100, Train Loss: 0.0412, Val Loss: 0.0413\n",
            "Epoch 27/100, Train Loss: 0.0413, Val Loss: 0.0413\n",
            "Epoch 28/100, Train Loss: 0.0412, Val Loss: 0.0413\n",
            "Epoch 29/100, Train Loss: 0.0405, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0400, Val Loss: 0.0413\n",
            "Epoch 31/100, Train Loss: 0.0424, Val Loss: 0.0413\n",
            "Epoch 32/100, Train Loss: 0.0419, Val Loss: 0.0413\n",
            "Epoch 33/100, Train Loss: 0.0424, Val Loss: 0.0413\n",
            "Epoch 34/100, Train Loss: 0.0403, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0407, Val Loss: 0.0413\n",
            "Epoch 36/100, Train Loss: 0.0411, Val Loss: 0.0413\n",
            "Epoch 37/100, Train Loss: 0.0411, Val Loss: 0.0413\n",
            "Epoch 38/100, Train Loss: 0.0408, Val Loss: 0.0413\n",
            "Epoch 39/100, Train Loss: 0.0409, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0403, Val Loss: 0.0413\n",
            "Epoch 41/100, Train Loss: 0.0422, Val Loss: 0.0413\n",
            "Epoch 42/100, Train Loss: 0.0406, Val Loss: 0.0413\n",
            "Epoch 43/100, Train Loss: 0.0396, Val Loss: 0.0413\n",
            "Epoch 44/100, Train Loss: 0.0403, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0416, Val Loss: 0.0413\n",
            "Epoch 46/100, Train Loss: 0.0457, Val Loss: 0.0413\n",
            "Epoch 47/100, Train Loss: 0.0420, Val Loss: 0.0413\n",
            "Epoch 48/100, Train Loss: 0.0401, Val Loss: 0.0413\n",
            "Epoch 49/100, Train Loss: 0.0403, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0454, Val Loss: 0.0413\n",
            "Epoch 51/100, Train Loss: 0.0403, Val Loss: 0.0413\n",
            "Epoch 52/100, Train Loss: 0.0404, Val Loss: 0.0413\n",
            "Epoch 53/100, Train Loss: 0.0454, Val Loss: 0.0413\n",
            "Epoch 54/100, Train Loss: 0.0413, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0408, Val Loss: 0.0413\n",
            "Epoch 56/100, Train Loss: 0.0454, Val Loss: 0.0413\n",
            "Epoch 57/100, Train Loss: 0.0407, Val Loss: 0.0413\n",
            "Epoch 58/100, Train Loss: 0.0405, Val Loss: 0.0413\n",
            "Epoch 59/100, Train Loss: 0.0408, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0410, Val Loss: 0.0413\n",
            "Epoch 61/100, Train Loss: 0.0417, Val Loss: 0.0413\n",
            "Epoch 62/100, Train Loss: 0.0411, Val Loss: 0.0413\n",
            "Epoch 63/100, Train Loss: 0.0398, Val Loss: 0.0413\n",
            "Epoch 64/100, Train Loss: 0.0409, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0409, Val Loss: 0.0413\n",
            "Epoch 66/100, Train Loss: 0.0407, Val Loss: 0.0413\n",
            "Epoch 67/100, Train Loss: 0.0418, Val Loss: 0.0413\n",
            "Epoch 68/100, Train Loss: 0.0428, Val Loss: 0.0413\n",
            "Epoch 69/100, Train Loss: 0.0423, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0406, Val Loss: 0.0413\n",
            "Epoch 71/100, Train Loss: 0.0414, Val Loss: 0.0413\n",
            "Epoch 72/100, Train Loss: 0.0408, Val Loss: 0.0413\n",
            "Epoch 73/100, Train Loss: 0.0413, Val Loss: 0.0413\n",
            "Epoch 74/100, Train Loss: 0.0411, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0448, Val Loss: 0.0413\n",
            "Epoch 76/100, Train Loss: 0.0427, Val Loss: 0.0413\n",
            "Epoch 77/100, Train Loss: 0.0436, Val Loss: 0.0413\n",
            "Epoch 78/100, Train Loss: 0.0453, Val Loss: 0.0413\n",
            "Epoch 79/100, Train Loss: 0.0416, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0408, Val Loss: 0.0413\n",
            "Epoch 81/100, Train Loss: 0.0409, Val Loss: 0.0413\n",
            "Epoch 82/100, Train Loss: 0.0398, Val Loss: 0.0413\n",
            "Epoch 83/100, Train Loss: 0.0417, Val Loss: 0.0413\n",
            "Epoch 84/100, Train Loss: 0.0424, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0413, Val Loss: 0.0413\n",
            "Epoch 86/100, Train Loss: 0.0405, Val Loss: 0.0413\n",
            "Epoch 87/100, Train Loss: 0.0419, Val Loss: 0.0413\n",
            "Epoch 88/100, Train Loss: 0.0412, Val Loss: 0.0413\n",
            "Epoch 89/100, Train Loss: 0.0414, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0411, Val Loss: 0.0413\n",
            "Epoch 91/100, Train Loss: 0.0408, Val Loss: 0.0413\n",
            "Epoch 92/100, Train Loss: 0.0410, Val Loss: 0.0413\n",
            "Epoch 93/100, Train Loss: 0.0407, Val Loss: 0.0413\n",
            "Epoch 94/100, Train Loss: 0.0408, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0403, Val Loss: 0.0413\n",
            "Epoch 96/100, Train Loss: 0.0405, Val Loss: 0.0413\n",
            "Epoch 97/100, Train Loss: 0.0415, Val Loss: 0.0413\n",
            "Epoch 98/100, Train Loss: 0.0411, Val Loss: 0.0413\n",
            "Epoch 99/100, Train Loss: 0.0405, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0421, Val Loss: 0.0413\n",
            "\n",
            "Test iteration 22/28\n",
            "Current training set size: 118 samples\n",
            "Epoch 1/100, Train Loss: 0.1501, Val Loss: 0.1880\n",
            "Epoch 2/100, Train Loss: 0.1485, Val Loss: 0.1842\n",
            "Epoch 3/100, Train Loss: 0.1384, Val Loss: 0.1691\n",
            "Epoch 4/100, Train Loss: 0.1317, Val Loss: 0.1597\n",
            "Epoch 5/100, Train Loss: 0.1270, Val Loss: 0.1234\n",
            "Epoch 6/100, Train Loss: 0.0927, Val Loss: 0.0667\n",
            "Epoch 7/100, Train Loss: 0.0793, Val Loss: 0.0325\n",
            "Epoch 8/100, Train Loss: 0.0657, Val Loss: 0.0608\n",
            "Epoch 9/100, Train Loss: 0.0638, Val Loss: 0.0364\n",
            "Epoch 10/100, Train Loss: 0.0553, Val Loss: 0.0566\n",
            "Epoch 11/100, Train Loss: 0.0692, Val Loss: 0.0550\n",
            "Epoch 12/100, Train Loss: 0.0591, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 13/100, Train Loss: 0.0479, Val Loss: 0.0371\n",
            "Epoch 14/100, Train Loss: 0.0452, Val Loss: 0.0375\n",
            "Epoch 15/100, Train Loss: 0.0434, Val Loss: 0.0384\n",
            "Epoch 16/100, Train Loss: 0.0424, Val Loss: 0.0387\n",
            "Epoch 17/100, Train Loss: 0.0425, Val Loss: 0.0387\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0424, Val Loss: 0.0387\n",
            "Epoch 19/100, Train Loss: 0.0433, Val Loss: 0.0387\n",
            "Epoch 20/100, Train Loss: 0.0426, Val Loss: 0.0387\n",
            "Epoch 21/100, Train Loss: 0.0422, Val Loss: 0.0387\n",
            "Epoch 22/100, Train Loss: 0.0422, Val Loss: 0.0387\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0422, Val Loss: 0.0387\n",
            "Epoch 24/100, Train Loss: 0.0425, Val Loss: 0.0387\n",
            "Epoch 25/100, Train Loss: 0.0422, Val Loss: 0.0387\n",
            "Epoch 26/100, Train Loss: 0.0422, Val Loss: 0.0387\n",
            "Epoch 27/100, Train Loss: 0.0471, Val Loss: 0.0387\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0430, Val Loss: 0.0387\n",
            "Epoch 29/100, Train Loss: 0.0420, Val Loss: 0.0387\n",
            "Epoch 30/100, Train Loss: 0.0427, Val Loss: 0.0387\n",
            "Epoch 31/100, Train Loss: 0.0424, Val Loss: 0.0387\n",
            "Epoch 32/100, Train Loss: 0.0422, Val Loss: 0.0387\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0422, Val Loss: 0.0387\n",
            "Epoch 34/100, Train Loss: 0.0422, Val Loss: 0.0387\n",
            "Epoch 35/100, Train Loss: 0.0425, Val Loss: 0.0387\n",
            "Epoch 36/100, Train Loss: 0.0424, Val Loss: 0.0387\n",
            "Epoch 37/100, Train Loss: 0.0422, Val Loss: 0.0387\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0417, Val Loss: 0.0387\n",
            "Epoch 39/100, Train Loss: 0.0418, Val Loss: 0.0387\n",
            "Epoch 40/100, Train Loss: 0.0420, Val Loss: 0.0387\n",
            "Epoch 41/100, Train Loss: 0.0422, Val Loss: 0.0387\n",
            "Epoch 42/100, Train Loss: 0.0427, Val Loss: 0.0387\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0427, Val Loss: 0.0387\n",
            "Epoch 44/100, Train Loss: 0.0423, Val Loss: 0.0387\n",
            "Epoch 45/100, Train Loss: 0.0422, Val Loss: 0.0387\n",
            "Epoch 46/100, Train Loss: 0.0416, Val Loss: 0.0387\n",
            "Epoch 47/100, Train Loss: 0.0422, Val Loss: 0.0387\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0422, Val Loss: 0.0387\n",
            "Epoch 49/100, Train Loss: 0.0422, Val Loss: 0.0387\n",
            "Epoch 50/100, Train Loss: 0.0421, Val Loss: 0.0387\n",
            "Epoch 51/100, Train Loss: 0.0428, Val Loss: 0.0387\n",
            "Epoch 52/100, Train Loss: 0.0432, Val Loss: 0.0387\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0421, Val Loss: 0.0387\n",
            "Epoch 54/100, Train Loss: 0.0429, Val Loss: 0.0387\n",
            "Epoch 55/100, Train Loss: 0.0427, Val Loss: 0.0387\n",
            "Epoch 56/100, Train Loss: 0.0469, Val Loss: 0.0387\n",
            "Epoch 57/100, Train Loss: 0.0422, Val Loss: 0.0387\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0420, Val Loss: 0.0387\n",
            "Epoch 59/100, Train Loss: 0.0439, Val Loss: 0.0387\n",
            "Epoch 60/100, Train Loss: 0.0423, Val Loss: 0.0387\n",
            "Epoch 61/100, Train Loss: 0.0423, Val Loss: 0.0387\n",
            "Epoch 62/100, Train Loss: 0.0424, Val Loss: 0.0387\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0417, Val Loss: 0.0387\n",
            "Epoch 64/100, Train Loss: 0.0423, Val Loss: 0.0387\n",
            "Epoch 65/100, Train Loss: 0.0417, Val Loss: 0.0387\n",
            "Epoch 66/100, Train Loss: 0.0417, Val Loss: 0.0387\n",
            "Epoch 67/100, Train Loss: 0.0420, Val Loss: 0.0387\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0418, Val Loss: 0.0387\n",
            "Epoch 69/100, Train Loss: 0.0425, Val Loss: 0.0387\n",
            "Epoch 70/100, Train Loss: 0.0426, Val Loss: 0.0387\n",
            "Epoch 71/100, Train Loss: 0.0418, Val Loss: 0.0387\n",
            "Epoch 72/100, Train Loss: 0.0428, Val Loss: 0.0387\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0421, Val Loss: 0.0387\n",
            "Epoch 74/100, Train Loss: 0.0427, Val Loss: 0.0387\n",
            "Epoch 75/100, Train Loss: 0.0430, Val Loss: 0.0387\n",
            "Epoch 76/100, Train Loss: 0.0421, Val Loss: 0.0387\n",
            "Epoch 77/100, Train Loss: 0.0468, Val Loss: 0.0387\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0422, Val Loss: 0.0387\n",
            "Epoch 79/100, Train Loss: 0.0424, Val Loss: 0.0387\n",
            "Epoch 80/100, Train Loss: 0.0422, Val Loss: 0.0387\n",
            "Epoch 81/100, Train Loss: 0.0430, Val Loss: 0.0387\n",
            "Epoch 82/100, Train Loss: 0.0423, Val Loss: 0.0387\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0424, Val Loss: 0.0387\n",
            "Epoch 84/100, Train Loss: 0.0417, Val Loss: 0.0387\n",
            "Epoch 85/100, Train Loss: 0.0421, Val Loss: 0.0387\n",
            "Epoch 86/100, Train Loss: 0.0425, Val Loss: 0.0387\n",
            "Epoch 87/100, Train Loss: 0.0423, Val Loss: 0.0387\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0428, Val Loss: 0.0387\n",
            "Epoch 89/100, Train Loss: 0.0424, Val Loss: 0.0387\n",
            "Epoch 90/100, Train Loss: 0.0429, Val Loss: 0.0387\n",
            "Epoch 91/100, Train Loss: 0.0422, Val Loss: 0.0387\n",
            "Epoch 92/100, Train Loss: 0.0420, Val Loss: 0.0387\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0419, Val Loss: 0.0387\n",
            "Epoch 94/100, Train Loss: 0.0422, Val Loss: 0.0387\n",
            "Epoch 95/100, Train Loss: 0.0426, Val Loss: 0.0387\n",
            "Epoch 96/100, Train Loss: 0.0422, Val Loss: 0.0387\n",
            "Epoch 97/100, Train Loss: 0.0422, Val Loss: 0.0387\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0420, Val Loss: 0.0387\n",
            "Epoch 99/100, Train Loss: 0.0421, Val Loss: 0.0387\n",
            "Epoch 100/100, Train Loss: 0.0423, Val Loss: 0.0387\n",
            "\n",
            "Test iteration 23/28\n",
            "Current training set size: 119 samples\n",
            "Epoch 1/100, Train Loss: 0.1550, Val Loss: 0.2175\n",
            "Epoch 2/100, Train Loss: 0.1533, Val Loss: 0.1906\n",
            "Epoch 3/100, Train Loss: 0.1464, Val Loss: 0.1718\n",
            "Epoch 4/100, Train Loss: 0.1350, Val Loss: 0.1287\n",
            "Epoch 5/100, Train Loss: 0.1077, Val Loss: 0.1409\n",
            "Epoch 6/100, Train Loss: 0.1011, Val Loss: 0.0643\n",
            "Epoch 7/100, Train Loss: 0.0825, Val Loss: 0.0344\n",
            "Epoch 8/100, Train Loss: 0.0683, Val Loss: 0.0369\n",
            "Epoch 9/100, Train Loss: 0.0730, Val Loss: 0.0429\n",
            "Epoch 10/100, Train Loss: 0.0574, Val Loss: 0.0503\n",
            "Epoch 11/100, Train Loss: 0.0554, Val Loss: 0.0426\n",
            "Epoch 12/100, Train Loss: 0.0485, Val Loss: 0.0428\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 13/100, Train Loss: 0.0447, Val Loss: 0.0417\n",
            "Epoch 14/100, Train Loss: 0.0433, Val Loss: 0.0405\n",
            "Epoch 15/100, Train Loss: 0.0433, Val Loss: 0.0397\n",
            "Epoch 16/100, Train Loss: 0.0432, Val Loss: 0.0396\n",
            "Epoch 17/100, Train Loss: 0.0430, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0427, Val Loss: 0.0390\n",
            "Epoch 19/100, Train Loss: 0.0426, Val Loss: 0.0390\n",
            "Epoch 20/100, Train Loss: 0.0423, Val Loss: 0.0390\n",
            "Epoch 21/100, Train Loss: 0.0427, Val Loss: 0.0390\n",
            "Epoch 22/100, Train Loss: 0.0426, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0432, Val Loss: 0.0390\n",
            "Epoch 24/100, Train Loss: 0.0427, Val Loss: 0.0390\n",
            "Epoch 25/100, Train Loss: 0.0426, Val Loss: 0.0390\n",
            "Epoch 26/100, Train Loss: 0.0444, Val Loss: 0.0390\n",
            "Epoch 27/100, Train Loss: 0.0425, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0428, Val Loss: 0.0390\n",
            "Epoch 29/100, Train Loss: 0.0421, Val Loss: 0.0390\n",
            "Epoch 30/100, Train Loss: 0.0426, Val Loss: 0.0390\n",
            "Epoch 31/100, Train Loss: 0.0426, Val Loss: 0.0390\n",
            "Epoch 32/100, Train Loss: 0.0429, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0422, Val Loss: 0.0390\n",
            "Epoch 34/100, Train Loss: 0.0427, Val Loss: 0.0390\n",
            "Epoch 35/100, Train Loss: 0.0425, Val Loss: 0.0390\n",
            "Epoch 36/100, Train Loss: 0.0435, Val Loss: 0.0390\n",
            "Epoch 37/100, Train Loss: 0.0433, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0427, Val Loss: 0.0390\n",
            "Epoch 39/100, Train Loss: 0.0435, Val Loss: 0.0390\n",
            "Epoch 40/100, Train Loss: 0.0430, Val Loss: 0.0390\n",
            "Epoch 41/100, Train Loss: 0.0428, Val Loss: 0.0390\n",
            "Epoch 42/100, Train Loss: 0.0430, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0432, Val Loss: 0.0390\n",
            "Epoch 44/100, Train Loss: 0.0424, Val Loss: 0.0390\n",
            "Epoch 45/100, Train Loss: 0.0448, Val Loss: 0.0390\n",
            "Epoch 46/100, Train Loss: 0.0427, Val Loss: 0.0390\n",
            "Epoch 47/100, Train Loss: 0.0427, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0423, Val Loss: 0.0390\n",
            "Epoch 49/100, Train Loss: 0.0428, Val Loss: 0.0390\n",
            "Epoch 50/100, Train Loss: 0.0424, Val Loss: 0.0390\n",
            "Epoch 51/100, Train Loss: 0.0424, Val Loss: 0.0390\n",
            "Epoch 52/100, Train Loss: 0.0426, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0426, Val Loss: 0.0390\n",
            "Epoch 54/100, Train Loss: 0.0430, Val Loss: 0.0390\n",
            "Epoch 55/100, Train Loss: 0.0429, Val Loss: 0.0390\n",
            "Epoch 56/100, Train Loss: 0.0426, Val Loss: 0.0390\n",
            "Epoch 57/100, Train Loss: 0.0429, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0429, Val Loss: 0.0390\n",
            "Epoch 59/100, Train Loss: 0.0434, Val Loss: 0.0390\n",
            "Epoch 60/100, Train Loss: 0.0442, Val Loss: 0.0390\n",
            "Epoch 61/100, Train Loss: 0.0428, Val Loss: 0.0390\n",
            "Epoch 62/100, Train Loss: 0.0431, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0427, Val Loss: 0.0390\n",
            "Epoch 64/100, Train Loss: 0.0428, Val Loss: 0.0390\n",
            "Epoch 65/100, Train Loss: 0.0427, Val Loss: 0.0390\n",
            "Epoch 66/100, Train Loss: 0.0442, Val Loss: 0.0390\n",
            "Epoch 67/100, Train Loss: 0.0426, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0429, Val Loss: 0.0390\n",
            "Epoch 69/100, Train Loss: 0.0423, Val Loss: 0.0390\n",
            "Epoch 70/100, Train Loss: 0.0433, Val Loss: 0.0390\n",
            "Epoch 71/100, Train Loss: 0.0422, Val Loss: 0.0390\n",
            "Epoch 72/100, Train Loss: 0.0435, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0444, Val Loss: 0.0390\n",
            "Epoch 74/100, Train Loss: 0.0428, Val Loss: 0.0390\n",
            "Epoch 75/100, Train Loss: 0.0427, Val Loss: 0.0390\n",
            "Epoch 76/100, Train Loss: 0.0424, Val Loss: 0.0390\n",
            "Epoch 77/100, Train Loss: 0.0430, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0428, Val Loss: 0.0390\n",
            "Epoch 79/100, Train Loss: 0.0426, Val Loss: 0.0390\n",
            "Epoch 80/100, Train Loss: 0.0431, Val Loss: 0.0390\n",
            "Epoch 81/100, Train Loss: 0.0427, Val Loss: 0.0390\n",
            "Epoch 82/100, Train Loss: 0.0443, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0427, Val Loss: 0.0390\n",
            "Epoch 84/100, Train Loss: 0.0427, Val Loss: 0.0390\n",
            "Epoch 85/100, Train Loss: 0.0430, Val Loss: 0.0390\n",
            "Epoch 86/100, Train Loss: 0.0427, Val Loss: 0.0390\n",
            "Epoch 87/100, Train Loss: 0.0441, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0431, Val Loss: 0.0390\n",
            "Epoch 89/100, Train Loss: 0.0426, Val Loss: 0.0390\n",
            "Epoch 90/100, Train Loss: 0.0425, Val Loss: 0.0390\n",
            "Epoch 91/100, Train Loss: 0.0427, Val Loss: 0.0390\n",
            "Epoch 92/100, Train Loss: 0.0426, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0432, Val Loss: 0.0390\n",
            "Epoch 94/100, Train Loss: 0.0435, Val Loss: 0.0390\n",
            "Epoch 95/100, Train Loss: 0.0427, Val Loss: 0.0390\n",
            "Epoch 96/100, Train Loss: 0.0439, Val Loss: 0.0390\n",
            "Epoch 97/100, Train Loss: 0.0432, Val Loss: 0.0390\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0429, Val Loss: 0.0390\n",
            "Epoch 99/100, Train Loss: 0.0430, Val Loss: 0.0390\n",
            "Epoch 100/100, Train Loss: 0.0433, Val Loss: 0.0390\n",
            "\n",
            "Test iteration 24/28\n",
            "Current training set size: 120 samples\n",
            "Epoch 1/100, Train Loss: 0.1592, Val Loss: 0.1897\n",
            "Epoch 2/100, Train Loss: 0.1454, Val Loss: 0.1852\n",
            "Epoch 3/100, Train Loss: 0.1478, Val Loss: 0.1777\n",
            "Epoch 4/100, Train Loss: 0.1341, Val Loss: 0.1436\n",
            "Epoch 5/100, Train Loss: 0.1077, Val Loss: 0.1009\n",
            "Epoch 6/100, Train Loss: 0.0905, Val Loss: 0.0581\n",
            "Epoch 7/100, Train Loss: 0.0772, Val Loss: 0.0470\n",
            "Epoch 8/100, Train Loss: 0.0719, Val Loss: 0.0373\n",
            "Epoch 9/100, Train Loss: 0.0650, Val Loss: 0.0428\n",
            "Epoch 10/100, Train Loss: 0.0662, Val Loss: 0.0435\n",
            "Epoch 11/100, Train Loss: 0.0687, Val Loss: 0.0625\n",
            "Epoch 12/100, Train Loss: 0.0571, Val Loss: 0.0528\n",
            "Epoch 13/100, Train Loss: 0.0483, Val Loss: 0.0482\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 14/100, Train Loss: 0.0494, Val Loss: 0.0460\n",
            "Epoch 15/100, Train Loss: 0.0474, Val Loss: 0.0441\n",
            "Epoch 16/100, Train Loss: 0.0450, Val Loss: 0.0414\n",
            "Epoch 17/100, Train Loss: 0.0444, Val Loss: 0.0395\n",
            "Epoch 18/100, Train Loss: 0.0441, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0432, Val Loss: 0.0393\n",
            "Epoch 20/100, Train Loss: 0.0432, Val Loss: 0.0393\n",
            "Epoch 21/100, Train Loss: 0.0436, Val Loss: 0.0393\n",
            "Epoch 22/100, Train Loss: 0.0432, Val Loss: 0.0393\n",
            "Epoch 23/100, Train Loss: 0.0435, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0433, Val Loss: 0.0393\n",
            "Epoch 25/100, Train Loss: 0.0430, Val Loss: 0.0393\n",
            "Epoch 26/100, Train Loss: 0.0431, Val Loss: 0.0393\n",
            "Epoch 27/100, Train Loss: 0.0435, Val Loss: 0.0393\n",
            "Epoch 28/100, Train Loss: 0.0433, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0436, Val Loss: 0.0393\n",
            "Epoch 30/100, Train Loss: 0.0431, Val Loss: 0.0393\n",
            "Epoch 31/100, Train Loss: 0.0426, Val Loss: 0.0393\n",
            "Epoch 32/100, Train Loss: 0.0435, Val Loss: 0.0393\n",
            "Epoch 33/100, Train Loss: 0.0443, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0435, Val Loss: 0.0393\n",
            "Epoch 35/100, Train Loss: 0.0437, Val Loss: 0.0393\n",
            "Epoch 36/100, Train Loss: 0.0432, Val Loss: 0.0393\n",
            "Epoch 37/100, Train Loss: 0.0438, Val Loss: 0.0393\n",
            "Epoch 38/100, Train Loss: 0.0442, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0437, Val Loss: 0.0393\n",
            "Epoch 40/100, Train Loss: 0.0433, Val Loss: 0.0393\n",
            "Epoch 41/100, Train Loss: 0.0435, Val Loss: 0.0393\n",
            "Epoch 42/100, Train Loss: 0.0429, Val Loss: 0.0393\n",
            "Epoch 43/100, Train Loss: 0.0445, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0435, Val Loss: 0.0393\n",
            "Epoch 45/100, Train Loss: 0.0431, Val Loss: 0.0393\n",
            "Epoch 46/100, Train Loss: 0.0433, Val Loss: 0.0393\n",
            "Epoch 47/100, Train Loss: 0.0431, Val Loss: 0.0393\n",
            "Epoch 48/100, Train Loss: 0.0433, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0435, Val Loss: 0.0393\n",
            "Epoch 50/100, Train Loss: 0.0432, Val Loss: 0.0393\n",
            "Epoch 51/100, Train Loss: 0.0440, Val Loss: 0.0393\n",
            "Epoch 52/100, Train Loss: 0.0439, Val Loss: 0.0393\n",
            "Epoch 53/100, Train Loss: 0.0434, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0433, Val Loss: 0.0393\n",
            "Epoch 55/100, Train Loss: 0.0437, Val Loss: 0.0393\n",
            "Epoch 56/100, Train Loss: 0.0434, Val Loss: 0.0393\n",
            "Epoch 57/100, Train Loss: 0.0436, Val Loss: 0.0393\n",
            "Epoch 58/100, Train Loss: 0.0438, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0433, Val Loss: 0.0393\n",
            "Epoch 60/100, Train Loss: 0.0435, Val Loss: 0.0393\n",
            "Epoch 61/100, Train Loss: 0.0435, Val Loss: 0.0393\n",
            "Epoch 62/100, Train Loss: 0.0435, Val Loss: 0.0393\n",
            "Epoch 63/100, Train Loss: 0.0434, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0433, Val Loss: 0.0393\n",
            "Epoch 65/100, Train Loss: 0.0435, Val Loss: 0.0393\n",
            "Epoch 66/100, Train Loss: 0.0435, Val Loss: 0.0393\n",
            "Epoch 67/100, Train Loss: 0.0434, Val Loss: 0.0393\n",
            "Epoch 68/100, Train Loss: 0.0438, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0431, Val Loss: 0.0393\n",
            "Epoch 70/100, Train Loss: 0.0434, Val Loss: 0.0393\n",
            "Epoch 71/100, Train Loss: 0.0434, Val Loss: 0.0393\n",
            "Epoch 72/100, Train Loss: 0.0432, Val Loss: 0.0393\n",
            "Epoch 73/100, Train Loss: 0.0430, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0432, Val Loss: 0.0393\n",
            "Epoch 75/100, Train Loss: 0.0435, Val Loss: 0.0393\n",
            "Epoch 76/100, Train Loss: 0.0434, Val Loss: 0.0393\n",
            "Epoch 77/100, Train Loss: 0.0438, Val Loss: 0.0393\n",
            "Epoch 78/100, Train Loss: 0.0429, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0437, Val Loss: 0.0393\n",
            "Epoch 80/100, Train Loss: 0.0431, Val Loss: 0.0393\n",
            "Epoch 81/100, Train Loss: 0.0434, Val Loss: 0.0393\n",
            "Epoch 82/100, Train Loss: 0.0434, Val Loss: 0.0393\n",
            "Epoch 83/100, Train Loss: 0.0434, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0434, Val Loss: 0.0393\n",
            "Epoch 85/100, Train Loss: 0.0440, Val Loss: 0.0393\n",
            "Epoch 86/100, Train Loss: 0.0434, Val Loss: 0.0393\n",
            "Epoch 87/100, Train Loss: 0.0434, Val Loss: 0.0393\n",
            "Epoch 88/100, Train Loss: 0.0439, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0429, Val Loss: 0.0393\n",
            "Epoch 90/100, Train Loss: 0.0434, Val Loss: 0.0393\n",
            "Epoch 91/100, Train Loss: 0.0432, Val Loss: 0.0393\n",
            "Epoch 92/100, Train Loss: 0.0433, Val Loss: 0.0393\n",
            "Epoch 93/100, Train Loss: 0.0434, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0436, Val Loss: 0.0393\n",
            "Epoch 95/100, Train Loss: 0.0435, Val Loss: 0.0393\n",
            "Epoch 96/100, Train Loss: 0.0433, Val Loss: 0.0393\n",
            "Epoch 97/100, Train Loss: 0.0430, Val Loss: 0.0393\n",
            "Epoch 98/100, Train Loss: 0.0431, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0437, Val Loss: 0.0393\n",
            "Epoch 100/100, Train Loss: 0.0430, Val Loss: 0.0393\n",
            "\n",
            "Test iteration 25/28\n",
            "Current training set size: 121 samples\n",
            "Epoch 1/100, Train Loss: 0.1716, Val Loss: 0.1904\n",
            "Epoch 2/100, Train Loss: 0.1561, Val Loss: 0.1879\n",
            "Epoch 3/100, Train Loss: 0.1397, Val Loss: 0.1902\n",
            "Epoch 4/100, Train Loss: 0.1377, Val Loss: 0.1635\n",
            "Epoch 5/100, Train Loss: 0.1143, Val Loss: 0.0823\n",
            "Epoch 6/100, Train Loss: 0.0911, Val Loss: 0.0476\n",
            "Epoch 7/100, Train Loss: 0.0798, Val Loss: 0.0409\n",
            "Epoch 8/100, Train Loss: 0.0747, Val Loss: 0.0518\n",
            "Epoch 9/100, Train Loss: 0.0678, Val Loss: 0.0385\n",
            "Epoch 10/100, Train Loss: 0.0583, Val Loss: 0.0435\n",
            "Epoch 11/100, Train Loss: 0.0600, Val Loss: 0.0456\n",
            "Epoch 12/100, Train Loss: 0.0531, Val Loss: 0.0489\n",
            "Epoch 13/100, Train Loss: 0.0531, Val Loss: 0.0570\n",
            "Epoch 14/100, Train Loss: 0.0609, Val Loss: 0.0440\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 15/100, Train Loss: 0.0438, Val Loss: 0.0431\n",
            "Epoch 16/100, Train Loss: 0.0425, Val Loss: 0.0420\n",
            "Epoch 17/100, Train Loss: 0.0412, Val Loss: 0.0415\n",
            "Epoch 18/100, Train Loss: 0.0414, Val Loss: 0.0415\n",
            "Epoch 19/100, Train Loss: 0.0419, Val Loss: 0.0414\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0422, Val Loss: 0.0414\n",
            "Epoch 21/100, Train Loss: 0.0414, Val Loss: 0.0414\n",
            "Epoch 22/100, Train Loss: 0.0424, Val Loss: 0.0414\n",
            "Epoch 23/100, Train Loss: 0.0408, Val Loss: 0.0414\n",
            "Epoch 24/100, Train Loss: 0.0414, Val Loss: 0.0414\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0416, Val Loss: 0.0414\n",
            "Epoch 26/100, Train Loss: 0.0403, Val Loss: 0.0414\n",
            "Epoch 27/100, Train Loss: 0.0414, Val Loss: 0.0414\n",
            "Epoch 28/100, Train Loss: 0.0411, Val Loss: 0.0414\n",
            "Epoch 29/100, Train Loss: 0.0417, Val Loss: 0.0414\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0412, Val Loss: 0.0414\n",
            "Epoch 31/100, Train Loss: 0.0419, Val Loss: 0.0414\n",
            "Epoch 32/100, Train Loss: 0.0411, Val Loss: 0.0414\n",
            "Epoch 33/100, Train Loss: 0.0445, Val Loss: 0.0414\n",
            "Epoch 34/100, Train Loss: 0.0428, Val Loss: 0.0414\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0408, Val Loss: 0.0414\n",
            "Epoch 36/100, Train Loss: 0.0411, Val Loss: 0.0414\n",
            "Epoch 37/100, Train Loss: 0.0447, Val Loss: 0.0414\n",
            "Epoch 38/100, Train Loss: 0.0407, Val Loss: 0.0414\n",
            "Epoch 39/100, Train Loss: 0.0406, Val Loss: 0.0414\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0410, Val Loss: 0.0414\n",
            "Epoch 41/100, Train Loss: 0.0424, Val Loss: 0.0414\n",
            "Epoch 42/100, Train Loss: 0.0531, Val Loss: 0.0414\n",
            "Epoch 43/100, Train Loss: 0.0410, Val Loss: 0.0414\n",
            "Epoch 44/100, Train Loss: 0.0417, Val Loss: 0.0414\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0420, Val Loss: 0.0414\n",
            "Epoch 46/100, Train Loss: 0.0405, Val Loss: 0.0414\n",
            "Epoch 47/100, Train Loss: 0.0418, Val Loss: 0.0414\n",
            "Epoch 48/100, Train Loss: 0.0418, Val Loss: 0.0414\n",
            "Epoch 49/100, Train Loss: 0.0406, Val Loss: 0.0414\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0524, Val Loss: 0.0414\n",
            "Epoch 51/100, Train Loss: 0.0411, Val Loss: 0.0414\n",
            "Epoch 52/100, Train Loss: 0.0415, Val Loss: 0.0414\n",
            "Epoch 53/100, Train Loss: 0.0406, Val Loss: 0.0414\n",
            "Epoch 54/100, Train Loss: 0.0413, Val Loss: 0.0414\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0414, Val Loss: 0.0414\n",
            "Epoch 56/100, Train Loss: 0.0411, Val Loss: 0.0414\n",
            "Epoch 57/100, Train Loss: 0.0541, Val Loss: 0.0414\n",
            "Epoch 58/100, Train Loss: 0.0426, Val Loss: 0.0414\n",
            "Epoch 59/100, Train Loss: 0.0410, Val Loss: 0.0414\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0406, Val Loss: 0.0414\n",
            "Epoch 61/100, Train Loss: 0.0409, Val Loss: 0.0414\n",
            "Epoch 62/100, Train Loss: 0.0419, Val Loss: 0.0414\n",
            "Epoch 63/100, Train Loss: 0.0414, Val Loss: 0.0414\n",
            "Epoch 64/100, Train Loss: 0.0415, Val Loss: 0.0414\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0413, Val Loss: 0.0414\n",
            "Epoch 66/100, Train Loss: 0.0412, Val Loss: 0.0414\n",
            "Epoch 67/100, Train Loss: 0.0415, Val Loss: 0.0414\n",
            "Epoch 68/100, Train Loss: 0.0449, Val Loss: 0.0414\n",
            "Epoch 69/100, Train Loss: 0.0418, Val Loss: 0.0414\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0416, Val Loss: 0.0414\n",
            "Epoch 71/100, Train Loss: 0.0423, Val Loss: 0.0414\n",
            "Epoch 72/100, Train Loss: 0.0413, Val Loss: 0.0414\n",
            "Epoch 73/100, Train Loss: 0.0415, Val Loss: 0.0414\n",
            "Epoch 74/100, Train Loss: 0.0414, Val Loss: 0.0414\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0413, Val Loss: 0.0414\n",
            "Epoch 76/100, Train Loss: 0.0425, Val Loss: 0.0414\n",
            "Epoch 77/100, Train Loss: 0.0416, Val Loss: 0.0414\n",
            "Epoch 78/100, Train Loss: 0.0542, Val Loss: 0.0414\n",
            "Epoch 79/100, Train Loss: 0.0413, Val Loss: 0.0414\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0419, Val Loss: 0.0414\n",
            "Epoch 81/100, Train Loss: 0.0416, Val Loss: 0.0414\n",
            "Epoch 82/100, Train Loss: 0.0409, Val Loss: 0.0414\n",
            "Epoch 83/100, Train Loss: 0.0403, Val Loss: 0.0414\n",
            "Epoch 84/100, Train Loss: 0.0411, Val Loss: 0.0414\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0416, Val Loss: 0.0414\n",
            "Epoch 86/100, Train Loss: 0.0412, Val Loss: 0.0414\n",
            "Epoch 87/100, Train Loss: 0.0402, Val Loss: 0.0414\n",
            "Epoch 88/100, Train Loss: 0.0404, Val Loss: 0.0414\n",
            "Epoch 89/100, Train Loss: 0.0410, Val Loss: 0.0414\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0416, Val Loss: 0.0414\n",
            "Epoch 91/100, Train Loss: 0.0426, Val Loss: 0.0414\n",
            "Epoch 92/100, Train Loss: 0.0419, Val Loss: 0.0414\n",
            "Epoch 93/100, Train Loss: 0.0410, Val Loss: 0.0414\n",
            "Epoch 94/100, Train Loss: 0.0412, Val Loss: 0.0414\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0423, Val Loss: 0.0414\n",
            "Epoch 96/100, Train Loss: 0.0408, Val Loss: 0.0414\n",
            "Epoch 97/100, Train Loss: 0.0413, Val Loss: 0.0414\n",
            "Epoch 98/100, Train Loss: 0.0405, Val Loss: 0.0414\n",
            "Epoch 99/100, Train Loss: 0.0407, Val Loss: 0.0414\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0411, Val Loss: 0.0414\n",
            "\n",
            "Test iteration 26/28\n",
            "Current training set size: 122 samples\n",
            "Epoch 1/100, Train Loss: 0.1537, Val Loss: 0.2032\n",
            "Epoch 2/100, Train Loss: 0.1475, Val Loss: 0.1841\n",
            "Epoch 3/100, Train Loss: 0.1518, Val Loss: 0.1751\n",
            "Epoch 4/100, Train Loss: 0.1278, Val Loss: 0.1269\n",
            "Epoch 5/100, Train Loss: 0.0943, Val Loss: 0.0389\n",
            "Epoch 6/100, Train Loss: 0.0791, Val Loss: 0.0324\n",
            "Epoch 7/100, Train Loss: 0.0765, Val Loss: 0.0604\n",
            "Epoch 8/100, Train Loss: 0.0658, Val Loss: 0.0458\n",
            "Epoch 9/100, Train Loss: 0.0628, Val Loss: 0.0382\n",
            "Epoch 10/100, Train Loss: 0.0558, Val Loss: 0.0417\n",
            "Epoch 11/100, Train Loss: 0.0512, Val Loss: 0.0627\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 12/100, Train Loss: 0.0667, Val Loss: 0.0502\n",
            "Epoch 13/100, Train Loss: 0.0495, Val Loss: 0.0391\n",
            "Epoch 14/100, Train Loss: 0.0453, Val Loss: 0.0366\n",
            "Epoch 15/100, Train Loss: 0.0465, Val Loss: 0.0362\n",
            "Epoch 16/100, Train Loss: 0.0447, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0443, Val Loss: 0.0356\n",
            "Epoch 18/100, Train Loss: 0.0429, Val Loss: 0.0356\n",
            "Epoch 19/100, Train Loss: 0.0432, Val Loss: 0.0356\n",
            "Epoch 20/100, Train Loss: 0.0436, Val Loss: 0.0356\n",
            "Epoch 21/100, Train Loss: 0.0431, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0431, Val Loss: 0.0356\n",
            "Epoch 23/100, Train Loss: 0.0439, Val Loss: 0.0356\n",
            "Epoch 24/100, Train Loss: 0.0438, Val Loss: 0.0356\n",
            "Epoch 25/100, Train Loss: 0.0437, Val Loss: 0.0356\n",
            "Epoch 26/100, Train Loss: 0.0459, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0429, Val Loss: 0.0356\n",
            "Epoch 28/100, Train Loss: 0.0428, Val Loss: 0.0356\n",
            "Epoch 29/100, Train Loss: 0.0434, Val Loss: 0.0356\n",
            "Epoch 30/100, Train Loss: 0.0437, Val Loss: 0.0356\n",
            "Epoch 31/100, Train Loss: 0.0436, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0433, Val Loss: 0.0356\n",
            "Epoch 33/100, Train Loss: 0.0448, Val Loss: 0.0356\n",
            "Epoch 34/100, Train Loss: 0.0430, Val Loss: 0.0356\n",
            "Epoch 35/100, Train Loss: 0.0429, Val Loss: 0.0356\n",
            "Epoch 36/100, Train Loss: 0.0525, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0432, Val Loss: 0.0356\n",
            "Epoch 38/100, Train Loss: 0.0435, Val Loss: 0.0356\n",
            "Epoch 39/100, Train Loss: 0.0436, Val Loss: 0.0356\n",
            "Epoch 40/100, Train Loss: 0.0435, Val Loss: 0.0356\n",
            "Epoch 41/100, Train Loss: 0.0430, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0437, Val Loss: 0.0356\n",
            "Epoch 43/100, Train Loss: 0.0438, Val Loss: 0.0356\n",
            "Epoch 44/100, Train Loss: 0.0442, Val Loss: 0.0356\n",
            "Epoch 45/100, Train Loss: 0.0437, Val Loss: 0.0356\n",
            "Epoch 46/100, Train Loss: 0.0432, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0434, Val Loss: 0.0356\n",
            "Epoch 48/100, Train Loss: 0.0436, Val Loss: 0.0356\n",
            "Epoch 49/100, Train Loss: 0.0436, Val Loss: 0.0356\n",
            "Epoch 50/100, Train Loss: 0.0431, Val Loss: 0.0356\n",
            "Epoch 51/100, Train Loss: 0.0482, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0429, Val Loss: 0.0356\n",
            "Epoch 53/100, Train Loss: 0.0436, Val Loss: 0.0356\n",
            "Epoch 54/100, Train Loss: 0.0435, Val Loss: 0.0356\n",
            "Epoch 55/100, Train Loss: 0.0437, Val Loss: 0.0356\n",
            "Epoch 56/100, Train Loss: 0.0428, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0438, Val Loss: 0.0356\n",
            "Epoch 58/100, Train Loss: 0.0437, Val Loss: 0.0356\n",
            "Epoch 59/100, Train Loss: 0.0433, Val Loss: 0.0356\n",
            "Epoch 60/100, Train Loss: 0.0440, Val Loss: 0.0356\n",
            "Epoch 61/100, Train Loss: 0.0431, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0430, Val Loss: 0.0356\n",
            "Epoch 63/100, Train Loss: 0.0427, Val Loss: 0.0356\n",
            "Epoch 64/100, Train Loss: 0.0438, Val Loss: 0.0356\n",
            "Epoch 65/100, Train Loss: 0.0436, Val Loss: 0.0356\n",
            "Epoch 66/100, Train Loss: 0.0435, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0436, Val Loss: 0.0356\n",
            "Epoch 68/100, Train Loss: 0.0442, Val Loss: 0.0356\n",
            "Epoch 69/100, Train Loss: 0.0429, Val Loss: 0.0356\n",
            "Epoch 70/100, Train Loss: 0.0436, Val Loss: 0.0356\n",
            "Epoch 71/100, Train Loss: 0.0439, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0430, Val Loss: 0.0356\n",
            "Epoch 73/100, Train Loss: 0.0437, Val Loss: 0.0356\n",
            "Epoch 74/100, Train Loss: 0.0432, Val Loss: 0.0356\n",
            "Epoch 75/100, Train Loss: 0.0430, Val Loss: 0.0356\n",
            "Epoch 76/100, Train Loss: 0.0440, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0431, Val Loss: 0.0356\n",
            "Epoch 78/100, Train Loss: 0.0432, Val Loss: 0.0356\n",
            "Epoch 79/100, Train Loss: 0.0437, Val Loss: 0.0356\n",
            "Epoch 80/100, Train Loss: 0.0435, Val Loss: 0.0356\n",
            "Epoch 81/100, Train Loss: 0.0436, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0434, Val Loss: 0.0356\n",
            "Epoch 83/100, Train Loss: 0.0437, Val Loss: 0.0356\n",
            "Epoch 84/100, Train Loss: 0.0430, Val Loss: 0.0356\n",
            "Epoch 85/100, Train Loss: 0.0443, Val Loss: 0.0356\n",
            "Epoch 86/100, Train Loss: 0.0434, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0484, Val Loss: 0.0356\n",
            "Epoch 88/100, Train Loss: 0.0432, Val Loss: 0.0356\n",
            "Epoch 89/100, Train Loss: 0.0430, Val Loss: 0.0356\n",
            "Epoch 90/100, Train Loss: 0.0430, Val Loss: 0.0356\n",
            "Epoch 91/100, Train Loss: 0.0437, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0444, Val Loss: 0.0356\n",
            "Epoch 93/100, Train Loss: 0.0435, Val Loss: 0.0356\n",
            "Epoch 94/100, Train Loss: 0.0431, Val Loss: 0.0356\n",
            "Epoch 95/100, Train Loss: 0.0432, Val Loss: 0.0356\n",
            "Epoch 96/100, Train Loss: 0.0431, Val Loss: 0.0356\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0431, Val Loss: 0.0356\n",
            "Epoch 98/100, Train Loss: 0.0435, Val Loss: 0.0356\n",
            "Epoch 99/100, Train Loss: 0.0434, Val Loss: 0.0356\n",
            "Epoch 100/100, Train Loss: 0.0433, Val Loss: 0.0356\n",
            "\n",
            "Test iteration 27/28\n",
            "Current training set size: 123 samples\n",
            "Epoch 1/100, Train Loss: 0.1491, Val Loss: 0.1870\n",
            "Epoch 2/100, Train Loss: 0.1482, Val Loss: 0.1906\n",
            "Epoch 3/100, Train Loss: 0.1406, Val Loss: 0.1628\n",
            "Epoch 4/100, Train Loss: 0.1269, Val Loss: 0.0987\n",
            "Epoch 5/100, Train Loss: 0.0907, Val Loss: 0.0622\n",
            "Epoch 6/100, Train Loss: 0.0781, Val Loss: 0.0893\n",
            "Epoch 7/100, Train Loss: 0.0720, Val Loss: 0.0619\n",
            "Epoch 8/100, Train Loss: 0.0662, Val Loss: 0.0479\n",
            "Epoch 9/100, Train Loss: 0.0626, Val Loss: 0.0536\n",
            "Epoch 10/100, Train Loss: 0.0550, Val Loss: 0.0606\n",
            "Epoch 11/100, Train Loss: 0.0580, Val Loss: 0.0691\n",
            "Epoch 12/100, Train Loss: 0.0586, Val Loss: 0.0509\n",
            "Epoch 13/100, Train Loss: 0.0486, Val Loss: 0.0512\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 14/100, Train Loss: 0.0483, Val Loss: 0.0457\n",
            "Epoch 15/100, Train Loss: 0.0444, Val Loss: 0.0418\n",
            "Epoch 16/100, Train Loss: 0.0421, Val Loss: 0.0404\n",
            "Epoch 17/100, Train Loss: 0.0412, Val Loss: 0.0392\n",
            "Epoch 18/100, Train Loss: 0.0410, Val Loss: 0.0389\n",
            "Epoch 19/100, Train Loss: 0.0413, Val Loss: 0.0389\n",
            "Epoch 20/100, Train Loss: 0.0409, Val Loss: 0.0385\n",
            "Epoch 21/100, Train Loss: 0.0414, Val Loss: 0.0385\n",
            "Epoch 22/100, Train Loss: 0.0410, Val Loss: 0.0383\n",
            "Epoch 23/100, Train Loss: 0.0400, Val Loss: 0.0389\n",
            "Epoch 24/100, Train Loss: 0.0407, Val Loss: 0.0390\n",
            "Epoch 25/100, Train Loss: 0.0398, Val Loss: 0.0390\n",
            "Epoch 26/100, Train Loss: 0.0410, Val Loss: 0.0388\n",
            "Epoch 27/100, Train Loss: 0.0398, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0401, Val Loss: 0.0389\n",
            "Epoch 29/100, Train Loss: 0.0398, Val Loss: 0.0389\n",
            "Epoch 30/100, Train Loss: 0.0402, Val Loss: 0.0389\n",
            "Epoch 31/100, Train Loss: 0.0403, Val Loss: 0.0389\n",
            "Epoch 32/100, Train Loss: 0.0396, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0404, Val Loss: 0.0389\n",
            "Epoch 34/100, Train Loss: 0.0399, Val Loss: 0.0389\n",
            "Epoch 35/100, Train Loss: 0.0402, Val Loss: 0.0389\n",
            "Epoch 36/100, Train Loss: 0.0403, Val Loss: 0.0389\n",
            "Epoch 37/100, Train Loss: 0.0402, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0401, Val Loss: 0.0389\n",
            "Epoch 39/100, Train Loss: 0.0407, Val Loss: 0.0389\n",
            "Epoch 40/100, Train Loss: 0.0394, Val Loss: 0.0389\n",
            "Epoch 41/100, Train Loss: 0.0414, Val Loss: 0.0389\n",
            "Epoch 42/100, Train Loss: 0.0402, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0399, Val Loss: 0.0389\n",
            "Epoch 44/100, Train Loss: 0.0396, Val Loss: 0.0389\n",
            "Epoch 45/100, Train Loss: 0.0414, Val Loss: 0.0389\n",
            "Epoch 46/100, Train Loss: 0.0400, Val Loss: 0.0389\n",
            "Epoch 47/100, Train Loss: 0.0396, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0402, Val Loss: 0.0389\n",
            "Epoch 49/100, Train Loss: 0.0397, Val Loss: 0.0389\n",
            "Epoch 50/100, Train Loss: 0.0405, Val Loss: 0.0389\n",
            "Epoch 51/100, Train Loss: 0.0396, Val Loss: 0.0389\n",
            "Epoch 52/100, Train Loss: 0.0400, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0399, Val Loss: 0.0389\n",
            "Epoch 54/100, Train Loss: 0.0405, Val Loss: 0.0389\n",
            "Epoch 55/100, Train Loss: 0.0403, Val Loss: 0.0389\n",
            "Epoch 56/100, Train Loss: 0.0401, Val Loss: 0.0389\n",
            "Epoch 57/100, Train Loss: 0.0400, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0406, Val Loss: 0.0389\n",
            "Epoch 59/100, Train Loss: 0.0397, Val Loss: 0.0389\n",
            "Epoch 60/100, Train Loss: 0.0397, Val Loss: 0.0389\n",
            "Epoch 61/100, Train Loss: 0.0397, Val Loss: 0.0389\n",
            "Epoch 62/100, Train Loss: 0.0401, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0397, Val Loss: 0.0389\n",
            "Epoch 64/100, Train Loss: 0.0399, Val Loss: 0.0389\n",
            "Epoch 65/100, Train Loss: 0.0401, Val Loss: 0.0389\n",
            "Epoch 66/100, Train Loss: 0.0396, Val Loss: 0.0389\n",
            "Epoch 67/100, Train Loss: 0.0401, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0407, Val Loss: 0.0389\n",
            "Epoch 69/100, Train Loss: 0.0401, Val Loss: 0.0389\n",
            "Epoch 70/100, Train Loss: 0.0399, Val Loss: 0.0389\n",
            "Epoch 71/100, Train Loss: 0.0402, Val Loss: 0.0389\n",
            "Epoch 72/100, Train Loss: 0.0398, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0403, Val Loss: 0.0389\n",
            "Epoch 74/100, Train Loss: 0.0399, Val Loss: 0.0389\n",
            "Epoch 75/100, Train Loss: 0.0400, Val Loss: 0.0389\n",
            "Epoch 76/100, Train Loss: 0.0399, Val Loss: 0.0389\n",
            "Epoch 77/100, Train Loss: 0.0402, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0401, Val Loss: 0.0389\n",
            "Epoch 79/100, Train Loss: 0.0399, Val Loss: 0.0389\n",
            "Epoch 80/100, Train Loss: 0.0401, Val Loss: 0.0389\n",
            "Epoch 81/100, Train Loss: 0.0400, Val Loss: 0.0389\n",
            "Epoch 82/100, Train Loss: 0.0397, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0395, Val Loss: 0.0389\n",
            "Epoch 84/100, Train Loss: 0.0406, Val Loss: 0.0389\n",
            "Epoch 85/100, Train Loss: 0.0396, Val Loss: 0.0389\n",
            "Epoch 86/100, Train Loss: 0.0401, Val Loss: 0.0389\n",
            "Epoch 87/100, Train Loss: 0.0403, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0396, Val Loss: 0.0389\n",
            "Epoch 89/100, Train Loss: 0.0397, Val Loss: 0.0389\n",
            "Epoch 90/100, Train Loss: 0.0406, Val Loss: 0.0389\n",
            "Epoch 91/100, Train Loss: 0.0397, Val Loss: 0.0389\n",
            "Epoch 92/100, Train Loss: 0.0399, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0397, Val Loss: 0.0389\n",
            "Epoch 94/100, Train Loss: 0.0406, Val Loss: 0.0389\n",
            "Epoch 95/100, Train Loss: 0.0401, Val Loss: 0.0389\n",
            "Epoch 96/100, Train Loss: 0.0402, Val Loss: 0.0389\n",
            "Epoch 97/100, Train Loss: 0.0408, Val Loss: 0.0389\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0402, Val Loss: 0.0389\n",
            "Epoch 99/100, Train Loss: 0.0399, Val Loss: 0.0389\n",
            "Epoch 100/100, Train Loss: 0.0400, Val Loss: 0.0389\n",
            "\n",
            "Test iteration 28/28\n",
            "Current training set size: 124 samples\n",
            "Epoch 1/100, Train Loss: 0.1664, Val Loss: 0.1903\n",
            "Epoch 2/100, Train Loss: 0.1463, Val Loss: 0.1832\n",
            "Epoch 3/100, Train Loss: 0.1387, Val Loss: 0.1327\n",
            "Epoch 4/100, Train Loss: 0.1356, Val Loss: 0.1779\n",
            "Epoch 5/100, Train Loss: 0.1254, Val Loss: 0.1062\n",
            "Epoch 6/100, Train Loss: 0.0943, Val Loss: 0.0697\n",
            "Epoch 7/100, Train Loss: 0.0835, Val Loss: 0.0821\n",
            "Epoch 8/100, Train Loss: 0.0743, Val Loss: 0.0574\n",
            "Epoch 9/100, Train Loss: 0.0717, Val Loss: 0.0427\n",
            "Epoch 10/100, Train Loss: 0.0774, Val Loss: 0.0440\n",
            "Epoch 11/100, Train Loss: 0.0650, Val Loss: 0.0581\n",
            "Epoch 12/100, Train Loss: 0.0579, Val Loss: 0.0539\n",
            "Epoch 13/100, Train Loss: 0.0564, Val Loss: 0.0384\n",
            "Epoch 14/100, Train Loss: 0.0463, Val Loss: 0.0439\n",
            "Epoch 15/100, Train Loss: 0.0532, Val Loss: 0.0398\n",
            "Epoch 16/100, Train Loss: 0.0446, Val Loss: 0.0435\n",
            "Epoch 17/100, Train Loss: 0.0434, Val Loss: 0.0548\n",
            "Epoch 18/100, Train Loss: 0.0481, Val Loss: 0.0654\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0551, Val Loss: 0.0558\n",
            "Epoch 20/100, Train Loss: 0.0463, Val Loss: 0.0479\n",
            "Epoch 21/100, Train Loss: 0.0421, Val Loss: 0.0449\n",
            "Epoch 22/100, Train Loss: 0.0399, Val Loss: 0.0441\n",
            "Epoch 23/100, Train Loss: 0.0394, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0390, Val Loss: 0.0437\n",
            "Epoch 25/100, Train Loss: 0.0388, Val Loss: 0.0437\n",
            "Epoch 26/100, Train Loss: 0.0395, Val Loss: 0.0437\n",
            "Epoch 27/100, Train Loss: 0.0390, Val Loss: 0.0437\n",
            "Epoch 28/100, Train Loss: 0.0392, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0394, Val Loss: 0.0437\n",
            "Epoch 30/100, Train Loss: 0.0393, Val Loss: 0.0437\n",
            "Epoch 31/100, Train Loss: 0.0387, Val Loss: 0.0437\n",
            "Epoch 32/100, Train Loss: 0.0395, Val Loss: 0.0437\n",
            "Epoch 33/100, Train Loss: 0.0390, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0396, Val Loss: 0.0437\n",
            "Epoch 35/100, Train Loss: 0.0389, Val Loss: 0.0437\n",
            "Epoch 36/100, Train Loss: 0.0392, Val Loss: 0.0437\n",
            "Epoch 37/100, Train Loss: 0.0388, Val Loss: 0.0437\n",
            "Epoch 38/100, Train Loss: 0.0393, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0398, Val Loss: 0.0437\n",
            "Epoch 40/100, Train Loss: 0.0389, Val Loss: 0.0437\n",
            "Epoch 41/100, Train Loss: 0.0397, Val Loss: 0.0437\n",
            "Epoch 42/100, Train Loss: 0.0391, Val Loss: 0.0437\n",
            "Epoch 43/100, Train Loss: 0.0393, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0400, Val Loss: 0.0437\n",
            "Epoch 45/100, Train Loss: 0.0393, Val Loss: 0.0437\n",
            "Epoch 46/100, Train Loss: 0.0394, Val Loss: 0.0437\n",
            "Epoch 47/100, Train Loss: 0.0391, Val Loss: 0.0437\n",
            "Epoch 48/100, Train Loss: 0.0390, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0389, Val Loss: 0.0437\n",
            "Epoch 50/100, Train Loss: 0.0387, Val Loss: 0.0437\n",
            "Epoch 51/100, Train Loss: 0.0400, Val Loss: 0.0437\n",
            "Epoch 52/100, Train Loss: 0.0394, Val Loss: 0.0437\n",
            "Epoch 53/100, Train Loss: 0.0389, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0393, Val Loss: 0.0437\n",
            "Epoch 55/100, Train Loss: 0.0395, Val Loss: 0.0437\n",
            "Epoch 56/100, Train Loss: 0.0389, Val Loss: 0.0437\n",
            "Epoch 57/100, Train Loss: 0.0383, Val Loss: 0.0437\n",
            "Epoch 58/100, Train Loss: 0.0393, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0395, Val Loss: 0.0437\n",
            "Epoch 60/100, Train Loss: 0.0393, Val Loss: 0.0437\n",
            "Epoch 61/100, Train Loss: 0.0397, Val Loss: 0.0437\n",
            "Epoch 62/100, Train Loss: 0.0387, Val Loss: 0.0437\n",
            "Epoch 63/100, Train Loss: 0.0390, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0385, Val Loss: 0.0437\n",
            "Epoch 65/100, Train Loss: 0.0395, Val Loss: 0.0437\n",
            "Epoch 66/100, Train Loss: 0.0386, Val Loss: 0.0437\n",
            "Epoch 67/100, Train Loss: 0.0390, Val Loss: 0.0437\n",
            "Epoch 68/100, Train Loss: 0.0387, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0386, Val Loss: 0.0437\n",
            "Epoch 70/100, Train Loss: 0.0389, Val Loss: 0.0437\n",
            "Epoch 71/100, Train Loss: 0.0397, Val Loss: 0.0437\n",
            "Epoch 72/100, Train Loss: 0.0396, Val Loss: 0.0437\n",
            "Epoch 73/100, Train Loss: 0.0394, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0393, Val Loss: 0.0437\n",
            "Epoch 75/100, Train Loss: 0.0394, Val Loss: 0.0437\n",
            "Epoch 76/100, Train Loss: 0.0390, Val Loss: 0.0437\n",
            "Epoch 77/100, Train Loss: 0.0394, Val Loss: 0.0437\n",
            "Epoch 78/100, Train Loss: 0.0391, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0393, Val Loss: 0.0437\n",
            "Epoch 80/100, Train Loss: 0.0394, Val Loss: 0.0437\n",
            "Epoch 81/100, Train Loss: 0.0389, Val Loss: 0.0437\n",
            "Epoch 82/100, Train Loss: 0.0393, Val Loss: 0.0437\n",
            "Epoch 83/100, Train Loss: 0.0389, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0385, Val Loss: 0.0437\n",
            "Epoch 85/100, Train Loss: 0.0387, Val Loss: 0.0437\n",
            "Epoch 86/100, Train Loss: 0.0397, Val Loss: 0.0437\n",
            "Epoch 87/100, Train Loss: 0.0393, Val Loss: 0.0437\n",
            "Epoch 88/100, Train Loss: 0.0398, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0393, Val Loss: 0.0437\n",
            "Epoch 90/100, Train Loss: 0.0386, Val Loss: 0.0437\n",
            "Epoch 91/100, Train Loss: 0.0391, Val Loss: 0.0437\n",
            "Epoch 92/100, Train Loss: 0.0395, Val Loss: 0.0437\n",
            "Epoch 93/100, Train Loss: 0.0389, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0390, Val Loss: 0.0437\n",
            "Epoch 95/100, Train Loss: 0.0394, Val Loss: 0.0437\n",
            "Epoch 96/100, Train Loss: 0.0396, Val Loss: 0.0437\n",
            "Epoch 97/100, Train Loss: 0.0388, Val Loss: 0.0437\n",
            "Epoch 98/100, Train Loss: 0.0396, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0390, Val Loss: 0.0437\n",
            "Epoch 100/100, Train Loss: 0.0387, Val Loss: 0.0437\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: lstm, Units: 512, Dropout: 0.2, Dense Units: 64\n",
            "Batch Size: 4, Epochs: 100\n",
            "Device: cuda\n",
            "Total test samples: 28\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/28\n",
            "Current training set size: 97 samples\n",
            "Epoch 1/100, Train Loss: 0.1460, Val Loss: 0.1916\n",
            "Epoch 2/100, Train Loss: 0.1394, Val Loss: 0.2062\n",
            "Epoch 3/100, Train Loss: 0.1505, Val Loss: 0.1732\n",
            "Epoch 4/100, Train Loss: 0.1231, Val Loss: 0.0793\n",
            "Epoch 5/100, Train Loss: 0.0813, Val Loss: 0.0437\n",
            "Epoch 6/100, Train Loss: 0.0771, Val Loss: 0.0346\n",
            "Epoch 7/100, Train Loss: 0.0798, Val Loss: 0.0748\n",
            "Epoch 8/100, Train Loss: 0.0634, Val Loss: 0.0334\n",
            "Epoch 9/100, Train Loss: 0.0576, Val Loss: 0.0420\n",
            "Epoch 10/100, Train Loss: 0.0576, Val Loss: 0.0517\n",
            "Epoch 11/100, Train Loss: 0.0495, Val Loss: 0.0359\n",
            "Epoch 12/100, Train Loss: 0.0441, Val Loss: 0.0348\n",
            "Epoch 13/100, Train Loss: 0.0383, Val Loss: 0.0326\n",
            "Epoch 14/100, Train Loss: 0.0466, Val Loss: 0.0374\n",
            "Epoch 15/100, Train Loss: 0.0453, Val Loss: 0.0367\n",
            "Epoch 16/100, Train Loss: 0.0420, Val Loss: 0.0358\n",
            "Epoch 17/100, Train Loss: 0.0412, Val Loss: 0.0400\n",
            "Epoch 18/100, Train Loss: 0.0352, Val Loss: 0.0368\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0331, Val Loss: 0.0361\n",
            "Epoch 20/100, Train Loss: 0.0315, Val Loss: 0.0352\n",
            "Epoch 21/100, Train Loss: 0.0310, Val Loss: 0.0352\n",
            "Epoch 22/100, Train Loss: 0.0306, Val Loss: 0.0353\n",
            "Epoch 23/100, Train Loss: 0.0304, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0291, Val Loss: 0.0351\n",
            "Epoch 25/100, Train Loss: 0.0307, Val Loss: 0.0351\n",
            "Epoch 26/100, Train Loss: 0.0300, Val Loss: 0.0351\n",
            "Epoch 27/100, Train Loss: 0.0296, Val Loss: 0.0351\n",
            "Epoch 28/100, Train Loss: 0.0304, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0293, Val Loss: 0.0351\n",
            "Epoch 30/100, Train Loss: 0.0299, Val Loss: 0.0351\n",
            "Epoch 31/100, Train Loss: 0.0299, Val Loss: 0.0351\n",
            "Epoch 32/100, Train Loss: 0.0298, Val Loss: 0.0351\n",
            "Epoch 33/100, Train Loss: 0.0305, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0298, Val Loss: 0.0351\n",
            "Epoch 35/100, Train Loss: 0.0306, Val Loss: 0.0351\n",
            "Epoch 36/100, Train Loss: 0.0300, Val Loss: 0.0351\n",
            "Epoch 37/100, Train Loss: 0.0310, Val Loss: 0.0351\n",
            "Epoch 38/100, Train Loss: 0.0306, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0297, Val Loss: 0.0351\n",
            "Epoch 40/100, Train Loss: 0.0301, Val Loss: 0.0351\n",
            "Epoch 41/100, Train Loss: 0.0293, Val Loss: 0.0351\n",
            "Epoch 42/100, Train Loss: 0.0289, Val Loss: 0.0351\n",
            "Epoch 43/100, Train Loss: 0.0301, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0291, Val Loss: 0.0351\n",
            "Epoch 45/100, Train Loss: 0.0295, Val Loss: 0.0351\n",
            "Epoch 46/100, Train Loss: 0.0292, Val Loss: 0.0351\n",
            "Epoch 47/100, Train Loss: 0.0298, Val Loss: 0.0351\n",
            "Epoch 48/100, Train Loss: 0.0294, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0357, Val Loss: 0.0351\n",
            "Epoch 50/100, Train Loss: 0.0294, Val Loss: 0.0351\n",
            "Epoch 51/100, Train Loss: 0.0302, Val Loss: 0.0351\n",
            "Epoch 52/100, Train Loss: 0.0300, Val Loss: 0.0351\n",
            "Epoch 53/100, Train Loss: 0.0291, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0300, Val Loss: 0.0351\n",
            "Epoch 55/100, Train Loss: 0.0309, Val Loss: 0.0351\n",
            "Epoch 56/100, Train Loss: 0.0304, Val Loss: 0.0351\n",
            "Epoch 57/100, Train Loss: 0.0306, Val Loss: 0.0351\n",
            "Epoch 58/100, Train Loss: 0.0293, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0294, Val Loss: 0.0351\n",
            "Epoch 60/100, Train Loss: 0.0295, Val Loss: 0.0351\n",
            "Epoch 61/100, Train Loss: 0.0292, Val Loss: 0.0351\n",
            "Epoch 62/100, Train Loss: 0.0297, Val Loss: 0.0351\n",
            "Epoch 63/100, Train Loss: 0.0310, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0304, Val Loss: 0.0351\n",
            "Epoch 65/100, Train Loss: 0.0303, Val Loss: 0.0351\n",
            "Epoch 66/100, Train Loss: 0.0294, Val Loss: 0.0351\n",
            "Epoch 67/100, Train Loss: 0.0307, Val Loss: 0.0351\n",
            "Epoch 68/100, Train Loss: 0.0295, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0310, Val Loss: 0.0351\n",
            "Epoch 70/100, Train Loss: 0.0293, Val Loss: 0.0351\n",
            "Epoch 71/100, Train Loss: 0.0297, Val Loss: 0.0351\n",
            "Epoch 72/100, Train Loss: 0.0300, Val Loss: 0.0351\n",
            "Epoch 73/100, Train Loss: 0.0295, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0312, Val Loss: 0.0351\n",
            "Epoch 75/100, Train Loss: 0.0304, Val Loss: 0.0351\n",
            "Epoch 76/100, Train Loss: 0.0302, Val Loss: 0.0351\n",
            "Epoch 77/100, Train Loss: 0.0298, Val Loss: 0.0351\n",
            "Epoch 78/100, Train Loss: 0.0310, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0306, Val Loss: 0.0351\n",
            "Epoch 80/100, Train Loss: 0.0299, Val Loss: 0.0351\n",
            "Epoch 81/100, Train Loss: 0.0310, Val Loss: 0.0351\n",
            "Epoch 82/100, Train Loss: 0.0298, Val Loss: 0.0351\n",
            "Epoch 83/100, Train Loss: 0.0309, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0294, Val Loss: 0.0351\n",
            "Epoch 85/100, Train Loss: 0.0300, Val Loss: 0.0351\n",
            "Epoch 86/100, Train Loss: 0.0292, Val Loss: 0.0351\n",
            "Epoch 87/100, Train Loss: 0.0305, Val Loss: 0.0351\n",
            "Epoch 88/100, Train Loss: 0.0296, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0311, Val Loss: 0.0351\n",
            "Epoch 90/100, Train Loss: 0.0295, Val Loss: 0.0351\n",
            "Epoch 91/100, Train Loss: 0.0298, Val Loss: 0.0351\n",
            "Epoch 92/100, Train Loss: 0.0301, Val Loss: 0.0351\n",
            "Epoch 93/100, Train Loss: 0.0293, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0297, Val Loss: 0.0351\n",
            "Epoch 95/100, Train Loss: 0.0298, Val Loss: 0.0351\n",
            "Epoch 96/100, Train Loss: 0.0300, Val Loss: 0.0351\n",
            "Epoch 97/100, Train Loss: 0.0311, Val Loss: 0.0351\n",
            "Epoch 98/100, Train Loss: 0.0294, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0316, Val Loss: 0.0351\n",
            "Epoch 100/100, Train Loss: 0.0310, Val Loss: 0.0351\n",
            "\n",
            "Test iteration 2/28\n",
            "Current training set size: 98 samples\n",
            "Epoch 1/100, Train Loss: 0.1714, Val Loss: 0.2065\n",
            "Epoch 2/100, Train Loss: 0.1522, Val Loss: 0.2085\n",
            "Epoch 3/100, Train Loss: 0.1448, Val Loss: 0.1826\n",
            "Epoch 4/100, Train Loss: 0.1425, Val Loss: 0.1734\n",
            "Epoch 5/100, Train Loss: 0.1300, Val Loss: 0.1630\n",
            "Epoch 6/100, Train Loss: 0.1092, Val Loss: 0.0639\n",
            "Epoch 7/100, Train Loss: 0.0989, Val Loss: 0.1017\n",
            "Epoch 8/100, Train Loss: 0.0910, Val Loss: 0.0812\n",
            "Epoch 9/100, Train Loss: 0.0739, Val Loss: 0.0652\n",
            "Epoch 10/100, Train Loss: 0.0704, Val Loss: 0.1016\n",
            "Epoch 11/100, Train Loss: 0.0691, Val Loss: 0.0399\n",
            "Epoch 12/100, Train Loss: 0.0589, Val Loss: 0.0388\n",
            "Epoch 13/100, Train Loss: 0.0511, Val Loss: 0.0414\n",
            "Epoch 14/100, Train Loss: 0.0486, Val Loss: 0.0478\n",
            "Epoch 15/100, Train Loss: 0.0513, Val Loss: 0.0498\n",
            "Epoch 16/100, Train Loss: 0.0465, Val Loss: 0.0611\n",
            "Epoch 17/100, Train Loss: 0.0444, Val Loss: 0.0532\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0470, Val Loss: 0.0485\n",
            "Epoch 19/100, Train Loss: 0.0406, Val Loss: 0.0438\n",
            "Epoch 20/100, Train Loss: 0.0365, Val Loss: 0.0409\n",
            "Epoch 21/100, Train Loss: 0.0366, Val Loss: 0.0401\n",
            "Epoch 22/100, Train Loss: 0.0355, Val Loss: 0.0393\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0369, Val Loss: 0.0393\n",
            "Epoch 24/100, Train Loss: 0.0348, Val Loss: 0.0392\n",
            "Epoch 25/100, Train Loss: 0.0350, Val Loss: 0.0392\n",
            "Epoch 26/100, Train Loss: 0.0353, Val Loss: 0.0392\n",
            "Epoch 27/100, Train Loss: 0.0356, Val Loss: 0.0392\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0353, Val Loss: 0.0392\n",
            "Epoch 29/100, Train Loss: 0.0358, Val Loss: 0.0392\n",
            "Epoch 30/100, Train Loss: 0.0355, Val Loss: 0.0392\n",
            "Epoch 31/100, Train Loss: 0.0351, Val Loss: 0.0392\n",
            "Epoch 32/100, Train Loss: 0.0360, Val Loss: 0.0392\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0355, Val Loss: 0.0392\n",
            "Epoch 34/100, Train Loss: 0.0359, Val Loss: 0.0392\n",
            "Epoch 35/100, Train Loss: 0.0350, Val Loss: 0.0392\n",
            "Epoch 36/100, Train Loss: 0.0354, Val Loss: 0.0392\n",
            "Epoch 37/100, Train Loss: 0.0352, Val Loss: 0.0392\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0359, Val Loss: 0.0392\n",
            "Epoch 39/100, Train Loss: 0.0351, Val Loss: 0.0392\n",
            "Epoch 40/100, Train Loss: 0.0353, Val Loss: 0.0392\n",
            "Epoch 41/100, Train Loss: 0.0349, Val Loss: 0.0392\n",
            "Epoch 42/100, Train Loss: 0.0344, Val Loss: 0.0392\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0370, Val Loss: 0.0392\n",
            "Epoch 44/100, Train Loss: 0.0351, Val Loss: 0.0392\n",
            "Epoch 45/100, Train Loss: 0.0351, Val Loss: 0.0392\n",
            "Epoch 46/100, Train Loss: 0.0392, Val Loss: 0.0392\n",
            "Epoch 47/100, Train Loss: 0.0355, Val Loss: 0.0392\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0347, Val Loss: 0.0392\n",
            "Epoch 49/100, Train Loss: 0.0354, Val Loss: 0.0392\n",
            "Epoch 50/100, Train Loss: 0.0363, Val Loss: 0.0392\n",
            "Epoch 51/100, Train Loss: 0.0351, Val Loss: 0.0392\n",
            "Epoch 52/100, Train Loss: 0.0354, Val Loss: 0.0392\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0345, Val Loss: 0.0392\n",
            "Epoch 54/100, Train Loss: 0.0348, Val Loss: 0.0392\n",
            "Epoch 55/100, Train Loss: 0.0343, Val Loss: 0.0392\n",
            "Epoch 56/100, Train Loss: 0.0365, Val Loss: 0.0392\n",
            "Epoch 57/100, Train Loss: 0.0351, Val Loss: 0.0392\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0352, Val Loss: 0.0392\n",
            "Epoch 59/100, Train Loss: 0.0353, Val Loss: 0.0392\n",
            "Epoch 60/100, Train Loss: 0.0354, Val Loss: 0.0392\n",
            "Epoch 61/100, Train Loss: 0.0359, Val Loss: 0.0392\n",
            "Epoch 62/100, Train Loss: 0.0359, Val Loss: 0.0392\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0348, Val Loss: 0.0392\n",
            "Epoch 64/100, Train Loss: 0.0349, Val Loss: 0.0392\n",
            "Epoch 65/100, Train Loss: 0.0357, Val Loss: 0.0392\n",
            "Epoch 66/100, Train Loss: 0.0349, Val Loss: 0.0392\n",
            "Epoch 67/100, Train Loss: 0.0353, Val Loss: 0.0392\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0350, Val Loss: 0.0392\n",
            "Epoch 69/100, Train Loss: 0.0354, Val Loss: 0.0392\n",
            "Epoch 70/100, Train Loss: 0.0361, Val Loss: 0.0392\n",
            "Epoch 71/100, Train Loss: 0.0391, Val Loss: 0.0392\n",
            "Epoch 72/100, Train Loss: 0.0394, Val Loss: 0.0392\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0353, Val Loss: 0.0392\n",
            "Epoch 74/100, Train Loss: 0.0353, Val Loss: 0.0392\n",
            "Epoch 75/100, Train Loss: 0.0358, Val Loss: 0.0392\n",
            "Epoch 76/100, Train Loss: 0.0354, Val Loss: 0.0392\n",
            "Epoch 77/100, Train Loss: 0.0355, Val Loss: 0.0392\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0353, Val Loss: 0.0392\n",
            "Epoch 79/100, Train Loss: 0.0351, Val Loss: 0.0392\n",
            "Epoch 80/100, Train Loss: 0.0347, Val Loss: 0.0392\n",
            "Epoch 81/100, Train Loss: 0.0349, Val Loss: 0.0392\n",
            "Epoch 82/100, Train Loss: 0.0357, Val Loss: 0.0392\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0360, Val Loss: 0.0392\n",
            "Epoch 84/100, Train Loss: 0.0363, Val Loss: 0.0392\n",
            "Epoch 85/100, Train Loss: 0.0353, Val Loss: 0.0392\n",
            "Epoch 86/100, Train Loss: 0.0354, Val Loss: 0.0392\n",
            "Epoch 87/100, Train Loss: 0.0352, Val Loss: 0.0392\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0355, Val Loss: 0.0392\n",
            "Epoch 89/100, Train Loss: 0.0354, Val Loss: 0.0392\n",
            "Epoch 90/100, Train Loss: 0.0356, Val Loss: 0.0392\n",
            "Epoch 91/100, Train Loss: 0.0399, Val Loss: 0.0392\n",
            "Epoch 92/100, Train Loss: 0.0355, Val Loss: 0.0392\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0354, Val Loss: 0.0392\n",
            "Epoch 94/100, Train Loss: 0.0359, Val Loss: 0.0392\n",
            "Epoch 95/100, Train Loss: 0.0360, Val Loss: 0.0392\n",
            "Epoch 96/100, Train Loss: 0.0353, Val Loss: 0.0392\n",
            "Epoch 97/100, Train Loss: 0.0356, Val Loss: 0.0392\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0352, Val Loss: 0.0392\n",
            "Epoch 99/100, Train Loss: 0.0344, Val Loss: 0.0392\n",
            "Epoch 100/100, Train Loss: 0.0354, Val Loss: 0.0392\n",
            "\n",
            "Test iteration 3/28\n",
            "Current training set size: 99 samples\n",
            "Epoch 1/100, Train Loss: 0.1585, Val Loss: 0.1899\n",
            "Epoch 2/100, Train Loss: 0.1448, Val Loss: 0.1921\n",
            "Epoch 3/100, Train Loss: 0.1347, Val Loss: 0.1655\n",
            "Epoch 4/100, Train Loss: 0.1186, Val Loss: 0.1159\n",
            "Epoch 5/100, Train Loss: 0.0929, Val Loss: 0.0471\n",
            "Epoch 6/100, Train Loss: 0.1022, Val Loss: 0.1057\n",
            "Epoch 7/100, Train Loss: 0.0918, Val Loss: 0.0549\n",
            "Epoch 8/100, Train Loss: 0.0669, Val Loss: 0.0768\n",
            "Epoch 9/100, Train Loss: 0.0681, Val Loss: 0.0484\n",
            "Epoch 10/100, Train Loss: 0.0593, Val Loss: 0.0760\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 11/100, Train Loss: 0.0789, Val Loss: 0.0608\n",
            "Epoch 12/100, Train Loss: 0.0609, Val Loss: 0.0463\n",
            "Epoch 13/100, Train Loss: 0.0519, Val Loss: 0.0395\n",
            "Epoch 14/100, Train Loss: 0.0497, Val Loss: 0.0357\n",
            "Epoch 15/100, Train Loss: 0.0489, Val Loss: 0.0342\n",
            "Epoch 16/100, Train Loss: 0.0490, Val Loss: 0.0324\n",
            "Epoch 17/100, Train Loss: 0.0482, Val Loss: 0.0319\n",
            "Epoch 18/100, Train Loss: 0.0477, Val Loss: 0.0317\n",
            "Epoch 19/100, Train Loss: 0.0476, Val Loss: 0.0318\n",
            "Epoch 20/100, Train Loss: 0.0469, Val Loss: 0.0320\n",
            "Epoch 21/100, Train Loss: 0.0493, Val Loss: 0.0323\n",
            "Epoch 22/100, Train Loss: 0.0466, Val Loss: 0.0327\n",
            "Epoch 23/100, Train Loss: 0.0460, Val Loss: 0.0331\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0460, Val Loss: 0.0331\n",
            "Epoch 25/100, Train Loss: 0.0460, Val Loss: 0.0331\n",
            "Epoch 26/100, Train Loss: 0.0458, Val Loss: 0.0331\n",
            "Epoch 27/100, Train Loss: 0.0459, Val Loss: 0.0331\n",
            "Epoch 28/100, Train Loss: 0.0466, Val Loss: 0.0331\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0461, Val Loss: 0.0331\n",
            "Epoch 30/100, Train Loss: 0.0462, Val Loss: 0.0331\n",
            "Epoch 31/100, Train Loss: 0.0460, Val Loss: 0.0331\n",
            "Epoch 32/100, Train Loss: 0.0462, Val Loss: 0.0331\n",
            "Epoch 33/100, Train Loss: 0.0463, Val Loss: 0.0331\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0458, Val Loss: 0.0331\n",
            "Epoch 35/100, Train Loss: 0.0460, Val Loss: 0.0331\n",
            "Epoch 36/100, Train Loss: 0.0461, Val Loss: 0.0331\n",
            "Epoch 37/100, Train Loss: 0.0458, Val Loss: 0.0331\n",
            "Epoch 38/100, Train Loss: 0.0458, Val Loss: 0.0331\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0468, Val Loss: 0.0331\n",
            "Epoch 40/100, Train Loss: 0.0462, Val Loss: 0.0331\n",
            "Epoch 41/100, Train Loss: 0.0457, Val Loss: 0.0331\n",
            "Epoch 42/100, Train Loss: 0.0460, Val Loss: 0.0331\n",
            "Epoch 43/100, Train Loss: 0.0462, Val Loss: 0.0331\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0462, Val Loss: 0.0331\n",
            "Epoch 45/100, Train Loss: 0.0462, Val Loss: 0.0331\n",
            "Epoch 46/100, Train Loss: 0.0459, Val Loss: 0.0331\n",
            "Epoch 47/100, Train Loss: 0.0457, Val Loss: 0.0331\n",
            "Epoch 48/100, Train Loss: 0.0458, Val Loss: 0.0331\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0462, Val Loss: 0.0331\n",
            "Epoch 50/100, Train Loss: 0.0462, Val Loss: 0.0331\n",
            "Epoch 51/100, Train Loss: 0.0462, Val Loss: 0.0331\n",
            "Epoch 52/100, Train Loss: 0.0460, Val Loss: 0.0331\n",
            "Epoch 53/100, Train Loss: 0.0459, Val Loss: 0.0331\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0455, Val Loss: 0.0331\n",
            "Epoch 55/100, Train Loss: 0.0475, Val Loss: 0.0331\n",
            "Epoch 56/100, Train Loss: 0.0460, Val Loss: 0.0331\n",
            "Epoch 57/100, Train Loss: 0.0460, Val Loss: 0.0331\n",
            "Epoch 58/100, Train Loss: 0.0454, Val Loss: 0.0331\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0460, Val Loss: 0.0331\n",
            "Epoch 60/100, Train Loss: 0.0486, Val Loss: 0.0331\n",
            "Epoch 61/100, Train Loss: 0.0459, Val Loss: 0.0331\n",
            "Epoch 62/100, Train Loss: 0.0463, Val Loss: 0.0331\n",
            "Epoch 63/100, Train Loss: 0.0459, Val Loss: 0.0331\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0457, Val Loss: 0.0331\n",
            "Epoch 65/100, Train Loss: 0.0468, Val Loss: 0.0331\n",
            "Epoch 66/100, Train Loss: 0.0457, Val Loss: 0.0331\n",
            "Epoch 67/100, Train Loss: 0.0458, Val Loss: 0.0331\n",
            "Epoch 68/100, Train Loss: 0.0457, Val Loss: 0.0331\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0455, Val Loss: 0.0331\n",
            "Epoch 70/100, Train Loss: 0.0463, Val Loss: 0.0331\n",
            "Epoch 71/100, Train Loss: 0.0459, Val Loss: 0.0331\n",
            "Epoch 72/100, Train Loss: 0.0457, Val Loss: 0.0331\n",
            "Epoch 73/100, Train Loss: 0.0458, Val Loss: 0.0331\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0462, Val Loss: 0.0331\n",
            "Epoch 75/100, Train Loss: 0.0460, Val Loss: 0.0331\n",
            "Epoch 76/100, Train Loss: 0.0458, Val Loss: 0.0331\n",
            "Epoch 77/100, Train Loss: 0.0465, Val Loss: 0.0331\n",
            "Epoch 78/100, Train Loss: 0.0463, Val Loss: 0.0331\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0460, Val Loss: 0.0331\n",
            "Epoch 80/100, Train Loss: 0.0459, Val Loss: 0.0331\n",
            "Epoch 81/100, Train Loss: 0.0455, Val Loss: 0.0331\n",
            "Epoch 82/100, Train Loss: 0.0463, Val Loss: 0.0331\n",
            "Epoch 83/100, Train Loss: 0.0460, Val Loss: 0.0331\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0464, Val Loss: 0.0331\n",
            "Epoch 85/100, Train Loss: 0.0459, Val Loss: 0.0331\n",
            "Epoch 86/100, Train Loss: 0.0457, Val Loss: 0.0331\n",
            "Epoch 87/100, Train Loss: 0.0465, Val Loss: 0.0331\n",
            "Epoch 88/100, Train Loss: 0.0460, Val Loss: 0.0331\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0466, Val Loss: 0.0331\n",
            "Epoch 90/100, Train Loss: 0.0454, Val Loss: 0.0331\n",
            "Epoch 91/100, Train Loss: 0.0457, Val Loss: 0.0331\n",
            "Epoch 92/100, Train Loss: 0.0461, Val Loss: 0.0331\n",
            "Epoch 93/100, Train Loss: 0.0471, Val Loss: 0.0331\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0460, Val Loss: 0.0331\n",
            "Epoch 95/100, Train Loss: 0.0462, Val Loss: 0.0331\n",
            "Epoch 96/100, Train Loss: 0.0459, Val Loss: 0.0331\n",
            "Epoch 97/100, Train Loss: 0.0460, Val Loss: 0.0331\n",
            "Epoch 98/100, Train Loss: 0.0454, Val Loss: 0.0331\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0460, Val Loss: 0.0331\n",
            "Epoch 100/100, Train Loss: 0.0469, Val Loss: 0.0331\n",
            "\n",
            "Test iteration 4/28\n",
            "Current training set size: 100 samples\n",
            "Epoch 1/100, Train Loss: 0.1476, Val Loss: 0.1929\n",
            "Epoch 2/100, Train Loss: 0.1350, Val Loss: 0.1670\n",
            "Epoch 3/100, Train Loss: 0.1291, Val Loss: 0.1628\n",
            "Epoch 4/100, Train Loss: 0.1314, Val Loss: 0.1458\n",
            "Epoch 5/100, Train Loss: 0.1050, Val Loss: 0.0969\n",
            "Epoch 6/100, Train Loss: 0.0832, Val Loss: 0.0908\n",
            "Epoch 7/100, Train Loss: 0.0818, Val Loss: 0.0527\n",
            "Epoch 8/100, Train Loss: 0.0742, Val Loss: 0.0663\n",
            "Epoch 9/100, Train Loss: 0.0681, Val Loss: 0.0799\n",
            "Epoch 10/100, Train Loss: 0.0701, Val Loss: 0.0353\n",
            "Epoch 11/100, Train Loss: 0.0517, Val Loss: 0.0396\n",
            "Epoch 12/100, Train Loss: 0.0465, Val Loss: 0.0374\n",
            "Epoch 13/100, Train Loss: 0.0504, Val Loss: 0.0371\n",
            "Epoch 14/100, Train Loss: 0.0476, Val Loss: 0.0368\n",
            "Epoch 15/100, Train Loss: 0.0486, Val Loss: 0.0388\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0371, Val Loss: 0.0369\n",
            "Epoch 17/100, Train Loss: 0.0358, Val Loss: 0.0359\n",
            "Epoch 18/100, Train Loss: 0.0357, Val Loss: 0.0354\n",
            "Epoch 19/100, Train Loss: 0.0352, Val Loss: 0.0350\n",
            "Epoch 20/100, Train Loss: 0.0351, Val Loss: 0.0349\n",
            "Epoch 21/100, Train Loss: 0.0354, Val Loss: 0.0353\n",
            "Epoch 22/100, Train Loss: 0.0351, Val Loss: 0.0352\n",
            "Epoch 23/100, Train Loss: 0.0354, Val Loss: 0.0353\n",
            "Epoch 24/100, Train Loss: 0.0359, Val Loss: 0.0356\n",
            "Epoch 25/100, Train Loss: 0.0348, Val Loss: 0.0348\n",
            "Epoch 26/100, Train Loss: 0.0352, Val Loss: 0.0350\n",
            "Epoch 27/100, Train Loss: 0.0353, Val Loss: 0.0349\n",
            "Epoch 28/100, Train Loss: 0.0356, Val Loss: 0.0351\n",
            "Epoch 29/100, Train Loss: 0.0350, Val Loss: 0.0351\n",
            "Epoch 30/100, Train Loss: 0.0349, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0356, Val Loss: 0.0352\n",
            "Epoch 32/100, Train Loss: 0.0353, Val Loss: 0.0352\n",
            "Epoch 33/100, Train Loss: 0.0351, Val Loss: 0.0352\n",
            "Epoch 34/100, Train Loss: 0.0354, Val Loss: 0.0352\n",
            "Epoch 35/100, Train Loss: 0.0349, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0345, Val Loss: 0.0352\n",
            "Epoch 37/100, Train Loss: 0.0355, Val Loss: 0.0352\n",
            "Epoch 38/100, Train Loss: 0.0350, Val Loss: 0.0352\n",
            "Epoch 39/100, Train Loss: 0.0349, Val Loss: 0.0352\n",
            "Epoch 40/100, Train Loss: 0.0350, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0346, Val Loss: 0.0352\n",
            "Epoch 42/100, Train Loss: 0.0353, Val Loss: 0.0352\n",
            "Epoch 43/100, Train Loss: 0.0354, Val Loss: 0.0352\n",
            "Epoch 44/100, Train Loss: 0.0347, Val Loss: 0.0352\n",
            "Epoch 45/100, Train Loss: 0.0355, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0355, Val Loss: 0.0352\n",
            "Epoch 47/100, Train Loss: 0.0355, Val Loss: 0.0352\n",
            "Epoch 48/100, Train Loss: 0.0357, Val Loss: 0.0352\n",
            "Epoch 49/100, Train Loss: 0.0358, Val Loss: 0.0352\n",
            "Epoch 50/100, Train Loss: 0.0350, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0352, Val Loss: 0.0352\n",
            "Epoch 52/100, Train Loss: 0.0350, Val Loss: 0.0352\n",
            "Epoch 53/100, Train Loss: 0.0349, Val Loss: 0.0352\n",
            "Epoch 54/100, Train Loss: 0.0350, Val Loss: 0.0352\n",
            "Epoch 55/100, Train Loss: 0.0354, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0347, Val Loss: 0.0352\n",
            "Epoch 57/100, Train Loss: 0.0348, Val Loss: 0.0352\n",
            "Epoch 58/100, Train Loss: 0.0353, Val Loss: 0.0352\n",
            "Epoch 59/100, Train Loss: 0.0353, Val Loss: 0.0352\n",
            "Epoch 60/100, Train Loss: 0.0351, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0353, Val Loss: 0.0352\n",
            "Epoch 62/100, Train Loss: 0.0353, Val Loss: 0.0352\n",
            "Epoch 63/100, Train Loss: 0.0351, Val Loss: 0.0352\n",
            "Epoch 64/100, Train Loss: 0.0355, Val Loss: 0.0352\n",
            "Epoch 65/100, Train Loss: 0.0359, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0353, Val Loss: 0.0352\n",
            "Epoch 67/100, Train Loss: 0.0351, Val Loss: 0.0352\n",
            "Epoch 68/100, Train Loss: 0.0347, Val Loss: 0.0352\n",
            "Epoch 69/100, Train Loss: 0.0357, Val Loss: 0.0352\n",
            "Epoch 70/100, Train Loss: 0.0349, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0355, Val Loss: 0.0352\n",
            "Epoch 72/100, Train Loss: 0.0353, Val Loss: 0.0352\n",
            "Epoch 73/100, Train Loss: 0.0349, Val Loss: 0.0352\n",
            "Epoch 74/100, Train Loss: 0.0357, Val Loss: 0.0352\n",
            "Epoch 75/100, Train Loss: 0.0356, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0349, Val Loss: 0.0352\n",
            "Epoch 77/100, Train Loss: 0.0357, Val Loss: 0.0352\n",
            "Epoch 78/100, Train Loss: 0.0349, Val Loss: 0.0352\n",
            "Epoch 79/100, Train Loss: 0.0350, Val Loss: 0.0352\n",
            "Epoch 80/100, Train Loss: 0.0352, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0351, Val Loss: 0.0352\n",
            "Epoch 82/100, Train Loss: 0.0350, Val Loss: 0.0352\n",
            "Epoch 83/100, Train Loss: 0.0348, Val Loss: 0.0352\n",
            "Epoch 84/100, Train Loss: 0.0354, Val Loss: 0.0352\n",
            "Epoch 85/100, Train Loss: 0.0348, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0357, Val Loss: 0.0352\n",
            "Epoch 87/100, Train Loss: 0.0354, Val Loss: 0.0352\n",
            "Epoch 88/100, Train Loss: 0.0349, Val Loss: 0.0352\n",
            "Epoch 89/100, Train Loss: 0.0361, Val Loss: 0.0352\n",
            "Epoch 90/100, Train Loss: 0.0351, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0346, Val Loss: 0.0352\n",
            "Epoch 92/100, Train Loss: 0.0350, Val Loss: 0.0352\n",
            "Epoch 93/100, Train Loss: 0.0351, Val Loss: 0.0352\n",
            "Epoch 94/100, Train Loss: 0.0355, Val Loss: 0.0352\n",
            "Epoch 95/100, Train Loss: 0.0355, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0356, Val Loss: 0.0352\n",
            "Epoch 97/100, Train Loss: 0.0351, Val Loss: 0.0352\n",
            "Epoch 98/100, Train Loss: 0.0345, Val Loss: 0.0352\n",
            "Epoch 99/100, Train Loss: 0.0351, Val Loss: 0.0352\n",
            "Epoch 100/100, Train Loss: 0.0350, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 5/28\n",
            "Current training set size: 101 samples\n",
            "Epoch 1/100, Train Loss: 0.1659, Val Loss: 0.2325\n",
            "Epoch 2/100, Train Loss: 0.1564, Val Loss: 0.2198\n",
            "Epoch 3/100, Train Loss: 0.1468, Val Loss: 0.1885\n",
            "Epoch 4/100, Train Loss: 0.1374, Val Loss: 0.1621\n",
            "Epoch 5/100, Train Loss: 0.1303, Val Loss: 0.1217\n",
            "Epoch 6/100, Train Loss: 0.0952, Val Loss: 0.0859\n",
            "Epoch 7/100, Train Loss: 0.0726, Val Loss: 0.1233\n",
            "Epoch 8/100, Train Loss: 0.1031, Val Loss: 0.0896\n",
            "Epoch 9/100, Train Loss: 0.0733, Val Loss: 0.0555\n",
            "Epoch 10/100, Train Loss: 0.0671, Val Loss: 0.0342\n",
            "Epoch 11/100, Train Loss: 0.0569, Val Loss: 0.0351\n",
            "Epoch 12/100, Train Loss: 0.0594, Val Loss: 0.0426\n",
            "Epoch 13/100, Train Loss: 0.0653, Val Loss: 0.0477\n",
            "Epoch 14/100, Train Loss: 0.0552, Val Loss: 0.0440\n",
            "Epoch 15/100, Train Loss: 0.0496, Val Loss: 0.0401\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0500, Val Loss: 0.0391\n",
            "Epoch 17/100, Train Loss: 0.0435, Val Loss: 0.0381\n",
            "Epoch 18/100, Train Loss: 0.0395, Val Loss: 0.0378\n",
            "Epoch 19/100, Train Loss: 0.0389, Val Loss: 0.0380\n",
            "Epoch 20/100, Train Loss: 0.0389, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0397, Val Loss: 0.0384\n",
            "Epoch 22/100, Train Loss: 0.0383, Val Loss: 0.0384\n",
            "Epoch 23/100, Train Loss: 0.0381, Val Loss: 0.0384\n",
            "Epoch 24/100, Train Loss: 0.0372, Val Loss: 0.0384\n",
            "Epoch 25/100, Train Loss: 0.0382, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0377, Val Loss: 0.0384\n",
            "Epoch 27/100, Train Loss: 0.0376, Val Loss: 0.0384\n",
            "Epoch 28/100, Train Loss: 0.0370, Val Loss: 0.0384\n",
            "Epoch 29/100, Train Loss: 0.0383, Val Loss: 0.0384\n",
            "Epoch 30/100, Train Loss: 0.0399, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0383, Val Loss: 0.0384\n",
            "Epoch 32/100, Train Loss: 0.0375, Val Loss: 0.0384\n",
            "Epoch 33/100, Train Loss: 0.0386, Val Loss: 0.0384\n",
            "Epoch 34/100, Train Loss: 0.0369, Val Loss: 0.0384\n",
            "Epoch 35/100, Train Loss: 0.0388, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0376, Val Loss: 0.0384\n",
            "Epoch 37/100, Train Loss: 0.0389, Val Loss: 0.0384\n",
            "Epoch 38/100, Train Loss: 0.0386, Val Loss: 0.0384\n",
            "Epoch 39/100, Train Loss: 0.0390, Val Loss: 0.0384\n",
            "Epoch 40/100, Train Loss: 0.0388, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0376, Val Loss: 0.0384\n",
            "Epoch 42/100, Train Loss: 0.0376, Val Loss: 0.0384\n",
            "Epoch 43/100, Train Loss: 0.0370, Val Loss: 0.0384\n",
            "Epoch 44/100, Train Loss: 0.0375, Val Loss: 0.0384\n",
            "Epoch 45/100, Train Loss: 0.0383, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0384, Val Loss: 0.0384\n",
            "Epoch 47/100, Train Loss: 0.0370, Val Loss: 0.0384\n",
            "Epoch 48/100, Train Loss: 0.0395, Val Loss: 0.0384\n",
            "Epoch 49/100, Train Loss: 0.0419, Val Loss: 0.0384\n",
            "Epoch 50/100, Train Loss: 0.0412, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0371, Val Loss: 0.0384\n",
            "Epoch 52/100, Train Loss: 0.0376, Val Loss: 0.0384\n",
            "Epoch 53/100, Train Loss: 0.0426, Val Loss: 0.0384\n",
            "Epoch 54/100, Train Loss: 0.0375, Val Loss: 0.0384\n",
            "Epoch 55/100, Train Loss: 0.0372, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0382, Val Loss: 0.0384\n",
            "Epoch 57/100, Train Loss: 0.0376, Val Loss: 0.0384\n",
            "Epoch 58/100, Train Loss: 0.0375, Val Loss: 0.0384\n",
            "Epoch 59/100, Train Loss: 0.0381, Val Loss: 0.0384\n",
            "Epoch 60/100, Train Loss: 0.0378, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0371, Val Loss: 0.0384\n",
            "Epoch 62/100, Train Loss: 0.0397, Val Loss: 0.0384\n",
            "Epoch 63/100, Train Loss: 0.0390, Val Loss: 0.0384\n",
            "Epoch 64/100, Train Loss: 0.0396, Val Loss: 0.0384\n",
            "Epoch 65/100, Train Loss: 0.0375, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0376, Val Loss: 0.0384\n",
            "Epoch 67/100, Train Loss: 0.0378, Val Loss: 0.0384\n",
            "Epoch 68/100, Train Loss: 0.0381, Val Loss: 0.0384\n",
            "Epoch 69/100, Train Loss: 0.0376, Val Loss: 0.0384\n",
            "Epoch 70/100, Train Loss: 0.0371, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0380, Val Loss: 0.0384\n",
            "Epoch 72/100, Train Loss: 0.0375, Val Loss: 0.0384\n",
            "Epoch 73/100, Train Loss: 0.0376, Val Loss: 0.0384\n",
            "Epoch 74/100, Train Loss: 0.0385, Val Loss: 0.0384\n",
            "Epoch 75/100, Train Loss: 0.0380, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0381, Val Loss: 0.0384\n",
            "Epoch 77/100, Train Loss: 0.0384, Val Loss: 0.0384\n",
            "Epoch 78/100, Train Loss: 0.0392, Val Loss: 0.0384\n",
            "Epoch 79/100, Train Loss: 0.0373, Val Loss: 0.0384\n",
            "Epoch 80/100, Train Loss: 0.0381, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0371, Val Loss: 0.0384\n",
            "Epoch 82/100, Train Loss: 0.0383, Val Loss: 0.0384\n",
            "Epoch 83/100, Train Loss: 0.0378, Val Loss: 0.0384\n",
            "Epoch 84/100, Train Loss: 0.0380, Val Loss: 0.0384\n",
            "Epoch 85/100, Train Loss: 0.0372, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0373, Val Loss: 0.0384\n",
            "Epoch 87/100, Train Loss: 0.0366, Val Loss: 0.0384\n",
            "Epoch 88/100, Train Loss: 0.0374, Val Loss: 0.0384\n",
            "Epoch 89/100, Train Loss: 0.0368, Val Loss: 0.0384\n",
            "Epoch 90/100, Train Loss: 0.0377, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0377, Val Loss: 0.0384\n",
            "Epoch 92/100, Train Loss: 0.0385, Val Loss: 0.0384\n",
            "Epoch 93/100, Train Loss: 0.0384, Val Loss: 0.0384\n",
            "Epoch 94/100, Train Loss: 0.0371, Val Loss: 0.0384\n",
            "Epoch 95/100, Train Loss: 0.0376, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0378, Val Loss: 0.0384\n",
            "Epoch 97/100, Train Loss: 0.0381, Val Loss: 0.0384\n",
            "Epoch 98/100, Train Loss: 0.0370, Val Loss: 0.0384\n",
            "Epoch 99/100, Train Loss: 0.0368, Val Loss: 0.0384\n",
            "Epoch 100/100, Train Loss: 0.0371, Val Loss: 0.0384\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 6/28\n",
            "Current training set size: 102 samples\n",
            "Epoch 1/100, Train Loss: 0.1533, Val Loss: 0.1908\n",
            "Epoch 2/100, Train Loss: 0.1460, Val Loss: 0.2132\n",
            "Epoch 3/100, Train Loss: 0.1488, Val Loss: 0.1805\n",
            "Epoch 4/100, Train Loss: 0.1403, Val Loss: 0.1565\n",
            "Epoch 5/100, Train Loss: 0.1097, Val Loss: 0.0819\n",
            "Epoch 6/100, Train Loss: 0.0889, Val Loss: 0.0634\n",
            "Epoch 7/100, Train Loss: 0.0720, Val Loss: 0.0601\n",
            "Epoch 8/100, Train Loss: 0.0658, Val Loss: 0.0581\n",
            "Epoch 9/100, Train Loss: 0.0652, Val Loss: 0.0557\n",
            "Epoch 10/100, Train Loss: 0.0562, Val Loss: 0.0427\n",
            "Epoch 11/100, Train Loss: 0.0486, Val Loss: 0.0452\n",
            "Epoch 12/100, Train Loss: 0.0521, Val Loss: 0.0565\n",
            "Epoch 13/100, Train Loss: 0.0528, Val Loss: 0.0559\n",
            "Epoch 14/100, Train Loss: 0.0534, Val Loss: 0.0362\n",
            "Epoch 15/100, Train Loss: 0.0492, Val Loss: 0.0354\n",
            "Epoch 16/100, Train Loss: 0.0466, Val Loss: 0.0458\n",
            "Epoch 17/100, Train Loss: 0.0417, Val Loss: 0.0398\n",
            "Epoch 18/100, Train Loss: 0.0430, Val Loss: 0.0429\n",
            "Epoch 19/100, Train Loss: 0.0405, Val Loss: 0.0524\n",
            "Epoch 20/100, Train Loss: 0.0480, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0560, Val Loss: 0.0461\n",
            "Epoch 22/100, Train Loss: 0.0447, Val Loss: 0.0439\n",
            "Epoch 23/100, Train Loss: 0.0366, Val Loss: 0.0433\n",
            "Epoch 24/100, Train Loss: 0.0358, Val Loss: 0.0431\n",
            "Epoch 25/100, Train Loss: 0.0344, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0346, Val Loss: 0.0435\n",
            "Epoch 27/100, Train Loss: 0.0343, Val Loss: 0.0435\n",
            "Epoch 28/100, Train Loss: 0.0376, Val Loss: 0.0435\n",
            "Epoch 29/100, Train Loss: 0.0347, Val Loss: 0.0435\n",
            "Epoch 30/100, Train Loss: 0.0347, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0347, Val Loss: 0.0435\n",
            "Epoch 32/100, Train Loss: 0.0346, Val Loss: 0.0435\n",
            "Epoch 33/100, Train Loss: 0.0344, Val Loss: 0.0435\n",
            "Epoch 34/100, Train Loss: 0.0343, Val Loss: 0.0435\n",
            "Epoch 35/100, Train Loss: 0.0342, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0343, Val Loss: 0.0435\n",
            "Epoch 37/100, Train Loss: 0.0347, Val Loss: 0.0435\n",
            "Epoch 38/100, Train Loss: 0.0342, Val Loss: 0.0435\n",
            "Epoch 39/100, Train Loss: 0.0340, Val Loss: 0.0435\n",
            "Epoch 40/100, Train Loss: 0.0351, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0343, Val Loss: 0.0435\n",
            "Epoch 42/100, Train Loss: 0.0346, Val Loss: 0.0435\n",
            "Epoch 43/100, Train Loss: 0.0343, Val Loss: 0.0435\n",
            "Epoch 44/100, Train Loss: 0.0337, Val Loss: 0.0435\n",
            "Epoch 45/100, Train Loss: 0.0340, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0338, Val Loss: 0.0435\n",
            "Epoch 47/100, Train Loss: 0.0338, Val Loss: 0.0435\n",
            "Epoch 48/100, Train Loss: 0.0342, Val Loss: 0.0435\n",
            "Epoch 49/100, Train Loss: 0.0349, Val Loss: 0.0435\n",
            "Epoch 50/100, Train Loss: 0.0339, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0355, Val Loss: 0.0435\n",
            "Epoch 52/100, Train Loss: 0.0343, Val Loss: 0.0435\n",
            "Epoch 53/100, Train Loss: 0.0337, Val Loss: 0.0435\n",
            "Epoch 54/100, Train Loss: 0.0343, Val Loss: 0.0435\n",
            "Epoch 55/100, Train Loss: 0.0376, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0361, Val Loss: 0.0435\n",
            "Epoch 57/100, Train Loss: 0.0340, Val Loss: 0.0435\n",
            "Epoch 58/100, Train Loss: 0.0341, Val Loss: 0.0435\n",
            "Epoch 59/100, Train Loss: 0.0337, Val Loss: 0.0435\n",
            "Epoch 60/100, Train Loss: 0.0345, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0340, Val Loss: 0.0435\n",
            "Epoch 62/100, Train Loss: 0.0337, Val Loss: 0.0435\n",
            "Epoch 63/100, Train Loss: 0.0344, Val Loss: 0.0435\n",
            "Epoch 64/100, Train Loss: 0.0345, Val Loss: 0.0435\n",
            "Epoch 65/100, Train Loss: 0.0352, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0346, Val Loss: 0.0435\n",
            "Epoch 67/100, Train Loss: 0.0338, Val Loss: 0.0435\n",
            "Epoch 68/100, Train Loss: 0.0342, Val Loss: 0.0435\n",
            "Epoch 69/100, Train Loss: 0.0339, Val Loss: 0.0435\n",
            "Epoch 70/100, Train Loss: 0.0343, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0349, Val Loss: 0.0435\n",
            "Epoch 72/100, Train Loss: 0.0347, Val Loss: 0.0435\n",
            "Epoch 73/100, Train Loss: 0.0348, Val Loss: 0.0435\n",
            "Epoch 74/100, Train Loss: 0.0336, Val Loss: 0.0435\n",
            "Epoch 75/100, Train Loss: 0.0348, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0344, Val Loss: 0.0435\n",
            "Epoch 77/100, Train Loss: 0.0349, Val Loss: 0.0435\n",
            "Epoch 78/100, Train Loss: 0.0347, Val Loss: 0.0435\n",
            "Epoch 79/100, Train Loss: 0.0335, Val Loss: 0.0435\n",
            "Epoch 80/100, Train Loss: 0.0342, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0340, Val Loss: 0.0435\n",
            "Epoch 82/100, Train Loss: 0.0345, Val Loss: 0.0435\n",
            "Epoch 83/100, Train Loss: 0.0332, Val Loss: 0.0435\n",
            "Epoch 84/100, Train Loss: 0.0343, Val Loss: 0.0435\n",
            "Epoch 85/100, Train Loss: 0.0343, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0340, Val Loss: 0.0435\n",
            "Epoch 87/100, Train Loss: 0.0344, Val Loss: 0.0435\n",
            "Epoch 88/100, Train Loss: 0.0339, Val Loss: 0.0435\n",
            "Epoch 89/100, Train Loss: 0.0343, Val Loss: 0.0435\n",
            "Epoch 90/100, Train Loss: 0.0355, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0343, Val Loss: 0.0435\n",
            "Epoch 92/100, Train Loss: 0.0349, Val Loss: 0.0435\n",
            "Epoch 93/100, Train Loss: 0.0336, Val Loss: 0.0435\n",
            "Epoch 94/100, Train Loss: 0.0357, Val Loss: 0.0435\n",
            "Epoch 95/100, Train Loss: 0.0345, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0348, Val Loss: 0.0435\n",
            "Epoch 97/100, Train Loss: 0.0349, Val Loss: 0.0435\n",
            "Epoch 98/100, Train Loss: 0.0362, Val Loss: 0.0435\n",
            "Epoch 99/100, Train Loss: 0.0345, Val Loss: 0.0435\n",
            "Epoch 100/100, Train Loss: 0.0343, Val Loss: 0.0435\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 7/28\n",
            "Current training set size: 103 samples\n",
            "Epoch 1/100, Train Loss: 0.1712, Val Loss: 0.2272\n",
            "Epoch 2/100, Train Loss: 0.1493, Val Loss: 0.2018\n",
            "Epoch 3/100, Train Loss: 0.1406, Val Loss: 0.1773\n",
            "Epoch 4/100, Train Loss: 0.1391, Val Loss: 0.1674\n",
            "Epoch 5/100, Train Loss: 0.1193, Val Loss: 0.1181\n",
            "Epoch 6/100, Train Loss: 0.1073, Val Loss: 0.1026\n",
            "Epoch 7/100, Train Loss: 0.0917, Val Loss: 0.1025\n",
            "Epoch 8/100, Train Loss: 0.0767, Val Loss: 0.0365\n",
            "Epoch 9/100, Train Loss: 0.0693, Val Loss: 0.0395\n",
            "Epoch 10/100, Train Loss: 0.0631, Val Loss: 0.0518\n",
            "Epoch 11/100, Train Loss: 0.0657, Val Loss: 0.0504\n",
            "Epoch 12/100, Train Loss: 0.0561, Val Loss: 0.0413\n",
            "Epoch 13/100, Train Loss: 0.0521, Val Loss: 0.0531\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 14/100, Train Loss: 0.0478, Val Loss: 0.0512\n",
            "Epoch 15/100, Train Loss: 0.0455, Val Loss: 0.0491\n",
            "Epoch 16/100, Train Loss: 0.0445, Val Loss: 0.0475\n",
            "Epoch 17/100, Train Loss: 0.0438, Val Loss: 0.0467\n",
            "Epoch 18/100, Train Loss: 0.0442, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0434, Val Loss: 0.0455\n",
            "Epoch 20/100, Train Loss: 0.0436, Val Loss: 0.0455\n",
            "Epoch 21/100, Train Loss: 0.0438, Val Loss: 0.0455\n",
            "Epoch 22/100, Train Loss: 0.0436, Val Loss: 0.0455\n",
            "Epoch 23/100, Train Loss: 0.0433, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0436, Val Loss: 0.0455\n",
            "Epoch 25/100, Train Loss: 0.0433, Val Loss: 0.0455\n",
            "Epoch 26/100, Train Loss: 0.0433, Val Loss: 0.0455\n",
            "Epoch 27/100, Train Loss: 0.0434, Val Loss: 0.0455\n",
            "Epoch 28/100, Train Loss: 0.0432, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0427, Val Loss: 0.0455\n",
            "Epoch 30/100, Train Loss: 0.0434, Val Loss: 0.0455\n",
            "Epoch 31/100, Train Loss: 0.0431, Val Loss: 0.0455\n",
            "Epoch 32/100, Train Loss: 0.0431, Val Loss: 0.0455\n",
            "Epoch 33/100, Train Loss: 0.0431, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0431, Val Loss: 0.0455\n",
            "Epoch 35/100, Train Loss: 0.0431, Val Loss: 0.0455\n",
            "Epoch 36/100, Train Loss: 0.0435, Val Loss: 0.0455\n",
            "Epoch 37/100, Train Loss: 0.0431, Val Loss: 0.0455\n",
            "Epoch 38/100, Train Loss: 0.0438, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0431, Val Loss: 0.0455\n",
            "Epoch 40/100, Train Loss: 0.0430, Val Loss: 0.0455\n",
            "Epoch 41/100, Train Loss: 0.0434, Val Loss: 0.0455\n",
            "Epoch 42/100, Train Loss: 0.0438, Val Loss: 0.0455\n",
            "Epoch 43/100, Train Loss: 0.0435, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0433, Val Loss: 0.0455\n",
            "Epoch 45/100, Train Loss: 0.0434, Val Loss: 0.0455\n",
            "Epoch 46/100, Train Loss: 0.0432, Val Loss: 0.0455\n",
            "Epoch 47/100, Train Loss: 0.0437, Val Loss: 0.0455\n",
            "Epoch 48/100, Train Loss: 0.0438, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0432, Val Loss: 0.0455\n",
            "Epoch 50/100, Train Loss: 0.0430, Val Loss: 0.0455\n",
            "Epoch 51/100, Train Loss: 0.0436, Val Loss: 0.0455\n",
            "Epoch 52/100, Train Loss: 0.0429, Val Loss: 0.0455\n",
            "Epoch 53/100, Train Loss: 0.0439, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0428, Val Loss: 0.0455\n",
            "Epoch 55/100, Train Loss: 0.0430, Val Loss: 0.0455\n",
            "Epoch 56/100, Train Loss: 0.0435, Val Loss: 0.0455\n",
            "Epoch 57/100, Train Loss: 0.0433, Val Loss: 0.0455\n",
            "Epoch 58/100, Train Loss: 0.0437, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0437, Val Loss: 0.0455\n",
            "Epoch 60/100, Train Loss: 0.0433, Val Loss: 0.0455\n",
            "Epoch 61/100, Train Loss: 0.0437, Val Loss: 0.0455\n",
            "Epoch 62/100, Train Loss: 0.0438, Val Loss: 0.0455\n",
            "Epoch 63/100, Train Loss: 0.0435, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0436, Val Loss: 0.0455\n",
            "Epoch 65/100, Train Loss: 0.0431, Val Loss: 0.0455\n",
            "Epoch 66/100, Train Loss: 0.0434, Val Loss: 0.0455\n",
            "Epoch 67/100, Train Loss: 0.0427, Val Loss: 0.0455\n",
            "Epoch 68/100, Train Loss: 0.0432, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0434, Val Loss: 0.0455\n",
            "Epoch 70/100, Train Loss: 0.0433, Val Loss: 0.0455\n",
            "Epoch 71/100, Train Loss: 0.0430, Val Loss: 0.0455\n",
            "Epoch 72/100, Train Loss: 0.0430, Val Loss: 0.0455\n",
            "Epoch 73/100, Train Loss: 0.0436, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0433, Val Loss: 0.0455\n",
            "Epoch 75/100, Train Loss: 0.0435, Val Loss: 0.0455\n",
            "Epoch 76/100, Train Loss: 0.0431, Val Loss: 0.0455\n",
            "Epoch 77/100, Train Loss: 0.0431, Val Loss: 0.0455\n",
            "Epoch 78/100, Train Loss: 0.0434, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0434, Val Loss: 0.0455\n",
            "Epoch 80/100, Train Loss: 0.0430, Val Loss: 0.0455\n",
            "Epoch 81/100, Train Loss: 0.0428, Val Loss: 0.0455\n",
            "Epoch 82/100, Train Loss: 0.0433, Val Loss: 0.0455\n",
            "Epoch 83/100, Train Loss: 0.0434, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0428, Val Loss: 0.0455\n",
            "Epoch 85/100, Train Loss: 0.0430, Val Loss: 0.0455\n",
            "Epoch 86/100, Train Loss: 0.0427, Val Loss: 0.0455\n",
            "Epoch 87/100, Train Loss: 0.0432, Val Loss: 0.0455\n",
            "Epoch 88/100, Train Loss: 0.0431, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0431, Val Loss: 0.0455\n",
            "Epoch 90/100, Train Loss: 0.0435, Val Loss: 0.0455\n",
            "Epoch 91/100, Train Loss: 0.0428, Val Loss: 0.0455\n",
            "Epoch 92/100, Train Loss: 0.0440, Val Loss: 0.0455\n",
            "Epoch 93/100, Train Loss: 0.0441, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0429, Val Loss: 0.0455\n",
            "Epoch 95/100, Train Loss: 0.0436, Val Loss: 0.0455\n",
            "Epoch 96/100, Train Loss: 0.0430, Val Loss: 0.0455\n",
            "Epoch 97/100, Train Loss: 0.0434, Val Loss: 0.0455\n",
            "Epoch 98/100, Train Loss: 0.0432, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0435, Val Loss: 0.0455\n",
            "Epoch 100/100, Train Loss: 0.0432, Val Loss: 0.0455\n",
            "\n",
            "Test iteration 8/28\n",
            "Current training set size: 104 samples\n",
            "Epoch 1/100, Train Loss: 0.1534, Val Loss: 0.1974\n",
            "Epoch 2/100, Train Loss: 0.1485, Val Loss: 0.1851\n",
            "Epoch 3/100, Train Loss: 0.1446, Val Loss: 0.1968\n",
            "Epoch 4/100, Train Loss: 0.1303, Val Loss: 0.1406\n",
            "Epoch 5/100, Train Loss: 0.0983, Val Loss: 0.0737\n",
            "Epoch 6/100, Train Loss: 0.0964, Val Loss: 0.0405\n",
            "Epoch 7/100, Train Loss: 0.0714, Val Loss: 0.0624\n",
            "Epoch 8/100, Train Loss: 0.0668, Val Loss: 0.0443\n",
            "Epoch 9/100, Train Loss: 0.0716, Val Loss: 0.0555\n",
            "Epoch 10/100, Train Loss: 0.0621, Val Loss: 0.0413\n",
            "Epoch 11/100, Train Loss: 0.0528, Val Loss: 0.0488\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 12/100, Train Loss: 0.0499, Val Loss: 0.0439\n",
            "Epoch 13/100, Train Loss: 0.0459, Val Loss: 0.0385\n",
            "Epoch 14/100, Train Loss: 0.0442, Val Loss: 0.0348\n",
            "Epoch 15/100, Train Loss: 0.0440, Val Loss: 0.0330\n",
            "Epoch 16/100, Train Loss: 0.0434, Val Loss: 0.0320\n",
            "Epoch 17/100, Train Loss: 0.0432, Val Loss: 0.0319\n",
            "Epoch 18/100, Train Loss: 0.0430, Val Loss: 0.0318\n",
            "Epoch 19/100, Train Loss: 0.0425, Val Loss: 0.0323\n",
            "Epoch 20/100, Train Loss: 0.0424, Val Loss: 0.0327\n",
            "Epoch 21/100, Train Loss: 0.0425, Val Loss: 0.0331\n",
            "Epoch 22/100, Train Loss: 0.0421, Val Loss: 0.0333\n",
            "Epoch 23/100, Train Loss: 0.0422, Val Loss: 0.0336\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0414, Val Loss: 0.0336\n",
            "Epoch 25/100, Train Loss: 0.0411, Val Loss: 0.0336\n",
            "Epoch 26/100, Train Loss: 0.0418, Val Loss: 0.0336\n",
            "Epoch 27/100, Train Loss: 0.0417, Val Loss: 0.0336\n",
            "Epoch 28/100, Train Loss: 0.0417, Val Loss: 0.0336\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0414, Val Loss: 0.0336\n",
            "Epoch 30/100, Train Loss: 0.0419, Val Loss: 0.0336\n",
            "Epoch 31/100, Train Loss: 0.0409, Val Loss: 0.0336\n",
            "Epoch 32/100, Train Loss: 0.0415, Val Loss: 0.0336\n",
            "Epoch 33/100, Train Loss: 0.0414, Val Loss: 0.0336\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0417, Val Loss: 0.0336\n",
            "Epoch 35/100, Train Loss: 0.0419, Val Loss: 0.0336\n",
            "Epoch 36/100, Train Loss: 0.0415, Val Loss: 0.0336\n",
            "Epoch 37/100, Train Loss: 0.0416, Val Loss: 0.0336\n",
            "Epoch 38/100, Train Loss: 0.0415, Val Loss: 0.0336\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0411, Val Loss: 0.0336\n",
            "Epoch 40/100, Train Loss: 0.0420, Val Loss: 0.0336\n",
            "Epoch 41/100, Train Loss: 0.0420, Val Loss: 0.0336\n",
            "Epoch 42/100, Train Loss: 0.0418, Val Loss: 0.0336\n",
            "Epoch 43/100, Train Loss: 0.0416, Val Loss: 0.0336\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0417, Val Loss: 0.0336\n",
            "Epoch 45/100, Train Loss: 0.0421, Val Loss: 0.0336\n",
            "Epoch 46/100, Train Loss: 0.0415, Val Loss: 0.0336\n",
            "Epoch 47/100, Train Loss: 0.0414, Val Loss: 0.0336\n",
            "Epoch 48/100, Train Loss: 0.0415, Val Loss: 0.0336\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0417, Val Loss: 0.0336\n",
            "Epoch 50/100, Train Loss: 0.0419, Val Loss: 0.0336\n",
            "Epoch 51/100, Train Loss: 0.0419, Val Loss: 0.0336\n",
            "Epoch 52/100, Train Loss: 0.0414, Val Loss: 0.0336\n",
            "Epoch 53/100, Train Loss: 0.0419, Val Loss: 0.0336\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0414, Val Loss: 0.0336\n",
            "Epoch 55/100, Train Loss: 0.0418, Val Loss: 0.0336\n",
            "Epoch 56/100, Train Loss: 0.0417, Val Loss: 0.0336\n",
            "Epoch 57/100, Train Loss: 0.0418, Val Loss: 0.0336\n",
            "Epoch 58/100, Train Loss: 0.0420, Val Loss: 0.0336\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0420, Val Loss: 0.0336\n",
            "Epoch 60/100, Train Loss: 0.0418, Val Loss: 0.0336\n",
            "Epoch 61/100, Train Loss: 0.0417, Val Loss: 0.0336\n",
            "Epoch 62/100, Train Loss: 0.0411, Val Loss: 0.0336\n",
            "Epoch 63/100, Train Loss: 0.0416, Val Loss: 0.0336\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0418, Val Loss: 0.0336\n",
            "Epoch 65/100, Train Loss: 0.0419, Val Loss: 0.0336\n",
            "Epoch 66/100, Train Loss: 0.0418, Val Loss: 0.0336\n",
            "Epoch 67/100, Train Loss: 0.0419, Val Loss: 0.0336\n",
            "Epoch 68/100, Train Loss: 0.0413, Val Loss: 0.0336\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0421, Val Loss: 0.0336\n",
            "Epoch 70/100, Train Loss: 0.0411, Val Loss: 0.0336\n",
            "Epoch 71/100, Train Loss: 0.0418, Val Loss: 0.0336\n",
            "Epoch 72/100, Train Loss: 0.0419, Val Loss: 0.0336\n",
            "Epoch 73/100, Train Loss: 0.0416, Val Loss: 0.0336\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0411, Val Loss: 0.0336\n",
            "Epoch 75/100, Train Loss: 0.0419, Val Loss: 0.0336\n",
            "Epoch 76/100, Train Loss: 0.0412, Val Loss: 0.0336\n",
            "Epoch 77/100, Train Loss: 0.0416, Val Loss: 0.0336\n",
            "Epoch 78/100, Train Loss: 0.0420, Val Loss: 0.0336\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0414, Val Loss: 0.0336\n",
            "Epoch 80/100, Train Loss: 0.0418, Val Loss: 0.0336\n",
            "Epoch 81/100, Train Loss: 0.0411, Val Loss: 0.0336\n",
            "Epoch 82/100, Train Loss: 0.0418, Val Loss: 0.0336\n",
            "Epoch 83/100, Train Loss: 0.0417, Val Loss: 0.0336\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0423, Val Loss: 0.0336\n",
            "Epoch 85/100, Train Loss: 0.0419, Val Loss: 0.0336\n",
            "Epoch 86/100, Train Loss: 0.0414, Val Loss: 0.0336\n",
            "Epoch 87/100, Train Loss: 0.0416, Val Loss: 0.0336\n",
            "Epoch 88/100, Train Loss: 0.0423, Val Loss: 0.0336\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0415, Val Loss: 0.0336\n",
            "Epoch 90/100, Train Loss: 0.0420, Val Loss: 0.0336\n",
            "Epoch 91/100, Train Loss: 0.0415, Val Loss: 0.0336\n",
            "Epoch 92/100, Train Loss: 0.0418, Val Loss: 0.0336\n",
            "Epoch 93/100, Train Loss: 0.0419, Val Loss: 0.0336\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0413, Val Loss: 0.0336\n",
            "Epoch 95/100, Train Loss: 0.0415, Val Loss: 0.0336\n",
            "Epoch 96/100, Train Loss: 0.0416, Val Loss: 0.0336\n",
            "Epoch 97/100, Train Loss: 0.0417, Val Loss: 0.0336\n",
            "Epoch 98/100, Train Loss: 0.0421, Val Loss: 0.0336\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0419, Val Loss: 0.0336\n",
            "Epoch 100/100, Train Loss: 0.0417, Val Loss: 0.0336\n",
            "\n",
            "Test iteration 9/28\n",
            "Current training set size: 105 samples\n",
            "Epoch 1/100, Train Loss: 0.1566, Val Loss: 0.1985\n",
            "Epoch 2/100, Train Loss: 0.1483, Val Loss: 0.1915\n",
            "Epoch 3/100, Train Loss: 0.1318, Val Loss: 0.1712\n",
            "Epoch 4/100, Train Loss: 0.1340, Val Loss: 0.1826\n",
            "Epoch 5/100, Train Loss: 0.1247, Val Loss: 0.1368\n",
            "Epoch 6/100, Train Loss: 0.0892, Val Loss: 0.0639\n",
            "Epoch 7/100, Train Loss: 0.1046, Val Loss: 0.1100\n",
            "Epoch 8/100, Train Loss: 0.0655, Val Loss: 0.0495\n",
            "Epoch 9/100, Train Loss: 0.0597, Val Loss: 0.0325\n",
            "Epoch 10/100, Train Loss: 0.0591, Val Loss: 0.0334\n",
            "Epoch 11/100, Train Loss: 0.0534, Val Loss: 0.0753\n",
            "Epoch 12/100, Train Loss: 0.0659, Val Loss: 0.0354\n",
            "Epoch 13/100, Train Loss: 0.0506, Val Loss: 0.0503\n",
            "Epoch 14/100, Train Loss: 0.0462, Val Loss: 0.0381\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 15/100, Train Loss: 0.0424, Val Loss: 0.0376\n",
            "Epoch 16/100, Train Loss: 0.0402, Val Loss: 0.0364\n",
            "Epoch 17/100, Train Loss: 0.0396, Val Loss: 0.0360\n",
            "Epoch 18/100, Train Loss: 0.0386, Val Loss: 0.0357\n",
            "Epoch 19/100, Train Loss: 0.0392, Val Loss: 0.0355\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0390, Val Loss: 0.0355\n",
            "Epoch 21/100, Train Loss: 0.0383, Val Loss: 0.0355\n",
            "Epoch 22/100, Train Loss: 0.0394, Val Loss: 0.0355\n",
            "Epoch 23/100, Train Loss: 0.0378, Val Loss: 0.0355\n",
            "Epoch 24/100, Train Loss: 0.0399, Val Loss: 0.0355\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0442, Val Loss: 0.0355\n",
            "Epoch 26/100, Train Loss: 0.0382, Val Loss: 0.0355\n",
            "Epoch 27/100, Train Loss: 0.0381, Val Loss: 0.0355\n",
            "Epoch 28/100, Train Loss: 0.0383, Val Loss: 0.0355\n",
            "Epoch 29/100, Train Loss: 0.0381, Val Loss: 0.0355\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0380, Val Loss: 0.0355\n",
            "Epoch 31/100, Train Loss: 0.0383, Val Loss: 0.0355\n",
            "Epoch 32/100, Train Loss: 0.0379, Val Loss: 0.0355\n",
            "Epoch 33/100, Train Loss: 0.0392, Val Loss: 0.0355\n",
            "Epoch 34/100, Train Loss: 0.0380, Val Loss: 0.0355\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0398, Val Loss: 0.0355\n",
            "Epoch 36/100, Train Loss: 0.0382, Val Loss: 0.0355\n",
            "Epoch 37/100, Train Loss: 0.0383, Val Loss: 0.0355\n",
            "Epoch 38/100, Train Loss: 0.0376, Val Loss: 0.0355\n",
            "Epoch 39/100, Train Loss: 0.0389, Val Loss: 0.0355\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0387, Val Loss: 0.0355\n",
            "Epoch 41/100, Train Loss: 0.0380, Val Loss: 0.0355\n",
            "Epoch 42/100, Train Loss: 0.0377, Val Loss: 0.0355\n",
            "Epoch 43/100, Train Loss: 0.0391, Val Loss: 0.0355\n",
            "Epoch 44/100, Train Loss: 0.0378, Val Loss: 0.0355\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0380, Val Loss: 0.0355\n",
            "Epoch 46/100, Train Loss: 0.0396, Val Loss: 0.0355\n",
            "Epoch 47/100, Train Loss: 0.0379, Val Loss: 0.0355\n",
            "Epoch 48/100, Train Loss: 0.0384, Val Loss: 0.0355\n",
            "Epoch 49/100, Train Loss: 0.0384, Val Loss: 0.0355\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0392, Val Loss: 0.0355\n",
            "Epoch 51/100, Train Loss: 0.0385, Val Loss: 0.0355\n",
            "Epoch 52/100, Train Loss: 0.0381, Val Loss: 0.0355\n",
            "Epoch 53/100, Train Loss: 0.0376, Val Loss: 0.0355\n",
            "Epoch 54/100, Train Loss: 0.0384, Val Loss: 0.0355\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0382, Val Loss: 0.0355\n",
            "Epoch 56/100, Train Loss: 0.0386, Val Loss: 0.0355\n",
            "Epoch 57/100, Train Loss: 0.0386, Val Loss: 0.0355\n",
            "Epoch 58/100, Train Loss: 0.0379, Val Loss: 0.0355\n",
            "Epoch 59/100, Train Loss: 0.0385, Val Loss: 0.0355\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0420, Val Loss: 0.0355\n",
            "Epoch 61/100, Train Loss: 0.0448, Val Loss: 0.0355\n",
            "Epoch 62/100, Train Loss: 0.0388, Val Loss: 0.0355\n",
            "Epoch 63/100, Train Loss: 0.0389, Val Loss: 0.0355\n",
            "Epoch 64/100, Train Loss: 0.0390, Val Loss: 0.0355\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0380, Val Loss: 0.0355\n",
            "Epoch 66/100, Train Loss: 0.0403, Val Loss: 0.0355\n",
            "Epoch 67/100, Train Loss: 0.0383, Val Loss: 0.0355\n",
            "Epoch 68/100, Train Loss: 0.0373, Val Loss: 0.0355\n",
            "Epoch 69/100, Train Loss: 0.0387, Val Loss: 0.0355\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0387, Val Loss: 0.0355\n",
            "Epoch 71/100, Train Loss: 0.0395, Val Loss: 0.0355\n",
            "Epoch 72/100, Train Loss: 0.0381, Val Loss: 0.0355\n",
            "Epoch 73/100, Train Loss: 0.0378, Val Loss: 0.0355\n",
            "Epoch 74/100, Train Loss: 0.0384, Val Loss: 0.0355\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0373, Val Loss: 0.0355\n",
            "Epoch 76/100, Train Loss: 0.0388, Val Loss: 0.0355\n",
            "Epoch 77/100, Train Loss: 0.0396, Val Loss: 0.0355\n",
            "Epoch 78/100, Train Loss: 0.0377, Val Loss: 0.0355\n",
            "Epoch 79/100, Train Loss: 0.0383, Val Loss: 0.0355\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0385, Val Loss: 0.0355\n",
            "Epoch 81/100, Train Loss: 0.0387, Val Loss: 0.0355\n",
            "Epoch 82/100, Train Loss: 0.0379, Val Loss: 0.0355\n",
            "Epoch 83/100, Train Loss: 0.0383, Val Loss: 0.0355\n",
            "Epoch 84/100, Train Loss: 0.0388, Val Loss: 0.0355\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0383, Val Loss: 0.0355\n",
            "Epoch 86/100, Train Loss: 0.0379, Val Loss: 0.0355\n",
            "Epoch 87/100, Train Loss: 0.0386, Val Loss: 0.0355\n",
            "Epoch 88/100, Train Loss: 0.0397, Val Loss: 0.0355\n",
            "Epoch 89/100, Train Loss: 0.0383, Val Loss: 0.0355\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0379, Val Loss: 0.0355\n",
            "Epoch 91/100, Train Loss: 0.0381, Val Loss: 0.0355\n",
            "Epoch 92/100, Train Loss: 0.0381, Val Loss: 0.0355\n",
            "Epoch 93/100, Train Loss: 0.0383, Val Loss: 0.0355\n",
            "Epoch 94/100, Train Loss: 0.0379, Val Loss: 0.0355\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0392, Val Loss: 0.0355\n",
            "Epoch 96/100, Train Loss: 0.0396, Val Loss: 0.0355\n",
            "Epoch 97/100, Train Loss: 0.0384, Val Loss: 0.0355\n",
            "Epoch 98/100, Train Loss: 0.0367, Val Loss: 0.0355\n",
            "Epoch 99/100, Train Loss: 0.0377, Val Loss: 0.0355\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0397, Val Loss: 0.0355\n",
            "\n",
            "Test iteration 10/28\n",
            "Current training set size: 106 samples\n",
            "Epoch 1/100, Train Loss: 0.1762, Val Loss: 0.2183\n",
            "Epoch 2/100, Train Loss: 0.1441, Val Loss: 0.1823\n",
            "Epoch 3/100, Train Loss: 0.1394, Val Loss: 0.1645\n",
            "Epoch 4/100, Train Loss: 0.1142, Val Loss: 0.1186\n",
            "Epoch 5/100, Train Loss: 0.0959, Val Loss: 0.0696\n",
            "Epoch 6/100, Train Loss: 0.0909, Val Loss: 0.0451\n",
            "Epoch 7/100, Train Loss: 0.0718, Val Loss: 0.0448\n",
            "Epoch 8/100, Train Loss: 0.0678, Val Loss: 0.0415\n",
            "Epoch 9/100, Train Loss: 0.0711, Val Loss: 0.0507\n",
            "Epoch 10/100, Train Loss: 0.0560, Val Loss: 0.0522\n",
            "Epoch 11/100, Train Loss: 0.0515, Val Loss: 0.0514\n",
            "Epoch 12/100, Train Loss: 0.0583, Val Loss: 0.0474\n",
            "Epoch 13/100, Train Loss: 0.0530, Val Loss: 0.0437\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 14/100, Train Loss: 0.0420, Val Loss: 0.0419\n",
            "Epoch 15/100, Train Loss: 0.0393, Val Loss: 0.0407\n",
            "Epoch 16/100, Train Loss: 0.0387, Val Loss: 0.0400\n",
            "Epoch 17/100, Train Loss: 0.0377, Val Loss: 0.0397\n",
            "Epoch 18/100, Train Loss: 0.0388, Val Loss: 0.0388\n",
            "Epoch 19/100, Train Loss: 0.0384, Val Loss: 0.0383\n",
            "Epoch 20/100, Train Loss: 0.0379, Val Loss: 0.0381\n",
            "Epoch 21/100, Train Loss: 0.0377, Val Loss: 0.0375\n",
            "Epoch 22/100, Train Loss: 0.0379, Val Loss: 0.0375\n",
            "Epoch 23/100, Train Loss: 0.0374, Val Loss: 0.0374\n",
            "Epoch 24/100, Train Loss: 0.0380, Val Loss: 0.0371\n",
            "Epoch 25/100, Train Loss: 0.0373, Val Loss: 0.0371\n",
            "Epoch 26/100, Train Loss: 0.0369, Val Loss: 0.0367\n",
            "Epoch 27/100, Train Loss: 0.0374, Val Loss: 0.0362\n",
            "Epoch 28/100, Train Loss: 0.0367, Val Loss: 0.0359\n",
            "Epoch 29/100, Train Loss: 0.0363, Val Loss: 0.0357\n",
            "Epoch 30/100, Train Loss: 0.0371, Val Loss: 0.0356\n",
            "Epoch 31/100, Train Loss: 0.0370, Val Loss: 0.0354\n",
            "Epoch 32/100, Train Loss: 0.0422, Val Loss: 0.0352\n",
            "Epoch 33/100, Train Loss: 0.0367, Val Loss: 0.0353\n",
            "Epoch 34/100, Train Loss: 0.0415, Val Loss: 0.0351\n",
            "Epoch 35/100, Train Loss: 0.0365, Val Loss: 0.0352\n",
            "Epoch 36/100, Train Loss: 0.0363, Val Loss: 0.0351\n",
            "Epoch 37/100, Train Loss: 0.0364, Val Loss: 0.0349\n",
            "Epoch 38/100, Train Loss: 0.0372, Val Loss: 0.0351\n",
            "Epoch 39/100, Train Loss: 0.0415, Val Loss: 0.0350\n",
            "Epoch 40/100, Train Loss: 0.0368, Val Loss: 0.0351\n",
            "Epoch 41/100, Train Loss: 0.0372, Val Loss: 0.0350\n",
            "Epoch 42/100, Train Loss: 0.0362, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0358, Val Loss: 0.0351\n",
            "Epoch 44/100, Train Loss: 0.0359, Val Loss: 0.0351\n",
            "Epoch 45/100, Train Loss: 0.0369, Val Loss: 0.0351\n",
            "Epoch 46/100, Train Loss: 0.0361, Val Loss: 0.0351\n",
            "Epoch 47/100, Train Loss: 0.0379, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0363, Val Loss: 0.0351\n",
            "Epoch 49/100, Train Loss: 0.0364, Val Loss: 0.0351\n",
            "Epoch 50/100, Train Loss: 0.0364, Val Loss: 0.0351\n",
            "Epoch 51/100, Train Loss: 0.0365, Val Loss: 0.0351\n",
            "Epoch 52/100, Train Loss: 0.0364, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0365, Val Loss: 0.0351\n",
            "Epoch 54/100, Train Loss: 0.0361, Val Loss: 0.0351\n",
            "Epoch 55/100, Train Loss: 0.0364, Val Loss: 0.0351\n",
            "Epoch 56/100, Train Loss: 0.0413, Val Loss: 0.0351\n",
            "Epoch 57/100, Train Loss: 0.0364, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0363, Val Loss: 0.0351\n",
            "Epoch 59/100, Train Loss: 0.0364, Val Loss: 0.0351\n",
            "Epoch 60/100, Train Loss: 0.0365, Val Loss: 0.0351\n",
            "Epoch 61/100, Train Loss: 0.0359, Val Loss: 0.0351\n",
            "Epoch 62/100, Train Loss: 0.0368, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0357, Val Loss: 0.0351\n",
            "Epoch 64/100, Train Loss: 0.0364, Val Loss: 0.0351\n",
            "Epoch 65/100, Train Loss: 0.0362, Val Loss: 0.0351\n",
            "Epoch 66/100, Train Loss: 0.0366, Val Loss: 0.0351\n",
            "Epoch 67/100, Train Loss: 0.0364, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0357, Val Loss: 0.0351\n",
            "Epoch 69/100, Train Loss: 0.0370, Val Loss: 0.0351\n",
            "Epoch 70/100, Train Loss: 0.0367, Val Loss: 0.0351\n",
            "Epoch 71/100, Train Loss: 0.0362, Val Loss: 0.0351\n",
            "Epoch 72/100, Train Loss: 0.0375, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0362, Val Loss: 0.0351\n",
            "Epoch 74/100, Train Loss: 0.0360, Val Loss: 0.0351\n",
            "Epoch 75/100, Train Loss: 0.0375, Val Loss: 0.0351\n",
            "Epoch 76/100, Train Loss: 0.0364, Val Loss: 0.0351\n",
            "Epoch 77/100, Train Loss: 0.0364, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0363, Val Loss: 0.0351\n",
            "Epoch 79/100, Train Loss: 0.0354, Val Loss: 0.0351\n",
            "Epoch 80/100, Train Loss: 0.0369, Val Loss: 0.0351\n",
            "Epoch 81/100, Train Loss: 0.0362, Val Loss: 0.0351\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0360, Val Loss: 0.0351\n",
            "Epoch 84/100, Train Loss: 0.0367, Val Loss: 0.0351\n",
            "Epoch 85/100, Train Loss: 0.0414, Val Loss: 0.0351\n",
            "Epoch 86/100, Train Loss: 0.0363, Val Loss: 0.0351\n",
            "Epoch 87/100, Train Loss: 0.0362, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0356, Val Loss: 0.0351\n",
            "Epoch 89/100, Train Loss: 0.0362, Val Loss: 0.0351\n",
            "Epoch 90/100, Train Loss: 0.0363, Val Loss: 0.0351\n",
            "Epoch 91/100, Train Loss: 0.0368, Val Loss: 0.0351\n",
            "Epoch 92/100, Train Loss: 0.0362, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0360, Val Loss: 0.0351\n",
            "Epoch 94/100, Train Loss: 0.0370, Val Loss: 0.0351\n",
            "Epoch 95/100, Train Loss: 0.0364, Val Loss: 0.0351\n",
            "Epoch 96/100, Train Loss: 0.0371, Val Loss: 0.0351\n",
            "Epoch 97/100, Train Loss: 0.0360, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0364, Val Loss: 0.0351\n",
            "Epoch 99/100, Train Loss: 0.0366, Val Loss: 0.0351\n",
            "Epoch 100/100, Train Loss: 0.0360, Val Loss: 0.0351\n",
            "\n",
            "Test iteration 11/28\n",
            "Current training set size: 107 samples\n",
            "Epoch 1/100, Train Loss: 0.1568, Val Loss: 0.1904\n",
            "Epoch 2/100, Train Loss: 0.1506, Val Loss: 0.1859\n",
            "Epoch 3/100, Train Loss: 0.1399, Val Loss: 0.1692\n",
            "Epoch 4/100, Train Loss: 0.1292, Val Loss: 0.1385\n",
            "Epoch 5/100, Train Loss: 0.1113, Val Loss: 0.0957\n",
            "Epoch 6/100, Train Loss: 0.0838, Val Loss: 0.0423\n",
            "Epoch 7/100, Train Loss: 0.0758, Val Loss: 0.0531\n",
            "Epoch 8/100, Train Loss: 0.0703, Val Loss: 0.0493\n",
            "Epoch 9/100, Train Loss: 0.0777, Val Loss: 0.0576\n",
            "Epoch 10/100, Train Loss: 0.0581, Val Loss: 0.0471\n",
            "Epoch 11/100, Train Loss: 0.0601, Val Loss: 0.0500\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 12/100, Train Loss: 0.0527, Val Loss: 0.0438\n",
            "Epoch 13/100, Train Loss: 0.0483, Val Loss: 0.0374\n",
            "Epoch 14/100, Train Loss: 0.0462, Val Loss: 0.0338\n",
            "Epoch 15/100, Train Loss: 0.0453, Val Loss: 0.0320\n",
            "Epoch 16/100, Train Loss: 0.0449, Val Loss: 0.0318\n",
            "Epoch 17/100, Train Loss: 0.0446, Val Loss: 0.0320\n",
            "Epoch 18/100, Train Loss: 0.0443, Val Loss: 0.0320\n",
            "Epoch 19/100, Train Loss: 0.0451, Val Loss: 0.0324\n",
            "Epoch 20/100, Train Loss: 0.0442, Val Loss: 0.0328\n",
            "Epoch 21/100, Train Loss: 0.0431, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0441, Val Loss: 0.0330\n",
            "Epoch 23/100, Train Loss: 0.0427, Val Loss: 0.0330\n",
            "Epoch 24/100, Train Loss: 0.0437, Val Loss: 0.0330\n",
            "Epoch 25/100, Train Loss: 0.0431, Val Loss: 0.0330\n",
            "Epoch 26/100, Train Loss: 0.0433, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0437, Val Loss: 0.0330\n",
            "Epoch 28/100, Train Loss: 0.0435, Val Loss: 0.0330\n",
            "Epoch 29/100, Train Loss: 0.0432, Val Loss: 0.0330\n",
            "Epoch 30/100, Train Loss: 0.0441, Val Loss: 0.0330\n",
            "Epoch 31/100, Train Loss: 0.0436, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0432, Val Loss: 0.0330\n",
            "Epoch 33/100, Train Loss: 0.0435, Val Loss: 0.0330\n",
            "Epoch 34/100, Train Loss: 0.0439, Val Loss: 0.0330\n",
            "Epoch 35/100, Train Loss: 0.0433, Val Loss: 0.0330\n",
            "Epoch 36/100, Train Loss: 0.0438, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0435, Val Loss: 0.0330\n",
            "Epoch 38/100, Train Loss: 0.0432, Val Loss: 0.0330\n",
            "Epoch 39/100, Train Loss: 0.0436, Val Loss: 0.0330\n",
            "Epoch 40/100, Train Loss: 0.0454, Val Loss: 0.0330\n",
            "Epoch 41/100, Train Loss: 0.0435, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0440, Val Loss: 0.0330\n",
            "Epoch 43/100, Train Loss: 0.0436, Val Loss: 0.0330\n",
            "Epoch 44/100, Train Loss: 0.0432, Val Loss: 0.0330\n",
            "Epoch 45/100, Train Loss: 0.0438, Val Loss: 0.0330\n",
            "Epoch 46/100, Train Loss: 0.0436, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0430, Val Loss: 0.0330\n",
            "Epoch 48/100, Train Loss: 0.0435, Val Loss: 0.0330\n",
            "Epoch 49/100, Train Loss: 0.0437, Val Loss: 0.0330\n",
            "Epoch 50/100, Train Loss: 0.0440, Val Loss: 0.0330\n",
            "Epoch 51/100, Train Loss: 0.0445, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0429, Val Loss: 0.0330\n",
            "Epoch 53/100, Train Loss: 0.0435, Val Loss: 0.0330\n",
            "Epoch 54/100, Train Loss: 0.0440, Val Loss: 0.0330\n",
            "Epoch 55/100, Train Loss: 0.0433, Val Loss: 0.0330\n",
            "Epoch 56/100, Train Loss: 0.0435, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0435, Val Loss: 0.0330\n",
            "Epoch 58/100, Train Loss: 0.0437, Val Loss: 0.0330\n",
            "Epoch 59/100, Train Loss: 0.0433, Val Loss: 0.0330\n",
            "Epoch 60/100, Train Loss: 0.0428, Val Loss: 0.0330\n",
            "Epoch 61/100, Train Loss: 0.0445, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0433, Val Loss: 0.0330\n",
            "Epoch 63/100, Train Loss: 0.0430, Val Loss: 0.0330\n",
            "Epoch 64/100, Train Loss: 0.0432, Val Loss: 0.0330\n",
            "Epoch 65/100, Train Loss: 0.0434, Val Loss: 0.0330\n",
            "Epoch 66/100, Train Loss: 0.0437, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0435, Val Loss: 0.0330\n",
            "Epoch 68/100, Train Loss: 0.0435, Val Loss: 0.0330\n",
            "Epoch 69/100, Train Loss: 0.0443, Val Loss: 0.0330\n",
            "Epoch 70/100, Train Loss: 0.0437, Val Loss: 0.0330\n",
            "Epoch 71/100, Train Loss: 0.0435, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0435, Val Loss: 0.0330\n",
            "Epoch 73/100, Train Loss: 0.0434, Val Loss: 0.0330\n",
            "Epoch 74/100, Train Loss: 0.0430, Val Loss: 0.0330\n",
            "Epoch 75/100, Train Loss: 0.0436, Val Loss: 0.0330\n",
            "Epoch 76/100, Train Loss: 0.0433, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0445, Val Loss: 0.0330\n",
            "Epoch 78/100, Train Loss: 0.0429, Val Loss: 0.0330\n",
            "Epoch 79/100, Train Loss: 0.0450, Val Loss: 0.0330\n",
            "Epoch 80/100, Train Loss: 0.0439, Val Loss: 0.0330\n",
            "Epoch 81/100, Train Loss: 0.0437, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0437, Val Loss: 0.0330\n",
            "Epoch 83/100, Train Loss: 0.0437, Val Loss: 0.0330\n",
            "Epoch 84/100, Train Loss: 0.0443, Val Loss: 0.0330\n",
            "Epoch 85/100, Train Loss: 0.0435, Val Loss: 0.0330\n",
            "Epoch 86/100, Train Loss: 0.0435, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0433, Val Loss: 0.0330\n",
            "Epoch 88/100, Train Loss: 0.0427, Val Loss: 0.0330\n",
            "Epoch 89/100, Train Loss: 0.0436, Val Loss: 0.0330\n",
            "Epoch 90/100, Train Loss: 0.0434, Val Loss: 0.0330\n",
            "Epoch 91/100, Train Loss: 0.0435, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0434, Val Loss: 0.0330\n",
            "Epoch 93/100, Train Loss: 0.0435, Val Loss: 0.0330\n",
            "Epoch 94/100, Train Loss: 0.0435, Val Loss: 0.0330\n",
            "Epoch 95/100, Train Loss: 0.0436, Val Loss: 0.0330\n",
            "Epoch 96/100, Train Loss: 0.0436, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0438, Val Loss: 0.0330\n",
            "Epoch 98/100, Train Loss: 0.0436, Val Loss: 0.0330\n",
            "Epoch 99/100, Train Loss: 0.0434, Val Loss: 0.0330\n",
            "Epoch 100/100, Train Loss: 0.0436, Val Loss: 0.0330\n",
            "\n",
            "Test iteration 12/28\n",
            "Current training set size: 108 samples\n",
            "Epoch 1/100, Train Loss: 0.1545, Val Loss: 0.1907\n",
            "Epoch 2/100, Train Loss: 0.1501, Val Loss: 0.1856\n",
            "Epoch 3/100, Train Loss: 0.1420, Val Loss: 0.1762\n",
            "Epoch 4/100, Train Loss: 0.1218, Val Loss: 0.1390\n",
            "Epoch 5/100, Train Loss: 0.1002, Val Loss: 0.0720\n",
            "Epoch 6/100, Train Loss: 0.0966, Val Loss: 0.0904\n",
            "Epoch 7/100, Train Loss: 0.0719, Val Loss: 0.1201\n",
            "Epoch 8/100, Train Loss: 0.0975, Val Loss: 0.0765\n",
            "Epoch 9/100, Train Loss: 0.0707, Val Loss: 0.0668\n",
            "Epoch 10/100, Train Loss: 0.0600, Val Loss: 0.0627\n",
            "Epoch 11/100, Train Loss: 0.0677, Val Loss: 0.0581\n",
            "Epoch 12/100, Train Loss: 0.0509, Val Loss: 0.0526\n",
            "Epoch 13/100, Train Loss: 0.0490, Val Loss: 0.0679\n",
            "Epoch 14/100, Train Loss: 0.0578, Val Loss: 0.0572\n",
            "Epoch 15/100, Train Loss: 0.0508, Val Loss: 0.0535\n",
            "Epoch 16/100, Train Loss: 0.0445, Val Loss: 0.0399\n",
            "Epoch 17/100, Train Loss: 0.0403, Val Loss: 0.0360\n",
            "Epoch 18/100, Train Loss: 0.0396, Val Loss: 0.0569\n",
            "Epoch 19/100, Train Loss: 0.0384, Val Loss: 0.0411\n",
            "Epoch 20/100, Train Loss: 0.0390, Val Loss: 0.0485\n",
            "Epoch 21/100, Train Loss: 0.0382, Val Loss: 0.0453\n",
            "Epoch 22/100, Train Loss: 0.0384, Val Loss: 0.0439\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0339, Val Loss: 0.0424\n",
            "Epoch 24/100, Train Loss: 0.0322, Val Loss: 0.0411\n",
            "Epoch 25/100, Train Loss: 0.0314, Val Loss: 0.0406\n",
            "Epoch 26/100, Train Loss: 0.0314, Val Loss: 0.0396\n",
            "Epoch 27/100, Train Loss: 0.0317, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0310, Val Loss: 0.0398\n",
            "Epoch 29/100, Train Loss: 0.0306, Val Loss: 0.0398\n",
            "Epoch 30/100, Train Loss: 0.0306, Val Loss: 0.0398\n",
            "Epoch 31/100, Train Loss: 0.0316, Val Loss: 0.0398\n",
            "Epoch 32/100, Train Loss: 0.0311, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0313, Val Loss: 0.0398\n",
            "Epoch 34/100, Train Loss: 0.0319, Val Loss: 0.0398\n",
            "Epoch 35/100, Train Loss: 0.0312, Val Loss: 0.0398\n",
            "Epoch 36/100, Train Loss: 0.0313, Val Loss: 0.0398\n",
            "Epoch 37/100, Train Loss: 0.0314, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0317, Val Loss: 0.0398\n",
            "Epoch 39/100, Train Loss: 0.0307, Val Loss: 0.0398\n",
            "Epoch 40/100, Train Loss: 0.0320, Val Loss: 0.0398\n",
            "Epoch 41/100, Train Loss: 0.0308, Val Loss: 0.0398\n",
            "Epoch 42/100, Train Loss: 0.0309, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0311, Val Loss: 0.0398\n",
            "Epoch 44/100, Train Loss: 0.0317, Val Loss: 0.0398\n",
            "Epoch 45/100, Train Loss: 0.0313, Val Loss: 0.0398\n",
            "Epoch 46/100, Train Loss: 0.0308, Val Loss: 0.0398\n",
            "Epoch 47/100, Train Loss: 0.0310, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0310, Val Loss: 0.0398\n",
            "Epoch 49/100, Train Loss: 0.0309, Val Loss: 0.0398\n",
            "Epoch 50/100, Train Loss: 0.0315, Val Loss: 0.0398\n",
            "Epoch 51/100, Train Loss: 0.0310, Val Loss: 0.0398\n",
            "Epoch 52/100, Train Loss: 0.0317, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0309, Val Loss: 0.0398\n",
            "Epoch 54/100, Train Loss: 0.0307, Val Loss: 0.0398\n",
            "Epoch 55/100, Train Loss: 0.0313, Val Loss: 0.0398\n",
            "Epoch 56/100, Train Loss: 0.0317, Val Loss: 0.0398\n",
            "Epoch 57/100, Train Loss: 0.0315, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0322, Val Loss: 0.0398\n",
            "Epoch 59/100, Train Loss: 0.0309, Val Loss: 0.0398\n",
            "Epoch 60/100, Train Loss: 0.0312, Val Loss: 0.0398\n",
            "Epoch 61/100, Train Loss: 0.0314, Val Loss: 0.0398\n",
            "Epoch 62/100, Train Loss: 0.0312, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0324, Val Loss: 0.0398\n",
            "Epoch 64/100, Train Loss: 0.0315, Val Loss: 0.0398\n",
            "Epoch 65/100, Train Loss: 0.0305, Val Loss: 0.0398\n",
            "Epoch 66/100, Train Loss: 0.0319, Val Loss: 0.0398\n",
            "Epoch 67/100, Train Loss: 0.0307, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0311, Val Loss: 0.0398\n",
            "Epoch 69/100, Train Loss: 0.0312, Val Loss: 0.0398\n",
            "Epoch 70/100, Train Loss: 0.0310, Val Loss: 0.0398\n",
            "Epoch 71/100, Train Loss: 0.0314, Val Loss: 0.0398\n",
            "Epoch 72/100, Train Loss: 0.0310, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0307, Val Loss: 0.0398\n",
            "Epoch 74/100, Train Loss: 0.0313, Val Loss: 0.0398\n",
            "Epoch 75/100, Train Loss: 0.0313, Val Loss: 0.0398\n",
            "Epoch 76/100, Train Loss: 0.0311, Val Loss: 0.0398\n",
            "Epoch 77/100, Train Loss: 0.0312, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0314, Val Loss: 0.0398\n",
            "Epoch 79/100, Train Loss: 0.0310, Val Loss: 0.0398\n",
            "Epoch 80/100, Train Loss: 0.0313, Val Loss: 0.0398\n",
            "Epoch 81/100, Train Loss: 0.0311, Val Loss: 0.0398\n",
            "Epoch 82/100, Train Loss: 0.0309, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0318, Val Loss: 0.0398\n",
            "Epoch 84/100, Train Loss: 0.0307, Val Loss: 0.0398\n",
            "Epoch 85/100, Train Loss: 0.0316, Val Loss: 0.0398\n",
            "Epoch 86/100, Train Loss: 0.0310, Val Loss: 0.0398\n",
            "Epoch 87/100, Train Loss: 0.0318, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0308, Val Loss: 0.0398\n",
            "Epoch 89/100, Train Loss: 0.0313, Val Loss: 0.0398\n",
            "Epoch 90/100, Train Loss: 0.0315, Val Loss: 0.0398\n",
            "Epoch 91/100, Train Loss: 0.0316, Val Loss: 0.0398\n",
            "Epoch 92/100, Train Loss: 0.0316, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0320, Val Loss: 0.0398\n",
            "Epoch 94/100, Train Loss: 0.0304, Val Loss: 0.0398\n",
            "Epoch 95/100, Train Loss: 0.0311, Val Loss: 0.0398\n",
            "Epoch 96/100, Train Loss: 0.0316, Val Loss: 0.0398\n",
            "Epoch 97/100, Train Loss: 0.0304, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0314, Val Loss: 0.0398\n",
            "Epoch 99/100, Train Loss: 0.0310, Val Loss: 0.0398\n",
            "Epoch 100/100, Train Loss: 0.0312, Val Loss: 0.0398\n",
            "\n",
            "Test iteration 13/28\n",
            "Current training set size: 109 samples\n",
            "Epoch 1/100, Train Loss: 0.1605, Val Loss: 0.1903\n",
            "Epoch 2/100, Train Loss: 0.1451, Val Loss: 0.2021\n",
            "Epoch 3/100, Train Loss: 0.1386, Val Loss: 0.1743\n",
            "Epoch 4/100, Train Loss: 0.1099, Val Loss: 0.0885\n",
            "Epoch 5/100, Train Loss: 0.1032, Val Loss: 0.0473\n",
            "Epoch 6/100, Train Loss: 0.0722, Val Loss: 0.0448\n",
            "Epoch 7/100, Train Loss: 0.0716, Val Loss: 0.0781\n",
            "Epoch 8/100, Train Loss: 0.0765, Val Loss: 0.0400\n",
            "Epoch 9/100, Train Loss: 0.0669, Val Loss: 0.0465\n",
            "Epoch 10/100, Train Loss: 0.0596, Val Loss: 0.0407\n",
            "Epoch 11/100, Train Loss: 0.0666, Val Loss: 0.0678\n",
            "Epoch 12/100, Train Loss: 0.0581, Val Loss: 0.0448\n",
            "Epoch 13/100, Train Loss: 0.0436, Val Loss: 0.0390\n",
            "Epoch 14/100, Train Loss: 0.0591, Val Loss: 0.0438\n",
            "Epoch 15/100, Train Loss: 0.0474, Val Loss: 0.0441\n",
            "Epoch 16/100, Train Loss: 0.0466, Val Loss: 0.0587\n",
            "Epoch 17/100, Train Loss: 0.0427, Val Loss: 0.0413\n",
            "Epoch 18/100, Train Loss: 0.0434, Val Loss: 0.0600\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0442, Val Loss: 0.0573\n",
            "Epoch 20/100, Train Loss: 0.0391, Val Loss: 0.0532\n",
            "Epoch 21/100, Train Loss: 0.0370, Val Loss: 0.0515\n",
            "Epoch 22/100, Train Loss: 0.0368, Val Loss: 0.0511\n",
            "Epoch 23/100, Train Loss: 0.0366, Val Loss: 0.0500\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0353, Val Loss: 0.0500\n",
            "Epoch 25/100, Train Loss: 0.0373, Val Loss: 0.0500\n",
            "Epoch 26/100, Train Loss: 0.0375, Val Loss: 0.0500\n",
            "Epoch 27/100, Train Loss: 0.0370, Val Loss: 0.0500\n",
            "Epoch 28/100, Train Loss: 0.0374, Val Loss: 0.0500\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0373, Val Loss: 0.0500\n",
            "Epoch 30/100, Train Loss: 0.0372, Val Loss: 0.0500\n",
            "Epoch 31/100, Train Loss: 0.0371, Val Loss: 0.0500\n",
            "Epoch 32/100, Train Loss: 0.0359, Val Loss: 0.0500\n",
            "Epoch 33/100, Train Loss: 0.0363, Val Loss: 0.0500\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0357, Val Loss: 0.0500\n",
            "Epoch 35/100, Train Loss: 0.0416, Val Loss: 0.0500\n",
            "Epoch 36/100, Train Loss: 0.0362, Val Loss: 0.0500\n",
            "Epoch 37/100, Train Loss: 0.0360, Val Loss: 0.0500\n",
            "Epoch 38/100, Train Loss: 0.0367, Val Loss: 0.0500\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0360, Val Loss: 0.0500\n",
            "Epoch 40/100, Train Loss: 0.0370, Val Loss: 0.0500\n",
            "Epoch 41/100, Train Loss: 0.0363, Val Loss: 0.0500\n",
            "Epoch 42/100, Train Loss: 0.0378, Val Loss: 0.0500\n",
            "Epoch 43/100, Train Loss: 0.0366, Val Loss: 0.0500\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0364, Val Loss: 0.0500\n",
            "Epoch 45/100, Train Loss: 0.0363, Val Loss: 0.0500\n",
            "Epoch 46/100, Train Loss: 0.0368, Val Loss: 0.0500\n",
            "Epoch 47/100, Train Loss: 0.0363, Val Loss: 0.0500\n",
            "Epoch 48/100, Train Loss: 0.0357, Val Loss: 0.0500\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0365, Val Loss: 0.0500\n",
            "Epoch 50/100, Train Loss: 0.0378, Val Loss: 0.0500\n",
            "Epoch 51/100, Train Loss: 0.0373, Val Loss: 0.0500\n",
            "Epoch 52/100, Train Loss: 0.0356, Val Loss: 0.0500\n",
            "Epoch 53/100, Train Loss: 0.0359, Val Loss: 0.0500\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0372, Val Loss: 0.0500\n",
            "Epoch 55/100, Train Loss: 0.0455, Val Loss: 0.0500\n",
            "Epoch 56/100, Train Loss: 0.0364, Val Loss: 0.0500\n",
            "Epoch 57/100, Train Loss: 0.0379, Val Loss: 0.0500\n",
            "Epoch 58/100, Train Loss: 0.0371, Val Loss: 0.0500\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0360, Val Loss: 0.0500\n",
            "Epoch 60/100, Train Loss: 0.0378, Val Loss: 0.0500\n",
            "Epoch 61/100, Train Loss: 0.0370, Val Loss: 0.0500\n",
            "Epoch 62/100, Train Loss: 0.0368, Val Loss: 0.0500\n",
            "Epoch 63/100, Train Loss: 0.0371, Val Loss: 0.0500\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0372, Val Loss: 0.0500\n",
            "Epoch 65/100, Train Loss: 0.0364, Val Loss: 0.0500\n",
            "Epoch 66/100, Train Loss: 0.0353, Val Loss: 0.0500\n",
            "Epoch 67/100, Train Loss: 0.0363, Val Loss: 0.0500\n",
            "Epoch 68/100, Train Loss: 0.0425, Val Loss: 0.0500\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0365, Val Loss: 0.0500\n",
            "Epoch 70/100, Train Loss: 0.0361, Val Loss: 0.0500\n",
            "Epoch 71/100, Train Loss: 0.0357, Val Loss: 0.0500\n",
            "Epoch 72/100, Train Loss: 0.0368, Val Loss: 0.0500\n",
            "Epoch 73/100, Train Loss: 0.0355, Val Loss: 0.0500\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0358, Val Loss: 0.0500\n",
            "Epoch 75/100, Train Loss: 0.0370, Val Loss: 0.0500\n",
            "Epoch 76/100, Train Loss: 0.0369, Val Loss: 0.0500\n",
            "Epoch 77/100, Train Loss: 0.0378, Val Loss: 0.0500\n",
            "Epoch 78/100, Train Loss: 0.0361, Val Loss: 0.0500\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0373, Val Loss: 0.0500\n",
            "Epoch 80/100, Train Loss: 0.0368, Val Loss: 0.0500\n",
            "Epoch 81/100, Train Loss: 0.0375, Val Loss: 0.0500\n",
            "Epoch 82/100, Train Loss: 0.0365, Val Loss: 0.0500\n",
            "Epoch 83/100, Train Loss: 0.0370, Val Loss: 0.0500\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0365, Val Loss: 0.0500\n",
            "Epoch 85/100, Train Loss: 0.0359, Val Loss: 0.0500\n",
            "Epoch 86/100, Train Loss: 0.0362, Val Loss: 0.0500\n",
            "Epoch 87/100, Train Loss: 0.0367, Val Loss: 0.0500\n",
            "Epoch 88/100, Train Loss: 0.0360, Val Loss: 0.0500\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0364, Val Loss: 0.0500\n",
            "Epoch 90/100, Train Loss: 0.0356, Val Loss: 0.0500\n",
            "Epoch 91/100, Train Loss: 0.0359, Val Loss: 0.0500\n",
            "Epoch 92/100, Train Loss: 0.0356, Val Loss: 0.0500\n",
            "Epoch 93/100, Train Loss: 0.0366, Val Loss: 0.0500\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0361, Val Loss: 0.0500\n",
            "Epoch 95/100, Train Loss: 0.0368, Val Loss: 0.0500\n",
            "Epoch 96/100, Train Loss: 0.0368, Val Loss: 0.0500\n",
            "Epoch 97/100, Train Loss: 0.0359, Val Loss: 0.0500\n",
            "Epoch 98/100, Train Loss: 0.0368, Val Loss: 0.0500\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0361, Val Loss: 0.0500\n",
            "Epoch 100/100, Train Loss: 0.0357, Val Loss: 0.0500\n",
            "\n",
            "Test iteration 14/28\n",
            "Current training set size: 110 samples\n",
            "Epoch 1/100, Train Loss: 0.1579, Val Loss: 0.1989\n",
            "Epoch 2/100, Train Loss: 0.1519, Val Loss: 0.1882\n",
            "Epoch 3/100, Train Loss: 0.1442, Val Loss: 0.1770\n",
            "Epoch 4/100, Train Loss: 0.1325, Val Loss: 0.1662\n",
            "Epoch 5/100, Train Loss: 0.1300, Val Loss: 0.1376\n",
            "Epoch 6/100, Train Loss: 0.0935, Val Loss: 0.0549\n",
            "Epoch 7/100, Train Loss: 0.0744, Val Loss: 0.0594\n",
            "Epoch 8/100, Train Loss: 0.0644, Val Loss: 0.0446\n",
            "Epoch 9/100, Train Loss: 0.0675, Val Loss: 0.0424\n",
            "Epoch 10/100, Train Loss: 0.0565, Val Loss: 0.0417\n",
            "Epoch 11/100, Train Loss: 0.0509, Val Loss: 0.0441\n",
            "Epoch 12/100, Train Loss: 0.0497, Val Loss: 0.0478\n",
            "Epoch 13/100, Train Loss: 0.0490, Val Loss: 0.0454\n",
            "Epoch 14/100, Train Loss: 0.0523, Val Loss: 0.0696\n",
            "Epoch 15/100, Train Loss: 0.0484, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0380, Val Loss: 0.0442\n",
            "Epoch 17/100, Train Loss: 0.0370, Val Loss: 0.0435\n",
            "Epoch 18/100, Train Loss: 0.0382, Val Loss: 0.0428\n",
            "Epoch 19/100, Train Loss: 0.0368, Val Loss: 0.0421\n",
            "Epoch 20/100, Train Loss: 0.0369, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0371, Val Loss: 0.0417\n",
            "Epoch 22/100, Train Loss: 0.0377, Val Loss: 0.0417\n",
            "Epoch 23/100, Train Loss: 0.0374, Val Loss: 0.0417\n",
            "Epoch 24/100, Train Loss: 0.0364, Val Loss: 0.0417\n",
            "Epoch 25/100, Train Loss: 0.0370, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0373, Val Loss: 0.0417\n",
            "Epoch 27/100, Train Loss: 0.0362, Val Loss: 0.0417\n",
            "Epoch 28/100, Train Loss: 0.0362, Val Loss: 0.0417\n",
            "Epoch 29/100, Train Loss: 0.0374, Val Loss: 0.0417\n",
            "Epoch 30/100, Train Loss: 0.0370, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0369, Val Loss: 0.0417\n",
            "Epoch 32/100, Train Loss: 0.0365, Val Loss: 0.0417\n",
            "Epoch 33/100, Train Loss: 0.0367, Val Loss: 0.0417\n",
            "Epoch 34/100, Train Loss: 0.0365, Val Loss: 0.0417\n",
            "Epoch 35/100, Train Loss: 0.0372, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0383, Val Loss: 0.0417\n",
            "Epoch 37/100, Train Loss: 0.0368, Val Loss: 0.0417\n",
            "Epoch 38/100, Train Loss: 0.0369, Val Loss: 0.0417\n",
            "Epoch 39/100, Train Loss: 0.0367, Val Loss: 0.0417\n",
            "Epoch 40/100, Train Loss: 0.0364, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0373, Val Loss: 0.0417\n",
            "Epoch 42/100, Train Loss: 0.0367, Val Loss: 0.0417\n",
            "Epoch 43/100, Train Loss: 0.0371, Val Loss: 0.0417\n",
            "Epoch 44/100, Train Loss: 0.0368, Val Loss: 0.0417\n",
            "Epoch 45/100, Train Loss: 0.0364, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0366, Val Loss: 0.0417\n",
            "Epoch 47/100, Train Loss: 0.0362, Val Loss: 0.0417\n",
            "Epoch 48/100, Train Loss: 0.0367, Val Loss: 0.0417\n",
            "Epoch 49/100, Train Loss: 0.0369, Val Loss: 0.0417\n",
            "Epoch 50/100, Train Loss: 0.0365, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0367, Val Loss: 0.0417\n",
            "Epoch 52/100, Train Loss: 0.0373, Val Loss: 0.0417\n",
            "Epoch 53/100, Train Loss: 0.0412, Val Loss: 0.0417\n",
            "Epoch 54/100, Train Loss: 0.0400, Val Loss: 0.0417\n",
            "Epoch 55/100, Train Loss: 0.0365, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0370, Val Loss: 0.0417\n",
            "Epoch 57/100, Train Loss: 0.0364, Val Loss: 0.0417\n",
            "Epoch 58/100, Train Loss: 0.0370, Val Loss: 0.0417\n",
            "Epoch 59/100, Train Loss: 0.0367, Val Loss: 0.0417\n",
            "Epoch 60/100, Train Loss: 0.0368, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0366, Val Loss: 0.0417\n",
            "Epoch 62/100, Train Loss: 0.0369, Val Loss: 0.0417\n",
            "Epoch 63/100, Train Loss: 0.0363, Val Loss: 0.0417\n",
            "Epoch 64/100, Train Loss: 0.0365, Val Loss: 0.0417\n",
            "Epoch 65/100, Train Loss: 0.0368, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0365, Val Loss: 0.0417\n",
            "Epoch 67/100, Train Loss: 0.0365, Val Loss: 0.0417\n",
            "Epoch 68/100, Train Loss: 0.0359, Val Loss: 0.0417\n",
            "Epoch 69/100, Train Loss: 0.0366, Val Loss: 0.0417\n",
            "Epoch 70/100, Train Loss: 0.0368, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0369, Val Loss: 0.0417\n",
            "Epoch 72/100, Train Loss: 0.0380, Val Loss: 0.0417\n",
            "Epoch 73/100, Train Loss: 0.0366, Val Loss: 0.0417\n",
            "Epoch 74/100, Train Loss: 0.0367, Val Loss: 0.0417\n",
            "Epoch 75/100, Train Loss: 0.0363, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0370, Val Loss: 0.0417\n",
            "Epoch 77/100, Train Loss: 0.0366, Val Loss: 0.0417\n",
            "Epoch 78/100, Train Loss: 0.0368, Val Loss: 0.0417\n",
            "Epoch 79/100, Train Loss: 0.0368, Val Loss: 0.0417\n",
            "Epoch 80/100, Train Loss: 0.0381, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0372, Val Loss: 0.0417\n",
            "Epoch 82/100, Train Loss: 0.0366, Val Loss: 0.0417\n",
            "Epoch 83/100, Train Loss: 0.0364, Val Loss: 0.0417\n",
            "Epoch 84/100, Train Loss: 0.0368, Val Loss: 0.0417\n",
            "Epoch 85/100, Train Loss: 0.0373, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0359, Val Loss: 0.0417\n",
            "Epoch 87/100, Train Loss: 0.0369, Val Loss: 0.0417\n",
            "Epoch 88/100, Train Loss: 0.0361, Val Loss: 0.0417\n",
            "Epoch 89/100, Train Loss: 0.0367, Val Loss: 0.0417\n",
            "Epoch 90/100, Train Loss: 0.0367, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0369, Val Loss: 0.0417\n",
            "Epoch 92/100, Train Loss: 0.0367, Val Loss: 0.0417\n",
            "Epoch 93/100, Train Loss: 0.0364, Val Loss: 0.0417\n",
            "Epoch 94/100, Train Loss: 0.0370, Val Loss: 0.0417\n",
            "Epoch 95/100, Train Loss: 0.0371, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0370, Val Loss: 0.0417\n",
            "Epoch 97/100, Train Loss: 0.0363, Val Loss: 0.0417\n",
            "Epoch 98/100, Train Loss: 0.0369, Val Loss: 0.0417\n",
            "Epoch 99/100, Train Loss: 0.0366, Val Loss: 0.0417\n",
            "Epoch 100/100, Train Loss: 0.0372, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 15/28\n",
            "Current training set size: 111 samples\n",
            "Epoch 1/100, Train Loss: 0.1521, Val Loss: 0.1888\n",
            "Epoch 2/100, Train Loss: 0.1428, Val Loss: 0.1810\n",
            "Epoch 3/100, Train Loss: 0.1422, Val Loss: 0.1553\n",
            "Epoch 4/100, Train Loss: 0.1179, Val Loss: 0.1077\n",
            "Epoch 5/100, Train Loss: 0.0787, Val Loss: 0.0412\n",
            "Epoch 6/100, Train Loss: 0.0990, Val Loss: 0.0756\n",
            "Epoch 7/100, Train Loss: 0.0736, Val Loss: 0.0467\n",
            "Epoch 8/100, Train Loss: 0.0665, Val Loss: 0.0554\n",
            "Epoch 9/100, Train Loss: 0.0672, Val Loss: 0.0743\n",
            "Epoch 10/100, Train Loss: 0.0580, Val Loss: 0.0704\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 11/100, Train Loss: 0.0650, Val Loss: 0.0694\n",
            "Epoch 12/100, Train Loss: 0.0636, Val Loss: 0.0672\n",
            "Epoch 13/100, Train Loss: 0.0619, Val Loss: 0.0657\n",
            "Epoch 14/100, Train Loss: 0.0602, Val Loss: 0.0635\n",
            "Epoch 15/100, Train Loss: 0.0584, Val Loss: 0.0612\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0573, Val Loss: 0.0612\n",
            "Epoch 17/100, Train Loss: 0.0571, Val Loss: 0.0612\n",
            "Epoch 18/100, Train Loss: 0.0573, Val Loss: 0.0612\n",
            "Epoch 19/100, Train Loss: 0.0586, Val Loss: 0.0611\n",
            "Epoch 20/100, Train Loss: 0.0580, Val Loss: 0.0611\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0577, Val Loss: 0.0611\n",
            "Epoch 22/100, Train Loss: 0.0598, Val Loss: 0.0611\n",
            "Epoch 23/100, Train Loss: 0.0575, Val Loss: 0.0611\n",
            "Epoch 24/100, Train Loss: 0.0575, Val Loss: 0.0611\n",
            "Epoch 25/100, Train Loss: 0.0578, Val Loss: 0.0611\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0577, Val Loss: 0.0611\n",
            "Epoch 27/100, Train Loss: 0.0585, Val Loss: 0.0611\n",
            "Epoch 28/100, Train Loss: 0.0571, Val Loss: 0.0611\n",
            "Epoch 29/100, Train Loss: 0.0578, Val Loss: 0.0611\n",
            "Epoch 30/100, Train Loss: 0.0586, Val Loss: 0.0611\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0579, Val Loss: 0.0611\n",
            "Epoch 32/100, Train Loss: 0.0583, Val Loss: 0.0611\n",
            "Epoch 33/100, Train Loss: 0.0577, Val Loss: 0.0611\n",
            "Epoch 34/100, Train Loss: 0.0576, Val Loss: 0.0611\n",
            "Epoch 35/100, Train Loss: 0.0574, Val Loss: 0.0611\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0576, Val Loss: 0.0611\n",
            "Epoch 37/100, Train Loss: 0.0572, Val Loss: 0.0611\n",
            "Epoch 38/100, Train Loss: 0.0576, Val Loss: 0.0611\n",
            "Epoch 39/100, Train Loss: 0.0574, Val Loss: 0.0611\n",
            "Epoch 40/100, Train Loss: 0.0574, Val Loss: 0.0611\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0577, Val Loss: 0.0611\n",
            "Epoch 42/100, Train Loss: 0.0570, Val Loss: 0.0611\n",
            "Epoch 43/100, Train Loss: 0.0575, Val Loss: 0.0611\n",
            "Epoch 44/100, Train Loss: 0.0574, Val Loss: 0.0611\n",
            "Epoch 45/100, Train Loss: 0.0576, Val Loss: 0.0611\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0575, Val Loss: 0.0611\n",
            "Epoch 47/100, Train Loss: 0.0573, Val Loss: 0.0611\n",
            "Epoch 48/100, Train Loss: 0.0574, Val Loss: 0.0611\n",
            "Epoch 49/100, Train Loss: 0.0574, Val Loss: 0.0611\n",
            "Epoch 50/100, Train Loss: 0.0576, Val Loss: 0.0611\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0574, Val Loss: 0.0611\n",
            "Epoch 52/100, Train Loss: 0.0570, Val Loss: 0.0611\n",
            "Epoch 53/100, Train Loss: 0.0578, Val Loss: 0.0611\n",
            "Epoch 54/100, Train Loss: 0.0575, Val Loss: 0.0611\n",
            "Epoch 55/100, Train Loss: 0.0576, Val Loss: 0.0611\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0572, Val Loss: 0.0611\n",
            "Epoch 57/100, Train Loss: 0.0575, Val Loss: 0.0611\n",
            "Epoch 58/100, Train Loss: 0.0574, Val Loss: 0.0611\n",
            "Epoch 59/100, Train Loss: 0.0575, Val Loss: 0.0611\n",
            "Epoch 60/100, Train Loss: 0.0573, Val Loss: 0.0611\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0572, Val Loss: 0.0611\n",
            "Epoch 62/100, Train Loss: 0.0575, Val Loss: 0.0611\n",
            "Epoch 63/100, Train Loss: 0.0580, Val Loss: 0.0611\n",
            "Epoch 64/100, Train Loss: 0.0574, Val Loss: 0.0611\n",
            "Epoch 65/100, Train Loss: 0.0577, Val Loss: 0.0611\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0575, Val Loss: 0.0611\n",
            "Epoch 67/100, Train Loss: 0.0591, Val Loss: 0.0611\n",
            "Epoch 68/100, Train Loss: 0.0574, Val Loss: 0.0611\n",
            "Epoch 69/100, Train Loss: 0.0577, Val Loss: 0.0611\n",
            "Epoch 70/100, Train Loss: 0.0578, Val Loss: 0.0611\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0576, Val Loss: 0.0611\n",
            "Epoch 72/100, Train Loss: 0.0578, Val Loss: 0.0611\n",
            "Epoch 73/100, Train Loss: 0.0579, Val Loss: 0.0611\n",
            "Epoch 74/100, Train Loss: 0.0575, Val Loss: 0.0611\n",
            "Epoch 75/100, Train Loss: 0.0576, Val Loss: 0.0611\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0576, Val Loss: 0.0611\n",
            "Epoch 77/100, Train Loss: 0.0579, Val Loss: 0.0611\n",
            "Epoch 78/100, Train Loss: 0.0573, Val Loss: 0.0611\n",
            "Epoch 79/100, Train Loss: 0.0578, Val Loss: 0.0611\n",
            "Epoch 80/100, Train Loss: 0.0574, Val Loss: 0.0611\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0582, Val Loss: 0.0611\n",
            "Epoch 82/100, Train Loss: 0.0590, Val Loss: 0.0611\n",
            "Epoch 83/100, Train Loss: 0.0579, Val Loss: 0.0611\n",
            "Epoch 84/100, Train Loss: 0.0578, Val Loss: 0.0611\n",
            "Epoch 85/100, Train Loss: 0.0579, Val Loss: 0.0611\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0580, Val Loss: 0.0611\n",
            "Epoch 87/100, Train Loss: 0.0583, Val Loss: 0.0611\n",
            "Epoch 88/100, Train Loss: 0.0578, Val Loss: 0.0611\n",
            "Epoch 89/100, Train Loss: 0.0575, Val Loss: 0.0611\n",
            "Epoch 90/100, Train Loss: 0.0575, Val Loss: 0.0611\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0577, Val Loss: 0.0611\n",
            "Epoch 92/100, Train Loss: 0.0582, Val Loss: 0.0611\n",
            "Epoch 93/100, Train Loss: 0.0576, Val Loss: 0.0611\n",
            "Epoch 94/100, Train Loss: 0.0575, Val Loss: 0.0611\n",
            "Epoch 95/100, Train Loss: 0.0572, Val Loss: 0.0611\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0581, Val Loss: 0.0611\n",
            "Epoch 97/100, Train Loss: 0.0594, Val Loss: 0.0611\n",
            "Epoch 98/100, Train Loss: 0.0578, Val Loss: 0.0611\n",
            "Epoch 99/100, Train Loss: 0.0584, Val Loss: 0.0611\n",
            "Epoch 100/100, Train Loss: 0.0576, Val Loss: 0.0611\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 16/28\n",
            "Current training set size: 112 samples\n",
            "Epoch 1/100, Train Loss: 0.1538, Val Loss: 0.1989\n",
            "Epoch 2/100, Train Loss: 0.1400, Val Loss: 0.1996\n",
            "Epoch 3/100, Train Loss: 0.1358, Val Loss: 0.1208\n",
            "Epoch 4/100, Train Loss: 0.1290, Val Loss: 0.0795\n",
            "Epoch 5/100, Train Loss: 0.1084, Val Loss: 0.0667\n",
            "Epoch 6/100, Train Loss: 0.0863, Val Loss: 0.0490\n",
            "Epoch 7/100, Train Loss: 0.0634, Val Loss: 0.0394\n",
            "Epoch 8/100, Train Loss: 0.0642, Val Loss: 0.0675\n",
            "Epoch 9/100, Train Loss: 0.0562, Val Loss: 0.0444\n",
            "Epoch 10/100, Train Loss: 0.0503, Val Loss: 0.0502\n",
            "Epoch 11/100, Train Loss: 0.0539, Val Loss: 0.0534\n",
            "Epoch 12/100, Train Loss: 0.0491, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 13/100, Train Loss: 0.0416, Val Loss: 0.0456\n",
            "Epoch 14/100, Train Loss: 0.0401, Val Loss: 0.0443\n",
            "Epoch 15/100, Train Loss: 0.0388, Val Loss: 0.0425\n",
            "Epoch 16/100, Train Loss: 0.0382, Val Loss: 0.0412\n",
            "Epoch 17/100, Train Loss: 0.0374, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0380, Val Loss: 0.0407\n",
            "Epoch 19/100, Train Loss: 0.0373, Val Loss: 0.0407\n",
            "Epoch 20/100, Train Loss: 0.0370, Val Loss: 0.0407\n",
            "Epoch 21/100, Train Loss: 0.0378, Val Loss: 0.0407\n",
            "Epoch 22/100, Train Loss: 0.0374, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0377, Val Loss: 0.0407\n",
            "Epoch 24/100, Train Loss: 0.0375, Val Loss: 0.0407\n",
            "Epoch 25/100, Train Loss: 0.0373, Val Loss: 0.0407\n",
            "Epoch 26/100, Train Loss: 0.0376, Val Loss: 0.0407\n",
            "Epoch 27/100, Train Loss: 0.0374, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0376, Val Loss: 0.0407\n",
            "Epoch 29/100, Train Loss: 0.0377, Val Loss: 0.0407\n",
            "Epoch 30/100, Train Loss: 0.0374, Val Loss: 0.0407\n",
            "Epoch 31/100, Train Loss: 0.0369, Val Loss: 0.0407\n",
            "Epoch 32/100, Train Loss: 0.0373, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0380, Val Loss: 0.0407\n",
            "Epoch 34/100, Train Loss: 0.0374, Val Loss: 0.0407\n",
            "Epoch 35/100, Train Loss: 0.0371, Val Loss: 0.0407\n",
            "Epoch 36/100, Train Loss: 0.0379, Val Loss: 0.0407\n",
            "Epoch 37/100, Train Loss: 0.0375, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0373, Val Loss: 0.0407\n",
            "Epoch 39/100, Train Loss: 0.0372, Val Loss: 0.0407\n",
            "Epoch 40/100, Train Loss: 0.0376, Val Loss: 0.0407\n",
            "Epoch 41/100, Train Loss: 0.0378, Val Loss: 0.0407\n",
            "Epoch 42/100, Train Loss: 0.0369, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0378, Val Loss: 0.0407\n",
            "Epoch 44/100, Train Loss: 0.0372, Val Loss: 0.0407\n",
            "Epoch 45/100, Train Loss: 0.0375, Val Loss: 0.0407\n",
            "Epoch 46/100, Train Loss: 0.0376, Val Loss: 0.0407\n",
            "Epoch 47/100, Train Loss: 0.0373, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0376, Val Loss: 0.0407\n",
            "Epoch 49/100, Train Loss: 0.0375, Val Loss: 0.0407\n",
            "Epoch 50/100, Train Loss: 0.0370, Val Loss: 0.0407\n",
            "Epoch 51/100, Train Loss: 0.0378, Val Loss: 0.0407\n",
            "Epoch 52/100, Train Loss: 0.0372, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0378, Val Loss: 0.0407\n",
            "Epoch 54/100, Train Loss: 0.0385, Val Loss: 0.0407\n",
            "Epoch 55/100, Train Loss: 0.0373, Val Loss: 0.0407\n",
            "Epoch 56/100, Train Loss: 0.0374, Val Loss: 0.0407\n",
            "Epoch 57/100, Train Loss: 0.0372, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0375, Val Loss: 0.0407\n",
            "Epoch 59/100, Train Loss: 0.0377, Val Loss: 0.0407\n",
            "Epoch 60/100, Train Loss: 0.0375, Val Loss: 0.0407\n",
            "Epoch 61/100, Train Loss: 0.0374, Val Loss: 0.0407\n",
            "Epoch 62/100, Train Loss: 0.0373, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0379, Val Loss: 0.0407\n",
            "Epoch 64/100, Train Loss: 0.0380, Val Loss: 0.0407\n",
            "Epoch 65/100, Train Loss: 0.0382, Val Loss: 0.0407\n",
            "Epoch 66/100, Train Loss: 0.0378, Val Loss: 0.0407\n",
            "Epoch 67/100, Train Loss: 0.0378, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0371, Val Loss: 0.0407\n",
            "Epoch 69/100, Train Loss: 0.0376, Val Loss: 0.0407\n",
            "Epoch 70/100, Train Loss: 0.0379, Val Loss: 0.0407\n",
            "Epoch 71/100, Train Loss: 0.0373, Val Loss: 0.0407\n",
            "Epoch 72/100, Train Loss: 0.0375, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0365, Val Loss: 0.0407\n",
            "Epoch 74/100, Train Loss: 0.0377, Val Loss: 0.0407\n",
            "Epoch 75/100, Train Loss: 0.0379, Val Loss: 0.0407\n",
            "Epoch 76/100, Train Loss: 0.0371, Val Loss: 0.0407\n",
            "Epoch 77/100, Train Loss: 0.0367, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0373, Val Loss: 0.0407\n",
            "Epoch 79/100, Train Loss: 0.0378, Val Loss: 0.0407\n",
            "Epoch 80/100, Train Loss: 0.0377, Val Loss: 0.0407\n",
            "Epoch 81/100, Train Loss: 0.0374, Val Loss: 0.0407\n",
            "Epoch 82/100, Train Loss: 0.0379, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0376, Val Loss: 0.0407\n",
            "Epoch 84/100, Train Loss: 0.0376, Val Loss: 0.0407\n",
            "Epoch 85/100, Train Loss: 0.0377, Val Loss: 0.0407\n",
            "Epoch 86/100, Train Loss: 0.0376, Val Loss: 0.0407\n",
            "Epoch 87/100, Train Loss: 0.0377, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0373, Val Loss: 0.0407\n",
            "Epoch 89/100, Train Loss: 0.0380, Val Loss: 0.0407\n",
            "Epoch 90/100, Train Loss: 0.0378, Val Loss: 0.0407\n",
            "Epoch 91/100, Train Loss: 0.0374, Val Loss: 0.0407\n",
            "Epoch 92/100, Train Loss: 0.0373, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0375, Val Loss: 0.0407\n",
            "Epoch 94/100, Train Loss: 0.0372, Val Loss: 0.0407\n",
            "Epoch 95/100, Train Loss: 0.0378, Val Loss: 0.0407\n",
            "Epoch 96/100, Train Loss: 0.0369, Val Loss: 0.0407\n",
            "Epoch 97/100, Train Loss: 0.0383, Val Loss: 0.0407\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0376, Val Loss: 0.0407\n",
            "Epoch 99/100, Train Loss: 0.0377, Val Loss: 0.0407\n",
            "Epoch 100/100, Train Loss: 0.0373, Val Loss: 0.0407\n",
            "\n",
            "Test iteration 17/28\n",
            "Current training set size: 113 samples\n",
            "Epoch 1/100, Train Loss: 0.1519, Val Loss: 0.1861\n",
            "Epoch 2/100, Train Loss: 0.1379, Val Loss: 0.1752\n",
            "Epoch 3/100, Train Loss: 0.1270, Val Loss: 0.1439\n",
            "Epoch 4/100, Train Loss: 0.1132, Val Loss: 0.0492\n",
            "Epoch 5/100, Train Loss: 0.0854, Val Loss: 0.0493\n",
            "Epoch 6/100, Train Loss: 0.0718, Val Loss: 0.0627\n",
            "Epoch 7/100, Train Loss: 0.0670, Val Loss: 0.0417\n",
            "Epoch 8/100, Train Loss: 0.0642, Val Loss: 0.0563\n",
            "Epoch 9/100, Train Loss: 0.0585, Val Loss: 0.0677\n",
            "Epoch 10/100, Train Loss: 0.0602, Val Loss: 0.0455\n",
            "Epoch 11/100, Train Loss: 0.0521, Val Loss: 0.0439\n",
            "Epoch 12/100, Train Loss: 0.0408, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 13/100, Train Loss: 0.0476, Val Loss: 0.0480\n",
            "Epoch 14/100, Train Loss: 0.0406, Val Loss: 0.0430\n",
            "Epoch 15/100, Train Loss: 0.0375, Val Loss: 0.0397\n",
            "Epoch 16/100, Train Loss: 0.0382, Val Loss: 0.0386\n",
            "Epoch 17/100, Train Loss: 0.0367, Val Loss: 0.0383\n",
            "Epoch 18/100, Train Loss: 0.0357, Val Loss: 0.0381\n",
            "Epoch 19/100, Train Loss: 0.0361, Val Loss: 0.0381\n",
            "Epoch 20/100, Train Loss: 0.0352, Val Loss: 0.0375\n",
            "Epoch 21/100, Train Loss: 0.0359, Val Loss: 0.0377\n",
            "Epoch 22/100, Train Loss: 0.0358, Val Loss: 0.0378\n",
            "Epoch 23/100, Train Loss: 0.0358, Val Loss: 0.0382\n",
            "Epoch 24/100, Train Loss: 0.0370, Val Loss: 0.0379\n",
            "Epoch 25/100, Train Loss: 0.0357, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0371, Val Loss: 0.0376\n",
            "Epoch 27/100, Train Loss: 0.0363, Val Loss: 0.0376\n",
            "Epoch 28/100, Train Loss: 0.0347, Val Loss: 0.0376\n",
            "Epoch 29/100, Train Loss: 0.0370, Val Loss: 0.0376\n",
            "Epoch 30/100, Train Loss: 0.0359, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0356, Val Loss: 0.0376\n",
            "Epoch 32/100, Train Loss: 0.0351, Val Loss: 0.0376\n",
            "Epoch 33/100, Train Loss: 0.0360, Val Loss: 0.0376\n",
            "Epoch 34/100, Train Loss: 0.0344, Val Loss: 0.0376\n",
            "Epoch 35/100, Train Loss: 0.0354, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0366, Val Loss: 0.0376\n",
            "Epoch 37/100, Train Loss: 0.0357, Val Loss: 0.0376\n",
            "Epoch 38/100, Train Loss: 0.0353, Val Loss: 0.0376\n",
            "Epoch 39/100, Train Loss: 0.0350, Val Loss: 0.0376\n",
            "Epoch 40/100, Train Loss: 0.0357, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0349, Val Loss: 0.0376\n",
            "Epoch 42/100, Train Loss: 0.0356, Val Loss: 0.0376\n",
            "Epoch 43/100, Train Loss: 0.0353, Val Loss: 0.0376\n",
            "Epoch 44/100, Train Loss: 0.0360, Val Loss: 0.0376\n",
            "Epoch 45/100, Train Loss: 0.0353, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0358, Val Loss: 0.0376\n",
            "Epoch 47/100, Train Loss: 0.0351, Val Loss: 0.0376\n",
            "Epoch 48/100, Train Loss: 0.0345, Val Loss: 0.0376\n",
            "Epoch 49/100, Train Loss: 0.0346, Val Loss: 0.0376\n",
            "Epoch 50/100, Train Loss: 0.0357, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0357, Val Loss: 0.0376\n",
            "Epoch 52/100, Train Loss: 0.0363, Val Loss: 0.0376\n",
            "Epoch 53/100, Train Loss: 0.0355, Val Loss: 0.0376\n",
            "Epoch 54/100, Train Loss: 0.0370, Val Loss: 0.0376\n",
            "Epoch 55/100, Train Loss: 0.0361, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0350, Val Loss: 0.0376\n",
            "Epoch 57/100, Train Loss: 0.0358, Val Loss: 0.0376\n",
            "Epoch 58/100, Train Loss: 0.0349, Val Loss: 0.0376\n",
            "Epoch 59/100, Train Loss: 0.0359, Val Loss: 0.0376\n",
            "Epoch 60/100, Train Loss: 0.0357, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0345, Val Loss: 0.0376\n",
            "Epoch 62/100, Train Loss: 0.0350, Val Loss: 0.0376\n",
            "Epoch 63/100, Train Loss: 0.0345, Val Loss: 0.0376\n",
            "Epoch 64/100, Train Loss: 0.0349, Val Loss: 0.0376\n",
            "Epoch 65/100, Train Loss: 0.0361, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0359, Val Loss: 0.0376\n",
            "Epoch 67/100, Train Loss: 0.0349, Val Loss: 0.0376\n",
            "Epoch 68/100, Train Loss: 0.0354, Val Loss: 0.0376\n",
            "Epoch 69/100, Train Loss: 0.0345, Val Loss: 0.0376\n",
            "Epoch 70/100, Train Loss: 0.0361, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0354, Val Loss: 0.0376\n",
            "Epoch 72/100, Train Loss: 0.0348, Val Loss: 0.0376\n",
            "Epoch 73/100, Train Loss: 0.0353, Val Loss: 0.0376\n",
            "Epoch 74/100, Train Loss: 0.0361, Val Loss: 0.0376\n",
            "Epoch 75/100, Train Loss: 0.0356, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0352, Val Loss: 0.0376\n",
            "Epoch 77/100, Train Loss: 0.0349, Val Loss: 0.0376\n",
            "Epoch 78/100, Train Loss: 0.0363, Val Loss: 0.0376\n",
            "Epoch 79/100, Train Loss: 0.0360, Val Loss: 0.0376\n",
            "Epoch 80/100, Train Loss: 0.0357, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0353, Val Loss: 0.0376\n",
            "Epoch 82/100, Train Loss: 0.0350, Val Loss: 0.0376\n",
            "Epoch 83/100, Train Loss: 0.0353, Val Loss: 0.0376\n",
            "Epoch 84/100, Train Loss: 0.0350, Val Loss: 0.0376\n",
            "Epoch 85/100, Train Loss: 0.0353, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0347, Val Loss: 0.0376\n",
            "Epoch 87/100, Train Loss: 0.0350, Val Loss: 0.0376\n",
            "Epoch 88/100, Train Loss: 0.0350, Val Loss: 0.0376\n",
            "Epoch 89/100, Train Loss: 0.0360, Val Loss: 0.0376\n",
            "Epoch 90/100, Train Loss: 0.0349, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0350, Val Loss: 0.0376\n",
            "Epoch 92/100, Train Loss: 0.0347, Val Loss: 0.0376\n",
            "Epoch 93/100, Train Loss: 0.0403, Val Loss: 0.0376\n",
            "Epoch 94/100, Train Loss: 0.0364, Val Loss: 0.0376\n",
            "Epoch 95/100, Train Loss: 0.0359, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0360, Val Loss: 0.0376\n",
            "Epoch 97/100, Train Loss: 0.0348, Val Loss: 0.0376\n",
            "Epoch 98/100, Train Loss: 0.0370, Val Loss: 0.0376\n",
            "Epoch 99/100, Train Loss: 0.0351, Val Loss: 0.0376\n",
            "Epoch 100/100, Train Loss: 0.0347, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 18/28\n",
            "Current training set size: 114 samples\n",
            "Epoch 1/100, Train Loss: 0.1607, Val Loss: 0.2166\n",
            "Epoch 2/100, Train Loss: 0.1436, Val Loss: 0.1848\n",
            "Epoch 3/100, Train Loss: 0.1407, Val Loss: 0.1647\n",
            "Epoch 4/100, Train Loss: 0.1257, Val Loss: 0.0955\n",
            "Epoch 5/100, Train Loss: 0.1122, Val Loss: 0.0702\n",
            "Epoch 6/100, Train Loss: 0.0789, Val Loss: 0.0447\n",
            "Epoch 7/100, Train Loss: 0.0754, Val Loss: 0.0571\n",
            "Epoch 8/100, Train Loss: 0.0673, Val Loss: 0.0441\n",
            "Epoch 9/100, Train Loss: 0.0587, Val Loss: 0.0542\n",
            "Epoch 10/100, Train Loss: 0.0512, Val Loss: 0.0561\n",
            "Epoch 11/100, Train Loss: 0.0547, Val Loss: 0.0368\n",
            "Epoch 12/100, Train Loss: 0.0496, Val Loss: 0.0518\n",
            "Epoch 13/100, Train Loss: 0.0411, Val Loss: 0.0378\n",
            "Epoch 14/100, Train Loss: 0.0518, Val Loss: 0.0453\n",
            "Epoch 15/100, Train Loss: 0.0409, Val Loss: 0.0552\n",
            "Epoch 16/100, Train Loss: 0.0485, Val Loss: 0.0454\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0344, Val Loss: 0.0461\n",
            "Epoch 18/100, Train Loss: 0.0341, Val Loss: 0.0449\n",
            "Epoch 19/100, Train Loss: 0.0331, Val Loss: 0.0455\n",
            "Epoch 20/100, Train Loss: 0.0335, Val Loss: 0.0448\n",
            "Epoch 21/100, Train Loss: 0.0329, Val Loss: 0.0450\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0323, Val Loss: 0.0450\n",
            "Epoch 23/100, Train Loss: 0.0329, Val Loss: 0.0450\n",
            "Epoch 24/100, Train Loss: 0.0339, Val Loss: 0.0450\n",
            "Epoch 25/100, Train Loss: 0.0338, Val Loss: 0.0450\n",
            "Epoch 26/100, Train Loss: 0.0325, Val Loss: 0.0450\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0327, Val Loss: 0.0450\n",
            "Epoch 28/100, Train Loss: 0.0331, Val Loss: 0.0450\n",
            "Epoch 29/100, Train Loss: 0.0324, Val Loss: 0.0450\n",
            "Epoch 30/100, Train Loss: 0.0330, Val Loss: 0.0450\n",
            "Epoch 31/100, Train Loss: 0.0337, Val Loss: 0.0450\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0332, Val Loss: 0.0450\n",
            "Epoch 33/100, Train Loss: 0.0324, Val Loss: 0.0450\n",
            "Epoch 34/100, Train Loss: 0.0328, Val Loss: 0.0450\n",
            "Epoch 35/100, Train Loss: 0.0324, Val Loss: 0.0450\n",
            "Epoch 36/100, Train Loss: 0.0320, Val Loss: 0.0450\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0332, Val Loss: 0.0450\n",
            "Epoch 38/100, Train Loss: 0.0333, Val Loss: 0.0450\n",
            "Epoch 39/100, Train Loss: 0.0327, Val Loss: 0.0450\n",
            "Epoch 40/100, Train Loss: 0.0328, Val Loss: 0.0450\n",
            "Epoch 41/100, Train Loss: 0.0334, Val Loss: 0.0450\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0336, Val Loss: 0.0450\n",
            "Epoch 43/100, Train Loss: 0.0324, Val Loss: 0.0450\n",
            "Epoch 44/100, Train Loss: 0.0324, Val Loss: 0.0450\n",
            "Epoch 45/100, Train Loss: 0.0323, Val Loss: 0.0450\n",
            "Epoch 46/100, Train Loss: 0.0332, Val Loss: 0.0450\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0328, Val Loss: 0.0450\n",
            "Epoch 48/100, Train Loss: 0.0325, Val Loss: 0.0450\n",
            "Epoch 49/100, Train Loss: 0.0328, Val Loss: 0.0450\n",
            "Epoch 50/100, Train Loss: 0.0330, Val Loss: 0.0450\n",
            "Epoch 51/100, Train Loss: 0.0325, Val Loss: 0.0450\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0322, Val Loss: 0.0450\n",
            "Epoch 53/100, Train Loss: 0.0359, Val Loss: 0.0450\n",
            "Epoch 54/100, Train Loss: 0.0334, Val Loss: 0.0450\n",
            "Epoch 55/100, Train Loss: 0.0329, Val Loss: 0.0450\n",
            "Epoch 56/100, Train Loss: 0.0335, Val Loss: 0.0450\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0339, Val Loss: 0.0450\n",
            "Epoch 58/100, Train Loss: 0.0333, Val Loss: 0.0450\n",
            "Epoch 59/100, Train Loss: 0.0337, Val Loss: 0.0450\n",
            "Epoch 60/100, Train Loss: 0.0329, Val Loss: 0.0450\n",
            "Epoch 61/100, Train Loss: 0.0331, Val Loss: 0.0450\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0326, Val Loss: 0.0450\n",
            "Epoch 63/100, Train Loss: 0.0327, Val Loss: 0.0450\n",
            "Epoch 64/100, Train Loss: 0.0335, Val Loss: 0.0450\n",
            "Epoch 65/100, Train Loss: 0.0330, Val Loss: 0.0450\n",
            "Epoch 66/100, Train Loss: 0.0329, Val Loss: 0.0450\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0332, Val Loss: 0.0450\n",
            "Epoch 68/100, Train Loss: 0.0328, Val Loss: 0.0450\n",
            "Epoch 69/100, Train Loss: 0.0336, Val Loss: 0.0450\n",
            "Epoch 70/100, Train Loss: 0.0324, Val Loss: 0.0450\n",
            "Epoch 71/100, Train Loss: 0.0328, Val Loss: 0.0450\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0332, Val Loss: 0.0450\n",
            "Epoch 73/100, Train Loss: 0.0327, Val Loss: 0.0450\n",
            "Epoch 74/100, Train Loss: 0.0331, Val Loss: 0.0450\n",
            "Epoch 75/100, Train Loss: 0.0338, Val Loss: 0.0450\n",
            "Epoch 76/100, Train Loss: 0.0330, Val Loss: 0.0450\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0326, Val Loss: 0.0450\n",
            "Epoch 78/100, Train Loss: 0.0326, Val Loss: 0.0450\n",
            "Epoch 79/100, Train Loss: 0.0326, Val Loss: 0.0450\n",
            "Epoch 80/100, Train Loss: 0.0327, Val Loss: 0.0450\n",
            "Epoch 81/100, Train Loss: 0.0329, Val Loss: 0.0450\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0336, Val Loss: 0.0450\n",
            "Epoch 83/100, Train Loss: 0.0330, Val Loss: 0.0450\n",
            "Epoch 84/100, Train Loss: 0.0336, Val Loss: 0.0450\n",
            "Epoch 85/100, Train Loss: 0.0334, Val Loss: 0.0450\n",
            "Epoch 86/100, Train Loss: 0.0331, Val Loss: 0.0450\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0329, Val Loss: 0.0450\n",
            "Epoch 88/100, Train Loss: 0.0333, Val Loss: 0.0450\n",
            "Epoch 89/100, Train Loss: 0.0331, Val Loss: 0.0450\n",
            "Epoch 90/100, Train Loss: 0.0322, Val Loss: 0.0450\n",
            "Epoch 91/100, Train Loss: 0.0336, Val Loss: 0.0450\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0329, Val Loss: 0.0450\n",
            "Epoch 93/100, Train Loss: 0.0330, Val Loss: 0.0450\n",
            "Epoch 94/100, Train Loss: 0.0336, Val Loss: 0.0450\n",
            "Epoch 95/100, Train Loss: 0.0330, Val Loss: 0.0450\n",
            "Epoch 96/100, Train Loss: 0.0327, Val Loss: 0.0450\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0322, Val Loss: 0.0450\n",
            "Epoch 98/100, Train Loss: 0.0339, Val Loss: 0.0450\n",
            "Epoch 99/100, Train Loss: 0.0332, Val Loss: 0.0450\n",
            "Epoch 100/100, Train Loss: 0.0329, Val Loss: 0.0450\n",
            "\n",
            "Test iteration 19/28\n",
            "Current training set size: 115 samples\n",
            "Epoch 1/100, Train Loss: 0.1530, Val Loss: 0.1910\n",
            "Epoch 2/100, Train Loss: 0.1523, Val Loss: 0.1951\n",
            "Epoch 3/100, Train Loss: 0.1333, Val Loss: 0.1356\n",
            "Epoch 4/100, Train Loss: 0.1087, Val Loss: 0.0823\n",
            "Epoch 5/100, Train Loss: 0.0807, Val Loss: 0.1107\n",
            "Epoch 6/100, Train Loss: 0.0786, Val Loss: 0.0440\n",
            "Epoch 7/100, Train Loss: 0.0796, Val Loss: 0.0416\n",
            "Epoch 8/100, Train Loss: 0.0629, Val Loss: 0.0331\n",
            "Epoch 9/100, Train Loss: 0.0595, Val Loss: 0.0538\n",
            "Epoch 10/100, Train Loss: 0.0596, Val Loss: 0.0549\n",
            "Epoch 11/100, Train Loss: 0.0583, Val Loss: 0.0409\n",
            "Epoch 12/100, Train Loss: 0.0526, Val Loss: 0.0475\n",
            "Epoch 13/100, Train Loss: 0.0460, Val Loss: 0.0419\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 14/100, Train Loss: 0.0405, Val Loss: 0.0410\n",
            "Epoch 15/100, Train Loss: 0.0388, Val Loss: 0.0407\n",
            "Epoch 16/100, Train Loss: 0.0390, Val Loss: 0.0402\n",
            "Epoch 17/100, Train Loss: 0.0383, Val Loss: 0.0396\n",
            "Epoch 18/100, Train Loss: 0.0386, Val Loss: 0.0395\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0378, Val Loss: 0.0395\n",
            "Epoch 20/100, Train Loss: 0.0376, Val Loss: 0.0395\n",
            "Epoch 21/100, Train Loss: 0.0384, Val Loss: 0.0395\n",
            "Epoch 22/100, Train Loss: 0.0378, Val Loss: 0.0395\n",
            "Epoch 23/100, Train Loss: 0.0384, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0377, Val Loss: 0.0394\n",
            "Epoch 25/100, Train Loss: 0.0382, Val Loss: 0.0394\n",
            "Epoch 26/100, Train Loss: 0.0382, Val Loss: 0.0394\n",
            "Epoch 27/100, Train Loss: 0.0381, Val Loss: 0.0394\n",
            "Epoch 28/100, Train Loss: 0.0374, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0377, Val Loss: 0.0394\n",
            "Epoch 30/100, Train Loss: 0.0375, Val Loss: 0.0394\n",
            "Epoch 31/100, Train Loss: 0.0376, Val Loss: 0.0394\n",
            "Epoch 32/100, Train Loss: 0.0377, Val Loss: 0.0394\n",
            "Epoch 33/100, Train Loss: 0.0379, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0381, Val Loss: 0.0394\n",
            "Epoch 35/100, Train Loss: 0.0378, Val Loss: 0.0394\n",
            "Epoch 36/100, Train Loss: 0.0379, Val Loss: 0.0394\n",
            "Epoch 37/100, Train Loss: 0.0380, Val Loss: 0.0394\n",
            "Epoch 38/100, Train Loss: 0.0382, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0378, Val Loss: 0.0394\n",
            "Epoch 40/100, Train Loss: 0.0378, Val Loss: 0.0394\n",
            "Epoch 41/100, Train Loss: 0.0379, Val Loss: 0.0394\n",
            "Epoch 42/100, Train Loss: 0.0379, Val Loss: 0.0394\n",
            "Epoch 43/100, Train Loss: 0.0375, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0377, Val Loss: 0.0394\n",
            "Epoch 45/100, Train Loss: 0.0375, Val Loss: 0.0394\n",
            "Epoch 46/100, Train Loss: 0.0378, Val Loss: 0.0394\n",
            "Epoch 47/100, Train Loss: 0.0383, Val Loss: 0.0394\n",
            "Epoch 48/100, Train Loss: 0.0379, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0380, Val Loss: 0.0394\n",
            "Epoch 50/100, Train Loss: 0.0373, Val Loss: 0.0394\n",
            "Epoch 51/100, Train Loss: 0.0384, Val Loss: 0.0394\n",
            "Epoch 52/100, Train Loss: 0.0380, Val Loss: 0.0394\n",
            "Epoch 53/100, Train Loss: 0.0381, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0383, Val Loss: 0.0394\n",
            "Epoch 55/100, Train Loss: 0.0381, Val Loss: 0.0394\n",
            "Epoch 56/100, Train Loss: 0.0385, Val Loss: 0.0394\n",
            "Epoch 57/100, Train Loss: 0.0383, Val Loss: 0.0394\n",
            "Epoch 58/100, Train Loss: 0.0381, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0384, Val Loss: 0.0394\n",
            "Epoch 60/100, Train Loss: 0.0377, Val Loss: 0.0394\n",
            "Epoch 61/100, Train Loss: 0.0379, Val Loss: 0.0394\n",
            "Epoch 62/100, Train Loss: 0.0382, Val Loss: 0.0394\n",
            "Epoch 63/100, Train Loss: 0.0377, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0377, Val Loss: 0.0394\n",
            "Epoch 65/100, Train Loss: 0.0377, Val Loss: 0.0394\n",
            "Epoch 66/100, Train Loss: 0.0377, Val Loss: 0.0394\n",
            "Epoch 67/100, Train Loss: 0.0376, Val Loss: 0.0394\n",
            "Epoch 68/100, Train Loss: 0.0378, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0379, Val Loss: 0.0394\n",
            "Epoch 70/100, Train Loss: 0.0378, Val Loss: 0.0394\n",
            "Epoch 71/100, Train Loss: 0.0384, Val Loss: 0.0394\n",
            "Epoch 72/100, Train Loss: 0.0381, Val Loss: 0.0394\n",
            "Epoch 73/100, Train Loss: 0.0383, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0382, Val Loss: 0.0394\n",
            "Epoch 75/100, Train Loss: 0.0380, Val Loss: 0.0394\n",
            "Epoch 76/100, Train Loss: 0.0382, Val Loss: 0.0394\n",
            "Epoch 77/100, Train Loss: 0.0376, Val Loss: 0.0394\n",
            "Epoch 78/100, Train Loss: 0.0384, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0378, Val Loss: 0.0394\n",
            "Epoch 80/100, Train Loss: 0.0378, Val Loss: 0.0394\n",
            "Epoch 81/100, Train Loss: 0.0383, Val Loss: 0.0394\n",
            "Epoch 82/100, Train Loss: 0.0384, Val Loss: 0.0394\n",
            "Epoch 83/100, Train Loss: 0.0398, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0382, Val Loss: 0.0394\n",
            "Epoch 85/100, Train Loss: 0.0395, Val Loss: 0.0394\n",
            "Epoch 86/100, Train Loss: 0.0375, Val Loss: 0.0394\n",
            "Epoch 87/100, Train Loss: 0.0378, Val Loss: 0.0394\n",
            "Epoch 88/100, Train Loss: 0.0382, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0381, Val Loss: 0.0394\n",
            "Epoch 90/100, Train Loss: 0.0378, Val Loss: 0.0394\n",
            "Epoch 91/100, Train Loss: 0.0377, Val Loss: 0.0394\n",
            "Epoch 92/100, Train Loss: 0.0380, Val Loss: 0.0394\n",
            "Epoch 93/100, Train Loss: 0.0381, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0378, Val Loss: 0.0394\n",
            "Epoch 95/100, Train Loss: 0.0377, Val Loss: 0.0394\n",
            "Epoch 96/100, Train Loss: 0.0385, Val Loss: 0.0394\n",
            "Epoch 97/100, Train Loss: 0.0386, Val Loss: 0.0394\n",
            "Epoch 98/100, Train Loss: 0.0381, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0383, Val Loss: 0.0394\n",
            "Epoch 100/100, Train Loss: 0.0382, Val Loss: 0.0394\n",
            "\n",
            "Test iteration 20/28\n",
            "Current training set size: 116 samples\n",
            "Epoch 1/100, Train Loss: 0.1530, Val Loss: 0.1910\n",
            "Epoch 2/100, Train Loss: 0.1424, Val Loss: 0.1770\n",
            "Epoch 3/100, Train Loss: 0.1320, Val Loss: 0.1421\n",
            "Epoch 4/100, Train Loss: 0.1049, Val Loss: 0.1094\n",
            "Epoch 5/100, Train Loss: 0.1059, Val Loss: 0.0649\n",
            "Epoch 6/100, Train Loss: 0.0757, Val Loss: 0.0356\n",
            "Epoch 7/100, Train Loss: 0.0809, Val Loss: 0.0583\n",
            "Epoch 8/100, Train Loss: 0.0761, Val Loss: 0.0375\n",
            "Epoch 9/100, Train Loss: 0.0775, Val Loss: 0.0683\n",
            "Epoch 10/100, Train Loss: 0.0634, Val Loss: 0.0513\n",
            "Epoch 11/100, Train Loss: 0.0557, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 12/100, Train Loss: 0.0451, Val Loss: 0.0403\n",
            "Epoch 13/100, Train Loss: 0.0446, Val Loss: 0.0399\n",
            "Epoch 14/100, Train Loss: 0.0438, Val Loss: 0.0394\n",
            "Epoch 15/100, Train Loss: 0.0441, Val Loss: 0.0392\n",
            "Epoch 16/100, Train Loss: 0.0437, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0437, Val Loss: 0.0391\n",
            "Epoch 18/100, Train Loss: 0.0440, Val Loss: 0.0391\n",
            "Epoch 19/100, Train Loss: 0.0441, Val Loss: 0.0391\n",
            "Epoch 20/100, Train Loss: 0.0439, Val Loss: 0.0391\n",
            "Epoch 21/100, Train Loss: 0.0437, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0438, Val Loss: 0.0391\n",
            "Epoch 23/100, Train Loss: 0.0441, Val Loss: 0.0391\n",
            "Epoch 24/100, Train Loss: 0.0435, Val Loss: 0.0391\n",
            "Epoch 25/100, Train Loss: 0.0438, Val Loss: 0.0391\n",
            "Epoch 26/100, Train Loss: 0.0438, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0439, Val Loss: 0.0391\n",
            "Epoch 28/100, Train Loss: 0.0441, Val Loss: 0.0391\n",
            "Epoch 29/100, Train Loss: 0.0434, Val Loss: 0.0391\n",
            "Epoch 30/100, Train Loss: 0.0436, Val Loss: 0.0391\n",
            "Epoch 31/100, Train Loss: 0.0436, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0438, Val Loss: 0.0391\n",
            "Epoch 33/100, Train Loss: 0.0438, Val Loss: 0.0391\n",
            "Epoch 34/100, Train Loss: 0.0437, Val Loss: 0.0391\n",
            "Epoch 35/100, Train Loss: 0.0433, Val Loss: 0.0391\n",
            "Epoch 36/100, Train Loss: 0.0437, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0442, Val Loss: 0.0391\n",
            "Epoch 38/100, Train Loss: 0.0437, Val Loss: 0.0391\n",
            "Epoch 39/100, Train Loss: 0.0439, Val Loss: 0.0391\n",
            "Epoch 40/100, Train Loss: 0.0436, Val Loss: 0.0391\n",
            "Epoch 41/100, Train Loss: 0.0438, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0435, Val Loss: 0.0391\n",
            "Epoch 43/100, Train Loss: 0.0441, Val Loss: 0.0391\n",
            "Epoch 44/100, Train Loss: 0.0440, Val Loss: 0.0391\n",
            "Epoch 45/100, Train Loss: 0.0436, Val Loss: 0.0391\n",
            "Epoch 46/100, Train Loss: 0.0436, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0434, Val Loss: 0.0391\n",
            "Epoch 48/100, Train Loss: 0.0434, Val Loss: 0.0391\n",
            "Epoch 49/100, Train Loss: 0.0438, Val Loss: 0.0391\n",
            "Epoch 50/100, Train Loss: 0.0438, Val Loss: 0.0391\n",
            "Epoch 51/100, Train Loss: 0.0438, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0441, Val Loss: 0.0391\n",
            "Epoch 53/100, Train Loss: 0.0442, Val Loss: 0.0391\n",
            "Epoch 54/100, Train Loss: 0.0438, Val Loss: 0.0391\n",
            "Epoch 55/100, Train Loss: 0.0435, Val Loss: 0.0391\n",
            "Epoch 56/100, Train Loss: 0.0440, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0433, Val Loss: 0.0391\n",
            "Epoch 58/100, Train Loss: 0.0435, Val Loss: 0.0391\n",
            "Epoch 59/100, Train Loss: 0.0432, Val Loss: 0.0391\n",
            "Epoch 60/100, Train Loss: 0.0435, Val Loss: 0.0391\n",
            "Epoch 61/100, Train Loss: 0.0438, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0440, Val Loss: 0.0391\n",
            "Epoch 63/100, Train Loss: 0.0438, Val Loss: 0.0391\n",
            "Epoch 64/100, Train Loss: 0.0440, Val Loss: 0.0391\n",
            "Epoch 65/100, Train Loss: 0.0433, Val Loss: 0.0391\n",
            "Epoch 66/100, Train Loss: 0.0434, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0435, Val Loss: 0.0391\n",
            "Epoch 68/100, Train Loss: 0.0437, Val Loss: 0.0391\n",
            "Epoch 69/100, Train Loss: 0.0438, Val Loss: 0.0391\n",
            "Epoch 70/100, Train Loss: 0.0443, Val Loss: 0.0391\n",
            "Epoch 71/100, Train Loss: 0.0435, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0435, Val Loss: 0.0391\n",
            "Epoch 73/100, Train Loss: 0.0435, Val Loss: 0.0391\n",
            "Epoch 74/100, Train Loss: 0.0436, Val Loss: 0.0391\n",
            "Epoch 75/100, Train Loss: 0.0434, Val Loss: 0.0391\n",
            "Epoch 76/100, Train Loss: 0.0438, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0436, Val Loss: 0.0391\n",
            "Epoch 78/100, Train Loss: 0.0436, Val Loss: 0.0391\n",
            "Epoch 79/100, Train Loss: 0.0437, Val Loss: 0.0391\n",
            "Epoch 80/100, Train Loss: 0.0433, Val Loss: 0.0391\n",
            "Epoch 81/100, Train Loss: 0.0438, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0441, Val Loss: 0.0391\n",
            "Epoch 83/100, Train Loss: 0.0436, Val Loss: 0.0391\n",
            "Epoch 84/100, Train Loss: 0.0439, Val Loss: 0.0391\n",
            "Epoch 85/100, Train Loss: 0.0433, Val Loss: 0.0391\n",
            "Epoch 86/100, Train Loss: 0.0439, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0438, Val Loss: 0.0391\n",
            "Epoch 88/100, Train Loss: 0.0441, Val Loss: 0.0391\n",
            "Epoch 89/100, Train Loss: 0.0438, Val Loss: 0.0391\n",
            "Epoch 90/100, Train Loss: 0.0437, Val Loss: 0.0391\n",
            "Epoch 91/100, Train Loss: 0.0435, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0437, Val Loss: 0.0391\n",
            "Epoch 93/100, Train Loss: 0.0436, Val Loss: 0.0391\n",
            "Epoch 94/100, Train Loss: 0.0439, Val Loss: 0.0391\n",
            "Epoch 95/100, Train Loss: 0.0440, Val Loss: 0.0391\n",
            "Epoch 96/100, Train Loss: 0.0434, Val Loss: 0.0391\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0436, Val Loss: 0.0391\n",
            "Epoch 98/100, Train Loss: 0.0445, Val Loss: 0.0391\n",
            "Epoch 99/100, Train Loss: 0.0439, Val Loss: 0.0391\n",
            "Epoch 100/100, Train Loss: 0.0439, Val Loss: 0.0391\n",
            "\n",
            "Test iteration 21/28\n",
            "Current training set size: 117 samples\n",
            "Epoch 1/100, Train Loss: 0.1581, Val Loss: 0.1894\n",
            "Epoch 2/100, Train Loss: 0.1468, Val Loss: 0.1758\n",
            "Epoch 3/100, Train Loss: 0.1556, Val Loss: 0.1777\n",
            "Epoch 4/100, Train Loss: 0.1315, Val Loss: 0.1791\n",
            "Epoch 5/100, Train Loss: 0.1132, Val Loss: 0.0978\n",
            "Epoch 6/100, Train Loss: 0.0879, Val Loss: 0.0833\n",
            "Epoch 7/100, Train Loss: 0.0651, Val Loss: 0.0688\n",
            "Epoch 8/100, Train Loss: 0.0680, Val Loss: 0.0650\n",
            "Epoch 9/100, Train Loss: 0.0674, Val Loss: 0.0606\n",
            "Epoch 10/100, Train Loss: 0.0598, Val Loss: 0.0499\n",
            "Epoch 11/100, Train Loss: 0.0518, Val Loss: 0.0536\n",
            "Epoch 12/100, Train Loss: 0.0508, Val Loss: 0.0381\n",
            "Epoch 13/100, Train Loss: 0.0455, Val Loss: 0.0388\n",
            "Epoch 14/100, Train Loss: 0.0496, Val Loss: 0.0450\n",
            "Epoch 15/100, Train Loss: 0.0525, Val Loss: 0.0433\n",
            "Epoch 16/100, Train Loss: 0.0482, Val Loss: 0.0440\n",
            "Epoch 17/100, Train Loss: 0.0412, Val Loss: 0.0476\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0392, Val Loss: 0.0453\n",
            "Epoch 19/100, Train Loss: 0.0380, Val Loss: 0.0432\n",
            "Epoch 20/100, Train Loss: 0.0365, Val Loss: 0.0422\n",
            "Epoch 21/100, Train Loss: 0.0361, Val Loss: 0.0416\n",
            "Epoch 22/100, Train Loss: 0.0360, Val Loss: 0.0409\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0360, Val Loss: 0.0409\n",
            "Epoch 24/100, Train Loss: 0.0357, Val Loss: 0.0409\n",
            "Epoch 25/100, Train Loss: 0.0358, Val Loss: 0.0409\n",
            "Epoch 26/100, Train Loss: 0.0357, Val Loss: 0.0409\n",
            "Epoch 27/100, Train Loss: 0.0361, Val Loss: 0.0409\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0355, Val Loss: 0.0409\n",
            "Epoch 29/100, Train Loss: 0.0351, Val Loss: 0.0409\n",
            "Epoch 30/100, Train Loss: 0.0354, Val Loss: 0.0409\n",
            "Epoch 31/100, Train Loss: 0.0383, Val Loss: 0.0409\n",
            "Epoch 32/100, Train Loss: 0.0369, Val Loss: 0.0409\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0371, Val Loss: 0.0409\n",
            "Epoch 34/100, Train Loss: 0.0354, Val Loss: 0.0409\n",
            "Epoch 35/100, Train Loss: 0.0356, Val Loss: 0.0409\n",
            "Epoch 36/100, Train Loss: 0.0352, Val Loss: 0.0409\n",
            "Epoch 37/100, Train Loss: 0.0368, Val Loss: 0.0409\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0357, Val Loss: 0.0409\n",
            "Epoch 39/100, Train Loss: 0.0364, Val Loss: 0.0409\n",
            "Epoch 40/100, Train Loss: 0.0357, Val Loss: 0.0409\n",
            "Epoch 41/100, Train Loss: 0.0369, Val Loss: 0.0409\n",
            "Epoch 42/100, Train Loss: 0.0359, Val Loss: 0.0409\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0349, Val Loss: 0.0409\n",
            "Epoch 44/100, Train Loss: 0.0354, Val Loss: 0.0409\n",
            "Epoch 45/100, Train Loss: 0.0366, Val Loss: 0.0409\n",
            "Epoch 46/100, Train Loss: 0.0402, Val Loss: 0.0409\n",
            "Epoch 47/100, Train Loss: 0.0367, Val Loss: 0.0409\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0349, Val Loss: 0.0409\n",
            "Epoch 49/100, Train Loss: 0.0357, Val Loss: 0.0409\n",
            "Epoch 50/100, Train Loss: 0.0375, Val Loss: 0.0409\n",
            "Epoch 51/100, Train Loss: 0.0349, Val Loss: 0.0409\n",
            "Epoch 52/100, Train Loss: 0.0356, Val Loss: 0.0409\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0403, Val Loss: 0.0409\n",
            "Epoch 54/100, Train Loss: 0.0354, Val Loss: 0.0409\n",
            "Epoch 55/100, Train Loss: 0.0355, Val Loss: 0.0409\n",
            "Epoch 56/100, Train Loss: 0.0404, Val Loss: 0.0409\n",
            "Epoch 57/100, Train Loss: 0.0359, Val Loss: 0.0409\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0355, Val Loss: 0.0409\n",
            "Epoch 59/100, Train Loss: 0.0352, Val Loss: 0.0409\n",
            "Epoch 60/100, Train Loss: 0.0361, Val Loss: 0.0409\n",
            "Epoch 61/100, Train Loss: 0.0370, Val Loss: 0.0409\n",
            "Epoch 62/100, Train Loss: 0.0366, Val Loss: 0.0409\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0351, Val Loss: 0.0409\n",
            "Epoch 64/100, Train Loss: 0.0363, Val Loss: 0.0409\n",
            "Epoch 65/100, Train Loss: 0.0369, Val Loss: 0.0409\n",
            "Epoch 66/100, Train Loss: 0.0358, Val Loss: 0.0409\n",
            "Epoch 67/100, Train Loss: 0.0364, Val Loss: 0.0409\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0386, Val Loss: 0.0409\n",
            "Epoch 69/100, Train Loss: 0.0363, Val Loss: 0.0409\n",
            "Epoch 70/100, Train Loss: 0.0344, Val Loss: 0.0409\n",
            "Epoch 71/100, Train Loss: 0.0364, Val Loss: 0.0409\n",
            "Epoch 72/100, Train Loss: 0.0363, Val Loss: 0.0409\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0360, Val Loss: 0.0409\n",
            "Epoch 74/100, Train Loss: 0.0356, Val Loss: 0.0409\n",
            "Epoch 75/100, Train Loss: 0.0397, Val Loss: 0.0409\n",
            "Epoch 76/100, Train Loss: 0.0362, Val Loss: 0.0409\n",
            "Epoch 77/100, Train Loss: 0.0386, Val Loss: 0.0409\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0373, Val Loss: 0.0409\n",
            "Epoch 79/100, Train Loss: 0.0363, Val Loss: 0.0409\n",
            "Epoch 80/100, Train Loss: 0.0356, Val Loss: 0.0409\n",
            "Epoch 81/100, Train Loss: 0.0354, Val Loss: 0.0409\n",
            "Epoch 82/100, Train Loss: 0.0353, Val Loss: 0.0409\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0359, Val Loss: 0.0409\n",
            "Epoch 84/100, Train Loss: 0.0357, Val Loss: 0.0409\n",
            "Epoch 85/100, Train Loss: 0.0359, Val Loss: 0.0409\n",
            "Epoch 86/100, Train Loss: 0.0350, Val Loss: 0.0409\n",
            "Epoch 87/100, Train Loss: 0.0367, Val Loss: 0.0409\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0357, Val Loss: 0.0409\n",
            "Epoch 89/100, Train Loss: 0.0357, Val Loss: 0.0409\n",
            "Epoch 90/100, Train Loss: 0.0352, Val Loss: 0.0409\n",
            "Epoch 91/100, Train Loss: 0.0358, Val Loss: 0.0409\n",
            "Epoch 92/100, Train Loss: 0.0360, Val Loss: 0.0409\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0363, Val Loss: 0.0409\n",
            "Epoch 94/100, Train Loss: 0.0353, Val Loss: 0.0409\n",
            "Epoch 95/100, Train Loss: 0.0362, Val Loss: 0.0409\n",
            "Epoch 96/100, Train Loss: 0.0350, Val Loss: 0.0409\n",
            "Epoch 97/100, Train Loss: 0.0357, Val Loss: 0.0409\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0367, Val Loss: 0.0409\n",
            "Epoch 99/100, Train Loss: 0.0349, Val Loss: 0.0409\n",
            "Epoch 100/100, Train Loss: 0.0367, Val Loss: 0.0409\n",
            "\n",
            "Test iteration 22/28\n",
            "Current training set size: 118 samples\n",
            "Epoch 1/100, Train Loss: 0.1444, Val Loss: 0.1863\n",
            "Epoch 2/100, Train Loss: 0.1503, Val Loss: 0.1841\n",
            "Epoch 3/100, Train Loss: 0.1376, Val Loss: 0.1664\n",
            "Epoch 4/100, Train Loss: 0.1319, Val Loss: 0.1581\n",
            "Epoch 5/100, Train Loss: 0.1240, Val Loss: 0.0929\n",
            "Epoch 6/100, Train Loss: 0.0911, Val Loss: 0.0504\n",
            "Epoch 7/100, Train Loss: 0.0706, Val Loss: 0.0333\n",
            "Epoch 8/100, Train Loss: 0.0628, Val Loss: 0.0434\n",
            "Epoch 9/100, Train Loss: 0.0593, Val Loss: 0.0385\n",
            "Epoch 10/100, Train Loss: 0.0541, Val Loss: 0.0409\n",
            "Epoch 11/100, Train Loss: 0.0526, Val Loss: 0.0525\n",
            "Epoch 12/100, Train Loss: 0.0547, Val Loss: 0.0492\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 13/100, Train Loss: 0.0519, Val Loss: 0.0459\n",
            "Epoch 14/100, Train Loss: 0.0464, Val Loss: 0.0427\n",
            "Epoch 15/100, Train Loss: 0.0435, Val Loss: 0.0423\n",
            "Epoch 16/100, Train Loss: 0.0423, Val Loss: 0.0415\n",
            "Epoch 17/100, Train Loss: 0.0416, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0410, Val Loss: 0.0416\n",
            "Epoch 19/100, Train Loss: 0.0421, Val Loss: 0.0416\n",
            "Epoch 20/100, Train Loss: 0.0408, Val Loss: 0.0416\n",
            "Epoch 21/100, Train Loss: 0.0412, Val Loss: 0.0416\n",
            "Epoch 22/100, Train Loss: 0.0409, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0408, Val Loss: 0.0416\n",
            "Epoch 24/100, Train Loss: 0.0412, Val Loss: 0.0416\n",
            "Epoch 25/100, Train Loss: 0.0416, Val Loss: 0.0416\n",
            "Epoch 26/100, Train Loss: 0.0408, Val Loss: 0.0416\n",
            "Epoch 27/100, Train Loss: 0.0455, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0412, Val Loss: 0.0416\n",
            "Epoch 29/100, Train Loss: 0.0410, Val Loss: 0.0416\n",
            "Epoch 30/100, Train Loss: 0.0412, Val Loss: 0.0416\n",
            "Epoch 31/100, Train Loss: 0.0409, Val Loss: 0.0416\n",
            "Epoch 32/100, Train Loss: 0.0418, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0408, Val Loss: 0.0416\n",
            "Epoch 34/100, Train Loss: 0.0412, Val Loss: 0.0416\n",
            "Epoch 35/100, Train Loss: 0.0411, Val Loss: 0.0416\n",
            "Epoch 36/100, Train Loss: 0.0422, Val Loss: 0.0416\n",
            "Epoch 37/100, Train Loss: 0.0415, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0407, Val Loss: 0.0416\n",
            "Epoch 39/100, Train Loss: 0.0413, Val Loss: 0.0416\n",
            "Epoch 40/100, Train Loss: 0.0412, Val Loss: 0.0416\n",
            "Epoch 41/100, Train Loss: 0.0416, Val Loss: 0.0416\n",
            "Epoch 42/100, Train Loss: 0.0422, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0411, Val Loss: 0.0416\n",
            "Epoch 44/100, Train Loss: 0.0407, Val Loss: 0.0416\n",
            "Epoch 45/100, Train Loss: 0.0411, Val Loss: 0.0416\n",
            "Epoch 46/100, Train Loss: 0.0406, Val Loss: 0.0416\n",
            "Epoch 47/100, Train Loss: 0.0418, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0418, Val Loss: 0.0416\n",
            "Epoch 49/100, Train Loss: 0.0411, Val Loss: 0.0416\n",
            "Epoch 50/100, Train Loss: 0.0412, Val Loss: 0.0416\n",
            "Epoch 51/100, Train Loss: 0.0416, Val Loss: 0.0416\n",
            "Epoch 52/100, Train Loss: 0.0418, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0410, Val Loss: 0.0416\n",
            "Epoch 54/100, Train Loss: 0.0411, Val Loss: 0.0416\n",
            "Epoch 55/100, Train Loss: 0.0420, Val Loss: 0.0416\n",
            "Epoch 56/100, Train Loss: 0.0447, Val Loss: 0.0416\n",
            "Epoch 57/100, Train Loss: 0.0414, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0413, Val Loss: 0.0416\n",
            "Epoch 59/100, Train Loss: 0.0423, Val Loss: 0.0416\n",
            "Epoch 60/100, Train Loss: 0.0412, Val Loss: 0.0416\n",
            "Epoch 61/100, Train Loss: 0.0411, Val Loss: 0.0416\n",
            "Epoch 62/100, Train Loss: 0.0414, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0408, Val Loss: 0.0416\n",
            "Epoch 64/100, Train Loss: 0.0408, Val Loss: 0.0416\n",
            "Epoch 65/100, Train Loss: 0.0407, Val Loss: 0.0416\n",
            "Epoch 66/100, Train Loss: 0.0408, Val Loss: 0.0416\n",
            "Epoch 67/100, Train Loss: 0.0416, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0412, Val Loss: 0.0416\n",
            "Epoch 69/100, Train Loss: 0.0414, Val Loss: 0.0416\n",
            "Epoch 70/100, Train Loss: 0.0417, Val Loss: 0.0416\n",
            "Epoch 71/100, Train Loss: 0.0408, Val Loss: 0.0416\n",
            "Epoch 72/100, Train Loss: 0.0421, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0414, Val Loss: 0.0416\n",
            "Epoch 74/100, Train Loss: 0.0415, Val Loss: 0.0416\n",
            "Epoch 75/100, Train Loss: 0.0413, Val Loss: 0.0416\n",
            "Epoch 76/100, Train Loss: 0.0416, Val Loss: 0.0416\n",
            "Epoch 77/100, Train Loss: 0.0456, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0413, Val Loss: 0.0416\n",
            "Epoch 79/100, Train Loss: 0.0415, Val Loss: 0.0416\n",
            "Epoch 80/100, Train Loss: 0.0410, Val Loss: 0.0416\n",
            "Epoch 81/100, Train Loss: 0.0422, Val Loss: 0.0416\n",
            "Epoch 82/100, Train Loss: 0.0413, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0415, Val Loss: 0.0416\n",
            "Epoch 84/100, Train Loss: 0.0404, Val Loss: 0.0416\n",
            "Epoch 85/100, Train Loss: 0.0415, Val Loss: 0.0416\n",
            "Epoch 86/100, Train Loss: 0.0410, Val Loss: 0.0416\n",
            "Epoch 87/100, Train Loss: 0.0415, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0422, Val Loss: 0.0416\n",
            "Epoch 89/100, Train Loss: 0.0411, Val Loss: 0.0416\n",
            "Epoch 90/100, Train Loss: 0.0417, Val Loss: 0.0416\n",
            "Epoch 91/100, Train Loss: 0.0417, Val Loss: 0.0416\n",
            "Epoch 92/100, Train Loss: 0.0406, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0404, Val Loss: 0.0416\n",
            "Epoch 94/100, Train Loss: 0.0414, Val Loss: 0.0416\n",
            "Epoch 95/100, Train Loss: 0.0412, Val Loss: 0.0416\n",
            "Epoch 96/100, Train Loss: 0.0418, Val Loss: 0.0416\n",
            "Epoch 97/100, Train Loss: 0.0410, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0407, Val Loss: 0.0416\n",
            "Epoch 99/100, Train Loss: 0.0419, Val Loss: 0.0416\n",
            "Epoch 100/100, Train Loss: 0.0410, Val Loss: 0.0416\n",
            "\n",
            "Test iteration 23/28\n",
            "Current training set size: 119 samples\n",
            "Epoch 1/100, Train Loss: 0.1563, Val Loss: 0.1928\n",
            "Epoch 2/100, Train Loss: 0.1447, Val Loss: 0.1881\n",
            "Epoch 3/100, Train Loss: 0.1425, Val Loss: 0.1621\n",
            "Epoch 4/100, Train Loss: 0.1314, Val Loss: 0.0905\n",
            "Epoch 5/100, Train Loss: 0.0848, Val Loss: 0.0498\n",
            "Epoch 6/100, Train Loss: 0.0838, Val Loss: 0.0651\n",
            "Epoch 7/100, Train Loss: 0.0847, Val Loss: 0.0434\n",
            "Epoch 8/100, Train Loss: 0.0695, Val Loss: 0.0345\n",
            "Epoch 9/100, Train Loss: 0.0702, Val Loss: 0.0520\n",
            "Epoch 10/100, Train Loss: 0.0645, Val Loss: 0.0410\n",
            "Epoch 11/100, Train Loss: 0.0531, Val Loss: 0.0363\n",
            "Epoch 12/100, Train Loss: 0.0465, Val Loss: 0.0454\n",
            "Epoch 13/100, Train Loss: 0.0434, Val Loss: 0.0478\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 14/100, Train Loss: 0.0456, Val Loss: 0.0431\n",
            "Epoch 15/100, Train Loss: 0.0427, Val Loss: 0.0387\n",
            "Epoch 16/100, Train Loss: 0.0410, Val Loss: 0.0378\n",
            "Epoch 17/100, Train Loss: 0.0406, Val Loss: 0.0381\n",
            "Epoch 18/100, Train Loss: 0.0403, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0401, Val Loss: 0.0379\n",
            "Epoch 20/100, Train Loss: 0.0396, Val Loss: 0.0379\n",
            "Epoch 21/100, Train Loss: 0.0396, Val Loss: 0.0379\n",
            "Epoch 22/100, Train Loss: 0.0399, Val Loss: 0.0379\n",
            "Epoch 23/100, Train Loss: 0.0402, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0399, Val Loss: 0.0379\n",
            "Epoch 25/100, Train Loss: 0.0399, Val Loss: 0.0379\n",
            "Epoch 26/100, Train Loss: 0.0412, Val Loss: 0.0379\n",
            "Epoch 27/100, Train Loss: 0.0396, Val Loss: 0.0379\n",
            "Epoch 28/100, Train Loss: 0.0405, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0392, Val Loss: 0.0379\n",
            "Epoch 30/100, Train Loss: 0.0395, Val Loss: 0.0379\n",
            "Epoch 31/100, Train Loss: 0.0405, Val Loss: 0.0379\n",
            "Epoch 32/100, Train Loss: 0.0401, Val Loss: 0.0379\n",
            "Epoch 33/100, Train Loss: 0.0398, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0402, Val Loss: 0.0379\n",
            "Epoch 35/100, Train Loss: 0.0399, Val Loss: 0.0379\n",
            "Epoch 36/100, Train Loss: 0.0406, Val Loss: 0.0379\n",
            "Epoch 37/100, Train Loss: 0.0404, Val Loss: 0.0379\n",
            "Epoch 38/100, Train Loss: 0.0401, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0406, Val Loss: 0.0379\n",
            "Epoch 40/100, Train Loss: 0.0398, Val Loss: 0.0379\n",
            "Epoch 41/100, Train Loss: 0.0408, Val Loss: 0.0379\n",
            "Epoch 42/100, Train Loss: 0.0401, Val Loss: 0.0379\n",
            "Epoch 43/100, Train Loss: 0.0402, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0400, Val Loss: 0.0379\n",
            "Epoch 45/100, Train Loss: 0.0414, Val Loss: 0.0379\n",
            "Epoch 46/100, Train Loss: 0.0403, Val Loss: 0.0379\n",
            "Epoch 47/100, Train Loss: 0.0404, Val Loss: 0.0379\n",
            "Epoch 48/100, Train Loss: 0.0401, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0399, Val Loss: 0.0379\n",
            "Epoch 50/100, Train Loss: 0.0394, Val Loss: 0.0379\n",
            "Epoch 51/100, Train Loss: 0.0402, Val Loss: 0.0379\n",
            "Epoch 52/100, Train Loss: 0.0399, Val Loss: 0.0379\n",
            "Epoch 53/100, Train Loss: 0.0398, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0399, Val Loss: 0.0379\n",
            "Epoch 55/100, Train Loss: 0.0399, Val Loss: 0.0379\n",
            "Epoch 56/100, Train Loss: 0.0393, Val Loss: 0.0379\n",
            "Epoch 57/100, Train Loss: 0.0396, Val Loss: 0.0379\n",
            "Epoch 58/100, Train Loss: 0.0401, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0401, Val Loss: 0.0379\n",
            "Epoch 60/100, Train Loss: 0.0412, Val Loss: 0.0379\n",
            "Epoch 61/100, Train Loss: 0.0397, Val Loss: 0.0379\n",
            "Epoch 62/100, Train Loss: 0.0398, Val Loss: 0.0379\n",
            "Epoch 63/100, Train Loss: 0.0403, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0401, Val Loss: 0.0379\n",
            "Epoch 65/100, Train Loss: 0.0393, Val Loss: 0.0379\n",
            "Epoch 66/100, Train Loss: 0.0416, Val Loss: 0.0379\n",
            "Epoch 67/100, Train Loss: 0.0398, Val Loss: 0.0379\n",
            "Epoch 68/100, Train Loss: 0.0413, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0398, Val Loss: 0.0379\n",
            "Epoch 70/100, Train Loss: 0.0407, Val Loss: 0.0379\n",
            "Epoch 71/100, Train Loss: 0.0398, Val Loss: 0.0379\n",
            "Epoch 72/100, Train Loss: 0.0405, Val Loss: 0.0379\n",
            "Epoch 73/100, Train Loss: 0.0417, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0396, Val Loss: 0.0379\n",
            "Epoch 75/100, Train Loss: 0.0401, Val Loss: 0.0379\n",
            "Epoch 76/100, Train Loss: 0.0401, Val Loss: 0.0379\n",
            "Epoch 77/100, Train Loss: 0.0397, Val Loss: 0.0379\n",
            "Epoch 78/100, Train Loss: 0.0398, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0404, Val Loss: 0.0379\n",
            "Epoch 80/100, Train Loss: 0.0402, Val Loss: 0.0379\n",
            "Epoch 81/100, Train Loss: 0.0393, Val Loss: 0.0379\n",
            "Epoch 82/100, Train Loss: 0.0419, Val Loss: 0.0379\n",
            "Epoch 83/100, Train Loss: 0.0399, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0404, Val Loss: 0.0379\n",
            "Epoch 85/100, Train Loss: 0.0400, Val Loss: 0.0379\n",
            "Epoch 86/100, Train Loss: 0.0396, Val Loss: 0.0379\n",
            "Epoch 87/100, Train Loss: 0.0420, Val Loss: 0.0379\n",
            "Epoch 88/100, Train Loss: 0.0393, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0404, Val Loss: 0.0379\n",
            "Epoch 90/100, Train Loss: 0.0401, Val Loss: 0.0379\n",
            "Epoch 91/100, Train Loss: 0.0399, Val Loss: 0.0379\n",
            "Epoch 92/100, Train Loss: 0.0402, Val Loss: 0.0379\n",
            "Epoch 93/100, Train Loss: 0.0404, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0405, Val Loss: 0.0379\n",
            "Epoch 95/100, Train Loss: 0.0404, Val Loss: 0.0379\n",
            "Epoch 96/100, Train Loss: 0.0414, Val Loss: 0.0379\n",
            "Epoch 97/100, Train Loss: 0.0405, Val Loss: 0.0379\n",
            "Epoch 98/100, Train Loss: 0.0395, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0396, Val Loss: 0.0379\n",
            "Epoch 100/100, Train Loss: 0.0399, Val Loss: 0.0379\n",
            "\n",
            "Test iteration 24/28\n",
            "Current training set size: 120 samples\n",
            "Epoch 1/100, Train Loss: 0.1542, Val Loss: 0.1903\n",
            "Epoch 2/100, Train Loss: 0.1382, Val Loss: 0.1949\n",
            "Epoch 3/100, Train Loss: 0.1508, Val Loss: 0.1797\n",
            "Epoch 4/100, Train Loss: 0.1313, Val Loss: 0.1335\n",
            "Epoch 5/100, Train Loss: 0.1139, Val Loss: 0.0998\n",
            "Epoch 6/100, Train Loss: 0.0810, Val Loss: 0.0367\n",
            "Epoch 7/100, Train Loss: 0.0859, Val Loss: 0.0842\n",
            "Epoch 8/100, Train Loss: 0.0802, Val Loss: 0.0517\n",
            "Epoch 9/100, Train Loss: 0.0655, Val Loss: 0.0416\n",
            "Epoch 10/100, Train Loss: 0.0703, Val Loss: 0.0422\n",
            "Epoch 11/100, Train Loss: 0.0716, Val Loss: 0.0388\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 12/100, Train Loss: 0.0480, Val Loss: 0.0387\n",
            "Epoch 13/100, Train Loss: 0.0477, Val Loss: 0.0390\n",
            "Epoch 14/100, Train Loss: 0.0473, Val Loss: 0.0391\n",
            "Epoch 15/100, Train Loss: 0.0474, Val Loss: 0.0390\n",
            "Epoch 16/100, Train Loss: 0.0474, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0472, Val Loss: 0.0394\n",
            "Epoch 18/100, Train Loss: 0.0477, Val Loss: 0.0394\n",
            "Epoch 19/100, Train Loss: 0.0473, Val Loss: 0.0394\n",
            "Epoch 20/100, Train Loss: 0.0470, Val Loss: 0.0394\n",
            "Epoch 21/100, Train Loss: 0.0474, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0477, Val Loss: 0.0394\n",
            "Epoch 23/100, Train Loss: 0.0473, Val Loss: 0.0394\n",
            "Epoch 24/100, Train Loss: 0.0472, Val Loss: 0.0394\n",
            "Epoch 25/100, Train Loss: 0.0473, Val Loss: 0.0394\n",
            "Epoch 26/100, Train Loss: 0.0469, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0474, Val Loss: 0.0394\n",
            "Epoch 28/100, Train Loss: 0.0469, Val Loss: 0.0394\n",
            "Epoch 29/100, Train Loss: 0.0472, Val Loss: 0.0394\n",
            "Epoch 30/100, Train Loss: 0.0472, Val Loss: 0.0394\n",
            "Epoch 31/100, Train Loss: 0.0472, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0474, Val Loss: 0.0394\n",
            "Epoch 33/100, Train Loss: 0.0478, Val Loss: 0.0394\n",
            "Epoch 34/100, Train Loss: 0.0473, Val Loss: 0.0394\n",
            "Epoch 35/100, Train Loss: 0.0472, Val Loss: 0.0394\n",
            "Epoch 36/100, Train Loss: 0.0473, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0473, Val Loss: 0.0394\n",
            "Epoch 38/100, Train Loss: 0.0476, Val Loss: 0.0394\n",
            "Epoch 39/100, Train Loss: 0.0475, Val Loss: 0.0394\n",
            "Epoch 40/100, Train Loss: 0.0472, Val Loss: 0.0394\n",
            "Epoch 41/100, Train Loss: 0.0476, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0470, Val Loss: 0.0394\n",
            "Epoch 43/100, Train Loss: 0.0478, Val Loss: 0.0394\n",
            "Epoch 44/100, Train Loss: 0.0475, Val Loss: 0.0394\n",
            "Epoch 45/100, Train Loss: 0.0477, Val Loss: 0.0394\n",
            "Epoch 46/100, Train Loss: 0.0472, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0469, Val Loss: 0.0394\n",
            "Epoch 48/100, Train Loss: 0.0470, Val Loss: 0.0394\n",
            "Epoch 49/100, Train Loss: 0.0474, Val Loss: 0.0394\n",
            "Epoch 50/100, Train Loss: 0.0470, Val Loss: 0.0394\n",
            "Epoch 51/100, Train Loss: 0.0477, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0475, Val Loss: 0.0394\n",
            "Epoch 53/100, Train Loss: 0.0472, Val Loss: 0.0394\n",
            "Epoch 54/100, Train Loss: 0.0472, Val Loss: 0.0394\n",
            "Epoch 55/100, Train Loss: 0.0472, Val Loss: 0.0394\n",
            "Epoch 56/100, Train Loss: 0.0472, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0475, Val Loss: 0.0394\n",
            "Epoch 58/100, Train Loss: 0.0471, Val Loss: 0.0394\n",
            "Epoch 59/100, Train Loss: 0.0472, Val Loss: 0.0394\n",
            "Epoch 60/100, Train Loss: 0.0472, Val Loss: 0.0394\n",
            "Epoch 61/100, Train Loss: 0.0469, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0472, Val Loss: 0.0394\n",
            "Epoch 63/100, Train Loss: 0.0473, Val Loss: 0.0394\n",
            "Epoch 64/100, Train Loss: 0.0472, Val Loss: 0.0394\n",
            "Epoch 65/100, Train Loss: 0.0473, Val Loss: 0.0394\n",
            "Epoch 66/100, Train Loss: 0.0470, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0474, Val Loss: 0.0394\n",
            "Epoch 68/100, Train Loss: 0.0477, Val Loss: 0.0394\n",
            "Epoch 69/100, Train Loss: 0.0477, Val Loss: 0.0394\n",
            "Epoch 70/100, Train Loss: 0.0472, Val Loss: 0.0394\n",
            "Epoch 71/100, Train Loss: 0.0475, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0475, Val Loss: 0.0394\n",
            "Epoch 73/100, Train Loss: 0.0472, Val Loss: 0.0394\n",
            "Epoch 74/100, Train Loss: 0.0472, Val Loss: 0.0394\n",
            "Epoch 75/100, Train Loss: 0.0475, Val Loss: 0.0394\n",
            "Epoch 76/100, Train Loss: 0.0471, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0474, Val Loss: 0.0394\n",
            "Epoch 78/100, Train Loss: 0.0470, Val Loss: 0.0394\n",
            "Epoch 79/100, Train Loss: 0.0473, Val Loss: 0.0394\n",
            "Epoch 80/100, Train Loss: 0.0471, Val Loss: 0.0394\n",
            "Epoch 81/100, Train Loss: 0.0470, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0472, Val Loss: 0.0394\n",
            "Epoch 83/100, Train Loss: 0.0471, Val Loss: 0.0394\n",
            "Epoch 84/100, Train Loss: 0.0473, Val Loss: 0.0394\n",
            "Epoch 85/100, Train Loss: 0.0473, Val Loss: 0.0394\n",
            "Epoch 86/100, Train Loss: 0.0475, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0466, Val Loss: 0.0394\n",
            "Epoch 88/100, Train Loss: 0.0475, Val Loss: 0.0394\n",
            "Epoch 89/100, Train Loss: 0.0474, Val Loss: 0.0394\n",
            "Epoch 90/100, Train Loss: 0.0476, Val Loss: 0.0394\n",
            "Epoch 91/100, Train Loss: 0.0475, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0471, Val Loss: 0.0394\n",
            "Epoch 93/100, Train Loss: 0.0475, Val Loss: 0.0394\n",
            "Epoch 94/100, Train Loss: 0.0473, Val Loss: 0.0394\n",
            "Epoch 95/100, Train Loss: 0.0473, Val Loss: 0.0394\n",
            "Epoch 96/100, Train Loss: 0.0470, Val Loss: 0.0394\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0475, Val Loss: 0.0394\n",
            "Epoch 98/100, Train Loss: 0.0473, Val Loss: 0.0394\n",
            "Epoch 99/100, Train Loss: 0.0476, Val Loss: 0.0394\n",
            "Epoch 100/100, Train Loss: 0.0473, Val Loss: 0.0394\n",
            "\n",
            "Test iteration 25/28\n",
            "Current training set size: 121 samples\n",
            "Epoch 1/100, Train Loss: 0.1653, Val Loss: 0.1909\n",
            "Epoch 2/100, Train Loss: 0.1559, Val Loss: 0.1881\n",
            "Epoch 3/100, Train Loss: 0.1396, Val Loss: 0.1888\n",
            "Epoch 4/100, Train Loss: 0.1351, Val Loss: 0.1527\n",
            "Epoch 5/100, Train Loss: 0.1190, Val Loss: 0.1013\n",
            "Epoch 6/100, Train Loss: 0.0920, Val Loss: 0.0464\n",
            "Epoch 7/100, Train Loss: 0.0763, Val Loss: 0.0408\n",
            "Epoch 8/100, Train Loss: 0.0745, Val Loss: 0.0327\n",
            "Epoch 9/100, Train Loss: 0.0621, Val Loss: 0.0423\n",
            "Epoch 10/100, Train Loss: 0.0572, Val Loss: 0.0520\n",
            "Epoch 11/100, Train Loss: 0.0602, Val Loss: 0.0625\n",
            "Epoch 12/100, Train Loss: 0.0481, Val Loss: 0.0412\n",
            "Epoch 13/100, Train Loss: 0.0458, Val Loss: 0.0515\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 14/100, Train Loss: 0.0479, Val Loss: 0.0475\n",
            "Epoch 15/100, Train Loss: 0.0440, Val Loss: 0.0432\n",
            "Epoch 16/100, Train Loss: 0.0404, Val Loss: 0.0415\n",
            "Epoch 17/100, Train Loss: 0.0396, Val Loss: 0.0404\n",
            "Epoch 18/100, Train Loss: 0.0394, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0408, Val Loss: 0.0398\n",
            "Epoch 20/100, Train Loss: 0.0398, Val Loss: 0.0398\n",
            "Epoch 21/100, Train Loss: 0.0403, Val Loss: 0.0398\n",
            "Epoch 22/100, Train Loss: 0.0415, Val Loss: 0.0398\n",
            "Epoch 23/100, Train Loss: 0.0396, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0396, Val Loss: 0.0398\n",
            "Epoch 25/100, Train Loss: 0.0399, Val Loss: 0.0398\n",
            "Epoch 26/100, Train Loss: 0.0390, Val Loss: 0.0398\n",
            "Epoch 27/100, Train Loss: 0.0397, Val Loss: 0.0398\n",
            "Epoch 28/100, Train Loss: 0.0400, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0395, Val Loss: 0.0398\n",
            "Epoch 30/100, Train Loss: 0.0402, Val Loss: 0.0398\n",
            "Epoch 31/100, Train Loss: 0.0407, Val Loss: 0.0398\n",
            "Epoch 32/100, Train Loss: 0.0392, Val Loss: 0.0398\n",
            "Epoch 33/100, Train Loss: 0.0435, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0403, Val Loss: 0.0398\n",
            "Epoch 35/100, Train Loss: 0.0391, Val Loss: 0.0398\n",
            "Epoch 36/100, Train Loss: 0.0394, Val Loss: 0.0398\n",
            "Epoch 37/100, Train Loss: 0.0438, Val Loss: 0.0398\n",
            "Epoch 38/100, Train Loss: 0.0392, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0393, Val Loss: 0.0398\n",
            "Epoch 40/100, Train Loss: 0.0396, Val Loss: 0.0398\n",
            "Epoch 41/100, Train Loss: 0.0404, Val Loss: 0.0398\n",
            "Epoch 42/100, Train Loss: 0.0521, Val Loss: 0.0398\n",
            "Epoch 43/100, Train Loss: 0.0397, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0400, Val Loss: 0.0398\n",
            "Epoch 45/100, Train Loss: 0.0407, Val Loss: 0.0398\n",
            "Epoch 46/100, Train Loss: 0.0393, Val Loss: 0.0398\n",
            "Epoch 47/100, Train Loss: 0.0400, Val Loss: 0.0398\n",
            "Epoch 48/100, Train Loss: 0.0403, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0392, Val Loss: 0.0398\n",
            "Epoch 50/100, Train Loss: 0.0512, Val Loss: 0.0398\n",
            "Epoch 51/100, Train Loss: 0.0392, Val Loss: 0.0398\n",
            "Epoch 52/100, Train Loss: 0.0404, Val Loss: 0.0398\n",
            "Epoch 53/100, Train Loss: 0.0390, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0391, Val Loss: 0.0398\n",
            "Epoch 55/100, Train Loss: 0.0397, Val Loss: 0.0398\n",
            "Epoch 56/100, Train Loss: 0.0394, Val Loss: 0.0398\n",
            "Epoch 57/100, Train Loss: 0.0530, Val Loss: 0.0398\n",
            "Epoch 58/100, Train Loss: 0.0398, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0397, Val Loss: 0.0398\n",
            "Epoch 60/100, Train Loss: 0.0390, Val Loss: 0.0398\n",
            "Epoch 61/100, Train Loss: 0.0401, Val Loss: 0.0398\n",
            "Epoch 62/100, Train Loss: 0.0407, Val Loss: 0.0398\n",
            "Epoch 63/100, Train Loss: 0.0395, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0396, Val Loss: 0.0398\n",
            "Epoch 65/100, Train Loss: 0.0403, Val Loss: 0.0398\n",
            "Epoch 66/100, Train Loss: 0.0406, Val Loss: 0.0398\n",
            "Epoch 67/100, Train Loss: 0.0401, Val Loss: 0.0398\n",
            "Epoch 68/100, Train Loss: 0.0441, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0404, Val Loss: 0.0398\n",
            "Epoch 70/100, Train Loss: 0.0403, Val Loss: 0.0398\n",
            "Epoch 71/100, Train Loss: 0.0403, Val Loss: 0.0398\n",
            "Epoch 72/100, Train Loss: 0.0397, Val Loss: 0.0398\n",
            "Epoch 73/100, Train Loss: 0.0400, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0396, Val Loss: 0.0398\n",
            "Epoch 75/100, Train Loss: 0.0401, Val Loss: 0.0398\n",
            "Epoch 76/100, Train Loss: 0.0408, Val Loss: 0.0398\n",
            "Epoch 77/100, Train Loss: 0.0406, Val Loss: 0.0398\n",
            "Epoch 78/100, Train Loss: 0.0530, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0399, Val Loss: 0.0398\n",
            "Epoch 80/100, Train Loss: 0.0402, Val Loss: 0.0398\n",
            "Epoch 81/100, Train Loss: 0.0399, Val Loss: 0.0398\n",
            "Epoch 82/100, Train Loss: 0.0391, Val Loss: 0.0398\n",
            "Epoch 83/100, Train Loss: 0.0395, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0394, Val Loss: 0.0398\n",
            "Epoch 85/100, Train Loss: 0.0399, Val Loss: 0.0398\n",
            "Epoch 86/100, Train Loss: 0.0398, Val Loss: 0.0398\n",
            "Epoch 87/100, Train Loss: 0.0386, Val Loss: 0.0398\n",
            "Epoch 88/100, Train Loss: 0.0405, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0402, Val Loss: 0.0398\n",
            "Epoch 90/100, Train Loss: 0.0399, Val Loss: 0.0398\n",
            "Epoch 91/100, Train Loss: 0.0412, Val Loss: 0.0398\n",
            "Epoch 92/100, Train Loss: 0.0422, Val Loss: 0.0398\n",
            "Epoch 93/100, Train Loss: 0.0401, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0397, Val Loss: 0.0398\n",
            "Epoch 95/100, Train Loss: 0.0397, Val Loss: 0.0398\n",
            "Epoch 96/100, Train Loss: 0.0394, Val Loss: 0.0398\n",
            "Epoch 97/100, Train Loss: 0.0406, Val Loss: 0.0398\n",
            "Epoch 98/100, Train Loss: 0.0393, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0395, Val Loss: 0.0398\n",
            "Epoch 100/100, Train Loss: 0.0396, Val Loss: 0.0398\n",
            "\n",
            "Test iteration 26/28\n",
            "Current training set size: 122 samples\n",
            "Epoch 1/100, Train Loss: 0.1539, Val Loss: 0.2191\n",
            "Epoch 2/100, Train Loss: 0.1544, Val Loss: 0.1901\n",
            "Epoch 3/100, Train Loss: 0.1489, Val Loss: 0.1748\n",
            "Epoch 4/100, Train Loss: 0.1291, Val Loss: 0.1058\n",
            "Epoch 5/100, Train Loss: 0.1168, Val Loss: 0.1435\n",
            "Epoch 6/100, Train Loss: 0.0874, Val Loss: 0.0337\n",
            "Epoch 7/100, Train Loss: 0.0747, Val Loss: 0.0559\n",
            "Epoch 8/100, Train Loss: 0.0658, Val Loss: 0.0481\n",
            "Epoch 9/100, Train Loss: 0.0626, Val Loss: 0.0489\n",
            "Epoch 10/100, Train Loss: 0.0575, Val Loss: 0.0335\n",
            "Epoch 11/100, Train Loss: 0.0485, Val Loss: 0.0691\n",
            "Epoch 12/100, Train Loss: 0.0574, Val Loss: 0.0498\n",
            "Epoch 13/100, Train Loss: 0.0459, Val Loss: 0.0618\n",
            "Epoch 14/100, Train Loss: 0.0466, Val Loss: 0.0396\n",
            "Epoch 15/100, Train Loss: 0.0460, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0433, Val Loss: 0.0431\n",
            "Epoch 17/100, Train Loss: 0.0392, Val Loss: 0.0429\n",
            "Epoch 18/100, Train Loss: 0.0379, Val Loss: 0.0430\n",
            "Epoch 19/100, Train Loss: 0.0374, Val Loss: 0.0433\n",
            "Epoch 20/100, Train Loss: 0.0372, Val Loss: 0.0432\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0369, Val Loss: 0.0432\n",
            "Epoch 22/100, Train Loss: 0.0363, Val Loss: 0.0432\n",
            "Epoch 23/100, Train Loss: 0.0369, Val Loss: 0.0432\n",
            "Epoch 24/100, Train Loss: 0.0370, Val Loss: 0.0432\n",
            "Epoch 25/100, Train Loss: 0.0373, Val Loss: 0.0432\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0379, Val Loss: 0.0432\n",
            "Epoch 27/100, Train Loss: 0.0364, Val Loss: 0.0432\n",
            "Epoch 28/100, Train Loss: 0.0364, Val Loss: 0.0432\n",
            "Epoch 29/100, Train Loss: 0.0368, Val Loss: 0.0432\n",
            "Epoch 30/100, Train Loss: 0.0369, Val Loss: 0.0432\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0365, Val Loss: 0.0432\n",
            "Epoch 32/100, Train Loss: 0.0363, Val Loss: 0.0432\n",
            "Epoch 33/100, Train Loss: 0.0379, Val Loss: 0.0432\n",
            "Epoch 34/100, Train Loss: 0.0367, Val Loss: 0.0432\n",
            "Epoch 35/100, Train Loss: 0.0364, Val Loss: 0.0432\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0439, Val Loss: 0.0432\n",
            "Epoch 37/100, Train Loss: 0.0358, Val Loss: 0.0432\n",
            "Epoch 38/100, Train Loss: 0.0367, Val Loss: 0.0432\n",
            "Epoch 39/100, Train Loss: 0.0367, Val Loss: 0.0432\n",
            "Epoch 40/100, Train Loss: 0.0367, Val Loss: 0.0432\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0365, Val Loss: 0.0432\n",
            "Epoch 42/100, Train Loss: 0.0371, Val Loss: 0.0432\n",
            "Epoch 43/100, Train Loss: 0.0370, Val Loss: 0.0432\n",
            "Epoch 44/100, Train Loss: 0.0372, Val Loss: 0.0432\n",
            "Epoch 45/100, Train Loss: 0.0374, Val Loss: 0.0432\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0366, Val Loss: 0.0432\n",
            "Epoch 47/100, Train Loss: 0.0368, Val Loss: 0.0432\n",
            "Epoch 48/100, Train Loss: 0.0365, Val Loss: 0.0432\n",
            "Epoch 49/100, Train Loss: 0.0370, Val Loss: 0.0432\n",
            "Epoch 50/100, Train Loss: 0.0368, Val Loss: 0.0432\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0396, Val Loss: 0.0432\n",
            "Epoch 52/100, Train Loss: 0.0364, Val Loss: 0.0432\n",
            "Epoch 53/100, Train Loss: 0.0367, Val Loss: 0.0432\n",
            "Epoch 54/100, Train Loss: 0.0367, Val Loss: 0.0432\n",
            "Epoch 55/100, Train Loss: 0.0366, Val Loss: 0.0432\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0363, Val Loss: 0.0432\n",
            "Epoch 57/100, Train Loss: 0.0366, Val Loss: 0.0432\n",
            "Epoch 58/100, Train Loss: 0.0370, Val Loss: 0.0432\n",
            "Epoch 59/100, Train Loss: 0.0363, Val Loss: 0.0432\n",
            "Epoch 60/100, Train Loss: 0.0372, Val Loss: 0.0432\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0364, Val Loss: 0.0432\n",
            "Epoch 62/100, Train Loss: 0.0363, Val Loss: 0.0432\n",
            "Epoch 63/100, Train Loss: 0.0367, Val Loss: 0.0432\n",
            "Epoch 64/100, Train Loss: 0.0366, Val Loss: 0.0432\n",
            "Epoch 65/100, Train Loss: 0.0374, Val Loss: 0.0432\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0368, Val Loss: 0.0432\n",
            "Epoch 67/100, Train Loss: 0.0365, Val Loss: 0.0432\n",
            "Epoch 68/100, Train Loss: 0.0372, Val Loss: 0.0432\n",
            "Epoch 69/100, Train Loss: 0.0361, Val Loss: 0.0432\n",
            "Epoch 70/100, Train Loss: 0.0365, Val Loss: 0.0432\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0371, Val Loss: 0.0432\n",
            "Epoch 72/100, Train Loss: 0.0363, Val Loss: 0.0432\n",
            "Epoch 73/100, Train Loss: 0.0366, Val Loss: 0.0432\n",
            "Epoch 74/100, Train Loss: 0.0365, Val Loss: 0.0432\n",
            "Epoch 75/100, Train Loss: 0.0362, Val Loss: 0.0432\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0368, Val Loss: 0.0432\n",
            "Epoch 77/100, Train Loss: 0.0371, Val Loss: 0.0432\n",
            "Epoch 78/100, Train Loss: 0.0368, Val Loss: 0.0432\n",
            "Epoch 79/100, Train Loss: 0.0370, Val Loss: 0.0432\n",
            "Epoch 80/100, Train Loss: 0.0365, Val Loss: 0.0432\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0367, Val Loss: 0.0432\n",
            "Epoch 82/100, Train Loss: 0.0361, Val Loss: 0.0432\n",
            "Epoch 83/100, Train Loss: 0.0369, Val Loss: 0.0432\n",
            "Epoch 84/100, Train Loss: 0.0365, Val Loss: 0.0432\n",
            "Epoch 85/100, Train Loss: 0.0372, Val Loss: 0.0432\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0371, Val Loss: 0.0432\n",
            "Epoch 87/100, Train Loss: 0.0410, Val Loss: 0.0432\n",
            "Epoch 88/100, Train Loss: 0.0364, Val Loss: 0.0432\n",
            "Epoch 89/100, Train Loss: 0.0363, Val Loss: 0.0432\n",
            "Epoch 90/100, Train Loss: 0.0364, Val Loss: 0.0432\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0373, Val Loss: 0.0432\n",
            "Epoch 92/100, Train Loss: 0.0375, Val Loss: 0.0432\n",
            "Epoch 93/100, Train Loss: 0.0367, Val Loss: 0.0432\n",
            "Epoch 94/100, Train Loss: 0.0365, Val Loss: 0.0432\n",
            "Epoch 95/100, Train Loss: 0.0368, Val Loss: 0.0432\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0368, Val Loss: 0.0432\n",
            "Epoch 97/100, Train Loss: 0.0367, Val Loss: 0.0432\n",
            "Epoch 98/100, Train Loss: 0.0374, Val Loss: 0.0432\n",
            "Epoch 99/100, Train Loss: 0.0364, Val Loss: 0.0432\n",
            "Epoch 100/100, Train Loss: 0.0365, Val Loss: 0.0432\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 27/28\n",
            "Current training set size: 123 samples\n",
            "Epoch 1/100, Train Loss: 0.1500, Val Loss: 0.1883\n",
            "Epoch 2/100, Train Loss: 0.1469, Val Loss: 0.1899\n",
            "Epoch 3/100, Train Loss: 0.1393, Val Loss: 0.1521\n",
            "Epoch 4/100, Train Loss: 0.1150, Val Loss: 0.0973\n",
            "Epoch 5/100, Train Loss: 0.0837, Val Loss: 0.0811\n",
            "Epoch 6/100, Train Loss: 0.0765, Val Loss: 0.0756\n",
            "Epoch 7/100, Train Loss: 0.0709, Val Loss: 0.0665\n",
            "Epoch 8/100, Train Loss: 0.0693, Val Loss: 0.0666\n",
            "Epoch 9/100, Train Loss: 0.0627, Val Loss: 0.0453\n",
            "Epoch 10/100, Train Loss: 0.0571, Val Loss: 0.0521\n",
            "Epoch 11/100, Train Loss: 0.0597, Val Loss: 0.0521\n",
            "Epoch 12/100, Train Loss: 0.0516, Val Loss: 0.0561\n",
            "Epoch 13/100, Train Loss: 0.0482, Val Loss: 0.0521\n",
            "Epoch 14/100, Train Loss: 0.0448, Val Loss: 0.0382\n",
            "Epoch 15/100, Train Loss: 0.0458, Val Loss: 0.0426\n",
            "Epoch 16/100, Train Loss: 0.0434, Val Loss: 0.0581\n",
            "Epoch 17/100, Train Loss: 0.0477, Val Loss: 0.0517\n",
            "Epoch 18/100, Train Loss: 0.0412, Val Loss: 0.0522\n",
            "Epoch 19/100, Train Loss: 0.0452, Val Loss: 0.0520\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0395, Val Loss: 0.0505\n",
            "Epoch 21/100, Train Loss: 0.0381, Val Loss: 0.0488\n",
            "Epoch 22/100, Train Loss: 0.0374, Val Loss: 0.0470\n",
            "Epoch 23/100, Train Loss: 0.0367, Val Loss: 0.0470\n",
            "Epoch 24/100, Train Loss: 0.0365, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0362, Val Loss: 0.0466\n",
            "Epoch 26/100, Train Loss: 0.0366, Val Loss: 0.0466\n",
            "Epoch 27/100, Train Loss: 0.0361, Val Loss: 0.0466\n",
            "Epoch 28/100, Train Loss: 0.0362, Val Loss: 0.0466\n",
            "Epoch 29/100, Train Loss: 0.0357, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0364, Val Loss: 0.0466\n",
            "Epoch 31/100, Train Loss: 0.0368, Val Loss: 0.0466\n",
            "Epoch 32/100, Train Loss: 0.0353, Val Loss: 0.0466\n",
            "Epoch 33/100, Train Loss: 0.0358, Val Loss: 0.0466\n",
            "Epoch 34/100, Train Loss: 0.0359, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0359, Val Loss: 0.0466\n",
            "Epoch 36/100, Train Loss: 0.0363, Val Loss: 0.0466\n",
            "Epoch 37/100, Train Loss: 0.0360, Val Loss: 0.0466\n",
            "Epoch 38/100, Train Loss: 0.0362, Val Loss: 0.0466\n",
            "Epoch 39/100, Train Loss: 0.0363, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0360, Val Loss: 0.0466\n",
            "Epoch 41/100, Train Loss: 0.0371, Val Loss: 0.0466\n",
            "Epoch 42/100, Train Loss: 0.0361, Val Loss: 0.0466\n",
            "Epoch 43/100, Train Loss: 0.0366, Val Loss: 0.0466\n",
            "Epoch 44/100, Train Loss: 0.0350, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0372, Val Loss: 0.0466\n",
            "Epoch 46/100, Train Loss: 0.0366, Val Loss: 0.0466\n",
            "Epoch 47/100, Train Loss: 0.0356, Val Loss: 0.0466\n",
            "Epoch 48/100, Train Loss: 0.0364, Val Loss: 0.0466\n",
            "Epoch 49/100, Train Loss: 0.0367, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0362, Val Loss: 0.0466\n",
            "Epoch 51/100, Train Loss: 0.0360, Val Loss: 0.0466\n",
            "Epoch 52/100, Train Loss: 0.0361, Val Loss: 0.0466\n",
            "Epoch 53/100, Train Loss: 0.0361, Val Loss: 0.0466\n",
            "Epoch 54/100, Train Loss: 0.0364, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0358, Val Loss: 0.0466\n",
            "Epoch 56/100, Train Loss: 0.0360, Val Loss: 0.0466\n",
            "Epoch 57/100, Train Loss: 0.0361, Val Loss: 0.0466\n",
            "Epoch 58/100, Train Loss: 0.0368, Val Loss: 0.0466\n",
            "Epoch 59/100, Train Loss: 0.0362, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0355, Val Loss: 0.0466\n",
            "Epoch 61/100, Train Loss: 0.0361, Val Loss: 0.0466\n",
            "Epoch 62/100, Train Loss: 0.0356, Val Loss: 0.0466\n",
            "Epoch 63/100, Train Loss: 0.0360, Val Loss: 0.0466\n",
            "Epoch 64/100, Train Loss: 0.0360, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0356, Val Loss: 0.0466\n",
            "Epoch 66/100, Train Loss: 0.0356, Val Loss: 0.0466\n",
            "Epoch 67/100, Train Loss: 0.0364, Val Loss: 0.0466\n",
            "Epoch 68/100, Train Loss: 0.0360, Val Loss: 0.0466\n",
            "Epoch 69/100, Train Loss: 0.0355, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0355, Val Loss: 0.0466\n",
            "Epoch 71/100, Train Loss: 0.0364, Val Loss: 0.0466\n",
            "Epoch 72/100, Train Loss: 0.0368, Val Loss: 0.0466\n",
            "Epoch 73/100, Train Loss: 0.0365, Val Loss: 0.0466\n",
            "Epoch 74/100, Train Loss: 0.0361, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0353, Val Loss: 0.0466\n",
            "Epoch 76/100, Train Loss: 0.0360, Val Loss: 0.0466\n",
            "Epoch 77/100, Train Loss: 0.0356, Val Loss: 0.0466\n",
            "Epoch 78/100, Train Loss: 0.0359, Val Loss: 0.0466\n",
            "Epoch 79/100, Train Loss: 0.0367, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0361, Val Loss: 0.0466\n",
            "Epoch 81/100, Train Loss: 0.0356, Val Loss: 0.0466\n",
            "Epoch 82/100, Train Loss: 0.0362, Val Loss: 0.0466\n",
            "Epoch 83/100, Train Loss: 0.0355, Val Loss: 0.0466\n",
            "Epoch 84/100, Train Loss: 0.0361, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0355, Val Loss: 0.0466\n",
            "Epoch 86/100, Train Loss: 0.0364, Val Loss: 0.0466\n",
            "Epoch 87/100, Train Loss: 0.0362, Val Loss: 0.0466\n",
            "Epoch 88/100, Train Loss: 0.0355, Val Loss: 0.0466\n",
            "Epoch 89/100, Train Loss: 0.0364, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0353, Val Loss: 0.0466\n",
            "Epoch 91/100, Train Loss: 0.0361, Val Loss: 0.0466\n",
            "Epoch 92/100, Train Loss: 0.0357, Val Loss: 0.0466\n",
            "Epoch 93/100, Train Loss: 0.0358, Val Loss: 0.0466\n",
            "Epoch 94/100, Train Loss: 0.0363, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0357, Val Loss: 0.0466\n",
            "Epoch 96/100, Train Loss: 0.0361, Val Loss: 0.0466\n",
            "Epoch 97/100, Train Loss: 0.0362, Val Loss: 0.0466\n",
            "Epoch 98/100, Train Loss: 0.0359, Val Loss: 0.0466\n",
            "Epoch 99/100, Train Loss: 0.0361, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0362, Val Loss: 0.0466\n",
            "\n",
            "Test iteration 28/28\n",
            "Current training set size: 124 samples\n",
            "Epoch 1/100, Train Loss: 0.1693, Val Loss: 0.1914\n",
            "Epoch 2/100, Train Loss: 0.1461, Val Loss: 0.1833\n",
            "Epoch 3/100, Train Loss: 0.1398, Val Loss: 0.1539\n",
            "Epoch 4/100, Train Loss: 0.1074, Val Loss: 0.0480\n",
            "Epoch 5/100, Train Loss: 0.0783, Val Loss: 0.0375\n",
            "Epoch 6/100, Train Loss: 0.0950, Val Loss: 0.0401\n",
            "Epoch 7/100, Train Loss: 0.0728, Val Loss: 0.0667\n",
            "Epoch 8/100, Train Loss: 0.0713, Val Loss: 0.0614\n",
            "Epoch 9/100, Train Loss: 0.0658, Val Loss: 0.0428\n",
            "Epoch 10/100, Train Loss: 0.0650, Val Loss: 0.0696\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 11/100, Train Loss: 0.0695, Val Loss: 0.0505\n",
            "Epoch 12/100, Train Loss: 0.0542, Val Loss: 0.0374\n",
            "Epoch 13/100, Train Loss: 0.0503, Val Loss: 0.0339\n",
            "Epoch 14/100, Train Loss: 0.0491, Val Loss: 0.0334\n",
            "Epoch 15/100, Train Loss: 0.0489, Val Loss: 0.0327\n",
            "Epoch 16/100, Train Loss: 0.0487, Val Loss: 0.0326\n",
            "Epoch 17/100, Train Loss: 0.0481, Val Loss: 0.0330\n",
            "Epoch 18/100, Train Loss: 0.0469, Val Loss: 0.0340\n",
            "Epoch 19/100, Train Loss: 0.0476, Val Loss: 0.0345\n",
            "Epoch 20/100, Train Loss: 0.0469, Val Loss: 0.0349\n",
            "Epoch 21/100, Train Loss: 0.0470, Val Loss: 0.0350\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0469, Val Loss: 0.0350\n",
            "Epoch 23/100, Train Loss: 0.0469, Val Loss: 0.0350\n",
            "Epoch 24/100, Train Loss: 0.0466, Val Loss: 0.0350\n",
            "Epoch 25/100, Train Loss: 0.0471, Val Loss: 0.0351\n",
            "Epoch 26/100, Train Loss: 0.0470, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0471, Val Loss: 0.0351\n",
            "Epoch 28/100, Train Loss: 0.0472, Val Loss: 0.0351\n",
            "Epoch 29/100, Train Loss: 0.0474, Val Loss: 0.0351\n",
            "Epoch 30/100, Train Loss: 0.0467, Val Loss: 0.0351\n",
            "Epoch 31/100, Train Loss: 0.0470, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0469, Val Loss: 0.0351\n",
            "Epoch 33/100, Train Loss: 0.0474, Val Loss: 0.0351\n",
            "Epoch 34/100, Train Loss: 0.0468, Val Loss: 0.0351\n",
            "Epoch 35/100, Train Loss: 0.0470, Val Loss: 0.0351\n",
            "Epoch 36/100, Train Loss: 0.0471, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0472, Val Loss: 0.0351\n",
            "Epoch 38/100, Train Loss: 0.0466, Val Loss: 0.0351\n",
            "Epoch 39/100, Train Loss: 0.0472, Val Loss: 0.0351\n",
            "Epoch 40/100, Train Loss: 0.0468, Val Loss: 0.0351\n",
            "Epoch 41/100, Train Loss: 0.0472, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0466, Val Loss: 0.0351\n",
            "Epoch 43/100, Train Loss: 0.0473, Val Loss: 0.0351\n",
            "Epoch 44/100, Train Loss: 0.0467, Val Loss: 0.0351\n",
            "Epoch 45/100, Train Loss: 0.0471, Val Loss: 0.0351\n",
            "Epoch 46/100, Train Loss: 0.0472, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0471, Val Loss: 0.0351\n",
            "Epoch 48/100, Train Loss: 0.0469, Val Loss: 0.0351\n",
            "Epoch 49/100, Train Loss: 0.0467, Val Loss: 0.0351\n",
            "Epoch 50/100, Train Loss: 0.0466, Val Loss: 0.0351\n",
            "Epoch 51/100, Train Loss: 0.0471, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0470, Val Loss: 0.0351\n",
            "Epoch 53/100, Train Loss: 0.0467, Val Loss: 0.0351\n",
            "Epoch 54/100, Train Loss: 0.0464, Val Loss: 0.0351\n",
            "Epoch 55/100, Train Loss: 0.0467, Val Loss: 0.0351\n",
            "Epoch 56/100, Train Loss: 0.0465, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0461, Val Loss: 0.0351\n",
            "Epoch 58/100, Train Loss: 0.0464, Val Loss: 0.0351\n",
            "Epoch 59/100, Train Loss: 0.0471, Val Loss: 0.0351\n",
            "Epoch 60/100, Train Loss: 0.0474, Val Loss: 0.0351\n",
            "Epoch 61/100, Train Loss: 0.0470, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0465, Val Loss: 0.0351\n",
            "Epoch 63/100, Train Loss: 0.0465, Val Loss: 0.0351\n",
            "Epoch 64/100, Train Loss: 0.0468, Val Loss: 0.0351\n",
            "Epoch 65/100, Train Loss: 0.0471, Val Loss: 0.0351\n",
            "Epoch 66/100, Train Loss: 0.0467, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0469, Val Loss: 0.0351\n",
            "Epoch 68/100, Train Loss: 0.0469, Val Loss: 0.0351\n",
            "Epoch 69/100, Train Loss: 0.0466, Val Loss: 0.0351\n",
            "Epoch 70/100, Train Loss: 0.0469, Val Loss: 0.0351\n",
            "Epoch 71/100, Train Loss: 0.0471, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0472, Val Loss: 0.0351\n",
            "Epoch 73/100, Train Loss: 0.0468, Val Loss: 0.0351\n",
            "Epoch 74/100, Train Loss: 0.0468, Val Loss: 0.0351\n",
            "Epoch 75/100, Train Loss: 0.0474, Val Loss: 0.0351\n",
            "Epoch 76/100, Train Loss: 0.0467, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0467, Val Loss: 0.0351\n",
            "Epoch 78/100, Train Loss: 0.0466, Val Loss: 0.0351\n",
            "Epoch 79/100, Train Loss: 0.0470, Val Loss: 0.0351\n",
            "Epoch 80/100, Train Loss: 0.0473, Val Loss: 0.0351\n",
            "Epoch 81/100, Train Loss: 0.0469, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0470, Val Loss: 0.0351\n",
            "Epoch 83/100, Train Loss: 0.0470, Val Loss: 0.0351\n",
            "Epoch 84/100, Train Loss: 0.0468, Val Loss: 0.0351\n",
            "Epoch 85/100, Train Loss: 0.0465, Val Loss: 0.0351\n",
            "Epoch 86/100, Train Loss: 0.0472, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0471, Val Loss: 0.0351\n",
            "Epoch 88/100, Train Loss: 0.0471, Val Loss: 0.0351\n",
            "Epoch 89/100, Train Loss: 0.0467, Val Loss: 0.0351\n",
            "Epoch 90/100, Train Loss: 0.0467, Val Loss: 0.0351\n",
            "Epoch 91/100, Train Loss: 0.0466, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0468, Val Loss: 0.0351\n",
            "Epoch 93/100, Train Loss: 0.0469, Val Loss: 0.0351\n",
            "Epoch 94/100, Train Loss: 0.0466, Val Loss: 0.0351\n",
            "Epoch 95/100, Train Loss: 0.0468, Val Loss: 0.0351\n",
            "Epoch 96/100, Train Loss: 0.0466, Val Loss: 0.0351\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0464, Val Loss: 0.0351\n",
            "Epoch 98/100, Train Loss: 0.0470, Val Loss: 0.0351\n",
            "Epoch 99/100, Train Loss: 0.0472, Val Loss: 0.0351\n",
            "Epoch 100/100, Train Loss: 0.0471, Val Loss: 0.0351\n",
            "Results:\n",
            "            RMSE            MAE       MAPE model_type  units  drop_rate  \\\n",
            "0  240585.072923  108315.003348   9.835603       lstm    256        0.1   \n",
            "1  252468.690125  111569.946429  10.189485       lstm    256        0.1   \n",
            "2  241551.867275  119975.191964   4.917729       lstm    256        0.2   \n",
            "3  212744.688882   99794.498884   9.813545       lstm    256        0.2   \n",
            "4  237389.188356  103371.850446   4.304721       lstm    512        0.1   \n",
            "5  233327.415288   99766.943080   8.443049       lstm    512        0.1   \n",
            "6  241813.786737  112261.626116   7.455519       lstm    512        0.2   \n",
            "7  223458.881703   99858.380580   5.419324       lstm    512        0.2   \n",
            "\n",
            "   dense_unit  batch_size  epochs  \n",
            "0          32           4     100  \n",
            "1          64           4     100  \n",
            "2          32           4     100  \n",
            "3          64           4     100  \n",
            "4          32           4     100  \n",
            "5          64           4     100  \n",
            "6          32           4     100  \n",
            "7          64           4     100  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adjusted_valuelist.to_csv( '/content/drive/MyDrive/LSTM_random.csv', index=False)\n",
        "pd.DataFrame(all_adjusted_predictions).to_csv(\"/content/drive/MyDrive/all_adjusted_predictions_LSTM_random.csv\", index=False)\n",
        "pd.DataFrame(all_ground_truths).to_csv(\"/content/drive/MyDrive/all_ground_truths_LSTM_random.csv\", index=False)"
      ],
      "metadata": {
        "id": "XXax4KMP4338"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adjusted_valuelist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "qUJQJfKT49Hg",
        "outputId": "cf172905-d27c-4fc8-9990-70c775325d8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            RMSE            MAE       MAPE model_type  units  drop_rate  \\\n",
              "0  240585.072923  108315.003348   9.835603       lstm    256        0.1   \n",
              "1  252468.690125  111569.946429  10.189485       lstm    256        0.1   \n",
              "2  241551.867275  119975.191964   4.917729       lstm    256        0.2   \n",
              "3  212744.688882   99794.498884   9.813545       lstm    256        0.2   \n",
              "4  237389.188356  103371.850446   4.304721       lstm    512        0.1   \n",
              "5  233327.415288   99766.943080   8.443049       lstm    512        0.1   \n",
              "6  241813.786737  112261.626116   7.455519       lstm    512        0.2   \n",
              "7  223458.881703   99858.380580   5.419324       lstm    512        0.2   \n",
              "\n",
              "   dense_unit  batch_size  epochs  \n",
              "0          32           4     100  \n",
              "1          64           4     100  \n",
              "2          32           4     100  \n",
              "3          64           4     100  \n",
              "4          32           4     100  \n",
              "5          64           4     100  \n",
              "6          32           4     100  \n",
              "7          64           4     100  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-00860ba3-ca99-49a8-b980-788e6262f581\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAE</th>\n",
              "      <th>MAPE</th>\n",
              "      <th>model_type</th>\n",
              "      <th>units</th>\n",
              "      <th>drop_rate</th>\n",
              "      <th>dense_unit</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>epochs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>240585.072923</td>\n",
              "      <td>108315.003348</td>\n",
              "      <td>9.835603</td>\n",
              "      <td>lstm</td>\n",
              "      <td>256</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>252468.690125</td>\n",
              "      <td>111569.946429</td>\n",
              "      <td>10.189485</td>\n",
              "      <td>lstm</td>\n",
              "      <td>256</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>241551.867275</td>\n",
              "      <td>119975.191964</td>\n",
              "      <td>4.917729</td>\n",
              "      <td>lstm</td>\n",
              "      <td>256</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>212744.688882</td>\n",
              "      <td>99794.498884</td>\n",
              "      <td>9.813545</td>\n",
              "      <td>lstm</td>\n",
              "      <td>256</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>237389.188356</td>\n",
              "      <td>103371.850446</td>\n",
              "      <td>4.304721</td>\n",
              "      <td>lstm</td>\n",
              "      <td>512</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>233327.415288</td>\n",
              "      <td>99766.943080</td>\n",
              "      <td>8.443049</td>\n",
              "      <td>lstm</td>\n",
              "      <td>512</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>241813.786737</td>\n",
              "      <td>112261.626116</td>\n",
              "      <td>7.455519</td>\n",
              "      <td>lstm</td>\n",
              "      <td>512</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>223458.881703</td>\n",
              "      <td>99858.380580</td>\n",
              "      <td>5.419324</td>\n",
              "      <td>lstm</td>\n",
              "      <td>512</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-00860ba3-ca99-49a8-b980-788e6262f581')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-00860ba3-ca99-49a8-b980-788e6262f581 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-00860ba3-ca99-49a8-b980-788e6262f581');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-55f9a449-bc98-4087-b9b6-99659224b595\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-55f9a449-bc98-4087-b9b6-99659224b595')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-55f9a449-bc98-4087-b9b6-99659224b595 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_956e9522-09e5-47a5-b366-e0cdcbbb1f6c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('adjusted_valuelist')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_956e9522-09e5-47a5-b366-e0cdcbbb1f6c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('adjusted_valuelist');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "adjusted_valuelist",
              "summary": "{\n  \"name\": \"adjusted_valuelist\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12313.237737743078,\n        \"min\": 212744.68888187592,\n        \"max\": 252468.69012497066,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          252468.69012497066,\n          233327.41528803724,\n          240585.07292278582\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7434.561564237152,\n        \"min\": 99766.94308035714,\n        \"max\": 119975.19196428571,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          111569.94642857143,\n          99766.94308035714,\n          108315.00334821429\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          10.189484596252441,\n          8.443049430847168,\n          9.835602760314941\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"lstm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"units\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 136,\n        \"min\": 256,\n        \"max\": 512,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          512\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"drop_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05345224838248488,\n        \"min\": 0.1,\n        \"max\": 0.2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dense_unit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17,\n        \"min\": 32,\n        \"max\": 64,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 4,\n        \"max\": 4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"epochs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 100,\n        \"max\": 100,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          100\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_adjusted_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hTKh7164-jF",
        "outputId": "96641d3c-36e8-4f5f-b04c-3d5c30f57b16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lstm_unit256_drop0.1_dense32_batch4_epochs100': array([      0.  ,  620281.2 ,  476042.4 , 1385132.1 ,  566537.3 ,\n",
              "         415190.6 ,  589163.9 ,  610282.06, 1265234.1 ,  608050.56,\n",
              "        1162219.  , 1493865.5 ,  684290.5 ,  416994.16,  810215.75,\n",
              "         599502.44,  415822.12,  450570.62, 1580895.9 , 1317135.  ,\n",
              "         513524.3 , 1295883.1 ,  568279.06,  382833.97,  669407.75,\n",
              "         459276.47, 1608031.1 ,  776279.8 ], dtype=float32),\n",
              " 'lstm_unit256_drop0.1_dense64_batch4_epochs100': array([      0.  ,  622279.7 ,  436755.4 , 1281191.2 ,  522234.6 ,\n",
              "         406610.1 ,  608010.3 ,  688814.7 , 1417494.6 ,  615734.56,\n",
              "        1199657.4 , 1626889.4 ,  607761.7 ,  417674.94,  805860.8 ,\n",
              "         604994.3 ,  400079.66,  460013.47, 1606530.6 , 1240840.8 ,\n",
              "         549384.5 , 1254397.4 ,  603040.8 ,  381222.66,  647270.06,\n",
              "         471475.94, 1447417.  ,  794483.44], dtype=float32),\n",
              " 'lstm_unit256_drop0.2_dense32_batch4_epochs100': array([      0.  ,  592508.2 ,  470278.72, 1379261.5 ,  525406.4 ,\n",
              "         394582.16,  597487.4 ,  637595.7 , 1473642.6 ,  619682.6 ,\n",
              "        1196357.9 , 1469127.9 ,  646718.44,  462131.06,  740847.9 ,\n",
              "         661940.5 ,  402562.7 ,  457311.53, 1560415.5 , 1301975.2 ,\n",
              "         526128.6 , 1314930.8 ,  653948.94,  377520.1 ,  647482.75,\n",
              "         439788.88, 1607049.1 ,  745495.94], dtype=float32),\n",
              " 'lstm_unit256_drop0.2_dense64_batch4_epochs100': array([      0.  ,  620156.6 ,  461986.8 , 1396148.2 ,  574688.25,\n",
              "         384917.34,  614853.5 ,  614700.94, 1396130.8 ,  613884.8 ,\n",
              "        1232626.6 , 1639718.9 ,  635286.9 ,  413579.03,  777345.  ,\n",
              "         622308.8 ,  390261.06,  442606.84, 1452823.8 , 1297376.6 ,\n",
              "         521267.44, 1232156.4 ,  614865.06,  381320.4 ,  701771.3 ,\n",
              "         457512.66, 1628518.9 ,  763231.56], dtype=float32),\n",
              " 'lstm_unit512_drop0.1_dense32_batch4_epochs100': array([      0.  ,  589046.3 ,  464331.12, 1310386.5 ,  543589.4 ,\n",
              "         388131.8 ,  606487.9 ,  623174.3 , 1285008.1 ,  615515.94,\n",
              "        1223044.6 , 1560349.2 ,  668356.1 ,  429325.34,  785056.44,\n",
              "         557046.94,  426130.  ,  450507.44, 1583120.9 , 1341372.8 ,\n",
              "         513022.78, 1355586.1 ,  604080.25,  378931.62,  682668.44,\n",
              "         447901.3 , 1617089.9 ,  785286.56], dtype=float32),\n",
              " 'lstm_unit512_drop0.1_dense64_batch4_epochs100': array([      0.  ,  612416.94,  509162.16, 1384493.  ,  536175.5 ,\n",
              "         419769.34,  594096.7 ,  639116.3 , 1338737.5 ,  617810.  ,\n",
              "        1131229.4 , 1532148.4 ,  678894.2 ,  444653.56,  825284.3 ,\n",
              "         617404.6 ,  398352.2 ,  451044.44, 1570724.1 , 1199412.1 ,\n",
              "         544418.9 , 1207380.6 ,  572718.3 ,  363482.84,  690519.06,\n",
              "         449937.75, 1695928.1 ,  779465.4 ], dtype=float32),\n",
              " 'lstm_unit512_drop0.2_dense32_batch4_epochs100': array([      0.  ,  606840.  ,  469336.66, 1348130.8 ,  547263.06,\n",
              "         418939.66,  571367.  ,  654776.06, 1280057.4 ,  616265.6 ,\n",
              "        1229190.1 , 1574639.4 ,  692078.94,  469384.06,  787466.6 ,\n",
              "         595655.44,  414667.66,  444419.4 , 1611156.4 , 1217819.9 ,\n",
              "         537716.7 , 1388785.5 ,  578678.5 ,  363503.1 ,  689466.56,\n",
              "         439284.88, 1692807.6 ,  775540.9 ], dtype=float32),\n",
              " 'lstm_unit512_drop0.2_dense64_batch4_epochs100': array([      0.  ,  595340.9 ,  438974.84, 1360950.6 ,  537509.1 ,\n",
              "         403659.72,  591556.4 ,  618978.  , 1322000.9 ,  602652.6 ,\n",
              "        1225049.2 , 1633791.5 ,  618724.6 ,  438118.06,  756909.9 ,\n",
              "         613160.6 ,  408400.28,  455964.88, 1536157.  , 1271543.8 ,\n",
              "         545593.8 , 1310143.5 ,  585208.5 ,  373849.53,  673042.1 ,\n",
              "         463261.6 , 1702885.2 ,  731237.5 ], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_ground_truths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mArShf7R4_wc",
        "outputId": "6f5afc38-603c-4543-af7e-dc8e430861a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lstm_unit256_drop0.1_dense32_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'lstm_unit256_drop0.1_dense64_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'lstm_unit256_drop0.2_dense32_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'lstm_unit256_drop0.2_dense64_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'lstm_unit512_drop0.1_dense32_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'lstm_unit512_drop0.1_dense64_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'lstm_unit512_drop0.2_dense32_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32),\n",
              " 'lstm_unit512_drop0.2_dense64_batch4_epochs100': array([      0. ,  564736. ,  552234. , 1301909. ,  466000. ,  416249. ,\n",
              "         613919. ,  590881. , 1315792. ,  676719. , 1161114. , 1809580. ,\n",
              "         626324. ,  427935. ,  830608. ,  559002. ,  376400. ,  460000. ,\n",
              "         449923. , 1431845. ,  630000. , 1205157. ,  593566. ,  292843. ,\n",
              "         756000. ,  475733. , 1994124.9,  776174. ], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config_key = 'lstm_unit256_drop0.2_dense32_batch4_epochs100'\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(all_ground_truths[config_key], label='Ground Truth')\n",
        "plt.plot(all_adjusted_predictions[config_key], label='LSTM Predictions')\n",
        "plt.title(f'LSTM Performance: {config_key}')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "lYUklPxsUPxP",
        "outputId": "cd6bce55-3fde-4364-d32f-2f2fc20337dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compare to Other Models"
      ],
      "metadata": {
        "id": "RdRdUjMpxjpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FIXED"
      ],
      "metadata": {
        "id": "8JNNYA2kxrWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Giảm số lượng/GRU unit, dense unit, epochs và sử dụng batch size nhỏ hơn để huấn luyện nhanh hơn.\n",
        "# model_types = ['multi-scale']\n",
        "# lstm_unit = [128, 256, 512]\n",
        "# gru_unit = [8, 16, 32]\n",
        "# drop_rate = [0.1, 0.2]\n",
        "# dense_unit = [16, 32, 64]\n",
        "# batch_size_num = [2, 4]\n",
        "# epochs = [100]\n",
        "\n",
        "model_types = ['cnn', 'rf', 'xgb', 'gbm']\n",
        "lstm_unit = [256,512]\n",
        "gru_unit = [8,16]\n",
        "drop_rate = [0.1,0.2]\n",
        "dense_unit = [32,64]\n",
        "batch_size_num = [4]\n",
        "epochs = [100]\n",
        "n_estimators = [100, 200]\n",
        "\n",
        "# # Replace the current parameter definitions\n",
        "# model_types = ['hybrid', 'sequential', 'stacked', 'bidirectional', 'cnn-rnn', 'multi-scale', 'transformer-rnn', 'ensemble', 'lstm', 'gru']\n",
        "# lstm_unit = [128]\n",
        "# gru_unit = [8]\n",
        "# drop_rate = [0.1]\n",
        "# dense_unit = [64]\n",
        "# batch_size_num = [2]\n",
        "# epochs = [100]\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import concatenate\n",
        "import itertools\n",
        "from math import sqrt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, dropout_rate=0.2, dense_units=64):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # CNN layers\n",
        "        self.conv1 = nn.Conv1d(input_dim, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(128, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Calculate the size after conv + pooling layers\n",
        "        cnn_output_size = 64 * (time_steps // 2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(cnn_output_size, dense_units)\n",
        "        self.fc2 = nn.Linear(dense_units, dense_units // 2)\n",
        "        self.fc3 = nn.Linear(dense_units // 2, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: (batch, time_steps, features)\n",
        "        # Reshape for CNN: (batch, features, time_steps)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # Apply CNN layers\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        # Apply fully connected layers\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "class AttentionGRU(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, gru_units, dropout_rate, dense_units):\n",
        "        super(AttentionGRU, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention1 = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # GRU layer\n",
        "        self.gru = nn.GRU(input_dim, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear(gru_units * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, time_steps, input_dim)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.attention1(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)  # (batch_size, time_steps, input_dim)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # GRU layer - output: (batch, seq, hidden_size)\n",
        "        gru_out, _ = self.gru(attention_mul)\n",
        "\n",
        "        # Flatten and Dense layers\n",
        "        flattened = self.flatten(gru_out)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class AttentionLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, dropout_rate, dense_units):\n",
        "        super(AttentionLSTM, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention1 = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear(lstm_units * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, time_steps, input_dim)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.attention1(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)  # (batch_size, time_steps, input_dim)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # LSTM layer - output: (batch, seq, hidden_size)\n",
        "        lstm_out, _ = self.lstm(attention_mul)\n",
        "\n",
        "        # Flatten and Dense layers\n",
        "        flattened = self.flatten(lstm_out)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class HybridLSTM_GRU(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(HybridLSTM_GRU, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention1 = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # LSTM and GRU layers\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "        self.gru = nn.GRU(input_dim, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        # Combined size from both LSTM and GRU\n",
        "        self.dense1 = nn.Linear((lstm_units + gru_units) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, time_steps, input_dim)\n",
        "\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.attention1(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)  # (batch_size, time_steps, input_dim)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # LSTM and GRU layers\n",
        "        lstm_out, _ = self.lstm(attention_mul)\n",
        "        gru_out, _ = self.gru(attention_mul)\n",
        "\n",
        "        # Concatenate LSTM and GRU outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Flatten and Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class SequentialHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(SequentialHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # LSTM followed by GRU\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "        self.gru = nn.GRU(lstm_units, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear(gru_units * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)\n",
        "        a = self.attention(a)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # Sequential processing: LSTM then GRU\n",
        "        lstm_out, _ = self.lstm(attention_mul)\n",
        "        gru_out, _ = self.gru(lstm_out)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(gru_out)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class StackedHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(StackedHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention\n",
        "        self.attention = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # Two stacked LSTM layers\n",
        "        self.lstm1 = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(lstm_units, lstm_units//2, batch_first=True)\n",
        "\n",
        "        # Two stacked GRU layers\n",
        "        self.gru1 = nn.GRU(input_dim, gru_units, batch_first=True)\n",
        "        self.gru2 = nn.GRU(gru_units, gru_units//2, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units//2 + gru_units//2) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)\n",
        "        a = self.attention(a)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # Stacked LSTM path\n",
        "        lstm_out1, _ = self.lstm1(attention_mul)\n",
        "        lstm_out2, _ = self.lstm2(lstm_out1)\n",
        "\n",
        "        # Stacked GRU path\n",
        "        gru_out1, _ = self.gru1(attention_mul)\n",
        "        gru_out2, _ = self.gru2(gru_out1)\n",
        "\n",
        "        # Concatenate final outputs\n",
        "        combined = torch.cat((lstm_out2, gru_out2), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class BidirectionalHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(BidirectionalHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # Bidirectional LSTM and GRU\n",
        "        self.bilstm = nn.LSTM(input_dim, lstm_units, batch_first=True, bidirectional=True)\n",
        "        self.bigru = nn.GRU(input_dim, gru_units, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units*2 + gru_units*2) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)\n",
        "        a = self.attention(a)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # Bidirectional networks\n",
        "        lstm_out, _ = self.bilstm(attention_mul)\n",
        "        gru_out, _ = self.bigru(attention_mul)\n",
        "\n",
        "        # Concatenate outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class CNNRNNHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(CNNRNNHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # 1D CNN for feature extraction\n",
        "        self.conv1 = nn.Conv1d(input_dim, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # RNN layers\n",
        "        self.lstm = nn.LSTM(64, lstm_units, batch_first=True)\n",
        "        self.gru = nn.GRU(64, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units + gru_units) * (time_steps-1), dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # CNN feature extraction\n",
        "        x = x.permute(0, 2, 1)  # (batch, features, seq_len)\n",
        "        cnn_out = self.relu(self.conv1(x))\n",
        "        cnn_out = self.maxpool(cnn_out)\n",
        "        cnn_out = self.relu(self.conv2(cnn_out))\n",
        "        cnn_out = cnn_out.permute(0, 2, 1)  # (batch, seq_len, features)\n",
        "\n",
        "        # RNN processing\n",
        "        lstm_out, _ = self.lstm(cnn_out)\n",
        "        gru_out, _ = self.gru(cnn_out)\n",
        "\n",
        "        # Combine outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class MultiScaleHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(MultiScaleHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # LSTM for long-term dependencies\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "\n",
        "        # GRU for shorter-term dependencies (operating on windows)\n",
        "        self.gru = nn.GRU(input_dim, gru_units, batch_first=True)\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units + gru_units) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Full sequence for LSTM (long-term)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Attention mechanism for GRU input\n",
        "        a = x.permute(0, 2, 1)\n",
        "        a = self.attention(a)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)\n",
        "\n",
        "        # Apply attention weights for GRU (short-term focus)\n",
        "        gru_input = torch.mul(x, a)\n",
        "        gru_out, _ = self.gru(gru_input)\n",
        "\n",
        "        # Combine outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class TransformerRNNHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units, nhead=4):\n",
        "        super(TransformerRNNHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Input projection for transformer\n",
        "        self.input_proj = nn.Linear(input_dim, 64)\n",
        "\n",
        "        # Transformer encoder layer\n",
        "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=64, nhead=nhead, dropout=dropout_rate, batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layer, num_layers=2)\n",
        "\n",
        "        # RNN layers\n",
        "        self.lstm = nn.LSTM(64, lstm_units, batch_first=True)\n",
        "        self.gru = nn.GRU(64, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units + gru_units) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Project input to transformer dimension\n",
        "        x_proj = self.input_proj(x)\n",
        "\n",
        "        # Apply transformer encoder\n",
        "        transformer_out = self.transformer_encoder(x_proj)\n",
        "\n",
        "        # Process with RNNs\n",
        "        lstm_out, _ = self.lstm(transformer_out)\n",
        "        gru_out, _ = self.gru(transformer_out)\n",
        "\n",
        "        # Combine outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class EnsembleHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(EnsembleHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Individual models\n",
        "        self.lstm_model = AttentionLSTM(input_dim, time_steps, lstm_units, dropout_rate, dense_units)\n",
        "        self.gru_model = AttentionGRU(input_dim, time_steps, gru_units, dropout_rate, dense_units)\n",
        "\n",
        "        # Combination layer\n",
        "        self.combine = nn.Linear(2, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get predictions from each model\n",
        "        lstm_pred = self.lstm_model(x)\n",
        "        gru_pred = self.gru_model(x)\n",
        "\n",
        "        # Combine predictions (learnable weights)\n",
        "        combined = torch.cat((lstm_pred, gru_pred), dim=1)\n",
        "        output = self.final_activation(self.combine(combined))\n",
        "\n",
        "        return output\n",
        "\n",
        "class StackingEnsemble(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(StackingEnsemble, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # First-level models (weak learners)\n",
        "        self.lstm_model = AttentionLSTM(input_dim, time_steps, lstm_units, dropout_rate, dense_units)\n",
        "        self.gru_model = AttentionGRU(input_dim, time_steps, gru_units, dropout_rate, dense_units)\n",
        "\n",
        "        # Enhanced meta-learner with more context awareness\n",
        "        # Takes base model predictions plus context features from the original input\n",
        "        self.context_extractor = nn.Sequential(\n",
        "            nn.Linear(input_dim * time_steps, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "\n",
        "        # Meta-learner network with enhanced capacity and context awareness\n",
        "        self.meta_learner = nn.Sequential(\n",
        "            nn.Linear(2 + 32, 64),  # 2 base predictions + 32 context features\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.Dropout(dropout_rate * 0.5),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Attention mechanism for base model outputs\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(2, 2),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Get predictions from base models\n",
        "        lstm_pred = self.lstm_model(x)\n",
        "        gru_pred = self.gru_model(x)\n",
        "\n",
        "        # Extract context features from input\n",
        "        flattened_input = x.reshape(batch_size, -1)\n",
        "        context_features = self.context_extractor(flattened_input)\n",
        "\n",
        "        # Combine base model predictions\n",
        "        base_preds = torch.cat((lstm_pred, gru_pred), dim=1)\n",
        "\n",
        "        # Calculate attention weights for base predictions\n",
        "        attention_weights = self.attention(base_preds)\n",
        "\n",
        "        # Apply attention to base predictions\n",
        "        weighted_preds = base_preds * attention_weights\n",
        "\n",
        "        # Combine everything for the meta-learner\n",
        "        meta_features = torch.cat((weighted_preds, context_features), dim=1)\n",
        "\n",
        "        # Final prediction\n",
        "        final_pred = self.meta_learner(meta_features)\n",
        "\n",
        "        return final_pred\n",
        "\n",
        "    def train_weak_learners(self, train_loader, val_loader, epochs=10, device='cpu'):\n",
        "        \"\"\"\n",
        "        Pre-train the weak learners separately before training the full ensemble\n",
        "        \"\"\"\n",
        "        print(\"Pre-training weak learners...\")\n",
        "        criterion = nn.L1Loss()\n",
        "\n",
        "        # Train LSTM model\n",
        "        print(\"Pre-training LSTM model...\")\n",
        "        optimizer_lstm = optim.Adam(self.lstm_model.parameters(), lr=0.001)\n",
        "        best_lstm_loss = float('inf')\n",
        "        lstm_patience = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            self.lstm_model.train()\n",
        "            train_loss = 0.0\n",
        "            for inputs, targets in train_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                optimizer_lstm.zero_grad()\n",
        "                outputs = self.lstm_model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                optimizer_lstm.step()\n",
        "                train_loss += loss.item()\n",
        "\n",
        "            # Validation\n",
        "            self.lstm_model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for inputs, targets in val_loader:\n",
        "                    inputs, targets = inputs.to(device), targets.to(device)\n",
        "                    outputs = self.lstm_model(inputs)\n",
        "                    val_loss += criterion(outputs, targets).item()\n",
        "\n",
        "            print(f\"LSTM Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n",
        "\n",
        "            # Early stopping check\n",
        "            if val_loss < best_lstm_loss:\n",
        "                best_lstm_loss = val_loss\n",
        "                lstm_patience = 0\n",
        "            else:\n",
        "                lstm_patience += 1\n",
        "                if lstm_patience >= 3:\n",
        "                    print(\"Early stopping LSTM training\")\n",
        "                    break\n",
        "\n",
        "        # Train GRU model\n",
        "        print(\"Pre-training GRU model...\")\n",
        "        optimizer_gru = optim.Adam(self.gru_model.parameters(), lr=0.001)\n",
        "        best_gru_loss = float('inf')\n",
        "        gru_patience = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            self.gru_model.train()\n",
        "            train_loss = 0.0\n",
        "            for inputs, targets in train_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                optimizer_gru.zero_grad()\n",
        "                outputs = self.gru_model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                optimizer_gru.step()\n",
        "                train_loss += loss.item()\n",
        "\n",
        "            # Validation\n",
        "            self.gru_model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for inputs, targets in val_loader:\n",
        "                    inputs, targets = inputs.to(device), targets.to(device)\n",
        "                    outputs = self.gru_model(inputs)\n",
        "                    val_loss += criterion(outputs, targets).item()\n",
        "\n",
        "            print(f\"GRU Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n",
        "\n",
        "            # Early stopping check\n",
        "            if val_loss < best_gru_loss:\n",
        "                best_gru_loss = val_loss\n",
        "                gru_patience = 0\n",
        "            else:\n",
        "                gru_patience += 1\n",
        "                if gru_patience >= 3:\n",
        "                    print(\"Early stopping GRU training\")\n",
        "                    break\n",
        "\n",
        "        # Generate predictions from trained base models for meta-learner warm-up\n",
        "        print(\"Preparing meta-learner with base model predictions...\")\n",
        "        self.lstm_model.eval()\n",
        "        self.gru_model.eval()\n",
        "\n",
        "        # Freeze the weights of weak learners\n",
        "        for param in self.lstm_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        for param in self.gru_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        print(\"Weak learners trained and frozen.\")\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, dropout_rate=0.2, dense_units=64):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # CNN layers\n",
        "        self.conv1 = nn.Conv1d(input_dim, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(128, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Calculate the size after conv + pooling layers\n",
        "        cnn_output_size = 64 * (time_steps // 2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(cnn_output_size, dense_units)\n",
        "        self.fc2 = nn.Linear(dense_units, dense_units // 2)\n",
        "        self.fc3 = nn.Linear(dense_units // 2, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: (batch, time_steps, features)\n",
        "        # Reshape for CNN: (batch, features, time_steps)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # Apply CNN layers\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        # Apply fully connected layers\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "class SklearnModelWrapper:\n",
        "    \"\"\"Wrapper for traditional ML models to maintain interface consistency with PyTorch models\"\"\"\n",
        "    def __init__(self, model_type='rf', **kwargs):\n",
        "        self.model_type = model_type\n",
        "\n",
        "        if model_type == 'rf':\n",
        "            self.model = RandomForestRegressor(**kwargs)\n",
        "        elif model_type == 'gbm':\n",
        "            self.model = GradientBoostingRegressor(**kwargs)\n",
        "        elif model_type == 'xgb':\n",
        "            self.model = xgb.XGBRegressor(**kwargs)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "        self.device = 'cpu'  # Traditional ML models run on CPU\n",
        "\n",
        "    def to(self, device):\n",
        "        # Dummy method for compatibility with PyTorch interface\n",
        "        self.device = device\n",
        "        return self\n",
        "\n",
        "    def train(self):\n",
        "        # Dummy method for compatibility with PyTorch interface\n",
        "        pass\n",
        "\n",
        "    def eval(self):\n",
        "        # Dummy method for compatibility with PyTorch interface\n",
        "        pass\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # For inference - handle both PyTorch tensors and numpy arrays\n",
        "        if isinstance(x, torch.Tensor):\n",
        "            x_np = x.detach().cpu().numpy()\n",
        "        else:\n",
        "            x_np = x\n",
        "\n",
        "        # Reshape input for traditional ML models: from (batch, time_steps, features) to (batch, time_steps*features)\n",
        "        batch_size = x_np.shape[0]\n",
        "        x_flat = x_np.reshape(batch_size, -1)\n",
        "\n",
        "        # Get predictions\n",
        "        preds = self.model.predict(x_flat)\n",
        "\n",
        "        # Convert back to appropriate format\n",
        "        if isinstance(x, torch.Tensor):\n",
        "            return torch.FloatTensor(preds.reshape(-1, 1))\n",
        "        return preds.reshape(-1, 1)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train the model using scikit-learn's fit method\"\"\"\n",
        "        # Reshape input if necessary\n",
        "        if len(X.shape) == 3:  # (batch, time_steps, features)\n",
        "            X_flat = X.reshape(X.shape[0], -1)\n",
        "        else:\n",
        "            X_flat = X\n",
        "\n",
        "        return self.model.fit(X_flat, y)\n",
        "\n",
        "def build_model(train_X, train_Y, val_X, val_Y, model_type='gru', lstm_units=128, gru_units=128,\n",
        "               drop_rate=0.3, dense_unit=64, batch_size=32, epochs=100, n_estimators=100):\n",
        "    # Print training parameters\n",
        "    train_X_tensor = torch.FloatTensor(train_X)\n",
        "    train_Y_tensor = torch.FloatTensor(train_Y.reshape(-1, 1))\n",
        "    val_X_tensor = torch.FloatTensor(val_X)\n",
        "    val_Y_tensor = torch.FloatTensor(val_Y.reshape(-1, 1))\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
        "    val_dataset = TensorDataset(val_X_tensor, val_Y_tensor)\n",
        "\n",
        "    # Create reproducible DataLoaders with fixed seeds\n",
        "    train_generator = torch.Generator()\n",
        "    train_generator.manual_seed(SEED)\n",
        "    val_generator = torch.Generator()\n",
        "    val_generator.manual_seed(SEED)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=train_generator)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, generator=val_generator)\n",
        "\n",
        "    # Initialize model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    time_steps = train_X.shape[1]\n",
        "    input_dim = train_X.shape[2]\n",
        "\n",
        "    # Initialize model with fixed initial weights\n",
        "    torch.manual_seed(SEED)\n",
        "\n",
        "    # Determine model type and create appropriate model\n",
        "    if model_type == 'gru':\n",
        "        model = AttentionGRU(input_dim, time_steps, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'lstm':\n",
        "        model = AttentionLSTM(input_dim, time_steps, lstm_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'hybrid':\n",
        "        model = HybridLSTM_GRU(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'sequential':\n",
        "        model = SequentialHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'stacked':\n",
        "        model = StackedHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'bidirectional':\n",
        "        model = BidirectionalHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'cnn-rnn':\n",
        "        model = CNNRNNHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'multi-scale':\n",
        "        model = MultiScaleHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'transformer-rnn':\n",
        "        model = TransformerRNNHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'ensemble':\n",
        "        model = EnsembleHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'stacking':\n",
        "        model = StackingEnsemble(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "        # Pre-train weak learners\n",
        "        model.train_weak_learners(train_loader, val_loader, epochs=10, device=device)\n",
        "    elif model_type == 'cnn':\n",
        "        model = CNNModel(input_dim, time_steps, drop_rate, dense_unit).to(device)\n",
        "    elif model_type in ['rf', 'xgb', 'gbm']:\n",
        "        # Initialize appropriate traditional ML model\n",
        "        if model_type == 'rf':\n",
        "            model = SklearnModelWrapper(model_type='rf', n_estimators=n_estimators, random_state=SEED)\n",
        "        elif model_type == 'xgb':\n",
        "            model = SklearnModelWrapper(model_type='xgb', n_estimators=n_estimators, random_state=SEED,\n",
        "                                       learning_rate=0.1, max_depth=6)\n",
        "        elif model_type == 'gbm':\n",
        "            model = SklearnModelWrapper(model_type='gbm', n_estimators=n_estimators, random_state=SEED,\n",
        "                                       learning_rate=0.1, max_depth=6)\n",
        "\n",
        "        # Directly train traditional ML models (no epochs needed)\n",
        "        train_X_flat = train_X.reshape(train_X.shape[0], -1)\n",
        "        model.fit(train_X_flat, train_Y)\n",
        "        return model\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "    # For deep learning models, continue with the existing training loop\n",
        "    if model_type not in ['rf', 'xgb', 'gbm']:\n",
        "        # Initialize optimizer and loss function\n",
        "        optimizer = optim.Adam(model.parameters())\n",
        "        criterion = nn.L1Loss()  # MAE loss\n",
        "\n",
        "        # Training loop\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "        patience = 5\n",
        "        lr_factor = 0.01\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            for inputs, targets in train_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for inputs, targets in val_loader:\n",
        "                    inputs, targets = inputs.to(device), targets.to(device)\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, targets)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "            # Print progress\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Train Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
        "\n",
        "            # Learning rate schedule based on validation loss\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    for param_group in optimizer.param_groups:\n",
        "                        param_group['lr'] *= lr_factor\n",
        "                    patience_counter = 0\n",
        "                    print(f'Reducing learning rate by factor of {lr_factor}')\n",
        "\n",
        "    return model\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    mask = y_true != 0\n",
        "    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "    return mape\n",
        "\n",
        "def walk_forward(train_X, train_Y, val_X, val_Y, test_X, test_Y, grid_search, scaler):\n",
        "    r, f, c = test_X.shape\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    all_predictions = {}\n",
        "    all_adjusted_predictions = {}\n",
        "    all_ground_truths = {}\n",
        "\n",
        "    # Create lists to store all evaluation results\n",
        "    original_valuelists = []\n",
        "    adjusted_valuelists = []\n",
        "\n",
        "    for x in grid_search:\n",
        "        history_x = np.array([x for x in train_X])\n",
        "        history_y = np.array([y for y in train_Y])\n",
        "        predictions = list()\n",
        "        adjusted_predictions = list()\n",
        "        groundtrue = list()\n",
        "\n",
        "        # Extract model type first to determine how to unpack the rest\n",
        "        model_type = x[0]\n",
        "\n",
        "        # Create the appropriate config_key and extract parameters based on model type\n",
        "        if model_type in ['hybrid', 'sequential', 'stacked', 'bidirectional', 'cnn-rnn', 'multi-scale', 'transformer-rnn', 'ensemble', 'stacking']:\n",
        "            # Hybrid model has 7 parameters\n",
        "            model_type, lstm_unit_val, gru_unit_val, drop, dense, batch, epoch = x\n",
        "            units = f\"L{lstm_unit_val}_G{gru_unit_val}\"  # For logging\n",
        "            config_key = f\"{model_type}_lstmUnit{lstm_unit_val}_gruUnit{gru_unit_val}_drop{drop}_dense{dense}_batch{batch}_epochs{epoch}\"\n",
        "        elif model_type in ['rf', 'xgb', 'gbm']:\n",
        "            # Tree-based models have only 2 parameters\n",
        "            model_type, n_estimators = x\n",
        "            units = f\"Est{n_estimators}\"  # For logging\n",
        "            config_key = f\"{model_type}_estimators{n_estimators}\"\n",
        "            drop = 0\n",
        "            dense = 0\n",
        "            batch = 0\n",
        "            epoch = 0\n",
        "        elif model_type == 'cnn':\n",
        "            # CNN model has 6 parameters but no specific units parameter\n",
        "            model_type, _, drop, dense, batch, epoch = x\n",
        "            units = \"CNN\"  # For logging\n",
        "            config_key = f\"{model_type}_drop{drop}_dense{dense}_batch{batch}_epochs{epoch}\"\n",
        "        else:\n",
        "            # LSTM and GRU models have 6 parameters\n",
        "            model_type, units, drop, dense, batch, epoch = x\n",
        "            config_key = f\"{model_type}_unit{units}_drop{drop}_dense{dense}_batch{batch}_epochs{epoch}\"\n",
        "\n",
        "        print(\"\\n\" + \"*\"*50)\n",
        "        print(f\"Starting walk-forward validation with parameters:\")\n",
        "        print(f\"Model Type: {model_type}, Units/Estimators: {units}\")\n",
        "        if model_type not in ['rf', 'xgb', 'gbm']:\n",
        "            print(f\"Dropout: {drop}, Dense Units: {dense}\")\n",
        "            print(f\"Batch Size: {batch}, Epochs: {epoch}\")\n",
        "        else:\n",
        "            print(f\"n_estimators: {n_estimators}\")\n",
        "        print(f\"Device: {device}\")\n",
        "        print(f\"Total test samples: {len(test_X)}\")\n",
        "        print(\"*\"*50 + \"\\n\")\n",
        "\n",
        "        for i in range(len(test_X)):\n",
        "            print(f\"\\nTest iteration {i+1}/{len(test_X)}\")\n",
        "            print(f\"Current training set size: {history_x.shape[0]} samples\")\n",
        "\n",
        "            # Build model based on model type\n",
        "            if model_type in ['hybrid', 'sequential', 'stacked', 'bidirectional', 'cnn-rnn', 'multi-scale', 'transformer-rnn', 'ensemble', 'stacking']:\n",
        "                model = build_model(history_x, history_y, val_X, val_Y, model_type=model_type,\n",
        "                                lstm_units=lstm_unit_val, gru_units=gru_unit_val, drop_rate=drop,\n",
        "                                dense_unit=dense, batch_size=batch, epochs=epoch)\n",
        "            elif model_type in ['rf', 'xgb', 'gbm']:\n",
        "                model = build_model(history_x, history_y, val_X, val_Y, model_type=model_type,\n",
        "                                n_estimators=n_estimators)\n",
        "            elif model_type == 'cnn':\n",
        "                model = build_model(history_x, history_y, val_X, val_Y, model_type=model_type,\n",
        "                                drop_rate=drop, dense_unit=dense, batch_size=batch, epochs=epoch)\n",
        "            else:\n",
        "                model = build_model(history_x, history_y, val_X, val_Y, model_type=model_type,\n",
        "                                lstm_units=units if model_type == 'lstm' else 128,\n",
        "                                gru_units=units if model_type == 'gru' else 128,\n",
        "                                drop_rate=drop, dense_unit=dense, batch_size=batch, epochs=epoch)\n",
        "\n",
        "            # Set model to eval mode if it's a PyTorch model\n",
        "            if hasattr(model, 'eval'):\n",
        "                model.eval()\n",
        "\n",
        "            # Convert test data to appropriate format\n",
        "            if model_type in ['rf', 'xgb', 'gbm']:\n",
        "                # For traditional ML models, flatten the input\n",
        "                test_data = test_X[i].reshape(1, -1)\n",
        "                yhat = model(test_data)\n",
        "            else:\n",
        "                # For deep learning models, use tensors\n",
        "                test_tensor = torch.FloatTensor(test_X[i].reshape(1, f, c)).to(device)\n",
        "                with torch.no_grad():\n",
        "                    yhat = model(test_tensor).cpu().numpy()\n",
        "\n",
        "            inv_yhat, inv_y = inverscale(yhat, test_X[i], test_Y[i], scaler)\n",
        "            prev_month_lockdown = test_X[i][11][5]\n",
        "            adjusted_inv_yhat = inv_yhat * (1 - prev_month_lockdown)\n",
        "            predictions.append(inv_yhat)\n",
        "            adjusted_predictions.append(adjusted_inv_yhat)\n",
        "            groundtrue.append(inv_y)\n",
        "\n",
        "            # Observation\n",
        "            obs_x = test_X[i]\n",
        "            obs_y = test_Y[i]\n",
        "\n",
        "            history_x = np.append(history_x, [obs_x], axis=0)\n",
        "            history_y = np.append(history_y, obs_y)\n",
        "\n",
        "        # Store predictions and ground truth for this configuration\n",
        "        all_predictions[config_key] = np.array(predictions).flatten()\n",
        "        all_adjusted_predictions[config_key] = np.array(adjusted_predictions).flatten()\n",
        "        all_ground_truths[config_key] = np.array(groundtrue).flatten()\n",
        "\n",
        "        original_valuelist = evalue(predictions, groundtrue)\n",
        "        original_valuelist['model_type'] = model_type\n",
        "        if model_type in ['rf', 'xgb', 'gbm']:\n",
        "            original_valuelist['n_estimators'] = n_estimators\n",
        "            original_valuelist['units'] = \"N/A\"\n",
        "            original_valuelist['drop_rate'] = \"N/A\"\n",
        "            original_valuelist['dense_unit'] = \"N/A\"\n",
        "            original_valuelist['batch_size'] = \"N/A\"\n",
        "            original_valuelist['epochs'] = \"N/A\"\n",
        "        elif model_type == 'cnn':\n",
        "            original_valuelist['units'] = \"N/A\"\n",
        "            original_valuelist['drop_rate'] = drop\n",
        "            original_valuelist['dense_unit'] = dense\n",
        "            original_valuelist['batch_size'] = batch\n",
        "            original_valuelist['epochs'] = epoch\n",
        "        else:\n",
        "            original_valuelist['units'] = units\n",
        "            original_valuelist['drop_rate'] = drop\n",
        "            original_valuelist['dense_unit'] = dense\n",
        "            original_valuelist['batch_size'] = batch\n",
        "            original_valuelist['epochs'] = epoch\n",
        "\n",
        "        # Evaluate with adjusted predictions\n",
        "        adjusted_valuelist = evalue(np.array(adjusted_predictions).flatten(), np.array(groundtrue).flatten())\n",
        "        adjusted_valuelist['model_type'] = model_type\n",
        "        if model_type in ['rf', 'xgb', 'gbm']:\n",
        "            adjusted_valuelist['n_estimators'] = n_estimators\n",
        "            adjusted_valuelist['units'] = \"N/A\"\n",
        "            adjusted_valuelist['drop_rate'] = \"N/A\"\n",
        "            adjusted_valuelist['dense_unit'] = \"N/A\"\n",
        "            adjusted_valuelist['batch_size'] = \"N/A\"\n",
        "            adjusted_valuelist['epochs'] = \"N/A\"\n",
        "        elif model_type == 'cnn':\n",
        "            adjusted_valuelist['units'] = \"N/A\"\n",
        "            adjusted_valuelist['drop_rate'] = drop\n",
        "            adjusted_valuelist['dense_unit'] = dense\n",
        "            adjusted_valuelist['batch_size'] = batch\n",
        "            adjusted_valuelist['epochs'] = epoch\n",
        "        else:\n",
        "            adjusted_valuelist['units'] = units\n",
        "            adjusted_valuelist['drop_rate'] = drop\n",
        "            adjusted_valuelist['dense_unit'] = dense\n",
        "            adjusted_valuelist['batch_size'] = batch\n",
        "            adjusted_valuelist['epochs'] = epoch\n",
        "\n",
        "        # Append to the lists of results\n",
        "        original_valuelists.append(original_valuelist)\n",
        "        adjusted_valuelists.append(adjusted_valuelist)\n",
        "\n",
        "    # Combine all results\n",
        "    all_original_valuelist = pd.concat(original_valuelists, ignore_index=True)\n",
        "    all_adjusted_valuelist = pd.concat(adjusted_valuelists, ignore_index=True)\n",
        "\n",
        "    return all_original_valuelist, all_adjusted_valuelist, all_predictions, all_ground_truths, all_adjusted_predictions\n",
        "\n",
        "def evalue(yhat, inv_y):\n",
        "    valuelist = {}\n",
        "    DLM_rmse = sqrt(mean_squared_error(inv_y, yhat))\n",
        "    valuelist.update({'RMSE': {'DLM': DLM_rmse}})\n",
        "    DLM_mae = mean_absolute_error(inv_y, yhat)\n",
        "    valuelist.update({'MAE': {'DLM': DLM_mae}})\n",
        "    DLM_mape = mean_absolute_percentage_error(inv_y, yhat)\n",
        "    valuelist.update({'MAPE': {'DLM': DLM_mape}})\n",
        "    return pd.DataFrame(valuelist)\n",
        "\n",
        "def inverscale(yhat, test_X, test_Y, scaler):\n",
        "    feature = len(scaler.scale_)\n",
        "    test_Y = np.array(test_Y)\n",
        "    test_X = test_X[1, 0:feature]\n",
        "    test_X = test_X.reshape(1, test_X.shape[0])\n",
        "\n",
        "    if len(yhat.shape) == 1:\n",
        "        yhat = yhat.reshape(len(yhat), 1)\n",
        "\n",
        "    inv_yhat = concatenate((yhat, test_X[:, :-1]), axis=1)\n",
        "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "    inv_yhat = inv_yhat[:, 0]\n",
        "\n",
        "    test_Y = test_Y.reshape(1, 1)\n",
        "    inv_y = concatenate((test_Y, test_X[:, :-1]), axis=1)\n",
        "    inv_y = scaler.inverse_transform(inv_y)\n",
        "    inv_y = inv_y[:, 0]\n",
        "    return inv_yhat, inv_y\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    values = reframed.values\n",
        "    reframed_with_dates_values = reframed_with_dates.values\n",
        "\n",
        "    # Convert date strings to datetime objects\n",
        "    dates = pd.to_datetime(reframed_with_dates['date'])\n",
        "\n",
        "    # Create masks for each split according to the specified date ranges\n",
        "    train_mask = ((dates >= '2008-01-01') & (dates <= '2017-12-31')) | ((dates >= '2020-01-01') & (dates <= '2020-05-31'))\n",
        "    val_mask = (dates >= '2018-01-01') & (dates <= '2018-12-31')\n",
        "    test_mask = ((dates >= '2019-01-01') & (dates <= '2019-12-31')) | ((dates >= '2020-06-01') & (dates <= '2020-07-31'))\n",
        "\n",
        "    # Extract values for train, validation, and test sets (excluding the date column)\n",
        "    train_data = reframed.loc[train_mask].values\n",
        "    val_data = reframed.loc[val_mask].values\n",
        "    test_data = reframed.loc[test_mask].values\n",
        "\n",
        "    # Split into X and Y\n",
        "    train_X, train_Y = train_data[:, :-1], train_data[:, -1]\n",
        "    val_X, val_Y = val_data[:, :-1], val_data[:, -1]\n",
        "    test_X, test_Y = test_data[:, :-1], test_data[:, -1]\n",
        "\n",
        "    # Reshape input to be 3D [samples, timesteps, features]\n",
        "    train_X = train_X.reshape(train_X.shape[0], 12, int(train_X.shape[1]/12))\n",
        "    val_X = val_X.reshape(val_X.shape[0], 12, int(val_X.shape[1]/12))\n",
        "    test_X = test_X.reshape(test_X.shape[0], 12, int(test_X.shape[1]/12))\n",
        "\n",
        "    # Modified grid search creation for all model types\n",
        "    grid_search = []\n",
        "    for model_type in model_types:\n",
        "        if model_type == 'lstm':\n",
        "            grid_search.extend(\n",
        "                list(itertools.product([model_type], lstm_unit, drop_rate, dense_unit, batch_size_num, epochs))\n",
        "            )\n",
        "        elif model_type == 'gru':\n",
        "            grid_search.extend(\n",
        "                list(itertools.product([model_type], gru_unit, drop_rate, dense_unit, batch_size_num, epochs))\n",
        "            )\n",
        "        elif model_type == 'cnn':\n",
        "            grid_search.extend(\n",
        "                list(itertools.product([model_type], [None], drop_rate, dense_unit, batch_size_num, epochs))\n",
        "            )\n",
        "        elif model_type in ['rf', 'xgb', 'gbm']:\n",
        "            # For tree-based models, we use n_estimators instead of units\n",
        "            grid_search.extend(\n",
        "                list(itertools.product([model_type], n_estimators))\n",
        "            )\n",
        "        else:\n",
        "            # All other hybrid models need both LSTM and GRU units\n",
        "            grid_search.extend(\n",
        "                list(itertools.product([model_type], lstm_unit, gru_unit, drop_rate, dense_unit, batch_size_num, epochs))\n",
        "            )\n",
        "\n",
        "    original_valuelist, adjusted_valuelist, all_predictions, all_ground_truths, all_adjusted_predictions = walk_forward(train_X, train_Y, val_X, val_Y, test_X, test_Y, grid_search, scaler)\n",
        "\n",
        "    # Group results by model type\n",
        "    # gru_results = adjusted_valuelist[adjusted_valuelist['model_type'] == 'gru']\n",
        "    # lstm_results = adjusted_valuelist[adjusted_valuelist['model_type'] == 'lstm']\n",
        "\n",
        "    print(\"Results:\")\n",
        "    print(adjusted_valuelist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOyQurbLxq9-",
        "outputId": "036d5a95-bc5c-48c1-96d2-f48ce08aae1b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 40/100, Train Loss: 0.0292, Val Loss: 0.0453\n",
            "Epoch 41/100, Train Loss: 0.0271, Val Loss: 0.0453\n",
            "Epoch 42/100, Train Loss: 0.0269, Val Loss: 0.0453\n",
            "Epoch 43/100, Train Loss: 0.0282, Val Loss: 0.0453\n",
            "Epoch 44/100, Train Loss: 0.0275, Val Loss: 0.0453\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0255, Val Loss: 0.0453\n",
            "Epoch 46/100, Train Loss: 0.0309, Val Loss: 0.0453\n",
            "Epoch 47/100, Train Loss: 0.0304, Val Loss: 0.0453\n",
            "Epoch 48/100, Train Loss: 0.0282, Val Loss: 0.0453\n",
            "Epoch 49/100, Train Loss: 0.0289, Val Loss: 0.0453\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0286, Val Loss: 0.0453\n",
            "Epoch 51/100, Train Loss: 0.0273, Val Loss: 0.0453\n",
            "Epoch 52/100, Train Loss: 0.0276, Val Loss: 0.0453\n",
            "Epoch 53/100, Train Loss: 0.0264, Val Loss: 0.0453\n",
            "Epoch 54/100, Train Loss: 0.0296, Val Loss: 0.0453\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0249, Val Loss: 0.0453\n",
            "Epoch 56/100, Train Loss: 0.0265, Val Loss: 0.0453\n",
            "Epoch 57/100, Train Loss: 0.0262, Val Loss: 0.0453\n",
            "Epoch 58/100, Train Loss: 0.0275, Val Loss: 0.0453\n",
            "Epoch 59/100, Train Loss: 0.0305, Val Loss: 0.0453\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0281, Val Loss: 0.0453\n",
            "Epoch 61/100, Train Loss: 0.0284, Val Loss: 0.0453\n",
            "Epoch 62/100, Train Loss: 0.0303, Val Loss: 0.0453\n",
            "Epoch 63/100, Train Loss: 0.0283, Val Loss: 0.0453\n",
            "Epoch 64/100, Train Loss: 0.0272, Val Loss: 0.0453\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0299, Val Loss: 0.0453\n",
            "Epoch 66/100, Train Loss: 0.0283, Val Loss: 0.0453\n",
            "Epoch 67/100, Train Loss: 0.0267, Val Loss: 0.0453\n",
            "Epoch 68/100, Train Loss: 0.0287, Val Loss: 0.0453\n",
            "Epoch 69/100, Train Loss: 0.0271, Val Loss: 0.0453\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0293, Val Loss: 0.0453\n",
            "Epoch 71/100, Train Loss: 0.0296, Val Loss: 0.0453\n",
            "Epoch 72/100, Train Loss: 0.0278, Val Loss: 0.0453\n",
            "Epoch 73/100, Train Loss: 0.0274, Val Loss: 0.0453\n",
            "Epoch 74/100, Train Loss: 0.0284, Val Loss: 0.0453\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0273, Val Loss: 0.0453\n",
            "Epoch 76/100, Train Loss: 0.0291, Val Loss: 0.0453\n",
            "Epoch 77/100, Train Loss: 0.0298, Val Loss: 0.0453\n",
            "Epoch 78/100, Train Loss: 0.0262, Val Loss: 0.0453\n",
            "Epoch 79/100, Train Loss: 0.0305, Val Loss: 0.0453\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0287, Val Loss: 0.0453\n",
            "Epoch 81/100, Train Loss: 0.0256, Val Loss: 0.0453\n",
            "Epoch 82/100, Train Loss: 0.0280, Val Loss: 0.0453\n",
            "Epoch 83/100, Train Loss: 0.0288, Val Loss: 0.0453\n",
            "Epoch 84/100, Train Loss: 0.0283, Val Loss: 0.0453\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0299, Val Loss: 0.0453\n",
            "Epoch 86/100, Train Loss: 0.0290, Val Loss: 0.0453\n",
            "Epoch 87/100, Train Loss: 0.0297, Val Loss: 0.0453\n",
            "Epoch 88/100, Train Loss: 0.0295, Val Loss: 0.0453\n",
            "Epoch 89/100, Train Loss: 0.0294, Val Loss: 0.0453\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0283, Val Loss: 0.0453\n",
            "Epoch 91/100, Train Loss: 0.0276, Val Loss: 0.0453\n",
            "Epoch 92/100, Train Loss: 0.0286, Val Loss: 0.0453\n",
            "Epoch 93/100, Train Loss: 0.0315, Val Loss: 0.0453\n",
            "Epoch 94/100, Train Loss: 0.0289, Val Loss: 0.0453\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0295, Val Loss: 0.0453\n",
            "Epoch 96/100, Train Loss: 0.0285, Val Loss: 0.0453\n",
            "Epoch 97/100, Train Loss: 0.0313, Val Loss: 0.0453\n",
            "Epoch 98/100, Train Loss: 0.0297, Val Loss: 0.0453\n",
            "Epoch 99/100, Train Loss: 0.0286, Val Loss: 0.0453\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0288, Val Loss: 0.0453\n",
            "\n",
            "Test iteration 5/14\n",
            "Current training set size: 117 samples\n",
            "Epoch 1/100, Train Loss: 0.1257, Val Loss: 0.2769\n",
            "Epoch 2/100, Train Loss: 0.0765, Val Loss: 0.1929\n",
            "Epoch 3/100, Train Loss: 0.0696, Val Loss: 0.1674\n",
            "Epoch 4/100, Train Loss: 0.0662, Val Loss: 0.1436\n",
            "Epoch 5/100, Train Loss: 0.0676, Val Loss: 0.1086\n",
            "Epoch 6/100, Train Loss: 0.0588, Val Loss: 0.0884\n",
            "Epoch 7/100, Train Loss: 0.0510, Val Loss: 0.0752\n",
            "Epoch 8/100, Train Loss: 0.0520, Val Loss: 0.0638\n",
            "Epoch 9/100, Train Loss: 0.0460, Val Loss: 0.0473\n",
            "Epoch 10/100, Train Loss: 0.0435, Val Loss: 0.0393\n",
            "Epoch 11/100, Train Loss: 0.0450, Val Loss: 0.0927\n",
            "Epoch 12/100, Train Loss: 0.0497, Val Loss: 0.0489\n",
            "Epoch 13/100, Train Loss: 0.0429, Val Loss: 0.0478\n",
            "Epoch 14/100, Train Loss: 0.0405, Val Loss: 0.0760\n",
            "Epoch 15/100, Train Loss: 0.0440, Val Loss: 0.0468\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0364, Val Loss: 0.0468\n",
            "Epoch 17/100, Train Loss: 0.0346, Val Loss: 0.0468\n",
            "Epoch 18/100, Train Loss: 0.0340, Val Loss: 0.0467\n",
            "Epoch 19/100, Train Loss: 0.0334, Val Loss: 0.0467\n",
            "Epoch 20/100, Train Loss: 0.0340, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0351, Val Loss: 0.0466\n",
            "Epoch 22/100, Train Loss: 0.0362, Val Loss: 0.0466\n",
            "Epoch 23/100, Train Loss: 0.0330, Val Loss: 0.0466\n",
            "Epoch 24/100, Train Loss: 0.0355, Val Loss: 0.0466\n",
            "Epoch 25/100, Train Loss: 0.0346, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0349, Val Loss: 0.0466\n",
            "Epoch 27/100, Train Loss: 0.0339, Val Loss: 0.0466\n",
            "Epoch 28/100, Train Loss: 0.0360, Val Loss: 0.0466\n",
            "Epoch 29/100, Train Loss: 0.0381, Val Loss: 0.0466\n",
            "Epoch 30/100, Train Loss: 0.0335, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0348, Val Loss: 0.0466\n",
            "Epoch 32/100, Train Loss: 0.0345, Val Loss: 0.0466\n",
            "Epoch 33/100, Train Loss: 0.0321, Val Loss: 0.0466\n",
            "Epoch 34/100, Train Loss: 0.0328, Val Loss: 0.0466\n",
            "Epoch 35/100, Train Loss: 0.0345, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0368, Val Loss: 0.0466\n",
            "Epoch 37/100, Train Loss: 0.0347, Val Loss: 0.0466\n",
            "Epoch 38/100, Train Loss: 0.0352, Val Loss: 0.0466\n",
            "Epoch 39/100, Train Loss: 0.0358, Val Loss: 0.0466\n",
            "Epoch 40/100, Train Loss: 0.0365, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0343, Val Loss: 0.0466\n",
            "Epoch 42/100, Train Loss: 0.0359, Val Loss: 0.0466\n",
            "Epoch 43/100, Train Loss: 0.0365, Val Loss: 0.0466\n",
            "Epoch 44/100, Train Loss: 0.0364, Val Loss: 0.0466\n",
            "Epoch 45/100, Train Loss: 0.0335, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0310, Val Loss: 0.0466\n",
            "Epoch 47/100, Train Loss: 0.0373, Val Loss: 0.0466\n",
            "Epoch 48/100, Train Loss: 0.0347, Val Loss: 0.0466\n",
            "Epoch 49/100, Train Loss: 0.0344, Val Loss: 0.0466\n",
            "Epoch 50/100, Train Loss: 0.0354, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0462, Val Loss: 0.0466\n",
            "Epoch 52/100, Train Loss: 0.0337, Val Loss: 0.0466\n",
            "Epoch 53/100, Train Loss: 0.0349, Val Loss: 0.0466\n",
            "Epoch 54/100, Train Loss: 0.0338, Val Loss: 0.0466\n",
            "Epoch 55/100, Train Loss: 0.0338, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0334, Val Loss: 0.0466\n",
            "Epoch 57/100, Train Loss: 0.0350, Val Loss: 0.0466\n",
            "Epoch 58/100, Train Loss: 0.0335, Val Loss: 0.0466\n",
            "Epoch 59/100, Train Loss: 0.0353, Val Loss: 0.0466\n",
            "Epoch 60/100, Train Loss: 0.0355, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0339, Val Loss: 0.0466\n",
            "Epoch 62/100, Train Loss: 0.0323, Val Loss: 0.0466\n",
            "Epoch 63/100, Train Loss: 0.0344, Val Loss: 0.0466\n",
            "Epoch 64/100, Train Loss: 0.0353, Val Loss: 0.0466\n",
            "Epoch 65/100, Train Loss: 0.0361, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0364, Val Loss: 0.0466\n",
            "Epoch 67/100, Train Loss: 0.0336, Val Loss: 0.0466\n",
            "Epoch 68/100, Train Loss: 0.0330, Val Loss: 0.0466\n",
            "Epoch 69/100, Train Loss: 0.0370, Val Loss: 0.0466\n",
            "Epoch 70/100, Train Loss: 0.0341, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0362, Val Loss: 0.0466\n",
            "Epoch 72/100, Train Loss: 0.0320, Val Loss: 0.0466\n",
            "Epoch 73/100, Train Loss: 0.0365, Val Loss: 0.0466\n",
            "Epoch 74/100, Train Loss: 0.0340, Val Loss: 0.0466\n",
            "Epoch 75/100, Train Loss: 0.0313, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0342, Val Loss: 0.0466\n",
            "Epoch 77/100, Train Loss: 0.0336, Val Loss: 0.0466\n",
            "Epoch 78/100, Train Loss: 0.0324, Val Loss: 0.0466\n",
            "Epoch 79/100, Train Loss: 0.0342, Val Loss: 0.0466\n",
            "Epoch 80/100, Train Loss: 0.0322, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0345, Val Loss: 0.0466\n",
            "Epoch 82/100, Train Loss: 0.0342, Val Loss: 0.0466\n",
            "Epoch 83/100, Train Loss: 0.0347, Val Loss: 0.0466\n",
            "Epoch 84/100, Train Loss: 0.0368, Val Loss: 0.0466\n",
            "Epoch 85/100, Train Loss: 0.0359, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0335, Val Loss: 0.0466\n",
            "Epoch 87/100, Train Loss: 0.0343, Val Loss: 0.0466\n",
            "Epoch 88/100, Train Loss: 0.0347, Val Loss: 0.0466\n",
            "Epoch 89/100, Train Loss: 0.0350, Val Loss: 0.0466\n",
            "Epoch 90/100, Train Loss: 0.0355, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0356, Val Loss: 0.0466\n",
            "Epoch 92/100, Train Loss: 0.0361, Val Loss: 0.0466\n",
            "Epoch 93/100, Train Loss: 0.0351, Val Loss: 0.0466\n",
            "Epoch 94/100, Train Loss: 0.0331, Val Loss: 0.0466\n",
            "Epoch 95/100, Train Loss: 0.0371, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0350, Val Loss: 0.0466\n",
            "Epoch 97/100, Train Loss: 0.0340, Val Loss: 0.0466\n",
            "Epoch 98/100, Train Loss: 0.0332, Val Loss: 0.0466\n",
            "Epoch 99/100, Train Loss: 0.0318, Val Loss: 0.0466\n",
            "Epoch 100/100, Train Loss: 0.0355, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 6/14\n",
            "Current training set size: 118 samples\n",
            "Epoch 1/100, Train Loss: 0.1385, Val Loss: 0.2770\n",
            "Epoch 2/100, Train Loss: 0.0855, Val Loss: 0.1895\n",
            "Epoch 3/100, Train Loss: 0.0807, Val Loss: 0.1818\n",
            "Epoch 4/100, Train Loss: 0.0676, Val Loss: 0.1622\n",
            "Epoch 5/100, Train Loss: 0.0673, Val Loss: 0.1238\n",
            "Epoch 6/100, Train Loss: 0.0764, Val Loss: 0.1299\n",
            "Epoch 7/100, Train Loss: 0.0528, Val Loss: 0.0496\n",
            "Epoch 8/100, Train Loss: 0.0533, Val Loss: 0.0508\n",
            "Epoch 9/100, Train Loss: 0.0480, Val Loss: 0.0790\n",
            "Epoch 10/100, Train Loss: 0.0486, Val Loss: 0.0648\n",
            "Epoch 11/100, Train Loss: 0.0446, Val Loss: 0.0506\n",
            "Epoch 12/100, Train Loss: 0.0403, Val Loss: 0.0530\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 13/100, Train Loss: 0.0395, Val Loss: 0.0518\n",
            "Epoch 14/100, Train Loss: 0.0331, Val Loss: 0.0523\n",
            "Epoch 15/100, Train Loss: 0.0368, Val Loss: 0.0529\n",
            "Epoch 16/100, Train Loss: 0.0377, Val Loss: 0.0528\n",
            "Epoch 17/100, Train Loss: 0.0348, Val Loss: 0.0520\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0355, Val Loss: 0.0519\n",
            "Epoch 19/100, Train Loss: 0.0369, Val Loss: 0.0519\n",
            "Epoch 20/100, Train Loss: 0.0332, Val Loss: 0.0519\n",
            "Epoch 21/100, Train Loss: 0.0358, Val Loss: 0.0519\n",
            "Epoch 22/100, Train Loss: 0.0369, Val Loss: 0.0519\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0352, Val Loss: 0.0519\n",
            "Epoch 24/100, Train Loss: 0.0347, Val Loss: 0.0519\n",
            "Epoch 25/100, Train Loss: 0.0341, Val Loss: 0.0519\n",
            "Epoch 26/100, Train Loss: 0.0338, Val Loss: 0.0519\n",
            "Epoch 27/100, Train Loss: 0.0360, Val Loss: 0.0519\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0352, Val Loss: 0.0519\n",
            "Epoch 29/100, Train Loss: 0.0344, Val Loss: 0.0519\n",
            "Epoch 30/100, Train Loss: 0.0357, Val Loss: 0.0519\n",
            "Epoch 31/100, Train Loss: 0.0346, Val Loss: 0.0519\n",
            "Epoch 32/100, Train Loss: 0.0337, Val Loss: 0.0519\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0380, Val Loss: 0.0519\n",
            "Epoch 34/100, Train Loss: 0.0360, Val Loss: 0.0519\n",
            "Epoch 35/100, Train Loss: 0.0343, Val Loss: 0.0519\n",
            "Epoch 36/100, Train Loss: 0.0380, Val Loss: 0.0519\n",
            "Epoch 37/100, Train Loss: 0.0354, Val Loss: 0.0519\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0338, Val Loss: 0.0519\n",
            "Epoch 39/100, Train Loss: 0.0334, Val Loss: 0.0519\n",
            "Epoch 40/100, Train Loss: 0.0367, Val Loss: 0.0519\n",
            "Epoch 41/100, Train Loss: 0.0357, Val Loss: 0.0519\n",
            "Epoch 42/100, Train Loss: 0.0339, Val Loss: 0.0519\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0362, Val Loss: 0.0519\n",
            "Epoch 44/100, Train Loss: 0.0350, Val Loss: 0.0519\n",
            "Epoch 45/100, Train Loss: 0.0372, Val Loss: 0.0519\n",
            "Epoch 46/100, Train Loss: 0.0336, Val Loss: 0.0519\n",
            "Epoch 47/100, Train Loss: 0.0361, Val Loss: 0.0519\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0358, Val Loss: 0.0519\n",
            "Epoch 49/100, Train Loss: 0.0364, Val Loss: 0.0519\n",
            "Epoch 50/100, Train Loss: 0.0376, Val Loss: 0.0519\n",
            "Epoch 51/100, Train Loss: 0.0367, Val Loss: 0.0519\n",
            "Epoch 52/100, Train Loss: 0.0328, Val Loss: 0.0519\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0360, Val Loss: 0.0519\n",
            "Epoch 54/100, Train Loss: 0.0361, Val Loss: 0.0519\n",
            "Epoch 55/100, Train Loss: 0.0320, Val Loss: 0.0519\n",
            "Epoch 56/100, Train Loss: 0.0351, Val Loss: 0.0519\n",
            "Epoch 57/100, Train Loss: 0.0344, Val Loss: 0.0519\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0367, Val Loss: 0.0519\n",
            "Epoch 59/100, Train Loss: 0.0346, Val Loss: 0.0519\n",
            "Epoch 60/100, Train Loss: 0.0362, Val Loss: 0.0519\n",
            "Epoch 61/100, Train Loss: 0.0359, Val Loss: 0.0519\n",
            "Epoch 62/100, Train Loss: 0.0354, Val Loss: 0.0519\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0360, Val Loss: 0.0519\n",
            "Epoch 64/100, Train Loss: 0.0350, Val Loss: 0.0519\n",
            "Epoch 65/100, Train Loss: 0.0357, Val Loss: 0.0519\n",
            "Epoch 66/100, Train Loss: 0.0327, Val Loss: 0.0519\n",
            "Epoch 67/100, Train Loss: 0.0363, Val Loss: 0.0519\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0329, Val Loss: 0.0519\n",
            "Epoch 69/100, Train Loss: 0.0372, Val Loss: 0.0519\n",
            "Epoch 70/100, Train Loss: 0.0324, Val Loss: 0.0519\n",
            "Epoch 71/100, Train Loss: 0.0349, Val Loss: 0.0519\n",
            "Epoch 72/100, Train Loss: 0.0362, Val Loss: 0.0519\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0363, Val Loss: 0.0519\n",
            "Epoch 74/100, Train Loss: 0.0341, Val Loss: 0.0519\n",
            "Epoch 75/100, Train Loss: 0.0360, Val Loss: 0.0519\n",
            "Epoch 76/100, Train Loss: 0.0378, Val Loss: 0.0519\n",
            "Epoch 77/100, Train Loss: 0.0323, Val Loss: 0.0519\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0328, Val Loss: 0.0519\n",
            "Epoch 79/100, Train Loss: 0.0332, Val Loss: 0.0519\n",
            "Epoch 80/100, Train Loss: 0.0381, Val Loss: 0.0519\n",
            "Epoch 81/100, Train Loss: 0.0348, Val Loss: 0.0519\n",
            "Epoch 82/100, Train Loss: 0.0352, Val Loss: 0.0519\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0366, Val Loss: 0.0519\n",
            "Epoch 84/100, Train Loss: 0.0349, Val Loss: 0.0519\n",
            "Epoch 85/100, Train Loss: 0.0376, Val Loss: 0.0519\n",
            "Epoch 86/100, Train Loss: 0.0357, Val Loss: 0.0519\n",
            "Epoch 87/100, Train Loss: 0.0361, Val Loss: 0.0519\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0335, Val Loss: 0.0519\n",
            "Epoch 89/100, Train Loss: 0.0327, Val Loss: 0.0519\n",
            "Epoch 90/100, Train Loss: 0.0374, Val Loss: 0.0519\n",
            "Epoch 91/100, Train Loss: 0.0338, Val Loss: 0.0519\n",
            "Epoch 92/100, Train Loss: 0.0337, Val Loss: 0.0519\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0352, Val Loss: 0.0519\n",
            "Epoch 94/100, Train Loss: 0.0365, Val Loss: 0.0519\n",
            "Epoch 95/100, Train Loss: 0.0331, Val Loss: 0.0519\n",
            "Epoch 96/100, Train Loss: 0.0337, Val Loss: 0.0519\n",
            "Epoch 97/100, Train Loss: 0.0354, Val Loss: 0.0519\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0400, Val Loss: 0.0519\n",
            "Epoch 99/100, Train Loss: 0.0339, Val Loss: 0.0519\n",
            "Epoch 100/100, Train Loss: 0.0381, Val Loss: 0.0519\n",
            "\n",
            "Test iteration 7/14\n",
            "Current training set size: 119 samples\n",
            "Epoch 1/100, Train Loss: 0.1503, Val Loss: 0.2710\n",
            "Epoch 2/100, Train Loss: 0.1090, Val Loss: 0.2117\n",
            "Epoch 3/100, Train Loss: 0.0829, Val Loss: 0.1925\n",
            "Epoch 4/100, Train Loss: 0.0712, Val Loss: 0.1725\n",
            "Epoch 5/100, Train Loss: 0.0717, Val Loss: 0.1487\n",
            "Epoch 6/100, Train Loss: 0.0635, Val Loss: 0.0519\n",
            "Epoch 7/100, Train Loss: 0.0643, Val Loss: 0.1056\n",
            "Epoch 8/100, Train Loss: 0.0495, Val Loss: 0.0585\n",
            "Epoch 9/100, Train Loss: 0.0540, Val Loss: 0.0500\n",
            "Epoch 10/100, Train Loss: 0.0522, Val Loss: 0.0374\n",
            "Epoch 11/100, Train Loss: 0.0471, Val Loss: 0.0356\n",
            "Epoch 12/100, Train Loss: 0.0496, Val Loss: 0.1372\n",
            "Epoch 13/100, Train Loss: 0.0553, Val Loss: 0.0420\n",
            "Epoch 14/100, Train Loss: 0.0468, Val Loss: 0.0514\n",
            "Epoch 15/100, Train Loss: 0.0398, Val Loss: 0.0563\n",
            "Epoch 16/100, Train Loss: 0.0446, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0543, Val Loss: 0.0374\n",
            "Epoch 18/100, Train Loss: 0.0491, Val Loss: 0.0375\n",
            "Epoch 19/100, Train Loss: 0.0407, Val Loss: 0.0376\n",
            "Epoch 20/100, Train Loss: 0.0430, Val Loss: 0.0377\n",
            "Epoch 21/100, Train Loss: 0.0398, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0407, Val Loss: 0.0379\n",
            "Epoch 23/100, Train Loss: 0.0382, Val Loss: 0.0379\n",
            "Epoch 24/100, Train Loss: 0.0395, Val Loss: 0.0379\n",
            "Epoch 25/100, Train Loss: 0.0379, Val Loss: 0.0379\n",
            "Epoch 26/100, Train Loss: 0.0406, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0389, Val Loss: 0.0379\n",
            "Epoch 28/100, Train Loss: 0.0376, Val Loss: 0.0379\n",
            "Epoch 29/100, Train Loss: 0.0375, Val Loss: 0.0379\n",
            "Epoch 30/100, Train Loss: 0.0386, Val Loss: 0.0379\n",
            "Epoch 31/100, Train Loss: 0.0402, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0385, Val Loss: 0.0379\n",
            "Epoch 33/100, Train Loss: 0.0380, Val Loss: 0.0379\n",
            "Epoch 34/100, Train Loss: 0.0364, Val Loss: 0.0379\n",
            "Epoch 35/100, Train Loss: 0.0400, Val Loss: 0.0379\n",
            "Epoch 36/100, Train Loss: 0.0365, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0390, Val Loss: 0.0379\n",
            "Epoch 38/100, Train Loss: 0.0401, Val Loss: 0.0379\n",
            "Epoch 39/100, Train Loss: 0.0411, Val Loss: 0.0379\n",
            "Epoch 40/100, Train Loss: 0.0380, Val Loss: 0.0379\n",
            "Epoch 41/100, Train Loss: 0.0396, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0399, Val Loss: 0.0379\n",
            "Epoch 43/100, Train Loss: 0.0416, Val Loss: 0.0379\n",
            "Epoch 44/100, Train Loss: 0.0399, Val Loss: 0.0379\n",
            "Epoch 45/100, Train Loss: 0.0370, Val Loss: 0.0379\n",
            "Epoch 46/100, Train Loss: 0.0419, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0380, Val Loss: 0.0379\n",
            "Epoch 48/100, Train Loss: 0.0415, Val Loss: 0.0379\n",
            "Epoch 49/100, Train Loss: 0.0373, Val Loss: 0.0379\n",
            "Epoch 50/100, Train Loss: 0.0411, Val Loss: 0.0379\n",
            "Epoch 51/100, Train Loss: 0.0371, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0403, Val Loss: 0.0379\n",
            "Epoch 53/100, Train Loss: 0.0400, Val Loss: 0.0379\n",
            "Epoch 54/100, Train Loss: 0.0406, Val Loss: 0.0379\n",
            "Epoch 55/100, Train Loss: 0.0403, Val Loss: 0.0379\n",
            "Epoch 56/100, Train Loss: 0.0372, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0412, Val Loss: 0.0379\n",
            "Epoch 58/100, Train Loss: 0.0394, Val Loss: 0.0379\n",
            "Epoch 59/100, Train Loss: 0.0411, Val Loss: 0.0379\n",
            "Epoch 60/100, Train Loss: 0.0428, Val Loss: 0.0379\n",
            "Epoch 61/100, Train Loss: 0.0385, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0362, Val Loss: 0.0379\n",
            "Epoch 63/100, Train Loss: 0.0377, Val Loss: 0.0379\n",
            "Epoch 64/100, Train Loss: 0.0399, Val Loss: 0.0379\n",
            "Epoch 65/100, Train Loss: 0.0392, Val Loss: 0.0379\n",
            "Epoch 66/100, Train Loss: 0.0400, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0395, Val Loss: 0.0379\n",
            "Epoch 68/100, Train Loss: 0.0404, Val Loss: 0.0379\n",
            "Epoch 69/100, Train Loss: 0.0377, Val Loss: 0.0379\n",
            "Epoch 70/100, Train Loss: 0.0378, Val Loss: 0.0379\n",
            "Epoch 71/100, Train Loss: 0.0408, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0383, Val Loss: 0.0379\n",
            "Epoch 73/100, Train Loss: 0.0353, Val Loss: 0.0379\n",
            "Epoch 74/100, Train Loss: 0.0388, Val Loss: 0.0379\n",
            "Epoch 75/100, Train Loss: 0.0394, Val Loss: 0.0379\n",
            "Epoch 76/100, Train Loss: 0.0392, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0434, Val Loss: 0.0379\n",
            "Epoch 78/100, Train Loss: 0.0390, Val Loss: 0.0379\n",
            "Epoch 79/100, Train Loss: 0.0385, Val Loss: 0.0379\n",
            "Epoch 80/100, Train Loss: 0.0388, Val Loss: 0.0379\n",
            "Epoch 81/100, Train Loss: 0.0374, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0418, Val Loss: 0.0379\n",
            "Epoch 83/100, Train Loss: 0.0388, Val Loss: 0.0379\n",
            "Epoch 84/100, Train Loss: 0.0396, Val Loss: 0.0379\n",
            "Epoch 85/100, Train Loss: 0.0392, Val Loss: 0.0379\n",
            "Epoch 86/100, Train Loss: 0.0383, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0391, Val Loss: 0.0379\n",
            "Epoch 88/100, Train Loss: 0.0391, Val Loss: 0.0379\n",
            "Epoch 89/100, Train Loss: 0.0393, Val Loss: 0.0379\n",
            "Epoch 90/100, Train Loss: 0.0417, Val Loss: 0.0379\n",
            "Epoch 91/100, Train Loss: 0.0370, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0401, Val Loss: 0.0379\n",
            "Epoch 93/100, Train Loss: 0.0384, Val Loss: 0.0379\n",
            "Epoch 94/100, Train Loss: 0.0374, Val Loss: 0.0379\n",
            "Epoch 95/100, Train Loss: 0.0423, Val Loss: 0.0379\n",
            "Epoch 96/100, Train Loss: 0.0376, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0399, Val Loss: 0.0379\n",
            "Epoch 98/100, Train Loss: 0.0390, Val Loss: 0.0379\n",
            "Epoch 99/100, Train Loss: 0.0401, Val Loss: 0.0379\n",
            "Epoch 100/100, Train Loss: 0.0403, Val Loss: 0.0379\n",
            "\n",
            "Test iteration 8/14\n",
            "Current training set size: 120 samples\n",
            "Epoch 1/100, Train Loss: 0.1493, Val Loss: 0.2264\n",
            "Epoch 2/100, Train Loss: 0.0905, Val Loss: 0.1847\n",
            "Epoch 3/100, Train Loss: 0.0726, Val Loss: 0.1629\n",
            "Epoch 4/100, Train Loss: 0.0656, Val Loss: 0.1168\n",
            "Epoch 5/100, Train Loss: 0.0556, Val Loss: 0.0901\n",
            "Epoch 6/100, Train Loss: 0.0506, Val Loss: 0.0632\n",
            "Epoch 7/100, Train Loss: 0.0560, Val Loss: 0.0610\n",
            "Epoch 8/100, Train Loss: 0.0527, Val Loss: 0.0474\n",
            "Epoch 9/100, Train Loss: 0.0466, Val Loss: 0.0534\n",
            "Epoch 10/100, Train Loss: 0.0433, Val Loss: 0.0697\n",
            "Epoch 11/100, Train Loss: 0.0424, Val Loss: 0.0438\n",
            "Epoch 12/100, Train Loss: 0.0475, Val Loss: 0.0543\n",
            "Epoch 13/100, Train Loss: 0.0419, Val Loss: 0.0418\n",
            "Epoch 14/100, Train Loss: 0.0484, Val Loss: 0.0396\n",
            "Epoch 15/100, Train Loss: 0.0393, Val Loss: 0.0366\n",
            "Epoch 16/100, Train Loss: 0.0355, Val Loss: 0.0383\n",
            "Epoch 17/100, Train Loss: 0.0393, Val Loss: 0.0544\n",
            "Epoch 18/100, Train Loss: 0.0453, Val Loss: 0.0388\n",
            "Epoch 19/100, Train Loss: 0.0407, Val Loss: 0.0380\n",
            "Epoch 20/100, Train Loss: 0.0386, Val Loss: 0.0378\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0335, Val Loss: 0.0379\n",
            "Epoch 22/100, Train Loss: 0.0351, Val Loss: 0.0378\n",
            "Epoch 23/100, Train Loss: 0.0339, Val Loss: 0.0377\n",
            "Epoch 24/100, Train Loss: 0.0346, Val Loss: 0.0376\n",
            "Epoch 25/100, Train Loss: 0.0321, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0326, Val Loss: 0.0374\n",
            "Epoch 27/100, Train Loss: 0.0348, Val Loss: 0.0374\n",
            "Epoch 28/100, Train Loss: 0.0336, Val Loss: 0.0374\n",
            "Epoch 29/100, Train Loss: 0.0310, Val Loss: 0.0374\n",
            "Epoch 30/100, Train Loss: 0.0327, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0309, Val Loss: 0.0374\n",
            "Epoch 32/100, Train Loss: 0.0344, Val Loss: 0.0374\n",
            "Epoch 33/100, Train Loss: 0.0340, Val Loss: 0.0374\n",
            "Epoch 34/100, Train Loss: 0.0328, Val Loss: 0.0374\n",
            "Epoch 35/100, Train Loss: 0.0307, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0323, Val Loss: 0.0374\n",
            "Epoch 37/100, Train Loss: 0.0329, Val Loss: 0.0374\n",
            "Epoch 38/100, Train Loss: 0.0343, Val Loss: 0.0374\n",
            "Epoch 39/100, Train Loss: 0.0326, Val Loss: 0.0374\n",
            "Epoch 40/100, Train Loss: 0.0352, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0323, Val Loss: 0.0374\n",
            "Epoch 42/100, Train Loss: 0.0332, Val Loss: 0.0374\n",
            "Epoch 43/100, Train Loss: 0.0324, Val Loss: 0.0374\n",
            "Epoch 44/100, Train Loss: 0.0331, Val Loss: 0.0374\n",
            "Epoch 45/100, Train Loss: 0.0345, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0323, Val Loss: 0.0374\n",
            "Epoch 47/100, Train Loss: 0.0331, Val Loss: 0.0374\n",
            "Epoch 48/100, Train Loss: 0.0325, Val Loss: 0.0374\n",
            "Epoch 49/100, Train Loss: 0.0317, Val Loss: 0.0374\n",
            "Epoch 50/100, Train Loss: 0.0321, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0289, Val Loss: 0.0374\n",
            "Epoch 52/100, Train Loss: 0.0357, Val Loss: 0.0374\n",
            "Epoch 53/100, Train Loss: 0.0330, Val Loss: 0.0374\n",
            "Epoch 54/100, Train Loss: 0.0354, Val Loss: 0.0374\n",
            "Epoch 55/100, Train Loss: 0.0356, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0322, Val Loss: 0.0374\n",
            "Epoch 57/100, Train Loss: 0.0318, Val Loss: 0.0374\n",
            "Epoch 58/100, Train Loss: 0.0318, Val Loss: 0.0374\n",
            "Epoch 59/100, Train Loss: 0.0308, Val Loss: 0.0374\n",
            "Epoch 60/100, Train Loss: 0.0302, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0306, Val Loss: 0.0374\n",
            "Epoch 62/100, Train Loss: 0.0317, Val Loss: 0.0374\n",
            "Epoch 63/100, Train Loss: 0.0327, Val Loss: 0.0374\n",
            "Epoch 64/100, Train Loss: 0.0326, Val Loss: 0.0374\n",
            "Epoch 65/100, Train Loss: 0.0311, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0319, Val Loss: 0.0374\n",
            "Epoch 67/100, Train Loss: 0.0339, Val Loss: 0.0374\n",
            "Epoch 68/100, Train Loss: 0.0335, Val Loss: 0.0374\n",
            "Epoch 69/100, Train Loss: 0.0341, Val Loss: 0.0374\n",
            "Epoch 70/100, Train Loss: 0.0308, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0320, Val Loss: 0.0374\n",
            "Epoch 72/100, Train Loss: 0.0331, Val Loss: 0.0374\n",
            "Epoch 73/100, Train Loss: 0.0302, Val Loss: 0.0374\n",
            "Epoch 74/100, Train Loss: 0.0326, Val Loss: 0.0374\n",
            "Epoch 75/100, Train Loss: 0.0348, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0323, Val Loss: 0.0374\n",
            "Epoch 77/100, Train Loss: 0.0309, Val Loss: 0.0374\n",
            "Epoch 78/100, Train Loss: 0.0308, Val Loss: 0.0374\n",
            "Epoch 79/100, Train Loss: 0.0325, Val Loss: 0.0374\n",
            "Epoch 80/100, Train Loss: 0.0326, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0315, Val Loss: 0.0374\n",
            "Epoch 82/100, Train Loss: 0.0353, Val Loss: 0.0374\n",
            "Epoch 83/100, Train Loss: 0.0328, Val Loss: 0.0374\n",
            "Epoch 84/100, Train Loss: 0.0326, Val Loss: 0.0374\n",
            "Epoch 85/100, Train Loss: 0.0359, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0324, Val Loss: 0.0374\n",
            "Epoch 87/100, Train Loss: 0.0336, Val Loss: 0.0374\n",
            "Epoch 88/100, Train Loss: 0.0333, Val Loss: 0.0374\n",
            "Epoch 89/100, Train Loss: 0.0344, Val Loss: 0.0374\n",
            "Epoch 90/100, Train Loss: 0.0325, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0327, Val Loss: 0.0374\n",
            "Epoch 92/100, Train Loss: 0.0329, Val Loss: 0.0374\n",
            "Epoch 93/100, Train Loss: 0.0326, Val Loss: 0.0374\n",
            "Epoch 94/100, Train Loss: 0.0322, Val Loss: 0.0374\n",
            "Epoch 95/100, Train Loss: 0.0319, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0319, Val Loss: 0.0374\n",
            "Epoch 97/100, Train Loss: 0.0289, Val Loss: 0.0374\n",
            "Epoch 98/100, Train Loss: 0.0332, Val Loss: 0.0374\n",
            "Epoch 99/100, Train Loss: 0.0335, Val Loss: 0.0374\n",
            "Epoch 100/100, Train Loss: 0.0325, Val Loss: 0.0374\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 9/14\n",
            "Current training set size: 121 samples\n",
            "Epoch 1/100, Train Loss: 0.1500, Val Loss: 0.2490\n",
            "Epoch 2/100, Train Loss: 0.0826, Val Loss: 0.1814\n",
            "Epoch 3/100, Train Loss: 0.0797, Val Loss: 0.1601\n",
            "Epoch 4/100, Train Loss: 0.0722, Val Loss: 0.1438\n",
            "Epoch 5/100, Train Loss: 0.0712, Val Loss: 0.0781\n",
            "Epoch 6/100, Train Loss: 0.0565, Val Loss: 0.0537\n",
            "Epoch 7/100, Train Loss: 0.0539, Val Loss: 0.0471\n",
            "Epoch 8/100, Train Loss: 0.0535, Val Loss: 0.0937\n",
            "Epoch 9/100, Train Loss: 0.0491, Val Loss: 0.0542\n",
            "Epoch 10/100, Train Loss: 0.0504, Val Loss: 0.0483\n",
            "Epoch 11/100, Train Loss: 0.0444, Val Loss: 0.0488\n",
            "Epoch 12/100, Train Loss: 0.0477, Val Loss: 0.0494\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 13/100, Train Loss: 0.0448, Val Loss: 0.0486\n",
            "Epoch 14/100, Train Loss: 0.0445, Val Loss: 0.0482\n",
            "Epoch 15/100, Train Loss: 0.0435, Val Loss: 0.0479\n",
            "Epoch 16/100, Train Loss: 0.0430, Val Loss: 0.0479\n",
            "Epoch 17/100, Train Loss: 0.0403, Val Loss: 0.0478\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0412, Val Loss: 0.0478\n",
            "Epoch 19/100, Train Loss: 0.0419, Val Loss: 0.0478\n",
            "Epoch 20/100, Train Loss: 0.0406, Val Loss: 0.0478\n",
            "Epoch 21/100, Train Loss: 0.0404, Val Loss: 0.0478\n",
            "Epoch 22/100, Train Loss: 0.0403, Val Loss: 0.0478\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0414, Val Loss: 0.0478\n",
            "Epoch 24/100, Train Loss: 0.0410, Val Loss: 0.0478\n",
            "Epoch 25/100, Train Loss: 0.0403, Val Loss: 0.0478\n",
            "Epoch 26/100, Train Loss: 0.0412, Val Loss: 0.0478\n",
            "Epoch 27/100, Train Loss: 0.0385, Val Loss: 0.0478\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0379, Val Loss: 0.0478\n",
            "Epoch 29/100, Train Loss: 0.0420, Val Loss: 0.0478\n",
            "Epoch 30/100, Train Loss: 0.0424, Val Loss: 0.0478\n",
            "Epoch 31/100, Train Loss: 0.0405, Val Loss: 0.0478\n",
            "Epoch 32/100, Train Loss: 0.0390, Val Loss: 0.0478\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0377, Val Loss: 0.0478\n",
            "Epoch 34/100, Train Loss: 0.0393, Val Loss: 0.0478\n",
            "Epoch 35/100, Train Loss: 0.0398, Val Loss: 0.0478\n",
            "Epoch 36/100, Train Loss: 0.0399, Val Loss: 0.0478\n",
            "Epoch 37/100, Train Loss: 0.0390, Val Loss: 0.0478\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0396, Val Loss: 0.0478\n",
            "Epoch 39/100, Train Loss: 0.0383, Val Loss: 0.0478\n",
            "Epoch 40/100, Train Loss: 0.0408, Val Loss: 0.0478\n",
            "Epoch 41/100, Train Loss: 0.0378, Val Loss: 0.0478\n",
            "Epoch 42/100, Train Loss: 0.0420, Val Loss: 0.0478\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0401, Val Loss: 0.0478\n",
            "Epoch 44/100, Train Loss: 0.0387, Val Loss: 0.0478\n",
            "Epoch 45/100, Train Loss: 0.0392, Val Loss: 0.0478\n",
            "Epoch 46/100, Train Loss: 0.0391, Val Loss: 0.0478\n",
            "Epoch 47/100, Train Loss: 0.0407, Val Loss: 0.0478\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0391, Val Loss: 0.0478\n",
            "Epoch 49/100, Train Loss: 0.0396, Val Loss: 0.0478\n",
            "Epoch 50/100, Train Loss: 0.0416, Val Loss: 0.0478\n",
            "Epoch 51/100, Train Loss: 0.0400, Val Loss: 0.0478\n",
            "Epoch 52/100, Train Loss: 0.0412, Val Loss: 0.0478\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0391, Val Loss: 0.0478\n",
            "Epoch 54/100, Train Loss: 0.0402, Val Loss: 0.0478\n",
            "Epoch 55/100, Train Loss: 0.0396, Val Loss: 0.0478\n",
            "Epoch 56/100, Train Loss: 0.0396, Val Loss: 0.0478\n",
            "Epoch 57/100, Train Loss: 0.0423, Val Loss: 0.0478\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0397, Val Loss: 0.0478\n",
            "Epoch 59/100, Train Loss: 0.0386, Val Loss: 0.0478\n",
            "Epoch 60/100, Train Loss: 0.0400, Val Loss: 0.0478\n",
            "Epoch 61/100, Train Loss: 0.0391, Val Loss: 0.0478\n",
            "Epoch 62/100, Train Loss: 0.0395, Val Loss: 0.0478\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0413, Val Loss: 0.0478\n",
            "Epoch 64/100, Train Loss: 0.0407, Val Loss: 0.0478\n",
            "Epoch 65/100, Train Loss: 0.0404, Val Loss: 0.0478\n",
            "Epoch 66/100, Train Loss: 0.0391, Val Loss: 0.0478\n",
            "Epoch 67/100, Train Loss: 0.0391, Val Loss: 0.0478\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0391, Val Loss: 0.0478\n",
            "Epoch 69/100, Train Loss: 0.0400, Val Loss: 0.0478\n",
            "Epoch 70/100, Train Loss: 0.0396, Val Loss: 0.0478\n",
            "Epoch 71/100, Train Loss: 0.0383, Val Loss: 0.0478\n",
            "Epoch 72/100, Train Loss: 0.0392, Val Loss: 0.0478\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0408, Val Loss: 0.0478\n",
            "Epoch 74/100, Train Loss: 0.0392, Val Loss: 0.0478\n",
            "Epoch 75/100, Train Loss: 0.0412, Val Loss: 0.0478\n",
            "Epoch 76/100, Train Loss: 0.0413, Val Loss: 0.0478\n",
            "Epoch 77/100, Train Loss: 0.0400, Val Loss: 0.0478\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0436, Val Loss: 0.0478\n",
            "Epoch 79/100, Train Loss: 0.0418, Val Loss: 0.0478\n",
            "Epoch 80/100, Train Loss: 0.0415, Val Loss: 0.0478\n",
            "Epoch 81/100, Train Loss: 0.0397, Val Loss: 0.0478\n",
            "Epoch 82/100, Train Loss: 0.0429, Val Loss: 0.0478\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0407, Val Loss: 0.0478\n",
            "Epoch 84/100, Train Loss: 0.0415, Val Loss: 0.0478\n",
            "Epoch 85/100, Train Loss: 0.0402, Val Loss: 0.0478\n",
            "Epoch 86/100, Train Loss: 0.0371, Val Loss: 0.0478\n",
            "Epoch 87/100, Train Loss: 0.0409, Val Loss: 0.0478\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0399, Val Loss: 0.0478\n",
            "Epoch 89/100, Train Loss: 0.0389, Val Loss: 0.0478\n",
            "Epoch 90/100, Train Loss: 0.0429, Val Loss: 0.0478\n",
            "Epoch 91/100, Train Loss: 0.0387, Val Loss: 0.0478\n",
            "Epoch 92/100, Train Loss: 0.0412, Val Loss: 0.0478\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0397, Val Loss: 0.0478\n",
            "Epoch 94/100, Train Loss: 0.0401, Val Loss: 0.0478\n",
            "Epoch 95/100, Train Loss: 0.0387, Val Loss: 0.0478\n",
            "Epoch 96/100, Train Loss: 0.0404, Val Loss: 0.0478\n",
            "Epoch 97/100, Train Loss: 0.0415, Val Loss: 0.0478\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0411, Val Loss: 0.0478\n",
            "Epoch 99/100, Train Loss: 0.0376, Val Loss: 0.0478\n",
            "Epoch 100/100, Train Loss: 0.0401, Val Loss: 0.0478\n",
            "\n",
            "Test iteration 10/14\n",
            "Current training set size: 122 samples\n",
            "Epoch 1/100, Train Loss: 0.1507, Val Loss: 0.2545\n",
            "Epoch 2/100, Train Loss: 0.0872, Val Loss: 0.1837\n",
            "Epoch 3/100, Train Loss: 0.0721, Val Loss: 0.1574\n",
            "Epoch 4/100, Train Loss: 0.0653, Val Loss: 0.1068\n",
            "Epoch 5/100, Train Loss: 0.0641, Val Loss: 0.0462\n",
            "Epoch 6/100, Train Loss: 0.0629, Val Loss: 0.2178\n",
            "Epoch 7/100, Train Loss: 0.0503, Val Loss: 0.0515\n",
            "Epoch 8/100, Train Loss: 0.0479, Val Loss: 0.0464\n",
            "Epoch 9/100, Train Loss: 0.0459, Val Loss: 0.0455\n",
            "Epoch 10/100, Train Loss: 0.0507, Val Loss: 0.0791\n",
            "Epoch 11/100, Train Loss: 0.0497, Val Loss: 0.0452\n",
            "Epoch 12/100, Train Loss: 0.0441, Val Loss: 0.0431\n",
            "Epoch 13/100, Train Loss: 0.0456, Val Loss: 0.0409\n",
            "Epoch 14/100, Train Loss: 0.0485, Val Loss: 0.0484\n",
            "Epoch 15/100, Train Loss: 0.0481, Val Loss: 0.0496\n",
            "Epoch 16/100, Train Loss: 0.0433, Val Loss: 0.0491\n",
            "Epoch 17/100, Train Loss: 0.0415, Val Loss: 0.0485\n",
            "Epoch 18/100, Train Loss: 0.0428, Val Loss: 0.0550\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0473, Val Loss: 0.0509\n",
            "Epoch 20/100, Train Loss: 0.0421, Val Loss: 0.0484\n",
            "Epoch 21/100, Train Loss: 0.0407, Val Loss: 0.0468\n",
            "Epoch 22/100, Train Loss: 0.0405, Val Loss: 0.0447\n",
            "Epoch 23/100, Train Loss: 0.0350, Val Loss: 0.0427\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0369, Val Loss: 0.0427\n",
            "Epoch 25/100, Train Loss: 0.0351, Val Loss: 0.0427\n",
            "Epoch 26/100, Train Loss: 0.0332, Val Loss: 0.0427\n",
            "Epoch 27/100, Train Loss: 0.0344, Val Loss: 0.0427\n",
            "Epoch 28/100, Train Loss: 0.0372, Val Loss: 0.0427\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0374, Val Loss: 0.0427\n",
            "Epoch 30/100, Train Loss: 0.0360, Val Loss: 0.0427\n",
            "Epoch 31/100, Train Loss: 0.0353, Val Loss: 0.0427\n",
            "Epoch 32/100, Train Loss: 0.0377, Val Loss: 0.0427\n",
            "Epoch 33/100, Train Loss: 0.0363, Val Loss: 0.0427\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0363, Val Loss: 0.0427\n",
            "Epoch 35/100, Train Loss: 0.0352, Val Loss: 0.0427\n",
            "Epoch 36/100, Train Loss: 0.0383, Val Loss: 0.0427\n",
            "Epoch 37/100, Train Loss: 0.0353, Val Loss: 0.0427\n",
            "Epoch 38/100, Train Loss: 0.0358, Val Loss: 0.0427\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0350, Val Loss: 0.0427\n",
            "Epoch 40/100, Train Loss: 0.0348, Val Loss: 0.0427\n",
            "Epoch 41/100, Train Loss: 0.0355, Val Loss: 0.0427\n",
            "Epoch 42/100, Train Loss: 0.0402, Val Loss: 0.0427\n",
            "Epoch 43/100, Train Loss: 0.0372, Val Loss: 0.0427\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0362, Val Loss: 0.0427\n",
            "Epoch 45/100, Train Loss: 0.0349, Val Loss: 0.0427\n",
            "Epoch 46/100, Train Loss: 0.0341, Val Loss: 0.0427\n",
            "Epoch 47/100, Train Loss: 0.0340, Val Loss: 0.0427\n",
            "Epoch 48/100, Train Loss: 0.0361, Val Loss: 0.0427\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0334, Val Loss: 0.0427\n",
            "Epoch 50/100, Train Loss: 0.0341, Val Loss: 0.0427\n",
            "Epoch 51/100, Train Loss: 0.0392, Val Loss: 0.0427\n",
            "Epoch 52/100, Train Loss: 0.0358, Val Loss: 0.0427\n",
            "Epoch 53/100, Train Loss: 0.0360, Val Loss: 0.0427\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0358, Val Loss: 0.0427\n",
            "Epoch 55/100, Train Loss: 0.0364, Val Loss: 0.0427\n",
            "Epoch 56/100, Train Loss: 0.0371, Val Loss: 0.0427\n",
            "Epoch 57/100, Train Loss: 0.0331, Val Loss: 0.0427\n",
            "Epoch 58/100, Train Loss: 0.0356, Val Loss: 0.0427\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0350, Val Loss: 0.0427\n",
            "Epoch 60/100, Train Loss: 0.0369, Val Loss: 0.0427\n",
            "Epoch 61/100, Train Loss: 0.0346, Val Loss: 0.0427\n",
            "Epoch 62/100, Train Loss: 0.0362, Val Loss: 0.0427\n",
            "Epoch 63/100, Train Loss: 0.0381, Val Loss: 0.0427\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0364, Val Loss: 0.0427\n",
            "Epoch 65/100, Train Loss: 0.0361, Val Loss: 0.0427\n",
            "Epoch 66/100, Train Loss: 0.0347, Val Loss: 0.0427\n",
            "Epoch 67/100, Train Loss: 0.0336, Val Loss: 0.0427\n",
            "Epoch 68/100, Train Loss: 0.0365, Val Loss: 0.0427\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0352, Val Loss: 0.0427\n",
            "Epoch 70/100, Train Loss: 0.0388, Val Loss: 0.0427\n",
            "Epoch 71/100, Train Loss: 0.0371, Val Loss: 0.0427\n",
            "Epoch 72/100, Train Loss: 0.0353, Val Loss: 0.0427\n",
            "Epoch 73/100, Train Loss: 0.0370, Val Loss: 0.0427\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0348, Val Loss: 0.0427\n",
            "Epoch 75/100, Train Loss: 0.0390, Val Loss: 0.0427\n",
            "Epoch 76/100, Train Loss: 0.0360, Val Loss: 0.0427\n",
            "Epoch 77/100, Train Loss: 0.0328, Val Loss: 0.0427\n",
            "Epoch 78/100, Train Loss: 0.0370, Val Loss: 0.0427\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0329, Val Loss: 0.0427\n",
            "Epoch 80/100, Train Loss: 0.0331, Val Loss: 0.0427\n",
            "Epoch 81/100, Train Loss: 0.0338, Val Loss: 0.0427\n",
            "Epoch 82/100, Train Loss: 0.0372, Val Loss: 0.0427\n",
            "Epoch 83/100, Train Loss: 0.0362, Val Loss: 0.0427\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0344, Val Loss: 0.0427\n",
            "Epoch 85/100, Train Loss: 0.0326, Val Loss: 0.0427\n",
            "Epoch 86/100, Train Loss: 0.0367, Val Loss: 0.0427\n",
            "Epoch 87/100, Train Loss: 0.0342, Val Loss: 0.0427\n",
            "Epoch 88/100, Train Loss: 0.0356, Val Loss: 0.0427\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0387, Val Loss: 0.0427\n",
            "Epoch 90/100, Train Loss: 0.0326, Val Loss: 0.0427\n",
            "Epoch 91/100, Train Loss: 0.0373, Val Loss: 0.0427\n",
            "Epoch 92/100, Train Loss: 0.0372, Val Loss: 0.0427\n",
            "Epoch 93/100, Train Loss: 0.0347, Val Loss: 0.0427\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0368, Val Loss: 0.0427\n",
            "Epoch 95/100, Train Loss: 0.0370, Val Loss: 0.0427\n",
            "Epoch 96/100, Train Loss: 0.0344, Val Loss: 0.0427\n",
            "Epoch 97/100, Train Loss: 0.0364, Val Loss: 0.0427\n",
            "Epoch 98/100, Train Loss: 0.0346, Val Loss: 0.0427\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0319, Val Loss: 0.0427\n",
            "Epoch 100/100, Train Loss: 0.0354, Val Loss: 0.0427\n",
            "\n",
            "Test iteration 11/14\n",
            "Current training set size: 123 samples\n",
            "Epoch 1/100, Train Loss: 0.1472, Val Loss: 0.2152\n",
            "Epoch 2/100, Train Loss: 0.0869, Val Loss: 0.1688\n",
            "Epoch 3/100, Train Loss: 0.0759, Val Loss: 0.1042\n",
            "Epoch 4/100, Train Loss: 0.0710, Val Loss: 0.0522\n",
            "Epoch 5/100, Train Loss: 0.0608, Val Loss: 0.0501\n",
            "Epoch 6/100, Train Loss: 0.0537, Val Loss: 0.0470\n",
            "Epoch 7/100, Train Loss: 0.0526, Val Loss: 0.0543\n",
            "Epoch 8/100, Train Loss: 0.0568, Val Loss: 0.0734\n",
            "Epoch 9/100, Train Loss: 0.0537, Val Loss: 0.0630\n",
            "Epoch 10/100, Train Loss: 0.0527, Val Loss: 0.0513\n",
            "Epoch 11/100, Train Loss: 0.0531, Val Loss: 0.0526\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 12/100, Train Loss: 0.0443, Val Loss: 0.0523\n",
            "Epoch 13/100, Train Loss: 0.0397, Val Loss: 0.0516\n",
            "Epoch 14/100, Train Loss: 0.0415, Val Loss: 0.0512\n",
            "Epoch 15/100, Train Loss: 0.0382, Val Loss: 0.0506\n",
            "Epoch 16/100, Train Loss: 0.0378, Val Loss: 0.0505\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0401, Val Loss: 0.0505\n",
            "Epoch 18/100, Train Loss: 0.0395, Val Loss: 0.0505\n",
            "Epoch 19/100, Train Loss: 0.0411, Val Loss: 0.0505\n",
            "Epoch 20/100, Train Loss: 0.0400, Val Loss: 0.0505\n",
            "Epoch 21/100, Train Loss: 0.0414, Val Loss: 0.0505\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0423, Val Loss: 0.0505\n",
            "Epoch 23/100, Train Loss: 0.0388, Val Loss: 0.0505\n",
            "Epoch 24/100, Train Loss: 0.0389, Val Loss: 0.0505\n",
            "Epoch 25/100, Train Loss: 0.0406, Val Loss: 0.0505\n",
            "Epoch 26/100, Train Loss: 0.0380, Val Loss: 0.0505\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0386, Val Loss: 0.0505\n",
            "Epoch 28/100, Train Loss: 0.0398, Val Loss: 0.0505\n",
            "Epoch 29/100, Train Loss: 0.0374, Val Loss: 0.0505\n",
            "Epoch 30/100, Train Loss: 0.0394, Val Loss: 0.0505\n",
            "Epoch 31/100, Train Loss: 0.0397, Val Loss: 0.0505\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0389, Val Loss: 0.0505\n",
            "Epoch 33/100, Train Loss: 0.0398, Val Loss: 0.0505\n",
            "Epoch 34/100, Train Loss: 0.0382, Val Loss: 0.0505\n",
            "Epoch 35/100, Train Loss: 0.0346, Val Loss: 0.0505\n",
            "Epoch 36/100, Train Loss: 0.0384, Val Loss: 0.0505\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0396, Val Loss: 0.0505\n",
            "Epoch 38/100, Train Loss: 0.0398, Val Loss: 0.0505\n",
            "Epoch 39/100, Train Loss: 0.0394, Val Loss: 0.0505\n",
            "Epoch 40/100, Train Loss: 0.0406, Val Loss: 0.0505\n",
            "Epoch 41/100, Train Loss: 0.0393, Val Loss: 0.0505\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0411, Val Loss: 0.0505\n",
            "Epoch 43/100, Train Loss: 0.0403, Val Loss: 0.0505\n",
            "Epoch 44/100, Train Loss: 0.0369, Val Loss: 0.0505\n",
            "Epoch 45/100, Train Loss: 0.0399, Val Loss: 0.0505\n",
            "Epoch 46/100, Train Loss: 0.0401, Val Loss: 0.0505\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0392, Val Loss: 0.0505\n",
            "Epoch 48/100, Train Loss: 0.0376, Val Loss: 0.0505\n",
            "Epoch 49/100, Train Loss: 0.0374, Val Loss: 0.0505\n",
            "Epoch 50/100, Train Loss: 0.0389, Val Loss: 0.0505\n",
            "Epoch 51/100, Train Loss: 0.0378, Val Loss: 0.0505\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0395, Val Loss: 0.0505\n",
            "Epoch 53/100, Train Loss: 0.0394, Val Loss: 0.0505\n",
            "Epoch 54/100, Train Loss: 0.0409, Val Loss: 0.0505\n",
            "Epoch 55/100, Train Loss: 0.0393, Val Loss: 0.0505\n",
            "Epoch 56/100, Train Loss: 0.0391, Val Loss: 0.0505\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0401, Val Loss: 0.0505\n",
            "Epoch 58/100, Train Loss: 0.0405, Val Loss: 0.0505\n",
            "Epoch 59/100, Train Loss: 0.0415, Val Loss: 0.0505\n",
            "Epoch 60/100, Train Loss: 0.0381, Val Loss: 0.0505\n",
            "Epoch 61/100, Train Loss: 0.0367, Val Loss: 0.0505\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0395, Val Loss: 0.0505\n",
            "Epoch 63/100, Train Loss: 0.0387, Val Loss: 0.0505\n",
            "Epoch 64/100, Train Loss: 0.0386, Val Loss: 0.0505\n",
            "Epoch 65/100, Train Loss: 0.0391, Val Loss: 0.0505\n",
            "Epoch 66/100, Train Loss: 0.0405, Val Loss: 0.0505\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0399, Val Loss: 0.0505\n",
            "Epoch 68/100, Train Loss: 0.0414, Val Loss: 0.0505\n",
            "Epoch 69/100, Train Loss: 0.0390, Val Loss: 0.0505\n",
            "Epoch 70/100, Train Loss: 0.0402, Val Loss: 0.0505\n",
            "Epoch 71/100, Train Loss: 0.0361, Val Loss: 0.0505\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0389, Val Loss: 0.0505\n",
            "Epoch 73/100, Train Loss: 0.0396, Val Loss: 0.0505\n",
            "Epoch 74/100, Train Loss: 0.0401, Val Loss: 0.0505\n",
            "Epoch 75/100, Train Loss: 0.0387, Val Loss: 0.0505\n",
            "Epoch 76/100, Train Loss: 0.0389, Val Loss: 0.0505\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0377, Val Loss: 0.0505\n",
            "Epoch 78/100, Train Loss: 0.0406, Val Loss: 0.0505\n",
            "Epoch 79/100, Train Loss: 0.0411, Val Loss: 0.0505\n",
            "Epoch 80/100, Train Loss: 0.0393, Val Loss: 0.0505\n",
            "Epoch 81/100, Train Loss: 0.0398, Val Loss: 0.0505\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0422, Val Loss: 0.0505\n",
            "Epoch 83/100, Train Loss: 0.0393, Val Loss: 0.0505\n",
            "Epoch 84/100, Train Loss: 0.0387, Val Loss: 0.0505\n",
            "Epoch 85/100, Train Loss: 0.0375, Val Loss: 0.0505\n",
            "Epoch 86/100, Train Loss: 0.0389, Val Loss: 0.0505\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0385, Val Loss: 0.0505\n",
            "Epoch 88/100, Train Loss: 0.0384, Val Loss: 0.0505\n",
            "Epoch 89/100, Train Loss: 0.0356, Val Loss: 0.0505\n",
            "Epoch 90/100, Train Loss: 0.0376, Val Loss: 0.0505\n",
            "Epoch 91/100, Train Loss: 0.0425, Val Loss: 0.0505\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0410, Val Loss: 0.0505\n",
            "Epoch 93/100, Train Loss: 0.0399, Val Loss: 0.0505\n",
            "Epoch 94/100, Train Loss: 0.0367, Val Loss: 0.0505\n",
            "Epoch 95/100, Train Loss: 0.0371, Val Loss: 0.0505\n",
            "Epoch 96/100, Train Loss: 0.0394, Val Loss: 0.0505\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0389, Val Loss: 0.0505\n",
            "Epoch 98/100, Train Loss: 0.0392, Val Loss: 0.0505\n",
            "Epoch 99/100, Train Loss: 0.0398, Val Loss: 0.0505\n",
            "Epoch 100/100, Train Loss: 0.0395, Val Loss: 0.0505\n",
            "\n",
            "Test iteration 12/14\n",
            "Current training set size: 124 samples\n",
            "Epoch 1/100, Train Loss: 0.1492, Val Loss: 0.2735\n",
            "Epoch 2/100, Train Loss: 0.0979, Val Loss: 0.1716\n",
            "Epoch 3/100, Train Loss: 0.0733, Val Loss: 0.1195\n",
            "Epoch 4/100, Train Loss: 0.0676, Val Loss: 0.0555\n",
            "Epoch 5/100, Train Loss: 0.0621, Val Loss: 0.0484\n",
            "Epoch 6/100, Train Loss: 0.0608, Val Loss: 0.0486\n",
            "Epoch 7/100, Train Loss: 0.0523, Val Loss: 0.0986\n",
            "Epoch 8/100, Train Loss: 0.0561, Val Loss: 0.0554\n",
            "Epoch 9/100, Train Loss: 0.0494, Val Loss: 0.0510\n",
            "Epoch 10/100, Train Loss: 0.0507, Val Loss: 0.0706\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 11/100, Train Loss: 0.0489, Val Loss: 0.0681\n",
            "Epoch 12/100, Train Loss: 0.0438, Val Loss: 0.0647\n",
            "Epoch 13/100, Train Loss: 0.0442, Val Loss: 0.0625\n",
            "Epoch 14/100, Train Loss: 0.0449, Val Loss: 0.0601\n",
            "Epoch 15/100, Train Loss: 0.0449, Val Loss: 0.0583\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0429, Val Loss: 0.0583\n",
            "Epoch 17/100, Train Loss: 0.0423, Val Loss: 0.0583\n",
            "Epoch 18/100, Train Loss: 0.0419, Val Loss: 0.0583\n",
            "Epoch 19/100, Train Loss: 0.0427, Val Loss: 0.0583\n",
            "Epoch 20/100, Train Loss: 0.0405, Val Loss: 0.0583\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0432, Val Loss: 0.0583\n",
            "Epoch 22/100, Train Loss: 0.0417, Val Loss: 0.0583\n",
            "Epoch 23/100, Train Loss: 0.0445, Val Loss: 0.0583\n",
            "Epoch 24/100, Train Loss: 0.0403, Val Loss: 0.0583\n",
            "Epoch 25/100, Train Loss: 0.0430, Val Loss: 0.0583\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0429, Val Loss: 0.0583\n",
            "Epoch 27/100, Train Loss: 0.0426, Val Loss: 0.0583\n",
            "Epoch 28/100, Train Loss: 0.0401, Val Loss: 0.0583\n",
            "Epoch 29/100, Train Loss: 0.0421, Val Loss: 0.0583\n",
            "Epoch 30/100, Train Loss: 0.0411, Val Loss: 0.0583\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0406, Val Loss: 0.0583\n",
            "Epoch 32/100, Train Loss: 0.0429, Val Loss: 0.0583\n",
            "Epoch 33/100, Train Loss: 0.0415, Val Loss: 0.0583\n",
            "Epoch 34/100, Train Loss: 0.0419, Val Loss: 0.0583\n",
            "Epoch 35/100, Train Loss: 0.0473, Val Loss: 0.0583\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0419, Val Loss: 0.0583\n",
            "Epoch 37/100, Train Loss: 0.0408, Val Loss: 0.0583\n",
            "Epoch 38/100, Train Loss: 0.0450, Val Loss: 0.0583\n",
            "Epoch 39/100, Train Loss: 0.0413, Val Loss: 0.0583\n",
            "Epoch 40/100, Train Loss: 0.0423, Val Loss: 0.0583\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0412, Val Loss: 0.0583\n",
            "Epoch 42/100, Train Loss: 0.0427, Val Loss: 0.0583\n",
            "Epoch 43/100, Train Loss: 0.0417, Val Loss: 0.0583\n",
            "Epoch 44/100, Train Loss: 0.0436, Val Loss: 0.0583\n",
            "Epoch 45/100, Train Loss: 0.0447, Val Loss: 0.0583\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0433, Val Loss: 0.0583\n",
            "Epoch 47/100, Train Loss: 0.0415, Val Loss: 0.0583\n",
            "Epoch 48/100, Train Loss: 0.0424, Val Loss: 0.0583\n",
            "Epoch 49/100, Train Loss: 0.0434, Val Loss: 0.0583\n",
            "Epoch 50/100, Train Loss: 0.0442, Val Loss: 0.0583\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0418, Val Loss: 0.0583\n",
            "Epoch 52/100, Train Loss: 0.0425, Val Loss: 0.0583\n",
            "Epoch 53/100, Train Loss: 0.0446, Val Loss: 0.0583\n",
            "Epoch 54/100, Train Loss: 0.0418, Val Loss: 0.0583\n",
            "Epoch 55/100, Train Loss: 0.0444, Val Loss: 0.0583\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0434, Val Loss: 0.0583\n",
            "Epoch 57/100, Train Loss: 0.0431, Val Loss: 0.0583\n",
            "Epoch 58/100, Train Loss: 0.0434, Val Loss: 0.0583\n",
            "Epoch 59/100, Train Loss: 0.0426, Val Loss: 0.0583\n",
            "Epoch 60/100, Train Loss: 0.0456, Val Loss: 0.0583\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0463, Val Loss: 0.0583\n",
            "Epoch 62/100, Train Loss: 0.0412, Val Loss: 0.0583\n",
            "Epoch 63/100, Train Loss: 0.0429, Val Loss: 0.0583\n",
            "Epoch 64/100, Train Loss: 0.0445, Val Loss: 0.0583\n",
            "Epoch 65/100, Train Loss: 0.0431, Val Loss: 0.0583\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0428, Val Loss: 0.0583\n",
            "Epoch 67/100, Train Loss: 0.0398, Val Loss: 0.0583\n",
            "Epoch 68/100, Train Loss: 0.0406, Val Loss: 0.0583\n",
            "Epoch 69/100, Train Loss: 0.0443, Val Loss: 0.0583\n",
            "Epoch 70/100, Train Loss: 0.0423, Val Loss: 0.0583\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0417, Val Loss: 0.0583\n",
            "Epoch 72/100, Train Loss: 0.0426, Val Loss: 0.0583\n",
            "Epoch 73/100, Train Loss: 0.0433, Val Loss: 0.0583\n",
            "Epoch 74/100, Train Loss: 0.0430, Val Loss: 0.0583\n",
            "Epoch 75/100, Train Loss: 0.0413, Val Loss: 0.0583\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0421, Val Loss: 0.0583\n",
            "Epoch 77/100, Train Loss: 0.0412, Val Loss: 0.0583\n",
            "Epoch 78/100, Train Loss: 0.0445, Val Loss: 0.0583\n",
            "Epoch 79/100, Train Loss: 0.0417, Val Loss: 0.0583\n",
            "Epoch 80/100, Train Loss: 0.0416, Val Loss: 0.0583\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0428, Val Loss: 0.0583\n",
            "Epoch 82/100, Train Loss: 0.0426, Val Loss: 0.0583\n",
            "Epoch 83/100, Train Loss: 0.0430, Val Loss: 0.0583\n",
            "Epoch 84/100, Train Loss: 0.0432, Val Loss: 0.0583\n",
            "Epoch 85/100, Train Loss: 0.0432, Val Loss: 0.0583\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0404, Val Loss: 0.0583\n",
            "Epoch 87/100, Train Loss: 0.0425, Val Loss: 0.0583\n",
            "Epoch 88/100, Train Loss: 0.0407, Val Loss: 0.0583\n",
            "Epoch 89/100, Train Loss: 0.0419, Val Loss: 0.0583\n",
            "Epoch 90/100, Train Loss: 0.0402, Val Loss: 0.0583\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0420, Val Loss: 0.0583\n",
            "Epoch 92/100, Train Loss: 0.0420, Val Loss: 0.0583\n",
            "Epoch 93/100, Train Loss: 0.0420, Val Loss: 0.0583\n",
            "Epoch 94/100, Train Loss: 0.0409, Val Loss: 0.0583\n",
            "Epoch 95/100, Train Loss: 0.0435, Val Loss: 0.0583\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0432, Val Loss: 0.0583\n",
            "Epoch 97/100, Train Loss: 0.0441, Val Loss: 0.0583\n",
            "Epoch 98/100, Train Loss: 0.0423, Val Loss: 0.0583\n",
            "Epoch 99/100, Train Loss: 0.0448, Val Loss: 0.0583\n",
            "Epoch 100/100, Train Loss: 0.0434, Val Loss: 0.0583\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 13/14\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1505, Val Loss: 0.2308\n",
            "Epoch 2/100, Train Loss: 0.0884, Val Loss: 0.1680\n",
            "Epoch 3/100, Train Loss: 0.0792, Val Loss: 0.1273\n",
            "Epoch 4/100, Train Loss: 0.0918, Val Loss: 0.0664\n",
            "Epoch 5/100, Train Loss: 0.0713, Val Loss: 0.0500\n",
            "Epoch 6/100, Train Loss: 0.0661, Val Loss: 0.0469\n",
            "Epoch 7/100, Train Loss: 0.0543, Val Loss: 0.0522\n",
            "Epoch 8/100, Train Loss: 0.0512, Val Loss: 0.1091\n",
            "Epoch 9/100, Train Loss: 0.0529, Val Loss: 0.0814\n",
            "Epoch 10/100, Train Loss: 0.0496, Val Loss: 0.0781\n",
            "Epoch 11/100, Train Loss: 0.0508, Val Loss: 0.0587\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 12/100, Train Loss: 0.0539, Val Loss: 0.0581\n",
            "Epoch 13/100, Train Loss: 0.0446, Val Loss: 0.0573\n",
            "Epoch 14/100, Train Loss: 0.0392, Val Loss: 0.0563\n",
            "Epoch 15/100, Train Loss: 0.0396, Val Loss: 0.0560\n",
            "Epoch 16/100, Train Loss: 0.0347, Val Loss: 0.0556\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0403, Val Loss: 0.0556\n",
            "Epoch 18/100, Train Loss: 0.0373, Val Loss: 0.0556\n",
            "Epoch 19/100, Train Loss: 0.0357, Val Loss: 0.0556\n",
            "Epoch 20/100, Train Loss: 0.0376, Val Loss: 0.0556\n",
            "Epoch 21/100, Train Loss: 0.0386, Val Loss: 0.0556\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0375, Val Loss: 0.0556\n",
            "Epoch 23/100, Train Loss: 0.0389, Val Loss: 0.0556\n",
            "Epoch 24/100, Train Loss: 0.0403, Val Loss: 0.0556\n",
            "Epoch 25/100, Train Loss: 0.0394, Val Loss: 0.0556\n",
            "Epoch 26/100, Train Loss: 0.0368, Val Loss: 0.0556\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0408, Val Loss: 0.0556\n",
            "Epoch 28/100, Train Loss: 0.0424, Val Loss: 0.0556\n",
            "Epoch 29/100, Train Loss: 0.0383, Val Loss: 0.0556\n",
            "Epoch 30/100, Train Loss: 0.0377, Val Loss: 0.0556\n",
            "Epoch 31/100, Train Loss: 0.0364, Val Loss: 0.0556\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0374, Val Loss: 0.0556\n",
            "Epoch 33/100, Train Loss: 0.0397, Val Loss: 0.0556\n",
            "Epoch 34/100, Train Loss: 0.0389, Val Loss: 0.0556\n",
            "Epoch 35/100, Train Loss: 0.0381, Val Loss: 0.0556\n",
            "Epoch 36/100, Train Loss: 0.0400, Val Loss: 0.0556\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0405, Val Loss: 0.0556\n",
            "Epoch 38/100, Train Loss: 0.0368, Val Loss: 0.0556\n",
            "Epoch 39/100, Train Loss: 0.0366, Val Loss: 0.0556\n",
            "Epoch 40/100, Train Loss: 0.0390, Val Loss: 0.0556\n",
            "Epoch 41/100, Train Loss: 0.0401, Val Loss: 0.0556\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0362, Val Loss: 0.0556\n",
            "Epoch 43/100, Train Loss: 0.0361, Val Loss: 0.0556\n",
            "Epoch 44/100, Train Loss: 0.0368, Val Loss: 0.0556\n",
            "Epoch 45/100, Train Loss: 0.0396, Val Loss: 0.0556\n",
            "Epoch 46/100, Train Loss: 0.0365, Val Loss: 0.0556\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0380, Val Loss: 0.0556\n",
            "Epoch 48/100, Train Loss: 0.0369, Val Loss: 0.0556\n",
            "Epoch 49/100, Train Loss: 0.0372, Val Loss: 0.0556\n",
            "Epoch 50/100, Train Loss: 0.0386, Val Loss: 0.0556\n",
            "Epoch 51/100, Train Loss: 0.0399, Val Loss: 0.0556\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0410, Val Loss: 0.0556\n",
            "Epoch 53/100, Train Loss: 0.0393, Val Loss: 0.0556\n",
            "Epoch 54/100, Train Loss: 0.0381, Val Loss: 0.0556\n",
            "Epoch 55/100, Train Loss: 0.0381, Val Loss: 0.0556\n",
            "Epoch 56/100, Train Loss: 0.0420, Val Loss: 0.0556\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0373, Val Loss: 0.0556\n",
            "Epoch 58/100, Train Loss: 0.0421, Val Loss: 0.0556\n",
            "Epoch 59/100, Train Loss: 0.0365, Val Loss: 0.0556\n",
            "Epoch 60/100, Train Loss: 0.0387, Val Loss: 0.0556\n",
            "Epoch 61/100, Train Loss: 0.0395, Val Loss: 0.0556\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0367, Val Loss: 0.0556\n",
            "Epoch 63/100, Train Loss: 0.0399, Val Loss: 0.0556\n",
            "Epoch 64/100, Train Loss: 0.0392, Val Loss: 0.0556\n",
            "Epoch 65/100, Train Loss: 0.0383, Val Loss: 0.0556\n",
            "Epoch 66/100, Train Loss: 0.0382, Val Loss: 0.0556\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0389, Val Loss: 0.0556\n",
            "Epoch 68/100, Train Loss: 0.0367, Val Loss: 0.0556\n",
            "Epoch 69/100, Train Loss: 0.0377, Val Loss: 0.0556\n",
            "Epoch 70/100, Train Loss: 0.0399, Val Loss: 0.0556\n",
            "Epoch 71/100, Train Loss: 0.0404, Val Loss: 0.0556\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0388, Val Loss: 0.0556\n",
            "Epoch 73/100, Train Loss: 0.0362, Val Loss: 0.0556\n",
            "Epoch 74/100, Train Loss: 0.0362, Val Loss: 0.0556\n",
            "Epoch 75/100, Train Loss: 0.0388, Val Loss: 0.0556\n",
            "Epoch 76/100, Train Loss: 0.0412, Val Loss: 0.0556\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0362, Val Loss: 0.0556\n",
            "Epoch 78/100, Train Loss: 0.0492, Val Loss: 0.0556\n",
            "Epoch 79/100, Train Loss: 0.0377, Val Loss: 0.0556\n",
            "Epoch 80/100, Train Loss: 0.0394, Val Loss: 0.0556\n",
            "Epoch 81/100, Train Loss: 0.0378, Val Loss: 0.0556\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0556\n",
            "Epoch 83/100, Train Loss: 0.0432, Val Loss: 0.0556\n",
            "Epoch 84/100, Train Loss: 0.0395, Val Loss: 0.0556\n",
            "Epoch 85/100, Train Loss: 0.0391, Val Loss: 0.0556\n",
            "Epoch 86/100, Train Loss: 0.0383, Val Loss: 0.0556\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0374, Val Loss: 0.0556\n",
            "Epoch 88/100, Train Loss: 0.0401, Val Loss: 0.0556\n",
            "Epoch 89/100, Train Loss: 0.0436, Val Loss: 0.0556\n",
            "Epoch 90/100, Train Loss: 0.0368, Val Loss: 0.0556\n",
            "Epoch 91/100, Train Loss: 0.0369, Val Loss: 0.0556\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0369, Val Loss: 0.0556\n",
            "Epoch 93/100, Train Loss: 0.0375, Val Loss: 0.0556\n",
            "Epoch 94/100, Train Loss: 0.0369, Val Loss: 0.0556\n",
            "Epoch 95/100, Train Loss: 0.0411, Val Loss: 0.0556\n",
            "Epoch 96/100, Train Loss: 0.0379, Val Loss: 0.0556\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0388, Val Loss: 0.0556\n",
            "Epoch 98/100, Train Loss: 0.0387, Val Loss: 0.0556\n",
            "Epoch 99/100, Train Loss: 0.0390, Val Loss: 0.0556\n",
            "Epoch 100/100, Train Loss: 0.0379, Val Loss: 0.0556\n",
            "\n",
            "Test iteration 14/14\n",
            "Current training set size: 126 samples\n",
            "Epoch 1/100, Train Loss: 0.1511, Val Loss: 0.2642\n",
            "Epoch 2/100, Train Loss: 0.0908, Val Loss: 0.1666\n",
            "Epoch 3/100, Train Loss: 0.0760, Val Loss: 0.1001\n",
            "Epoch 4/100, Train Loss: 0.0669, Val Loss: 0.0557\n",
            "Epoch 5/100, Train Loss: 0.0626, Val Loss: 0.1014\n",
            "Epoch 6/100, Train Loss: 0.0538, Val Loss: 0.0606\n",
            "Epoch 7/100, Train Loss: 0.0500, Val Loss: 0.0636\n",
            "Epoch 8/100, Train Loss: 0.0495, Val Loss: 0.0499\n",
            "Epoch 9/100, Train Loss: 0.0438, Val Loss: 0.0675\n",
            "Epoch 10/100, Train Loss: 0.0439, Val Loss: 0.1032\n",
            "Epoch 11/100, Train Loss: 0.0420, Val Loss: 0.0735\n",
            "Epoch 12/100, Train Loss: 0.0449, Val Loss: 0.0625\n",
            "Epoch 13/100, Train Loss: 0.0411, Val Loss: 0.0680\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 14/100, Train Loss: 0.0447, Val Loss: 0.0684\n",
            "Epoch 15/100, Train Loss: 0.0398, Val Loss: 0.0691\n",
            "Epoch 16/100, Train Loss: 0.0383, Val Loss: 0.0682\n",
            "Epoch 17/100, Train Loss: 0.0337, Val Loss: 0.0671\n",
            "Epoch 18/100, Train Loss: 0.0356, Val Loss: 0.0673\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0315, Val Loss: 0.0673\n",
            "Epoch 20/100, Train Loss: 0.0362, Val Loss: 0.0672\n",
            "Epoch 21/100, Train Loss: 0.0352, Val Loss: 0.0672\n",
            "Epoch 22/100, Train Loss: 0.0359, Val Loss: 0.0672\n",
            "Epoch 23/100, Train Loss: 0.0351, Val Loss: 0.0672\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0360, Val Loss: 0.0672\n",
            "Epoch 25/100, Train Loss: 0.0370, Val Loss: 0.0672\n",
            "Epoch 26/100, Train Loss: 0.0354, Val Loss: 0.0672\n",
            "Epoch 27/100, Train Loss: 0.0361, Val Loss: 0.0672\n",
            "Epoch 28/100, Train Loss: 0.0360, Val Loss: 0.0672\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0352, Val Loss: 0.0672\n",
            "Epoch 30/100, Train Loss: 0.0390, Val Loss: 0.0672\n",
            "Epoch 31/100, Train Loss: 0.0353, Val Loss: 0.0672\n",
            "Epoch 32/100, Train Loss: 0.0340, Val Loss: 0.0672\n",
            "Epoch 33/100, Train Loss: 0.0334, Val Loss: 0.0672\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0351, Val Loss: 0.0672\n",
            "Epoch 35/100, Train Loss: 0.0370, Val Loss: 0.0672\n",
            "Epoch 36/100, Train Loss: 0.0383, Val Loss: 0.0672\n",
            "Epoch 37/100, Train Loss: 0.0370, Val Loss: 0.0672\n",
            "Epoch 38/100, Train Loss: 0.0339, Val Loss: 0.0672\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0368, Val Loss: 0.0672\n",
            "Epoch 40/100, Train Loss: 0.0373, Val Loss: 0.0672\n",
            "Epoch 41/100, Train Loss: 0.0352, Val Loss: 0.0672\n",
            "Epoch 42/100, Train Loss: 0.0366, Val Loss: 0.0672\n",
            "Epoch 43/100, Train Loss: 0.0360, Val Loss: 0.0672\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0334, Val Loss: 0.0672\n",
            "Epoch 45/100, Train Loss: 0.0360, Val Loss: 0.0672\n",
            "Epoch 46/100, Train Loss: 0.0358, Val Loss: 0.0672\n",
            "Epoch 47/100, Train Loss: 0.0365, Val Loss: 0.0672\n",
            "Epoch 48/100, Train Loss: 0.0378, Val Loss: 0.0672\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0351, Val Loss: 0.0672\n",
            "Epoch 50/100, Train Loss: 0.0362, Val Loss: 0.0672\n",
            "Epoch 51/100, Train Loss: 0.0349, Val Loss: 0.0672\n",
            "Epoch 52/100, Train Loss: 0.0354, Val Loss: 0.0672\n",
            "Epoch 53/100, Train Loss: 0.0353, Val Loss: 0.0672\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0335, Val Loss: 0.0672\n",
            "Epoch 55/100, Train Loss: 0.0349, Val Loss: 0.0672\n",
            "Epoch 56/100, Train Loss: 0.0386, Val Loss: 0.0672\n",
            "Epoch 57/100, Train Loss: 0.0375, Val Loss: 0.0672\n",
            "Epoch 58/100, Train Loss: 0.0367, Val Loss: 0.0672\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0337, Val Loss: 0.0672\n",
            "Epoch 60/100, Train Loss: 0.0364, Val Loss: 0.0672\n",
            "Epoch 61/100, Train Loss: 0.0366, Val Loss: 0.0672\n",
            "Epoch 62/100, Train Loss: 0.0391, Val Loss: 0.0672\n",
            "Epoch 63/100, Train Loss: 0.0371, Val Loss: 0.0672\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0348, Val Loss: 0.0672\n",
            "Epoch 65/100, Train Loss: 0.0356, Val Loss: 0.0672\n",
            "Epoch 66/100, Train Loss: 0.0351, Val Loss: 0.0672\n",
            "Epoch 67/100, Train Loss: 0.0350, Val Loss: 0.0672\n",
            "Epoch 68/100, Train Loss: 0.0360, Val Loss: 0.0672\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0367, Val Loss: 0.0672\n",
            "Epoch 70/100, Train Loss: 0.0356, Val Loss: 0.0672\n",
            "Epoch 71/100, Train Loss: 0.0360, Val Loss: 0.0672\n",
            "Epoch 72/100, Train Loss: 0.0377, Val Loss: 0.0672\n",
            "Epoch 73/100, Train Loss: 0.0353, Val Loss: 0.0672\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0342, Val Loss: 0.0672\n",
            "Epoch 75/100, Train Loss: 0.0358, Val Loss: 0.0672\n",
            "Epoch 76/100, Train Loss: 0.0356, Val Loss: 0.0672\n",
            "Epoch 77/100, Train Loss: 0.0331, Val Loss: 0.0672\n",
            "Epoch 78/100, Train Loss: 0.0342, Val Loss: 0.0672\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0356, Val Loss: 0.0672\n",
            "Epoch 80/100, Train Loss: 0.0340, Val Loss: 0.0672\n",
            "Epoch 81/100, Train Loss: 0.0354, Val Loss: 0.0672\n",
            "Epoch 82/100, Train Loss: 0.0361, Val Loss: 0.0672\n",
            "Epoch 83/100, Train Loss: 0.0355, Val Loss: 0.0672\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0366, Val Loss: 0.0672\n",
            "Epoch 85/100, Train Loss: 0.0357, Val Loss: 0.0672\n",
            "Epoch 86/100, Train Loss: 0.0374, Val Loss: 0.0672\n",
            "Epoch 87/100, Train Loss: 0.0373, Val Loss: 0.0672\n",
            "Epoch 88/100, Train Loss: 0.0347, Val Loss: 0.0672\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0380, Val Loss: 0.0672\n",
            "Epoch 90/100, Train Loss: 0.0356, Val Loss: 0.0672\n",
            "Epoch 91/100, Train Loss: 0.0352, Val Loss: 0.0672\n",
            "Epoch 92/100, Train Loss: 0.0351, Val Loss: 0.0672\n",
            "Epoch 93/100, Train Loss: 0.0366, Val Loss: 0.0672\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0364, Val Loss: 0.0672\n",
            "Epoch 95/100, Train Loss: 0.0345, Val Loss: 0.0672\n",
            "Epoch 96/100, Train Loss: 0.0353, Val Loss: 0.0672\n",
            "Epoch 97/100, Train Loss: 0.0349, Val Loss: 0.0672\n",
            "Epoch 98/100, Train Loss: 0.0371, Val Loss: 0.0672\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0346, Val Loss: 0.0672\n",
            "Epoch 100/100, Train Loss: 0.0373, Val Loss: 0.0672\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: cnn, Units/Estimators: CNN\n",
            "Dropout: 0.2, Dense Units: 32\n",
            "Batch Size: 4, Epochs: 100\n",
            "Device: cuda\n",
            "Total test samples: 14\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/14\n",
            "Current training set size: 113 samples\n",
            "Epoch 1/100, Train Loss: 0.1360, Val Loss: 0.3853\n",
            "Epoch 2/100, Train Loss: 0.0882, Val Loss: 0.2994\n",
            "Epoch 3/100, Train Loss: 0.0738, Val Loss: 0.2034\n",
            "Epoch 4/100, Train Loss: 0.0656, Val Loss: 0.1923\n",
            "Epoch 5/100, Train Loss: 0.0673, Val Loss: 0.1868\n",
            "Epoch 6/100, Train Loss: 0.0607, Val Loss: 0.1797\n",
            "Epoch 7/100, Train Loss: 0.0530, Val Loss: 0.1440\n",
            "Epoch 8/100, Train Loss: 0.0540, Val Loss: 0.1454\n",
            "Epoch 9/100, Train Loss: 0.0491, Val Loss: 0.1044\n",
            "Epoch 10/100, Train Loss: 0.0496, Val Loss: 0.0607\n",
            "Epoch 11/100, Train Loss: 0.0466, Val Loss: 0.0864\n",
            "Epoch 12/100, Train Loss: 0.0499, Val Loss: 0.0908\n",
            "Epoch 13/100, Train Loss: 0.0507, Val Loss: 0.1010\n",
            "Epoch 14/100, Train Loss: 0.0471, Val Loss: 0.0635\n",
            "Epoch 15/100, Train Loss: 0.0427, Val Loss: 0.0866\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0614, Val Loss: 0.0883\n",
            "Epoch 17/100, Train Loss: 0.0430, Val Loss: 0.0888\n",
            "Epoch 18/100, Train Loss: 0.0404, Val Loss: 0.0894\n",
            "Epoch 19/100, Train Loss: 0.0383, Val Loss: 0.0887\n",
            "Epoch 20/100, Train Loss: 0.0405, Val Loss: 0.0888\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0362, Val Loss: 0.0888\n",
            "Epoch 22/100, Train Loss: 0.0389, Val Loss: 0.0888\n",
            "Epoch 23/100, Train Loss: 0.0408, Val Loss: 0.0888\n",
            "Epoch 24/100, Train Loss: 0.0430, Val Loss: 0.0888\n",
            "Epoch 25/100, Train Loss: 0.0402, Val Loss: 0.0888\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0379, Val Loss: 0.0888\n",
            "Epoch 27/100, Train Loss: 0.0424, Val Loss: 0.0888\n",
            "Epoch 28/100, Train Loss: 0.0417, Val Loss: 0.0888\n",
            "Epoch 29/100, Train Loss: 0.0485, Val Loss: 0.0888\n",
            "Epoch 30/100, Train Loss: 0.0390, Val Loss: 0.0888\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0389, Val Loss: 0.0888\n",
            "Epoch 32/100, Train Loss: 0.0438, Val Loss: 0.0888\n",
            "Epoch 33/100, Train Loss: 0.0357, Val Loss: 0.0888\n",
            "Epoch 34/100, Train Loss: 0.0370, Val Loss: 0.0888\n",
            "Epoch 35/100, Train Loss: 0.0430, Val Loss: 0.0888\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0422, Val Loss: 0.0888\n",
            "Epoch 37/100, Train Loss: 0.0386, Val Loss: 0.0888\n",
            "Epoch 38/100, Train Loss: 0.0366, Val Loss: 0.0888\n",
            "Epoch 39/100, Train Loss: 0.0391, Val Loss: 0.0888\n",
            "Epoch 40/100, Train Loss: 0.0410, Val Loss: 0.0888\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0407, Val Loss: 0.0888\n",
            "Epoch 42/100, Train Loss: 0.0390, Val Loss: 0.0888\n",
            "Epoch 43/100, Train Loss: 0.0414, Val Loss: 0.0888\n",
            "Epoch 44/100, Train Loss: 0.0384, Val Loss: 0.0888\n",
            "Epoch 45/100, Train Loss: 0.0412, Val Loss: 0.0888\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0377, Val Loss: 0.0888\n",
            "Epoch 47/100, Train Loss: 0.0408, Val Loss: 0.0888\n",
            "Epoch 48/100, Train Loss: 0.0391, Val Loss: 0.0888\n",
            "Epoch 49/100, Train Loss: 0.0460, Val Loss: 0.0888\n",
            "Epoch 50/100, Train Loss: 0.0390, Val Loss: 0.0888\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0388, Val Loss: 0.0888\n",
            "Epoch 52/100, Train Loss: 0.0390, Val Loss: 0.0888\n",
            "Epoch 53/100, Train Loss: 0.0392, Val Loss: 0.0888\n",
            "Epoch 54/100, Train Loss: 0.0413, Val Loss: 0.0888\n",
            "Epoch 55/100, Train Loss: 0.0418, Val Loss: 0.0888\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0421, Val Loss: 0.0888\n",
            "Epoch 57/100, Train Loss: 0.0390, Val Loss: 0.0888\n",
            "Epoch 58/100, Train Loss: 0.0429, Val Loss: 0.0888\n",
            "Epoch 59/100, Train Loss: 0.0404, Val Loss: 0.0888\n",
            "Epoch 60/100, Train Loss: 0.0406, Val Loss: 0.0888\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0411, Val Loss: 0.0888\n",
            "Epoch 62/100, Train Loss: 0.0368, Val Loss: 0.0888\n",
            "Epoch 63/100, Train Loss: 0.0403, Val Loss: 0.0888\n",
            "Epoch 64/100, Train Loss: 0.0405, Val Loss: 0.0888\n",
            "Epoch 65/100, Train Loss: 0.0399, Val Loss: 0.0888\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0419, Val Loss: 0.0888\n",
            "Epoch 67/100, Train Loss: 0.0408, Val Loss: 0.0888\n",
            "Epoch 68/100, Train Loss: 0.0417, Val Loss: 0.0888\n",
            "Epoch 69/100, Train Loss: 0.0419, Val Loss: 0.0888\n",
            "Epoch 70/100, Train Loss: 0.0360, Val Loss: 0.0888\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0410, Val Loss: 0.0888\n",
            "Epoch 72/100, Train Loss: 0.0405, Val Loss: 0.0888\n",
            "Epoch 73/100, Train Loss: 0.0405, Val Loss: 0.0888\n",
            "Epoch 74/100, Train Loss: 0.0411, Val Loss: 0.0888\n",
            "Epoch 75/100, Train Loss: 0.0389, Val Loss: 0.0888\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0405, Val Loss: 0.0888\n",
            "Epoch 77/100, Train Loss: 0.0463, Val Loss: 0.0888\n",
            "Epoch 78/100, Train Loss: 0.0421, Val Loss: 0.0888\n",
            "Epoch 79/100, Train Loss: 0.0392, Val Loss: 0.0888\n",
            "Epoch 80/100, Train Loss: 0.0397, Val Loss: 0.0888\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0426, Val Loss: 0.0888\n",
            "Epoch 82/100, Train Loss: 0.0423, Val Loss: 0.0888\n",
            "Epoch 83/100, Train Loss: 0.0396, Val Loss: 0.0888\n",
            "Epoch 84/100, Train Loss: 0.0408, Val Loss: 0.0888\n",
            "Epoch 85/100, Train Loss: 0.0422, Val Loss: 0.0888\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0393, Val Loss: 0.0888\n",
            "Epoch 87/100, Train Loss: 0.0408, Val Loss: 0.0888\n",
            "Epoch 88/100, Train Loss: 0.0358, Val Loss: 0.0888\n",
            "Epoch 89/100, Train Loss: 0.0426, Val Loss: 0.0888\n",
            "Epoch 90/100, Train Loss: 0.0417, Val Loss: 0.0888\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0393, Val Loss: 0.0888\n",
            "Epoch 92/100, Train Loss: 0.0440, Val Loss: 0.0888\n",
            "Epoch 93/100, Train Loss: 0.0412, Val Loss: 0.0888\n",
            "Epoch 94/100, Train Loss: 0.0388, Val Loss: 0.0888\n",
            "Epoch 95/100, Train Loss: 0.0456, Val Loss: 0.0888\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0398, Val Loss: 0.0888\n",
            "Epoch 97/100, Train Loss: 0.0421, Val Loss: 0.0888\n",
            "Epoch 98/100, Train Loss: 0.0423, Val Loss: 0.0888\n",
            "Epoch 99/100, Train Loss: 0.0410, Val Loss: 0.0888\n",
            "Epoch 100/100, Train Loss: 0.0407, Val Loss: 0.0888\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 2/14\n",
            "Current training set size: 114 samples\n",
            "Epoch 1/100, Train Loss: 0.1429, Val Loss: 0.3484\n",
            "Epoch 2/100, Train Loss: 0.0948, Val Loss: 0.2578\n",
            "Epoch 3/100, Train Loss: 0.0739, Val Loss: 0.1951\n",
            "Epoch 4/100, Train Loss: 0.0686, Val Loss: 0.1895\n",
            "Epoch 5/100, Train Loss: 0.0681, Val Loss: 0.1750\n",
            "Epoch 6/100, Train Loss: 0.0648, Val Loss: 0.1618\n",
            "Epoch 7/100, Train Loss: 0.0559, Val Loss: 0.1147\n",
            "Epoch 8/100, Train Loss: 0.0539, Val Loss: 0.1133\n",
            "Epoch 9/100, Train Loss: 0.0583, Val Loss: 0.1096\n",
            "Epoch 10/100, Train Loss: 0.0501, Val Loss: 0.0970\n",
            "Epoch 11/100, Train Loss: 0.0506, Val Loss: 0.1266\n",
            "Epoch 12/100, Train Loss: 0.0449, Val Loss: 0.0598\n",
            "Epoch 13/100, Train Loss: 0.0415, Val Loss: 0.0996\n",
            "Epoch 14/100, Train Loss: 0.0466, Val Loss: 0.0636\n",
            "Epoch 15/100, Train Loss: 0.0393, Val Loss: 0.0495\n",
            "Epoch 16/100, Train Loss: 0.0377, Val Loss: 0.0471\n",
            "Epoch 17/100, Train Loss: 0.0439, Val Loss: 0.1621\n",
            "Epoch 18/100, Train Loss: 0.0455, Val Loss: 0.0660\n",
            "Epoch 19/100, Train Loss: 0.0398, Val Loss: 0.0746\n",
            "Epoch 20/100, Train Loss: 0.0403, Val Loss: 0.0630\n",
            "Epoch 21/100, Train Loss: 0.0367, Val Loss: 0.1570\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0373, Val Loss: 0.1517\n",
            "Epoch 23/100, Train Loss: 0.0379, Val Loss: 0.1463\n",
            "Epoch 24/100, Train Loss: 0.0342, Val Loss: 0.1372\n",
            "Epoch 25/100, Train Loss: 0.0401, Val Loss: 0.1335\n",
            "Epoch 26/100, Train Loss: 0.0358, Val Loss: 0.1295\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0343, Val Loss: 0.1295\n",
            "Epoch 28/100, Train Loss: 0.0407, Val Loss: 0.1294\n",
            "Epoch 29/100, Train Loss: 0.0355, Val Loss: 0.1294\n",
            "Epoch 30/100, Train Loss: 0.0335, Val Loss: 0.1293\n",
            "Epoch 31/100, Train Loss: 0.0393, Val Loss: 0.1293\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0373, Val Loss: 0.1293\n",
            "Epoch 33/100, Train Loss: 0.0394, Val Loss: 0.1293\n",
            "Epoch 34/100, Train Loss: 0.0379, Val Loss: 0.1293\n",
            "Epoch 35/100, Train Loss: 0.0361, Val Loss: 0.1293\n",
            "Epoch 36/100, Train Loss: 0.0344, Val Loss: 0.1293\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0334, Val Loss: 0.1293\n",
            "Epoch 38/100, Train Loss: 0.0345, Val Loss: 0.1293\n",
            "Epoch 39/100, Train Loss: 0.0373, Val Loss: 0.1293\n",
            "Epoch 40/100, Train Loss: 0.0341, Val Loss: 0.1293\n",
            "Epoch 41/100, Train Loss: 0.0398, Val Loss: 0.1293\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0344, Val Loss: 0.1293\n",
            "Epoch 43/100, Train Loss: 0.0332, Val Loss: 0.1293\n",
            "Epoch 44/100, Train Loss: 0.0374, Val Loss: 0.1293\n",
            "Epoch 45/100, Train Loss: 0.0376, Val Loss: 0.1293\n",
            "Epoch 46/100, Train Loss: 0.0332, Val Loss: 0.1293\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0361, Val Loss: 0.1293\n",
            "Epoch 48/100, Train Loss: 0.0344, Val Loss: 0.1293\n",
            "Epoch 49/100, Train Loss: 0.0341, Val Loss: 0.1293\n",
            "Epoch 50/100, Train Loss: 0.0359, Val Loss: 0.1293\n",
            "Epoch 51/100, Train Loss: 0.0358, Val Loss: 0.1293\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0396, Val Loss: 0.1293\n",
            "Epoch 53/100, Train Loss: 0.0346, Val Loss: 0.1293\n",
            "Epoch 54/100, Train Loss: 0.0369, Val Loss: 0.1293\n",
            "Epoch 55/100, Train Loss: 0.0375, Val Loss: 0.1293\n",
            "Epoch 56/100, Train Loss: 0.0344, Val Loss: 0.1293\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0351, Val Loss: 0.1293\n",
            "Epoch 58/100, Train Loss: 0.0347, Val Loss: 0.1293\n",
            "Epoch 59/100, Train Loss: 0.0365, Val Loss: 0.1293\n",
            "Epoch 60/100, Train Loss: 0.0363, Val Loss: 0.1293\n",
            "Epoch 61/100, Train Loss: 0.0356, Val Loss: 0.1293\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0365, Val Loss: 0.1293\n",
            "Epoch 63/100, Train Loss: 0.0351, Val Loss: 0.1293\n",
            "Epoch 64/100, Train Loss: 0.0394, Val Loss: 0.1293\n",
            "Epoch 65/100, Train Loss: 0.0412, Val Loss: 0.1293\n",
            "Epoch 66/100, Train Loss: 0.0404, Val Loss: 0.1293\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0341, Val Loss: 0.1293\n",
            "Epoch 68/100, Train Loss: 0.0359, Val Loss: 0.1293\n",
            "Epoch 69/100, Train Loss: 0.0374, Val Loss: 0.1293\n",
            "Epoch 70/100, Train Loss: 0.0341, Val Loss: 0.1293\n",
            "Epoch 71/100, Train Loss: 0.0353, Val Loss: 0.1293\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0341, Val Loss: 0.1293\n",
            "Epoch 73/100, Train Loss: 0.0351, Val Loss: 0.1293\n",
            "Epoch 74/100, Train Loss: 0.0394, Val Loss: 0.1293\n",
            "Epoch 75/100, Train Loss: 0.0332, Val Loss: 0.1293\n",
            "Epoch 76/100, Train Loss: 0.0357, Val Loss: 0.1293\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0361, Val Loss: 0.1293\n",
            "Epoch 78/100, Train Loss: 0.0351, Val Loss: 0.1293\n",
            "Epoch 79/100, Train Loss: 0.0344, Val Loss: 0.1293\n",
            "Epoch 80/100, Train Loss: 0.0335, Val Loss: 0.1293\n",
            "Epoch 81/100, Train Loss: 0.0344, Val Loss: 0.1293\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0372, Val Loss: 0.1293\n",
            "Epoch 83/100, Train Loss: 0.0322, Val Loss: 0.1293\n",
            "Epoch 84/100, Train Loss: 0.0338, Val Loss: 0.1293\n",
            "Epoch 85/100, Train Loss: 0.0356, Val Loss: 0.1293\n",
            "Epoch 86/100, Train Loss: 0.0349, Val Loss: 0.1293\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0357, Val Loss: 0.1293\n",
            "Epoch 88/100, Train Loss: 0.0378, Val Loss: 0.1293\n",
            "Epoch 89/100, Train Loss: 0.0330, Val Loss: 0.1293\n",
            "Epoch 90/100, Train Loss: 0.0322, Val Loss: 0.1293\n",
            "Epoch 91/100, Train Loss: 0.0370, Val Loss: 0.1293\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0347, Val Loss: 0.1293\n",
            "Epoch 93/100, Train Loss: 0.0358, Val Loss: 0.1293\n",
            "Epoch 94/100, Train Loss: 0.0382, Val Loss: 0.1293\n",
            "Epoch 95/100, Train Loss: 0.0364, Val Loss: 0.1293\n",
            "Epoch 96/100, Train Loss: 0.0374, Val Loss: 0.1293\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0355, Val Loss: 0.1293\n",
            "Epoch 98/100, Train Loss: 0.0382, Val Loss: 0.1293\n",
            "Epoch 99/100, Train Loss: 0.0371, Val Loss: 0.1293\n",
            "Epoch 100/100, Train Loss: 0.0318, Val Loss: 0.1293\n",
            "\n",
            "Test iteration 3/14\n",
            "Current training set size: 115 samples\n",
            "Epoch 1/100, Train Loss: 0.1368, Val Loss: 0.3700\n",
            "Epoch 2/100, Train Loss: 0.0912, Val Loss: 0.2330\n",
            "Epoch 3/100, Train Loss: 0.0811, Val Loss: 0.2088\n",
            "Epoch 4/100, Train Loss: 0.0686, Val Loss: 0.1911\n",
            "Epoch 5/100, Train Loss: 0.0684, Val Loss: 0.1866\n",
            "Epoch 6/100, Train Loss: 0.0626, Val Loss: 0.1432\n",
            "Epoch 7/100, Train Loss: 0.0608, Val Loss: 0.1351\n",
            "Epoch 8/100, Train Loss: 0.0586, Val Loss: 0.0984\n",
            "Epoch 9/100, Train Loss: 0.0527, Val Loss: 0.1175\n",
            "Epoch 10/100, Train Loss: 0.0498, Val Loss: 0.0898\n",
            "Epoch 11/100, Train Loss: 0.0515, Val Loss: 0.0553\n",
            "Epoch 12/100, Train Loss: 0.0449, Val Loss: 0.0630\n",
            "Epoch 13/100, Train Loss: 0.0450, Val Loss: 0.0488\n",
            "Epoch 14/100, Train Loss: 0.0423, Val Loss: 0.0976\n",
            "Epoch 15/100, Train Loss: 0.0511, Val Loss: 0.0506\n",
            "Epoch 16/100, Train Loss: 0.0437, Val Loss: 0.0985\n",
            "Epoch 17/100, Train Loss: 0.0400, Val Loss: 0.0527\n",
            "Epoch 18/100, Train Loss: 0.0503, Val Loss: 0.1074\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0461, Val Loss: 0.1024\n",
            "Epoch 20/100, Train Loss: 0.0414, Val Loss: 0.0967\n",
            "Epoch 21/100, Train Loss: 0.0379, Val Loss: 0.0923\n",
            "Epoch 22/100, Train Loss: 0.0396, Val Loss: 0.0900\n",
            "Epoch 23/100, Train Loss: 0.0379, Val Loss: 0.0880\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0386, Val Loss: 0.0880\n",
            "Epoch 25/100, Train Loss: 0.0384, Val Loss: 0.0879\n",
            "Epoch 26/100, Train Loss: 0.0372, Val Loss: 0.0879\n",
            "Epoch 27/100, Train Loss: 0.0400, Val Loss: 0.0879\n",
            "Epoch 28/100, Train Loss: 0.0358, Val Loss: 0.0879\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0381, Val Loss: 0.0879\n",
            "Epoch 30/100, Train Loss: 0.0383, Val Loss: 0.0879\n",
            "Epoch 31/100, Train Loss: 0.0395, Val Loss: 0.0879\n",
            "Epoch 32/100, Train Loss: 0.0370, Val Loss: 0.0879\n",
            "Epoch 33/100, Train Loss: 0.0371, Val Loss: 0.0879\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0372, Val Loss: 0.0879\n",
            "Epoch 35/100, Train Loss: 0.0390, Val Loss: 0.0879\n",
            "Epoch 36/100, Train Loss: 0.0422, Val Loss: 0.0879\n",
            "Epoch 37/100, Train Loss: 0.0376, Val Loss: 0.0879\n",
            "Epoch 38/100, Train Loss: 0.0386, Val Loss: 0.0879\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0387, Val Loss: 0.0879\n",
            "Epoch 40/100, Train Loss: 0.0355, Val Loss: 0.0879\n",
            "Epoch 41/100, Train Loss: 0.0404, Val Loss: 0.0879\n",
            "Epoch 42/100, Train Loss: 0.0372, Val Loss: 0.0879\n",
            "Epoch 43/100, Train Loss: 0.0378, Val Loss: 0.0879\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0377, Val Loss: 0.0879\n",
            "Epoch 45/100, Train Loss: 0.0399, Val Loss: 0.0879\n",
            "Epoch 46/100, Train Loss: 0.0384, Val Loss: 0.0879\n",
            "Epoch 47/100, Train Loss: 0.0387, Val Loss: 0.0879\n",
            "Epoch 48/100, Train Loss: 0.0337, Val Loss: 0.0879\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0389, Val Loss: 0.0879\n",
            "Epoch 50/100, Train Loss: 0.0357, Val Loss: 0.0879\n",
            "Epoch 51/100, Train Loss: 0.0379, Val Loss: 0.0879\n",
            "Epoch 52/100, Train Loss: 0.0331, Val Loss: 0.0879\n",
            "Epoch 53/100, Train Loss: 0.0386, Val Loss: 0.0879\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0394, Val Loss: 0.0879\n",
            "Epoch 55/100, Train Loss: 0.0385, Val Loss: 0.0879\n",
            "Epoch 56/100, Train Loss: 0.0385, Val Loss: 0.0879\n",
            "Epoch 57/100, Train Loss: 0.0374, Val Loss: 0.0879\n",
            "Epoch 58/100, Train Loss: 0.0368, Val Loss: 0.0879\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0381, Val Loss: 0.0879\n",
            "Epoch 60/100, Train Loss: 0.0374, Val Loss: 0.0879\n",
            "Epoch 61/100, Train Loss: 0.0401, Val Loss: 0.0879\n",
            "Epoch 62/100, Train Loss: 0.0371, Val Loss: 0.0879\n",
            "Epoch 63/100, Train Loss: 0.0379, Val Loss: 0.0879\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0381, Val Loss: 0.0879\n",
            "Epoch 65/100, Train Loss: 0.0415, Val Loss: 0.0879\n",
            "Epoch 66/100, Train Loss: 0.0355, Val Loss: 0.0879\n",
            "Epoch 67/100, Train Loss: 0.0366, Val Loss: 0.0879\n",
            "Epoch 68/100, Train Loss: 0.0365, Val Loss: 0.0879\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0351, Val Loss: 0.0879\n",
            "Epoch 70/100, Train Loss: 0.0389, Val Loss: 0.0879\n",
            "Epoch 71/100, Train Loss: 0.0367, Val Loss: 0.0879\n",
            "Epoch 72/100, Train Loss: 0.0409, Val Loss: 0.0879\n",
            "Epoch 73/100, Train Loss: 0.0389, Val Loss: 0.0879\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0360, Val Loss: 0.0879\n",
            "Epoch 75/100, Train Loss: 0.0397, Val Loss: 0.0879\n",
            "Epoch 76/100, Train Loss: 0.0368, Val Loss: 0.0879\n",
            "Epoch 77/100, Train Loss: 0.0375, Val Loss: 0.0879\n",
            "Epoch 78/100, Train Loss: 0.0406, Val Loss: 0.0879\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0375, Val Loss: 0.0879\n",
            "Epoch 80/100, Train Loss: 0.0376, Val Loss: 0.0879\n",
            "Epoch 81/100, Train Loss: 0.0394, Val Loss: 0.0879\n",
            "Epoch 82/100, Train Loss: 0.0371, Val Loss: 0.0879\n",
            "Epoch 83/100, Train Loss: 0.0389, Val Loss: 0.0879\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0373, Val Loss: 0.0879\n",
            "Epoch 85/100, Train Loss: 0.0381, Val Loss: 0.0879\n",
            "Epoch 86/100, Train Loss: 0.0398, Val Loss: 0.0879\n",
            "Epoch 87/100, Train Loss: 0.0409, Val Loss: 0.0879\n",
            "Epoch 88/100, Train Loss: 0.0379, Val Loss: 0.0879\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0365, Val Loss: 0.0879\n",
            "Epoch 90/100, Train Loss: 0.0386, Val Loss: 0.0879\n",
            "Epoch 91/100, Train Loss: 0.0385, Val Loss: 0.0879\n",
            "Epoch 92/100, Train Loss: 0.0387, Val Loss: 0.0879\n",
            "Epoch 93/100, Train Loss: 0.0368, Val Loss: 0.0879\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0396, Val Loss: 0.0879\n",
            "Epoch 95/100, Train Loss: 0.0402, Val Loss: 0.0879\n",
            "Epoch 96/100, Train Loss: 0.0367, Val Loss: 0.0879\n",
            "Epoch 97/100, Train Loss: 0.0369, Val Loss: 0.0879\n",
            "Epoch 98/100, Train Loss: 0.0377, Val Loss: 0.0879\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0380, Val Loss: 0.0879\n",
            "Epoch 100/100, Train Loss: 0.0365, Val Loss: 0.0879\n",
            "\n",
            "Test iteration 4/14\n",
            "Current training set size: 116 samples\n",
            "Epoch 1/100, Train Loss: 0.1453, Val Loss: 0.3524\n",
            "Epoch 2/100, Train Loss: 0.0976, Val Loss: 0.2506\n",
            "Epoch 3/100, Train Loss: 0.0728, Val Loss: 0.1960\n",
            "Epoch 4/100, Train Loss: 0.0687, Val Loss: 0.1906\n",
            "Epoch 5/100, Train Loss: 0.0676, Val Loss: 0.1777\n",
            "Epoch 6/100, Train Loss: 0.0657, Val Loss: 0.1401\n",
            "Epoch 7/100, Train Loss: 0.0638, Val Loss: 0.1310\n",
            "Epoch 8/100, Train Loss: 0.0562, Val Loss: 0.1290\n",
            "Epoch 9/100, Train Loss: 0.0527, Val Loss: 0.0755\n",
            "Epoch 10/100, Train Loss: 0.0487, Val Loss: 0.0660\n",
            "Epoch 11/100, Train Loss: 0.0441, Val Loss: 0.0732\n",
            "Epoch 12/100, Train Loss: 0.0486, Val Loss: 0.0492\n",
            "Epoch 13/100, Train Loss: 0.0494, Val Loss: 0.0910\n",
            "Epoch 14/100, Train Loss: 0.0514, Val Loss: 0.0466\n",
            "Epoch 15/100, Train Loss: 0.0484, Val Loss: 0.0511\n",
            "Epoch 16/100, Train Loss: 0.0431, Val Loss: 0.0608\n",
            "Epoch 17/100, Train Loss: 0.0440, Val Loss: 0.0480\n",
            "Epoch 18/100, Train Loss: 0.0415, Val Loss: 0.0477\n",
            "Epoch 19/100, Train Loss: 0.0399, Val Loss: 0.0901\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0376, Val Loss: 0.0861\n",
            "Epoch 21/100, Train Loss: 0.0376, Val Loss: 0.0823\n",
            "Epoch 22/100, Train Loss: 0.0354, Val Loss: 0.0783\n",
            "Epoch 23/100, Train Loss: 0.0355, Val Loss: 0.0739\n",
            "Epoch 24/100, Train Loss: 0.0366, Val Loss: 0.0704\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0358, Val Loss: 0.0704\n",
            "Epoch 26/100, Train Loss: 0.0364, Val Loss: 0.0703\n",
            "Epoch 27/100, Train Loss: 0.0375, Val Loss: 0.0703\n",
            "Epoch 28/100, Train Loss: 0.0361, Val Loss: 0.0702\n",
            "Epoch 29/100, Train Loss: 0.0369, Val Loss: 0.0702\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0341, Val Loss: 0.0702\n",
            "Epoch 31/100, Train Loss: 0.0334, Val Loss: 0.0702\n",
            "Epoch 32/100, Train Loss: 0.0374, Val Loss: 0.0702\n",
            "Epoch 33/100, Train Loss: 0.0348, Val Loss: 0.0702\n",
            "Epoch 34/100, Train Loss: 0.0400, Val Loss: 0.0702\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0329, Val Loss: 0.0702\n",
            "Epoch 36/100, Train Loss: 0.0349, Val Loss: 0.0702\n",
            "Epoch 37/100, Train Loss: 0.0383, Val Loss: 0.0702\n",
            "Epoch 38/100, Train Loss: 0.0379, Val Loss: 0.0702\n",
            "Epoch 39/100, Train Loss: 0.0373, Val Loss: 0.0702\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0369, Val Loss: 0.0702\n",
            "Epoch 41/100, Train Loss: 0.0390, Val Loss: 0.0702\n",
            "Epoch 42/100, Train Loss: 0.0368, Val Loss: 0.0702\n",
            "Epoch 43/100, Train Loss: 0.0383, Val Loss: 0.0702\n",
            "Epoch 44/100, Train Loss: 0.0366, Val Loss: 0.0702\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0383, Val Loss: 0.0702\n",
            "Epoch 46/100, Train Loss: 0.0367, Val Loss: 0.0702\n",
            "Epoch 47/100, Train Loss: 0.0363, Val Loss: 0.0702\n",
            "Epoch 48/100, Train Loss: 0.0372, Val Loss: 0.0702\n",
            "Epoch 49/100, Train Loss: 0.0347, Val Loss: 0.0702\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0357, Val Loss: 0.0702\n",
            "Epoch 51/100, Train Loss: 0.0391, Val Loss: 0.0702\n",
            "Epoch 52/100, Train Loss: 0.0370, Val Loss: 0.0702\n",
            "Epoch 53/100, Train Loss: 0.0365, Val Loss: 0.0702\n",
            "Epoch 54/100, Train Loss: 0.0367, Val Loss: 0.0702\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0376, Val Loss: 0.0702\n",
            "Epoch 56/100, Train Loss: 0.0367, Val Loss: 0.0702\n",
            "Epoch 57/100, Train Loss: 0.0363, Val Loss: 0.0702\n",
            "Epoch 58/100, Train Loss: 0.0352, Val Loss: 0.0702\n",
            "Epoch 59/100, Train Loss: 0.0386, Val Loss: 0.0702\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0353, Val Loss: 0.0702\n",
            "Epoch 61/100, Train Loss: 0.0392, Val Loss: 0.0702\n",
            "Epoch 62/100, Train Loss: 0.0342, Val Loss: 0.0702\n",
            "Epoch 63/100, Train Loss: 0.0374, Val Loss: 0.0702\n",
            "Epoch 64/100, Train Loss: 0.0386, Val Loss: 0.0702\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0369, Val Loss: 0.0702\n",
            "Epoch 66/100, Train Loss: 0.0348, Val Loss: 0.0702\n",
            "Epoch 67/100, Train Loss: 0.0346, Val Loss: 0.0702\n",
            "Epoch 68/100, Train Loss: 0.0370, Val Loss: 0.0702\n",
            "Epoch 69/100, Train Loss: 0.0354, Val Loss: 0.0702\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0321, Val Loss: 0.0702\n",
            "Epoch 71/100, Train Loss: 0.0322, Val Loss: 0.0702\n",
            "Epoch 72/100, Train Loss: 0.0373, Val Loss: 0.0702\n",
            "Epoch 73/100, Train Loss: 0.0412, Val Loss: 0.0702\n",
            "Epoch 74/100, Train Loss: 0.0375, Val Loss: 0.0702\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0370, Val Loss: 0.0702\n",
            "Epoch 76/100, Train Loss: 0.0374, Val Loss: 0.0702\n",
            "Epoch 77/100, Train Loss: 0.0386, Val Loss: 0.0702\n",
            "Epoch 78/100, Train Loss: 0.0364, Val Loss: 0.0702\n",
            "Epoch 79/100, Train Loss: 0.0381, Val Loss: 0.0702\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0352, Val Loss: 0.0702\n",
            "Epoch 81/100, Train Loss: 0.0362, Val Loss: 0.0702\n",
            "Epoch 82/100, Train Loss: 0.0363, Val Loss: 0.0702\n",
            "Epoch 83/100, Train Loss: 0.0367, Val Loss: 0.0702\n",
            "Epoch 84/100, Train Loss: 0.0373, Val Loss: 0.0702\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0335, Val Loss: 0.0702\n",
            "Epoch 86/100, Train Loss: 0.0391, Val Loss: 0.0702\n",
            "Epoch 87/100, Train Loss: 0.0375, Val Loss: 0.0702\n",
            "Epoch 88/100, Train Loss: 0.0413, Val Loss: 0.0702\n",
            "Epoch 89/100, Train Loss: 0.0389, Val Loss: 0.0702\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0388, Val Loss: 0.0702\n",
            "Epoch 91/100, Train Loss: 0.0380, Val Loss: 0.0702\n",
            "Epoch 92/100, Train Loss: 0.0377, Val Loss: 0.0702\n",
            "Epoch 93/100, Train Loss: 0.0379, Val Loss: 0.0702\n",
            "Epoch 94/100, Train Loss: 0.0346, Val Loss: 0.0702\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0377, Val Loss: 0.0702\n",
            "Epoch 96/100, Train Loss: 0.0353, Val Loss: 0.0702\n",
            "Epoch 97/100, Train Loss: 0.0358, Val Loss: 0.0702\n",
            "Epoch 98/100, Train Loss: 0.0375, Val Loss: 0.0702\n",
            "Epoch 99/100, Train Loss: 0.0369, Val Loss: 0.0702\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0392, Val Loss: 0.0702\n",
            "\n",
            "Test iteration 5/14\n",
            "Current training set size: 117 samples\n",
            "Epoch 1/100, Train Loss: 0.1394, Val Loss: 0.3772\n",
            "Epoch 2/100, Train Loss: 0.1027, Val Loss: 0.2583\n",
            "Epoch 3/100, Train Loss: 0.0777, Val Loss: 0.1908\n",
            "Epoch 4/100, Train Loss: 0.0752, Val Loss: 0.1829\n",
            "Epoch 5/100, Train Loss: 0.0655, Val Loss: 0.1534\n",
            "Epoch 6/100, Train Loss: 0.0600, Val Loss: 0.0734\n",
            "Epoch 7/100, Train Loss: 0.0624, Val Loss: 0.1241\n",
            "Epoch 8/100, Train Loss: 0.0523, Val Loss: 0.0530\n",
            "Epoch 9/100, Train Loss: 0.0508, Val Loss: 0.0894\n",
            "Epoch 10/100, Train Loss: 0.0619, Val Loss: 0.0754\n",
            "Epoch 11/100, Train Loss: 0.0518, Val Loss: 0.0518\n",
            "Epoch 12/100, Train Loss: 0.0520, Val Loss: 0.0452\n",
            "Epoch 13/100, Train Loss: 0.0463, Val Loss: 0.0386\n",
            "Epoch 14/100, Train Loss: 0.0520, Val Loss: 0.0454\n",
            "Epoch 15/100, Train Loss: 0.0426, Val Loss: 0.0374\n",
            "Epoch 16/100, Train Loss: 0.0470, Val Loss: 0.0611\n",
            "Epoch 17/100, Train Loss: 0.0454, Val Loss: 0.0530\n",
            "Epoch 18/100, Train Loss: 0.0478, Val Loss: 0.0431\n",
            "Epoch 19/100, Train Loss: 0.0440, Val Loss: 0.0478\n",
            "Epoch 20/100, Train Loss: 0.0376, Val Loss: 0.0419\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0391, Val Loss: 0.0409\n",
            "Epoch 22/100, Train Loss: 0.0394, Val Loss: 0.0397\n",
            "Epoch 23/100, Train Loss: 0.0370, Val Loss: 0.0396\n",
            "Epoch 24/100, Train Loss: 0.0390, Val Loss: 0.0398\n",
            "Epoch 25/100, Train Loss: 0.0363, Val Loss: 0.0399\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0370, Val Loss: 0.0400\n",
            "Epoch 27/100, Train Loss: 0.0398, Val Loss: 0.0400\n",
            "Epoch 28/100, Train Loss: 0.0333, Val Loss: 0.0400\n",
            "Epoch 29/100, Train Loss: 0.0390, Val Loss: 0.0400\n",
            "Epoch 30/100, Train Loss: 0.0352, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0350, Val Loss: 0.0400\n",
            "Epoch 32/100, Train Loss: 0.0343, Val Loss: 0.0400\n",
            "Epoch 33/100, Train Loss: 0.0377, Val Loss: 0.0400\n",
            "Epoch 34/100, Train Loss: 0.0396, Val Loss: 0.0400\n",
            "Epoch 35/100, Train Loss: 0.0360, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0366, Val Loss: 0.0400\n",
            "Epoch 37/100, Train Loss: 0.0406, Val Loss: 0.0400\n",
            "Epoch 38/100, Train Loss: 0.0376, Val Loss: 0.0400\n",
            "Epoch 39/100, Train Loss: 0.0370, Val Loss: 0.0400\n",
            "Epoch 40/100, Train Loss: 0.0369, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0360, Val Loss: 0.0400\n",
            "Epoch 42/100, Train Loss: 0.0347, Val Loss: 0.0400\n",
            "Epoch 43/100, Train Loss: 0.0410, Val Loss: 0.0400\n",
            "Epoch 44/100, Train Loss: 0.0385, Val Loss: 0.0400\n",
            "Epoch 45/100, Train Loss: 0.0389, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0375, Val Loss: 0.0400\n",
            "Epoch 47/100, Train Loss: 0.0368, Val Loss: 0.0400\n",
            "Epoch 48/100, Train Loss: 0.0353, Val Loss: 0.0400\n",
            "Epoch 49/100, Train Loss: 0.0411, Val Loss: 0.0400\n",
            "Epoch 50/100, Train Loss: 0.0386, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0428, Val Loss: 0.0400\n",
            "Epoch 52/100, Train Loss: 0.0367, Val Loss: 0.0400\n",
            "Epoch 53/100, Train Loss: 0.0385, Val Loss: 0.0400\n",
            "Epoch 54/100, Train Loss: 0.0352, Val Loss: 0.0400\n",
            "Epoch 55/100, Train Loss: 0.0384, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0355, Val Loss: 0.0400\n",
            "Epoch 57/100, Train Loss: 0.0376, Val Loss: 0.0400\n",
            "Epoch 58/100, Train Loss: 0.0367, Val Loss: 0.0400\n",
            "Epoch 59/100, Train Loss: 0.0383, Val Loss: 0.0400\n",
            "Epoch 60/100, Train Loss: 0.0388, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0355, Val Loss: 0.0400\n",
            "Epoch 62/100, Train Loss: 0.0395, Val Loss: 0.0400\n",
            "Epoch 63/100, Train Loss: 0.0397, Val Loss: 0.0400\n",
            "Epoch 64/100, Train Loss: 0.0356, Val Loss: 0.0400\n",
            "Epoch 65/100, Train Loss: 0.0324, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0427, Val Loss: 0.0400\n",
            "Epoch 67/100, Train Loss: 0.0357, Val Loss: 0.0400\n",
            "Epoch 68/100, Train Loss: 0.0336, Val Loss: 0.0400\n",
            "Epoch 69/100, Train Loss: 0.0390, Val Loss: 0.0400\n",
            "Epoch 70/100, Train Loss: 0.0427, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0404, Val Loss: 0.0400\n",
            "Epoch 72/100, Train Loss: 0.0348, Val Loss: 0.0400\n",
            "Epoch 73/100, Train Loss: 0.0394, Val Loss: 0.0400\n",
            "Epoch 74/100, Train Loss: 0.0337, Val Loss: 0.0400\n",
            "Epoch 75/100, Train Loss: 0.0393, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0367, Val Loss: 0.0400\n",
            "Epoch 77/100, Train Loss: 0.0376, Val Loss: 0.0400\n",
            "Epoch 78/100, Train Loss: 0.0407, Val Loss: 0.0400\n",
            "Epoch 79/100, Train Loss: 0.0405, Val Loss: 0.0400\n",
            "Epoch 80/100, Train Loss: 0.0400, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0394, Val Loss: 0.0400\n",
            "Epoch 82/100, Train Loss: 0.0346, Val Loss: 0.0400\n",
            "Epoch 83/100, Train Loss: 0.0378, Val Loss: 0.0400\n",
            "Epoch 84/100, Train Loss: 0.0418, Val Loss: 0.0400\n",
            "Epoch 85/100, Train Loss: 0.0373, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0401, Val Loss: 0.0400\n",
            "Epoch 87/100, Train Loss: 0.0408, Val Loss: 0.0400\n",
            "Epoch 88/100, Train Loss: 0.0400, Val Loss: 0.0400\n",
            "Epoch 89/100, Train Loss: 0.0369, Val Loss: 0.0400\n",
            "Epoch 90/100, Train Loss: 0.0373, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0320, Val Loss: 0.0400\n",
            "Epoch 92/100, Train Loss: 0.0341, Val Loss: 0.0400\n",
            "Epoch 93/100, Train Loss: 0.0375, Val Loss: 0.0400\n",
            "Epoch 94/100, Train Loss: 0.0365, Val Loss: 0.0400\n",
            "Epoch 95/100, Train Loss: 0.0349, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0378, Val Loss: 0.0400\n",
            "Epoch 97/100, Train Loss: 0.0356, Val Loss: 0.0400\n",
            "Epoch 98/100, Train Loss: 0.0360, Val Loss: 0.0400\n",
            "Epoch 99/100, Train Loss: 0.0358, Val Loss: 0.0400\n",
            "Epoch 100/100, Train Loss: 0.0379, Val Loss: 0.0400\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 6/14\n",
            "Current training set size: 118 samples\n",
            "Epoch 1/100, Train Loss: 0.1446, Val Loss: 0.3152\n",
            "Epoch 2/100, Train Loss: 0.0946, Val Loss: 0.2305\n",
            "Epoch 3/100, Train Loss: 0.0801, Val Loss: 0.1913\n",
            "Epoch 4/100, Train Loss: 0.0722, Val Loss: 0.1711\n",
            "Epoch 5/100, Train Loss: 0.0673, Val Loss: 0.1331\n",
            "Epoch 6/100, Train Loss: 0.0613, Val Loss: 0.1124\n",
            "Epoch 7/100, Train Loss: 0.0557, Val Loss: 0.0582\n",
            "Epoch 8/100, Train Loss: 0.0604, Val Loss: 0.0471\n",
            "Epoch 9/100, Train Loss: 0.0583, Val Loss: 0.0624\n",
            "Epoch 10/100, Train Loss: 0.0565, Val Loss: 0.0483\n",
            "Epoch 11/100, Train Loss: 0.0489, Val Loss: 0.0434\n",
            "Epoch 12/100, Train Loss: 0.0515, Val Loss: 0.0923\n",
            "Epoch 13/100, Train Loss: 0.0452, Val Loss: 0.0465\n",
            "Epoch 14/100, Train Loss: 0.0477, Val Loss: 0.0477\n",
            "Epoch 15/100, Train Loss: 0.0457, Val Loss: 0.0602\n",
            "Epoch 16/100, Train Loss: 0.0484, Val Loss: 0.0469\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0465, Val Loss: 0.0472\n",
            "Epoch 18/100, Train Loss: 0.0433, Val Loss: 0.0475\n",
            "Epoch 19/100, Train Loss: 0.0429, Val Loss: 0.0475\n",
            "Epoch 20/100, Train Loss: 0.0382, Val Loss: 0.0475\n",
            "Epoch 21/100, Train Loss: 0.0364, Val Loss: 0.0475\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0407, Val Loss: 0.0475\n",
            "Epoch 23/100, Train Loss: 0.0366, Val Loss: 0.0475\n",
            "Epoch 24/100, Train Loss: 0.0376, Val Loss: 0.0475\n",
            "Epoch 25/100, Train Loss: 0.0388, Val Loss: 0.0475\n",
            "Epoch 26/100, Train Loss: 0.0371, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0400, Val Loss: 0.0475\n",
            "Epoch 28/100, Train Loss: 0.0384, Val Loss: 0.0474\n",
            "Epoch 29/100, Train Loss: 0.0338, Val Loss: 0.0474\n",
            "Epoch 30/100, Train Loss: 0.0404, Val Loss: 0.0474\n",
            "Epoch 31/100, Train Loss: 0.0374, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0361, Val Loss: 0.0474\n",
            "Epoch 33/100, Train Loss: 0.0383, Val Loss: 0.0474\n",
            "Epoch 34/100, Train Loss: 0.0382, Val Loss: 0.0474\n",
            "Epoch 35/100, Train Loss: 0.0436, Val Loss: 0.0474\n",
            "Epoch 36/100, Train Loss: 0.0378, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0348, Val Loss: 0.0474\n",
            "Epoch 38/100, Train Loss: 0.0411, Val Loss: 0.0474\n",
            "Epoch 39/100, Train Loss: 0.0386, Val Loss: 0.0474\n",
            "Epoch 40/100, Train Loss: 0.0372, Val Loss: 0.0474\n",
            "Epoch 41/100, Train Loss: 0.0379, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0423, Val Loss: 0.0474\n",
            "Epoch 43/100, Train Loss: 0.0405, Val Loss: 0.0474\n",
            "Epoch 44/100, Train Loss: 0.0381, Val Loss: 0.0474\n",
            "Epoch 45/100, Train Loss: 0.0397, Val Loss: 0.0474\n",
            "Epoch 46/100, Train Loss: 0.0388, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0374, Val Loss: 0.0474\n",
            "Epoch 48/100, Train Loss: 0.0360, Val Loss: 0.0474\n",
            "Epoch 49/100, Train Loss: 0.0371, Val Loss: 0.0474\n",
            "Epoch 50/100, Train Loss: 0.0440, Val Loss: 0.0474\n",
            "Epoch 51/100, Train Loss: 0.0372, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0386, Val Loss: 0.0474\n",
            "Epoch 53/100, Train Loss: 0.0389, Val Loss: 0.0474\n",
            "Epoch 54/100, Train Loss: 0.0410, Val Loss: 0.0474\n",
            "Epoch 55/100, Train Loss: 0.0411, Val Loss: 0.0474\n",
            "Epoch 56/100, Train Loss: 0.0391, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0398, Val Loss: 0.0474\n",
            "Epoch 58/100, Train Loss: 0.0358, Val Loss: 0.0474\n",
            "Epoch 59/100, Train Loss: 0.0395, Val Loss: 0.0474\n",
            "Epoch 60/100, Train Loss: 0.0406, Val Loss: 0.0474\n",
            "Epoch 61/100, Train Loss: 0.0422, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0413, Val Loss: 0.0474\n",
            "Epoch 63/100, Train Loss: 0.0351, Val Loss: 0.0474\n",
            "Epoch 64/100, Train Loss: 0.0394, Val Loss: 0.0474\n",
            "Epoch 65/100, Train Loss: 0.0345, Val Loss: 0.0474\n",
            "Epoch 66/100, Train Loss: 0.0370, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0369, Val Loss: 0.0474\n",
            "Epoch 68/100, Train Loss: 0.0401, Val Loss: 0.0474\n",
            "Epoch 69/100, Train Loss: 0.0398, Val Loss: 0.0474\n",
            "Epoch 70/100, Train Loss: 0.0421, Val Loss: 0.0474\n",
            "Epoch 71/100, Train Loss: 0.0368, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0394, Val Loss: 0.0474\n",
            "Epoch 73/100, Train Loss: 0.0389, Val Loss: 0.0474\n",
            "Epoch 74/100, Train Loss: 0.0385, Val Loss: 0.0474\n",
            "Epoch 75/100, Train Loss: 0.0388, Val Loss: 0.0474\n",
            "Epoch 76/100, Train Loss: 0.0361, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0401, Val Loss: 0.0474\n",
            "Epoch 78/100, Train Loss: 0.0399, Val Loss: 0.0474\n",
            "Epoch 79/100, Train Loss: 0.0380, Val Loss: 0.0474\n",
            "Epoch 80/100, Train Loss: 0.0432, Val Loss: 0.0474\n",
            "Epoch 81/100, Train Loss: 0.0355, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0405, Val Loss: 0.0474\n",
            "Epoch 83/100, Train Loss: 0.0381, Val Loss: 0.0474\n",
            "Epoch 84/100, Train Loss: 0.0388, Val Loss: 0.0474\n",
            "Epoch 85/100, Train Loss: 0.0425, Val Loss: 0.0474\n",
            "Epoch 86/100, Train Loss: 0.0398, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0368, Val Loss: 0.0474\n",
            "Epoch 88/100, Train Loss: 0.0372, Val Loss: 0.0474\n",
            "Epoch 89/100, Train Loss: 0.0423, Val Loss: 0.0474\n",
            "Epoch 90/100, Train Loss: 0.0407, Val Loss: 0.0474\n",
            "Epoch 91/100, Train Loss: 0.0377, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0374, Val Loss: 0.0474\n",
            "Epoch 93/100, Train Loss: 0.0364, Val Loss: 0.0474\n",
            "Epoch 94/100, Train Loss: 0.0360, Val Loss: 0.0474\n",
            "Epoch 95/100, Train Loss: 0.0340, Val Loss: 0.0474\n",
            "Epoch 96/100, Train Loss: 0.0398, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0346, Val Loss: 0.0474\n",
            "Epoch 98/100, Train Loss: 0.0399, Val Loss: 0.0474\n",
            "Epoch 99/100, Train Loss: 0.0394, Val Loss: 0.0474\n",
            "Epoch 100/100, Train Loss: 0.0378, Val Loss: 0.0474\n",
            "\n",
            "Test iteration 7/14\n",
            "Current training set size: 119 samples\n",
            "Epoch 1/100, Train Loss: 0.1511, Val Loss: 0.3498\n",
            "Epoch 2/100, Train Loss: 0.0986, Val Loss: 0.2267\n",
            "Epoch 3/100, Train Loss: 0.0812, Val Loss: 0.1905\n",
            "Epoch 4/100, Train Loss: 0.0703, Val Loss: 0.1619\n",
            "Epoch 5/100, Train Loss: 0.0652, Val Loss: 0.0973\n",
            "Epoch 6/100, Train Loss: 0.0701, Val Loss: 0.0464\n",
            "Epoch 7/100, Train Loss: 0.0585, Val Loss: 0.0385\n",
            "Epoch 8/100, Train Loss: 0.0528, Val Loss: 0.0409\n",
            "Epoch 9/100, Train Loss: 0.0556, Val Loss: 0.0401\n",
            "Epoch 10/100, Train Loss: 0.0565, Val Loss: 0.0439\n",
            "Epoch 11/100, Train Loss: 0.0469, Val Loss: 0.0505\n",
            "Epoch 12/100, Train Loss: 0.0488, Val Loss: 0.1100\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 13/100, Train Loss: 0.0521, Val Loss: 0.1050\n",
            "Epoch 14/100, Train Loss: 0.0473, Val Loss: 0.0947\n",
            "Epoch 15/100, Train Loss: 0.0497, Val Loss: 0.0866\n",
            "Epoch 16/100, Train Loss: 0.0536, Val Loss: 0.0785\n",
            "Epoch 17/100, Train Loss: 0.0471, Val Loss: 0.0740\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0472, Val Loss: 0.0740\n",
            "Epoch 19/100, Train Loss: 0.0518, Val Loss: 0.0739\n",
            "Epoch 20/100, Train Loss: 0.0448, Val Loss: 0.0739\n",
            "Epoch 21/100, Train Loss: 0.0466, Val Loss: 0.0739\n",
            "Epoch 22/100, Train Loss: 0.0466, Val Loss: 0.0738\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0442, Val Loss: 0.0738\n",
            "Epoch 24/100, Train Loss: 0.0459, Val Loss: 0.0738\n",
            "Epoch 25/100, Train Loss: 0.0474, Val Loss: 0.0738\n",
            "Epoch 26/100, Train Loss: 0.0489, Val Loss: 0.0738\n",
            "Epoch 27/100, Train Loss: 0.0484, Val Loss: 0.0738\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0482, Val Loss: 0.0738\n",
            "Epoch 29/100, Train Loss: 0.0456, Val Loss: 0.0738\n",
            "Epoch 30/100, Train Loss: 0.0480, Val Loss: 0.0738\n",
            "Epoch 31/100, Train Loss: 0.0445, Val Loss: 0.0738\n",
            "Epoch 32/100, Train Loss: 0.0483, Val Loss: 0.0738\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0449, Val Loss: 0.0738\n",
            "Epoch 34/100, Train Loss: 0.0479, Val Loss: 0.0738\n",
            "Epoch 35/100, Train Loss: 0.0510, Val Loss: 0.0738\n",
            "Epoch 36/100, Train Loss: 0.0466, Val Loss: 0.0738\n",
            "Epoch 37/100, Train Loss: 0.0486, Val Loss: 0.0738\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0451, Val Loss: 0.0738\n",
            "Epoch 39/100, Train Loss: 0.0494, Val Loss: 0.0738\n",
            "Epoch 40/100, Train Loss: 0.0481, Val Loss: 0.0738\n",
            "Epoch 41/100, Train Loss: 0.0495, Val Loss: 0.0738\n",
            "Epoch 42/100, Train Loss: 0.0453, Val Loss: 0.0738\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0500, Val Loss: 0.0738\n",
            "Epoch 44/100, Train Loss: 0.0453, Val Loss: 0.0738\n",
            "Epoch 45/100, Train Loss: 0.0457, Val Loss: 0.0738\n",
            "Epoch 46/100, Train Loss: 0.0507, Val Loss: 0.0738\n",
            "Epoch 47/100, Train Loss: 0.0482, Val Loss: 0.0738\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0489, Val Loss: 0.0738\n",
            "Epoch 49/100, Train Loss: 0.0495, Val Loss: 0.0738\n",
            "Epoch 50/100, Train Loss: 0.0500, Val Loss: 0.0738\n",
            "Epoch 51/100, Train Loss: 0.0439, Val Loss: 0.0738\n",
            "Epoch 52/100, Train Loss: 0.0492, Val Loss: 0.0738\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0467, Val Loss: 0.0738\n",
            "Epoch 54/100, Train Loss: 0.0498, Val Loss: 0.0738\n",
            "Epoch 55/100, Train Loss: 0.0465, Val Loss: 0.0738\n",
            "Epoch 56/100, Train Loss: 0.0432, Val Loss: 0.0738\n",
            "Epoch 57/100, Train Loss: 0.0484, Val Loss: 0.0738\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0435, Val Loss: 0.0738\n",
            "Epoch 59/100, Train Loss: 0.0476, Val Loss: 0.0738\n",
            "Epoch 60/100, Train Loss: 0.0456, Val Loss: 0.0738\n",
            "Epoch 61/100, Train Loss: 0.0446, Val Loss: 0.0738\n",
            "Epoch 62/100, Train Loss: 0.0466, Val Loss: 0.0738\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0483, Val Loss: 0.0738\n",
            "Epoch 64/100, Train Loss: 0.0470, Val Loss: 0.0738\n",
            "Epoch 65/100, Train Loss: 0.0506, Val Loss: 0.0738\n",
            "Epoch 66/100, Train Loss: 0.0474, Val Loss: 0.0738\n",
            "Epoch 67/100, Train Loss: 0.0445, Val Loss: 0.0738\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0474, Val Loss: 0.0738\n",
            "Epoch 69/100, Train Loss: 0.0497, Val Loss: 0.0738\n",
            "Epoch 70/100, Train Loss: 0.0523, Val Loss: 0.0738\n",
            "Epoch 71/100, Train Loss: 0.0468, Val Loss: 0.0738\n",
            "Epoch 72/100, Train Loss: 0.0456, Val Loss: 0.0738\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0494, Val Loss: 0.0738\n",
            "Epoch 74/100, Train Loss: 0.0474, Val Loss: 0.0738\n",
            "Epoch 75/100, Train Loss: 0.0474, Val Loss: 0.0738\n",
            "Epoch 76/100, Train Loss: 0.0468, Val Loss: 0.0738\n",
            "Epoch 77/100, Train Loss: 0.0464, Val Loss: 0.0738\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0488, Val Loss: 0.0738\n",
            "Epoch 79/100, Train Loss: 0.0448, Val Loss: 0.0738\n",
            "Epoch 80/100, Train Loss: 0.0485, Val Loss: 0.0738\n",
            "Epoch 81/100, Train Loss: 0.0462, Val Loss: 0.0738\n",
            "Epoch 82/100, Train Loss: 0.0471, Val Loss: 0.0738\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0454, Val Loss: 0.0738\n",
            "Epoch 84/100, Train Loss: 0.0487, Val Loss: 0.0738\n",
            "Epoch 85/100, Train Loss: 0.0450, Val Loss: 0.0738\n",
            "Epoch 86/100, Train Loss: 0.0441, Val Loss: 0.0738\n",
            "Epoch 87/100, Train Loss: 0.0458, Val Loss: 0.0738\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0479, Val Loss: 0.0738\n",
            "Epoch 89/100, Train Loss: 0.0490, Val Loss: 0.0738\n",
            "Epoch 90/100, Train Loss: 0.0453, Val Loss: 0.0738\n",
            "Epoch 91/100, Train Loss: 0.0460, Val Loss: 0.0738\n",
            "Epoch 92/100, Train Loss: 0.0479, Val Loss: 0.0738\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0463, Val Loss: 0.0738\n",
            "Epoch 94/100, Train Loss: 0.0467, Val Loss: 0.0738\n",
            "Epoch 95/100, Train Loss: 0.0488, Val Loss: 0.0738\n",
            "Epoch 96/100, Train Loss: 0.0466, Val Loss: 0.0738\n",
            "Epoch 97/100, Train Loss: 0.0459, Val Loss: 0.0738\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0511, Val Loss: 0.0738\n",
            "Epoch 99/100, Train Loss: 0.0480, Val Loss: 0.0738\n",
            "Epoch 100/100, Train Loss: 0.0447, Val Loss: 0.0738\n",
            "\n",
            "Test iteration 8/14\n",
            "Current training set size: 120 samples\n",
            "Epoch 1/100, Train Loss: 0.1538, Val Loss: 0.3087\n",
            "Epoch 2/100, Train Loss: 0.1012, Val Loss: 0.2150\n",
            "Epoch 3/100, Train Loss: 0.0778, Val Loss: 0.1908\n",
            "Epoch 4/100, Train Loss: 0.0757, Val Loss: 0.1642\n",
            "Epoch 5/100, Train Loss: 0.0642, Val Loss: 0.1456\n",
            "Epoch 6/100, Train Loss: 0.0665, Val Loss: 0.1430\n",
            "Epoch 7/100, Train Loss: 0.0606, Val Loss: 0.0651\n",
            "Epoch 8/100, Train Loss: 0.0625, Val Loss: 0.0470\n",
            "Epoch 9/100, Train Loss: 0.0498, Val Loss: 0.0625\n",
            "Epoch 10/100, Train Loss: 0.0542, Val Loss: 0.1055\n",
            "Epoch 11/100, Train Loss: 0.0478, Val Loss: 0.0449\n",
            "Epoch 12/100, Train Loss: 0.0474, Val Loss: 0.0477\n",
            "Epoch 13/100, Train Loss: 0.0538, Val Loss: 0.0523\n",
            "Epoch 14/100, Train Loss: 0.0452, Val Loss: 0.0454\n",
            "Epoch 15/100, Train Loss: 0.0471, Val Loss: 0.0768\n",
            "Epoch 16/100, Train Loss: 0.0407, Val Loss: 0.0504\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0429, Val Loss: 0.0502\n",
            "Epoch 18/100, Train Loss: 0.0415, Val Loss: 0.0499\n",
            "Epoch 19/100, Train Loss: 0.0393, Val Loss: 0.0498\n",
            "Epoch 20/100, Train Loss: 0.0390, Val Loss: 0.0497\n",
            "Epoch 21/100, Train Loss: 0.0364, Val Loss: 0.0496\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0418, Val Loss: 0.0496\n",
            "Epoch 23/100, Train Loss: 0.0344, Val Loss: 0.0496\n",
            "Epoch 24/100, Train Loss: 0.0409, Val Loss: 0.0496\n",
            "Epoch 25/100, Train Loss: 0.0442, Val Loss: 0.0496\n",
            "Epoch 26/100, Train Loss: 0.0380, Val Loss: 0.0496\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0413, Val Loss: 0.0496\n",
            "Epoch 28/100, Train Loss: 0.0410, Val Loss: 0.0496\n",
            "Epoch 29/100, Train Loss: 0.0370, Val Loss: 0.0496\n",
            "Epoch 30/100, Train Loss: 0.0414, Val Loss: 0.0496\n",
            "Epoch 31/100, Train Loss: 0.0412, Val Loss: 0.0496\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0394, Val Loss: 0.0496\n",
            "Epoch 33/100, Train Loss: 0.0375, Val Loss: 0.0496\n",
            "Epoch 34/100, Train Loss: 0.0385, Val Loss: 0.0496\n",
            "Epoch 35/100, Train Loss: 0.0394, Val Loss: 0.0496\n",
            "Epoch 36/100, Train Loss: 0.0369, Val Loss: 0.0496\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0385, Val Loss: 0.0496\n",
            "Epoch 38/100, Train Loss: 0.0427, Val Loss: 0.0496\n",
            "Epoch 39/100, Train Loss: 0.0370, Val Loss: 0.0496\n",
            "Epoch 40/100, Train Loss: 0.0392, Val Loss: 0.0496\n",
            "Epoch 41/100, Train Loss: 0.0388, Val Loss: 0.0496\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0404, Val Loss: 0.0496\n",
            "Epoch 43/100, Train Loss: 0.0398, Val Loss: 0.0496\n",
            "Epoch 44/100, Train Loss: 0.0385, Val Loss: 0.0496\n",
            "Epoch 45/100, Train Loss: 0.0422, Val Loss: 0.0496\n",
            "Epoch 46/100, Train Loss: 0.0410, Val Loss: 0.0496\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0361, Val Loss: 0.0496\n",
            "Epoch 48/100, Train Loss: 0.0392, Val Loss: 0.0496\n",
            "Epoch 49/100, Train Loss: 0.0381, Val Loss: 0.0496\n",
            "Epoch 50/100, Train Loss: 0.0398, Val Loss: 0.0496\n",
            "Epoch 51/100, Train Loss: 0.0358, Val Loss: 0.0496\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0428, Val Loss: 0.0496\n",
            "Epoch 53/100, Train Loss: 0.0417, Val Loss: 0.0496\n",
            "Epoch 54/100, Train Loss: 0.0403, Val Loss: 0.0496\n",
            "Epoch 55/100, Train Loss: 0.0434, Val Loss: 0.0496\n",
            "Epoch 56/100, Train Loss: 0.0391, Val Loss: 0.0496\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0403, Val Loss: 0.0496\n",
            "Epoch 58/100, Train Loss: 0.0394, Val Loss: 0.0496\n",
            "Epoch 59/100, Train Loss: 0.0396, Val Loss: 0.0496\n",
            "Epoch 60/100, Train Loss: 0.0389, Val Loss: 0.0496\n",
            "Epoch 61/100, Train Loss: 0.0383, Val Loss: 0.0496\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0403, Val Loss: 0.0496\n",
            "Epoch 63/100, Train Loss: 0.0364, Val Loss: 0.0496\n",
            "Epoch 64/100, Train Loss: 0.0384, Val Loss: 0.0496\n",
            "Epoch 65/100, Train Loss: 0.0396, Val Loss: 0.0496\n",
            "Epoch 66/100, Train Loss: 0.0429, Val Loss: 0.0496\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0386, Val Loss: 0.0496\n",
            "Epoch 68/100, Train Loss: 0.0397, Val Loss: 0.0496\n",
            "Epoch 69/100, Train Loss: 0.0397, Val Loss: 0.0496\n",
            "Epoch 70/100, Train Loss: 0.0378, Val Loss: 0.0496\n",
            "Epoch 71/100, Train Loss: 0.0393, Val Loss: 0.0496\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0393, Val Loss: 0.0496\n",
            "Epoch 73/100, Train Loss: 0.0399, Val Loss: 0.0496\n",
            "Epoch 74/100, Train Loss: 0.0369, Val Loss: 0.0496\n",
            "Epoch 75/100, Train Loss: 0.0384, Val Loss: 0.0496\n",
            "Epoch 76/100, Train Loss: 0.0391, Val Loss: 0.0496\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0369, Val Loss: 0.0496\n",
            "Epoch 78/100, Train Loss: 0.0403, Val Loss: 0.0496\n",
            "Epoch 79/100, Train Loss: 0.0382, Val Loss: 0.0496\n",
            "Epoch 80/100, Train Loss: 0.0389, Val Loss: 0.0496\n",
            "Epoch 81/100, Train Loss: 0.0381, Val Loss: 0.0496\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0410, Val Loss: 0.0496\n",
            "Epoch 83/100, Train Loss: 0.0386, Val Loss: 0.0496\n",
            "Epoch 84/100, Train Loss: 0.0398, Val Loss: 0.0496\n",
            "Epoch 85/100, Train Loss: 0.0406, Val Loss: 0.0496\n",
            "Epoch 86/100, Train Loss: 0.0367, Val Loss: 0.0496\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0412, Val Loss: 0.0496\n",
            "Epoch 88/100, Train Loss: 0.0382, Val Loss: 0.0496\n",
            "Epoch 89/100, Train Loss: 0.0393, Val Loss: 0.0496\n",
            "Epoch 90/100, Train Loss: 0.0429, Val Loss: 0.0496\n",
            "Epoch 91/100, Train Loss: 0.0380, Val Loss: 0.0496\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0401, Val Loss: 0.0496\n",
            "Epoch 93/100, Train Loss: 0.0384, Val Loss: 0.0496\n",
            "Epoch 94/100, Train Loss: 0.0378, Val Loss: 0.0496\n",
            "Epoch 95/100, Train Loss: 0.0401, Val Loss: 0.0496\n",
            "Epoch 96/100, Train Loss: 0.0369, Val Loss: 0.0496\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0378, Val Loss: 0.0496\n",
            "Epoch 98/100, Train Loss: 0.0399, Val Loss: 0.0496\n",
            "Epoch 99/100, Train Loss: 0.0374, Val Loss: 0.0496\n",
            "Epoch 100/100, Train Loss: 0.0370, Val Loss: 0.0496\n",
            "\n",
            "Test iteration 9/14\n",
            "Current training set size: 121 samples\n",
            "Epoch 1/100, Train Loss: 0.1493, Val Loss: 0.3072\n",
            "Epoch 2/100, Train Loss: 0.0929, Val Loss: 0.2009\n",
            "Epoch 3/100, Train Loss: 0.0799, Val Loss: 0.1827\n",
            "Epoch 4/100, Train Loss: 0.0955, Val Loss: 0.1624\n",
            "Epoch 5/100, Train Loss: 0.0725, Val Loss: 0.1127\n",
            "Epoch 6/100, Train Loss: 0.0657, Val Loss: 0.0692\n",
            "Epoch 7/100, Train Loss: 0.0603, Val Loss: 0.0509\n",
            "Epoch 8/100, Train Loss: 0.0543, Val Loss: 0.0844\n",
            "Epoch 9/100, Train Loss: 0.0537, Val Loss: 0.0520\n",
            "Epoch 10/100, Train Loss: 0.0593, Val Loss: 0.0477\n",
            "Epoch 11/100, Train Loss: 0.0562, Val Loss: 0.0580\n",
            "Epoch 12/100, Train Loss: 0.0497, Val Loss: 0.0477\n",
            "Epoch 13/100, Train Loss: 0.0512, Val Loss: 0.0510\n",
            "Epoch 14/100, Train Loss: 0.0442, Val Loss: 0.0481\n",
            "Epoch 15/100, Train Loss: 0.0502, Val Loss: 0.0550\n",
            "Epoch 16/100, Train Loss: 0.0429, Val Loss: 0.0418\n",
            "Epoch 17/100, Train Loss: 0.0455, Val Loss: 0.0531\n",
            "Epoch 18/100, Train Loss: 0.0417, Val Loss: 0.0445\n",
            "Epoch 19/100, Train Loss: 0.0352, Val Loss: 0.0438\n",
            "Epoch 20/100, Train Loss: 0.0443, Val Loss: 0.0458\n",
            "Epoch 21/100, Train Loss: 0.0442, Val Loss: 0.0475\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0469, Val Loss: 0.0478\n",
            "Epoch 23/100, Train Loss: 0.0418, Val Loss: 0.0478\n",
            "Epoch 24/100, Train Loss: 0.0415, Val Loss: 0.0480\n",
            "Epoch 25/100, Train Loss: 0.0410, Val Loss: 0.0483\n",
            "Epoch 26/100, Train Loss: 0.0411, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0382, Val Loss: 0.0484\n",
            "Epoch 28/100, Train Loss: 0.0362, Val Loss: 0.0484\n",
            "Epoch 29/100, Train Loss: 0.0384, Val Loss: 0.0484\n",
            "Epoch 30/100, Train Loss: 0.0381, Val Loss: 0.0484\n",
            "Epoch 31/100, Train Loss: 0.0385, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0388, Val Loss: 0.0484\n",
            "Epoch 33/100, Train Loss: 0.0364, Val Loss: 0.0484\n",
            "Epoch 34/100, Train Loss: 0.0404, Val Loss: 0.0484\n",
            "Epoch 35/100, Train Loss: 0.0374, Val Loss: 0.0484\n",
            "Epoch 36/100, Train Loss: 0.0401, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0379, Val Loss: 0.0484\n",
            "Epoch 38/100, Train Loss: 0.0405, Val Loss: 0.0484\n",
            "Epoch 39/100, Train Loss: 0.0399, Val Loss: 0.0484\n",
            "Epoch 40/100, Train Loss: 0.0383, Val Loss: 0.0484\n",
            "Epoch 41/100, Train Loss: 0.0434, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0404, Val Loss: 0.0484\n",
            "Epoch 43/100, Train Loss: 0.0370, Val Loss: 0.0484\n",
            "Epoch 44/100, Train Loss: 0.0407, Val Loss: 0.0484\n",
            "Epoch 45/100, Train Loss: 0.0368, Val Loss: 0.0484\n",
            "Epoch 46/100, Train Loss: 0.0382, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0431, Val Loss: 0.0484\n",
            "Epoch 48/100, Train Loss: 0.0436, Val Loss: 0.0484\n",
            "Epoch 49/100, Train Loss: 0.0399, Val Loss: 0.0484\n",
            "Epoch 50/100, Train Loss: 0.0408, Val Loss: 0.0484\n",
            "Epoch 51/100, Train Loss: 0.0396, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0408, Val Loss: 0.0484\n",
            "Epoch 53/100, Train Loss: 0.0410, Val Loss: 0.0484\n",
            "Epoch 54/100, Train Loss: 0.0422, Val Loss: 0.0484\n",
            "Epoch 55/100, Train Loss: 0.0418, Val Loss: 0.0484\n",
            "Epoch 56/100, Train Loss: 0.0379, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0388, Val Loss: 0.0484\n",
            "Epoch 58/100, Train Loss: 0.0383, Val Loss: 0.0484\n",
            "Epoch 59/100, Train Loss: 0.0410, Val Loss: 0.0484\n",
            "Epoch 60/100, Train Loss: 0.0382, Val Loss: 0.0484\n",
            "Epoch 61/100, Train Loss: 0.0398, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0387, Val Loss: 0.0484\n",
            "Epoch 63/100, Train Loss: 0.0407, Val Loss: 0.0484\n",
            "Epoch 64/100, Train Loss: 0.0425, Val Loss: 0.0484\n",
            "Epoch 65/100, Train Loss: 0.0358, Val Loss: 0.0484\n",
            "Epoch 66/100, Train Loss: 0.0390, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0379, Val Loss: 0.0484\n",
            "Epoch 68/100, Train Loss: 0.0362, Val Loss: 0.0484\n",
            "Epoch 69/100, Train Loss: 0.0401, Val Loss: 0.0484\n",
            "Epoch 70/100, Train Loss: 0.0419, Val Loss: 0.0484\n",
            "Epoch 71/100, Train Loss: 0.0395, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0406, Val Loss: 0.0484\n",
            "Epoch 73/100, Train Loss: 0.0394, Val Loss: 0.0484\n",
            "Epoch 74/100, Train Loss: 0.0392, Val Loss: 0.0484\n",
            "Epoch 75/100, Train Loss: 0.0378, Val Loss: 0.0484\n",
            "Epoch 76/100, Train Loss: 0.0406, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0379, Val Loss: 0.0484\n",
            "Epoch 78/100, Train Loss: 0.0385, Val Loss: 0.0484\n",
            "Epoch 79/100, Train Loss: 0.0431, Val Loss: 0.0484\n",
            "Epoch 80/100, Train Loss: 0.0357, Val Loss: 0.0484\n",
            "Epoch 81/100, Train Loss: 0.0389, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0404, Val Loss: 0.0484\n",
            "Epoch 83/100, Train Loss: 0.0425, Val Loss: 0.0484\n",
            "Epoch 84/100, Train Loss: 0.0379, Val Loss: 0.0484\n",
            "Epoch 85/100, Train Loss: 0.0375, Val Loss: 0.0484\n",
            "Epoch 86/100, Train Loss: 0.0399, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0389, Val Loss: 0.0484\n",
            "Epoch 88/100, Train Loss: 0.0387, Val Loss: 0.0484\n",
            "Epoch 89/100, Train Loss: 0.0438, Val Loss: 0.0484\n",
            "Epoch 90/100, Train Loss: 0.0428, Val Loss: 0.0484\n",
            "Epoch 91/100, Train Loss: 0.0397, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0390, Val Loss: 0.0484\n",
            "Epoch 93/100, Train Loss: 0.0408, Val Loss: 0.0484\n",
            "Epoch 94/100, Train Loss: 0.0353, Val Loss: 0.0484\n",
            "Epoch 95/100, Train Loss: 0.0379, Val Loss: 0.0484\n",
            "Epoch 96/100, Train Loss: 0.0390, Val Loss: 0.0484\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0363, Val Loss: 0.0484\n",
            "Epoch 98/100, Train Loss: 0.0362, Val Loss: 0.0484\n",
            "Epoch 99/100, Train Loss: 0.0387, Val Loss: 0.0484\n",
            "Epoch 100/100, Train Loss: 0.0397, Val Loss: 0.0484\n",
            "\n",
            "Test iteration 10/14\n",
            "Current training set size: 122 samples\n",
            "Epoch 1/100, Train Loss: 0.1568, Val Loss: 0.2944\n",
            "Epoch 2/100, Train Loss: 0.1001, Val Loss: 0.1972\n",
            "Epoch 3/100, Train Loss: 0.0828, Val Loss: 0.1816\n",
            "Epoch 4/100, Train Loss: 0.0752, Val Loss: 0.1555\n",
            "Epoch 5/100, Train Loss: 0.0700, Val Loss: 0.1042\n",
            "Epoch 6/100, Train Loss: 0.0698, Val Loss: 0.0541\n",
            "Epoch 7/100, Train Loss: 0.0619, Val Loss: 0.0513\n",
            "Epoch 8/100, Train Loss: 0.0630, Val Loss: 0.0857\n",
            "Epoch 9/100, Train Loss: 0.0587, Val Loss: 0.0880\n",
            "Epoch 10/100, Train Loss: 0.0528, Val Loss: 0.0520\n",
            "Epoch 11/100, Train Loss: 0.0549, Val Loss: 0.0463\n",
            "Epoch 12/100, Train Loss: 0.0547, Val Loss: 0.0485\n",
            "Epoch 13/100, Train Loss: 0.0499, Val Loss: 0.0443\n",
            "Epoch 14/100, Train Loss: 0.0458, Val Loss: 0.0451\n",
            "Epoch 15/100, Train Loss: 0.0462, Val Loss: 0.0459\n",
            "Epoch 16/100, Train Loss: 0.0417, Val Loss: 0.0479\n",
            "Epoch 17/100, Train Loss: 0.0480, Val Loss: 0.0473\n",
            "Epoch 18/100, Train Loss: 0.0428, Val Loss: 0.0454\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0401, Val Loss: 0.0466\n",
            "Epoch 20/100, Train Loss: 0.0387, Val Loss: 0.0468\n",
            "Epoch 21/100, Train Loss: 0.0422, Val Loss: 0.0476\n",
            "Epoch 22/100, Train Loss: 0.0462, Val Loss: 0.0484\n",
            "Epoch 23/100, Train Loss: 0.0405, Val Loss: 0.0490\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0365, Val Loss: 0.0490\n",
            "Epoch 25/100, Train Loss: 0.0397, Val Loss: 0.0490\n",
            "Epoch 26/100, Train Loss: 0.0363, Val Loss: 0.0490\n",
            "Epoch 27/100, Train Loss: 0.0413, Val Loss: 0.0490\n",
            "Epoch 28/100, Train Loss: 0.0389, Val Loss: 0.0490\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0405, Val Loss: 0.0490\n",
            "Epoch 30/100, Train Loss: 0.0387, Val Loss: 0.0490\n",
            "Epoch 31/100, Train Loss: 0.0405, Val Loss: 0.0490\n",
            "Epoch 32/100, Train Loss: 0.0419, Val Loss: 0.0490\n",
            "Epoch 33/100, Train Loss: 0.0398, Val Loss: 0.0490\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0405, Val Loss: 0.0490\n",
            "Epoch 35/100, Train Loss: 0.0407, Val Loss: 0.0490\n",
            "Epoch 36/100, Train Loss: 0.0388, Val Loss: 0.0490\n",
            "Epoch 37/100, Train Loss: 0.0407, Val Loss: 0.0490\n",
            "Epoch 38/100, Train Loss: 0.0441, Val Loss: 0.0490\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0365, Val Loss: 0.0490\n",
            "Epoch 40/100, Train Loss: 0.0371, Val Loss: 0.0490\n",
            "Epoch 41/100, Train Loss: 0.0415, Val Loss: 0.0490\n",
            "Epoch 42/100, Train Loss: 0.0395, Val Loss: 0.0490\n",
            "Epoch 43/100, Train Loss: 0.0377, Val Loss: 0.0490\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0395, Val Loss: 0.0490\n",
            "Epoch 45/100, Train Loss: 0.0352, Val Loss: 0.0490\n",
            "Epoch 46/100, Train Loss: 0.0382, Val Loss: 0.0490\n",
            "Epoch 47/100, Train Loss: 0.0383, Val Loss: 0.0490\n",
            "Epoch 48/100, Train Loss: 0.0382, Val Loss: 0.0490\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0396, Val Loss: 0.0490\n",
            "Epoch 50/100, Train Loss: 0.0374, Val Loss: 0.0490\n",
            "Epoch 51/100, Train Loss: 0.0389, Val Loss: 0.0490\n",
            "Epoch 52/100, Train Loss: 0.0412, Val Loss: 0.0490\n",
            "Epoch 53/100, Train Loss: 0.0387, Val Loss: 0.0490\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0395, Val Loss: 0.0490\n",
            "Epoch 55/100, Train Loss: 0.0407, Val Loss: 0.0490\n",
            "Epoch 56/100, Train Loss: 0.0381, Val Loss: 0.0490\n",
            "Epoch 57/100, Train Loss: 0.0374, Val Loss: 0.0490\n",
            "Epoch 58/100, Train Loss: 0.0380, Val Loss: 0.0490\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0375, Val Loss: 0.0490\n",
            "Epoch 60/100, Train Loss: 0.0445, Val Loss: 0.0490\n",
            "Epoch 61/100, Train Loss: 0.0410, Val Loss: 0.0490\n",
            "Epoch 62/100, Train Loss: 0.0374, Val Loss: 0.0490\n",
            "Epoch 63/100, Train Loss: 0.0380, Val Loss: 0.0490\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0411, Val Loss: 0.0490\n",
            "Epoch 65/100, Train Loss: 0.0384, Val Loss: 0.0490\n",
            "Epoch 66/100, Train Loss: 0.0393, Val Loss: 0.0490\n",
            "Epoch 67/100, Train Loss: 0.0392, Val Loss: 0.0490\n",
            "Epoch 68/100, Train Loss: 0.0415, Val Loss: 0.0490\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0423, Val Loss: 0.0490\n",
            "Epoch 70/100, Train Loss: 0.0425, Val Loss: 0.0490\n",
            "Epoch 71/100, Train Loss: 0.0350, Val Loss: 0.0490\n",
            "Epoch 72/100, Train Loss: 0.0387, Val Loss: 0.0490\n",
            "Epoch 73/100, Train Loss: 0.0402, Val Loss: 0.0490\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0364, Val Loss: 0.0490\n",
            "Epoch 75/100, Train Loss: 0.0387, Val Loss: 0.0490\n",
            "Epoch 76/100, Train Loss: 0.0369, Val Loss: 0.0490\n",
            "Epoch 77/100, Train Loss: 0.0391, Val Loss: 0.0490\n",
            "Epoch 78/100, Train Loss: 0.0368, Val Loss: 0.0490\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0378, Val Loss: 0.0490\n",
            "Epoch 80/100, Train Loss: 0.0393, Val Loss: 0.0490\n",
            "Epoch 81/100, Train Loss: 0.0424, Val Loss: 0.0490\n",
            "Epoch 82/100, Train Loss: 0.0407, Val Loss: 0.0490\n",
            "Epoch 83/100, Train Loss: 0.0382, Val Loss: 0.0490\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0400, Val Loss: 0.0490\n",
            "Epoch 85/100, Train Loss: 0.0375, Val Loss: 0.0490\n",
            "Epoch 86/100, Train Loss: 0.0428, Val Loss: 0.0490\n",
            "Epoch 87/100, Train Loss: 0.0434, Val Loss: 0.0490\n",
            "Epoch 88/100, Train Loss: 0.0371, Val Loss: 0.0490\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0424, Val Loss: 0.0490\n",
            "Epoch 90/100, Train Loss: 0.0406, Val Loss: 0.0490\n",
            "Epoch 91/100, Train Loss: 0.0383, Val Loss: 0.0490\n",
            "Epoch 92/100, Train Loss: 0.0400, Val Loss: 0.0490\n",
            "Epoch 93/100, Train Loss: 0.0376, Val Loss: 0.0490\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0361, Val Loss: 0.0490\n",
            "Epoch 95/100, Train Loss: 0.0363, Val Loss: 0.0490\n",
            "Epoch 96/100, Train Loss: 0.0393, Val Loss: 0.0490\n",
            "Epoch 97/100, Train Loss: 0.0409, Val Loss: 0.0490\n",
            "Epoch 98/100, Train Loss: 0.0353, Val Loss: 0.0490\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0400, Val Loss: 0.0490\n",
            "Epoch 100/100, Train Loss: 0.0398, Val Loss: 0.0490\n",
            "\n",
            "Test iteration 11/14\n",
            "Current training set size: 123 samples\n",
            "Epoch 1/100, Train Loss: 0.1621, Val Loss: 0.2749\n",
            "Epoch 2/100, Train Loss: 0.1099, Val Loss: 0.1938\n",
            "Epoch 3/100, Train Loss: 0.0889, Val Loss: 0.1777\n",
            "Epoch 4/100, Train Loss: 0.0796, Val Loss: 0.1264\n",
            "Epoch 5/100, Train Loss: 0.0684, Val Loss: 0.0488\n",
            "Epoch 6/100, Train Loss: 0.0683, Val Loss: 0.0493\n",
            "Epoch 7/100, Train Loss: 0.0548, Val Loss: 0.0691\n",
            "Epoch 8/100, Train Loss: 0.0571, Val Loss: 0.0529\n",
            "Epoch 9/100, Train Loss: 0.0545, Val Loss: 0.0388\n",
            "Epoch 10/100, Train Loss: 0.0539, Val Loss: 0.1111\n",
            "Epoch 11/100, Train Loss: 0.0549, Val Loss: 0.0736\n",
            "Epoch 12/100, Train Loss: 0.0520, Val Loss: 0.0513\n",
            "Epoch 13/100, Train Loss: 0.0478, Val Loss: 0.0482\n",
            "Epoch 14/100, Train Loss: 0.0517, Val Loss: 0.0487\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 15/100, Train Loss: 0.0484, Val Loss: 0.0489\n",
            "Epoch 16/100, Train Loss: 0.0381, Val Loss: 0.0490\n",
            "Epoch 17/100, Train Loss: 0.0403, Val Loss: 0.0493\n",
            "Epoch 18/100, Train Loss: 0.0430, Val Loss: 0.0494\n",
            "Epoch 19/100, Train Loss: 0.0393, Val Loss: 0.0494\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0378, Val Loss: 0.0494\n",
            "Epoch 21/100, Train Loss: 0.0393, Val Loss: 0.0494\n",
            "Epoch 22/100, Train Loss: 0.0432, Val Loss: 0.0494\n",
            "Epoch 23/100, Train Loss: 0.0432, Val Loss: 0.0494\n",
            "Epoch 24/100, Train Loss: 0.0410, Val Loss: 0.0494\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0442, Val Loss: 0.0494\n",
            "Epoch 26/100, Train Loss: 0.0381, Val Loss: 0.0494\n",
            "Epoch 27/100, Train Loss: 0.0398, Val Loss: 0.0494\n",
            "Epoch 28/100, Train Loss: 0.0389, Val Loss: 0.0494\n",
            "Epoch 29/100, Train Loss: 0.0406, Val Loss: 0.0494\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0401, Val Loss: 0.0494\n",
            "Epoch 31/100, Train Loss: 0.0395, Val Loss: 0.0494\n",
            "Epoch 32/100, Train Loss: 0.0397, Val Loss: 0.0494\n",
            "Epoch 33/100, Train Loss: 0.0437, Val Loss: 0.0494\n",
            "Epoch 34/100, Train Loss: 0.0461, Val Loss: 0.0494\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0422, Val Loss: 0.0494\n",
            "Epoch 36/100, Train Loss: 0.0406, Val Loss: 0.0494\n",
            "Epoch 37/100, Train Loss: 0.0413, Val Loss: 0.0494\n",
            "Epoch 38/100, Train Loss: 0.0450, Val Loss: 0.0494\n",
            "Epoch 39/100, Train Loss: 0.0356, Val Loss: 0.0494\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0397, Val Loss: 0.0494\n",
            "Epoch 41/100, Train Loss: 0.0385, Val Loss: 0.0494\n",
            "Epoch 42/100, Train Loss: 0.0440, Val Loss: 0.0494\n",
            "Epoch 43/100, Train Loss: 0.0453, Val Loss: 0.0494\n",
            "Epoch 44/100, Train Loss: 0.0351, Val Loss: 0.0494\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0405, Val Loss: 0.0494\n",
            "Epoch 46/100, Train Loss: 0.0415, Val Loss: 0.0494\n",
            "Epoch 47/100, Train Loss: 0.0398, Val Loss: 0.0494\n",
            "Epoch 48/100, Train Loss: 0.0391, Val Loss: 0.0494\n",
            "Epoch 49/100, Train Loss: 0.0432, Val Loss: 0.0494\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0423, Val Loss: 0.0494\n",
            "Epoch 51/100, Train Loss: 0.0397, Val Loss: 0.0494\n",
            "Epoch 52/100, Train Loss: 0.0438, Val Loss: 0.0494\n",
            "Epoch 53/100, Train Loss: 0.0401, Val Loss: 0.0494\n",
            "Epoch 54/100, Train Loss: 0.0412, Val Loss: 0.0494\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0415, Val Loss: 0.0494\n",
            "Epoch 56/100, Train Loss: 0.0465, Val Loss: 0.0494\n",
            "Epoch 57/100, Train Loss: 0.0455, Val Loss: 0.0494\n",
            "Epoch 58/100, Train Loss: 0.0430, Val Loss: 0.0494\n",
            "Epoch 59/100, Train Loss: 0.0466, Val Loss: 0.0494\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0419, Val Loss: 0.0494\n",
            "Epoch 61/100, Train Loss: 0.0388, Val Loss: 0.0494\n",
            "Epoch 62/100, Train Loss: 0.0388, Val Loss: 0.0494\n",
            "Epoch 63/100, Train Loss: 0.0414, Val Loss: 0.0494\n",
            "Epoch 64/100, Train Loss: 0.0433, Val Loss: 0.0494\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0394, Val Loss: 0.0494\n",
            "Epoch 66/100, Train Loss: 0.0427, Val Loss: 0.0494\n",
            "Epoch 67/100, Train Loss: 0.0462, Val Loss: 0.0494\n",
            "Epoch 68/100, Train Loss: 0.0412, Val Loss: 0.0494\n",
            "Epoch 69/100, Train Loss: 0.0413, Val Loss: 0.0494\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0401, Val Loss: 0.0494\n",
            "Epoch 71/100, Train Loss: 0.0377, Val Loss: 0.0494\n",
            "Epoch 72/100, Train Loss: 0.0389, Val Loss: 0.0494\n",
            "Epoch 73/100, Train Loss: 0.0419, Val Loss: 0.0494\n",
            "Epoch 74/100, Train Loss: 0.0467, Val Loss: 0.0494\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0382, Val Loss: 0.0494\n",
            "Epoch 76/100, Train Loss: 0.0361, Val Loss: 0.0494\n",
            "Epoch 77/100, Train Loss: 0.0372, Val Loss: 0.0494\n",
            "Epoch 78/100, Train Loss: 0.0411, Val Loss: 0.0494\n",
            "Epoch 79/100, Train Loss: 0.0371, Val Loss: 0.0494\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0377, Val Loss: 0.0494\n",
            "Epoch 81/100, Train Loss: 0.0409, Val Loss: 0.0494\n",
            "Epoch 82/100, Train Loss: 0.0471, Val Loss: 0.0494\n",
            "Epoch 83/100, Train Loss: 0.0397, Val Loss: 0.0494\n",
            "Epoch 84/100, Train Loss: 0.0406, Val Loss: 0.0494\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0422, Val Loss: 0.0494\n",
            "Epoch 86/100, Train Loss: 0.0373, Val Loss: 0.0494\n",
            "Epoch 87/100, Train Loss: 0.0471, Val Loss: 0.0494\n",
            "Epoch 88/100, Train Loss: 0.0379, Val Loss: 0.0494\n",
            "Epoch 89/100, Train Loss: 0.0420, Val Loss: 0.0494\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0393, Val Loss: 0.0494\n",
            "Epoch 91/100, Train Loss: 0.0423, Val Loss: 0.0494\n",
            "Epoch 92/100, Train Loss: 0.0410, Val Loss: 0.0494\n",
            "Epoch 93/100, Train Loss: 0.0442, Val Loss: 0.0494\n",
            "Epoch 94/100, Train Loss: 0.0430, Val Loss: 0.0494\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0455, Val Loss: 0.0494\n",
            "Epoch 96/100, Train Loss: 0.0453, Val Loss: 0.0494\n",
            "Epoch 97/100, Train Loss: 0.0426, Val Loss: 0.0494\n",
            "Epoch 98/100, Train Loss: 0.0438, Val Loss: 0.0494\n",
            "Epoch 99/100, Train Loss: 0.0404, Val Loss: 0.0494\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0420, Val Loss: 0.0494\n",
            "\n",
            "Test iteration 12/14\n",
            "Current training set size: 124 samples\n",
            "Epoch 1/100, Train Loss: 0.1607, Val Loss: 0.3520\n",
            "Epoch 2/100, Train Loss: 0.1146, Val Loss: 0.1900\n",
            "Epoch 3/100, Train Loss: 0.0827, Val Loss: 0.1504\n",
            "Epoch 4/100, Train Loss: 0.0787, Val Loss: 0.0900\n",
            "Epoch 5/100, Train Loss: 0.0656, Val Loss: 0.0508\n",
            "Epoch 6/100, Train Loss: 0.0678, Val Loss: 0.0502\n",
            "Epoch 7/100, Train Loss: 0.0602, Val Loss: 0.0507\n",
            "Epoch 8/100, Train Loss: 0.0633, Val Loss: 0.1668\n",
            "Epoch 9/100, Train Loss: 0.0562, Val Loss: 0.0642\n",
            "Epoch 10/100, Train Loss: 0.0628, Val Loss: 0.0517\n",
            "Epoch 11/100, Train Loss: 0.0563, Val Loss: 0.0896\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 12/100, Train Loss: 0.0548, Val Loss: 0.0876\n",
            "Epoch 13/100, Train Loss: 0.0533, Val Loss: 0.0855\n",
            "Epoch 14/100, Train Loss: 0.0520, Val Loss: 0.0841\n",
            "Epoch 15/100, Train Loss: 0.0480, Val Loss: 0.0799\n",
            "Epoch 16/100, Train Loss: 0.0470, Val Loss: 0.0768\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0475, Val Loss: 0.0767\n",
            "Epoch 18/100, Train Loss: 0.0481, Val Loss: 0.0767\n",
            "Epoch 19/100, Train Loss: 0.0464, Val Loss: 0.0767\n",
            "Epoch 20/100, Train Loss: 0.0465, Val Loss: 0.0767\n",
            "Epoch 21/100, Train Loss: 0.0474, Val Loss: 0.0767\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0509, Val Loss: 0.0767\n",
            "Epoch 23/100, Train Loss: 0.0497, Val Loss: 0.0767\n",
            "Epoch 24/100, Train Loss: 0.0481, Val Loss: 0.0767\n",
            "Epoch 25/100, Train Loss: 0.0480, Val Loss: 0.0767\n",
            "Epoch 26/100, Train Loss: 0.0486, Val Loss: 0.0767\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0443, Val Loss: 0.0767\n",
            "Epoch 28/100, Train Loss: 0.0542, Val Loss: 0.0767\n",
            "Epoch 29/100, Train Loss: 0.0463, Val Loss: 0.0767\n",
            "Epoch 30/100, Train Loss: 0.0473, Val Loss: 0.0767\n",
            "Epoch 31/100, Train Loss: 0.0514, Val Loss: 0.0767\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0475, Val Loss: 0.0767\n",
            "Epoch 33/100, Train Loss: 0.0485, Val Loss: 0.0767\n",
            "Epoch 34/100, Train Loss: 0.0508, Val Loss: 0.0767\n",
            "Epoch 35/100, Train Loss: 0.0466, Val Loss: 0.0767\n",
            "Epoch 36/100, Train Loss: 0.0475, Val Loss: 0.0767\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0480, Val Loss: 0.0767\n",
            "Epoch 38/100, Train Loss: 0.0479, Val Loss: 0.0767\n",
            "Epoch 39/100, Train Loss: 0.0505, Val Loss: 0.0767\n",
            "Epoch 40/100, Train Loss: 0.0470, Val Loss: 0.0767\n",
            "Epoch 41/100, Train Loss: 0.0442, Val Loss: 0.0767\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0496, Val Loss: 0.0767\n",
            "Epoch 43/100, Train Loss: 0.0512, Val Loss: 0.0767\n",
            "Epoch 44/100, Train Loss: 0.0489, Val Loss: 0.0767\n",
            "Epoch 45/100, Train Loss: 0.0494, Val Loss: 0.0767\n",
            "Epoch 46/100, Train Loss: 0.0501, Val Loss: 0.0767\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0503, Val Loss: 0.0767\n",
            "Epoch 48/100, Train Loss: 0.0499, Val Loss: 0.0767\n",
            "Epoch 49/100, Train Loss: 0.0455, Val Loss: 0.0767\n",
            "Epoch 50/100, Train Loss: 0.0522, Val Loss: 0.0767\n",
            "Epoch 51/100, Train Loss: 0.0431, Val Loss: 0.0767\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0502, Val Loss: 0.0767\n",
            "Epoch 53/100, Train Loss: 0.0493, Val Loss: 0.0767\n",
            "Epoch 54/100, Train Loss: 0.0483, Val Loss: 0.0767\n",
            "Epoch 55/100, Train Loss: 0.0476, Val Loss: 0.0767\n",
            "Epoch 56/100, Train Loss: 0.0475, Val Loss: 0.0767\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0449, Val Loss: 0.0767\n",
            "Epoch 58/100, Train Loss: 0.0500, Val Loss: 0.0767\n",
            "Epoch 59/100, Train Loss: 0.0508, Val Loss: 0.0767\n",
            "Epoch 60/100, Train Loss: 0.0486, Val Loss: 0.0767\n",
            "Epoch 61/100, Train Loss: 0.0458, Val Loss: 0.0767\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0468, Val Loss: 0.0767\n",
            "Epoch 63/100, Train Loss: 0.0484, Val Loss: 0.0767\n",
            "Epoch 64/100, Train Loss: 0.0488, Val Loss: 0.0767\n",
            "Epoch 65/100, Train Loss: 0.0513, Val Loss: 0.0767\n",
            "Epoch 66/100, Train Loss: 0.0447, Val Loss: 0.0767\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0484, Val Loss: 0.0767\n",
            "Epoch 68/100, Train Loss: 0.0488, Val Loss: 0.0767\n",
            "Epoch 69/100, Train Loss: 0.0513, Val Loss: 0.0767\n",
            "Epoch 70/100, Train Loss: 0.0483, Val Loss: 0.0767\n",
            "Epoch 71/100, Train Loss: 0.0447, Val Loss: 0.0767\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0478, Val Loss: 0.0767\n",
            "Epoch 73/100, Train Loss: 0.0467, Val Loss: 0.0767\n",
            "Epoch 74/100, Train Loss: 0.0440, Val Loss: 0.0767\n",
            "Epoch 75/100, Train Loss: 0.0469, Val Loss: 0.0767\n",
            "Epoch 76/100, Train Loss: 0.0463, Val Loss: 0.0767\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0511, Val Loss: 0.0767\n",
            "Epoch 78/100, Train Loss: 0.0517, Val Loss: 0.0767\n",
            "Epoch 79/100, Train Loss: 0.0517, Val Loss: 0.0767\n",
            "Epoch 80/100, Train Loss: 0.0531, Val Loss: 0.0767\n",
            "Epoch 81/100, Train Loss: 0.0476, Val Loss: 0.0767\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0535, Val Loss: 0.0767\n",
            "Epoch 83/100, Train Loss: 0.0510, Val Loss: 0.0767\n",
            "Epoch 84/100, Train Loss: 0.0487, Val Loss: 0.0767\n",
            "Epoch 85/100, Train Loss: 0.0500, Val Loss: 0.0767\n",
            "Epoch 86/100, Train Loss: 0.0471, Val Loss: 0.0767\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0515, Val Loss: 0.0767\n",
            "Epoch 88/100, Train Loss: 0.0469, Val Loss: 0.0767\n",
            "Epoch 89/100, Train Loss: 0.0505, Val Loss: 0.0767\n",
            "Epoch 90/100, Train Loss: 0.0483, Val Loss: 0.0767\n",
            "Epoch 91/100, Train Loss: 0.0476, Val Loss: 0.0767\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0470, Val Loss: 0.0767\n",
            "Epoch 93/100, Train Loss: 0.0476, Val Loss: 0.0767\n",
            "Epoch 94/100, Train Loss: 0.0481, Val Loss: 0.0767\n",
            "Epoch 95/100, Train Loss: 0.0444, Val Loss: 0.0767\n",
            "Epoch 96/100, Train Loss: 0.0520, Val Loss: 0.0767\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0435, Val Loss: 0.0767\n",
            "Epoch 98/100, Train Loss: 0.0471, Val Loss: 0.0767\n",
            "Epoch 99/100, Train Loss: 0.0501, Val Loss: 0.0767\n",
            "Epoch 100/100, Train Loss: 0.0536, Val Loss: 0.0767\n",
            "\n",
            "Test iteration 13/14\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1585, Val Loss: 0.2425\n",
            "Epoch 2/100, Train Loss: 0.0963, Val Loss: 0.1904\n",
            "Epoch 3/100, Train Loss: 0.0883, Val Loss: 0.1707\n",
            "Epoch 4/100, Train Loss: 0.0853, Val Loss: 0.1273\n",
            "Epoch 5/100, Train Loss: 0.0705, Val Loss: 0.0680\n",
            "Epoch 6/100, Train Loss: 0.0682, Val Loss: 0.0478\n",
            "Epoch 7/100, Train Loss: 0.0606, Val Loss: 0.0861\n",
            "Epoch 8/100, Train Loss: 0.0523, Val Loss: 0.0820\n",
            "Epoch 9/100, Train Loss: 0.0632, Val Loss: 0.0542\n",
            "Epoch 10/100, Train Loss: 0.0545, Val Loss: 0.0988\n",
            "Epoch 11/100, Train Loss: 0.0538, Val Loss: 0.0632\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 12/100, Train Loss: 0.0554, Val Loss: 0.0627\n",
            "Epoch 13/100, Train Loss: 0.0477, Val Loss: 0.0621\n",
            "Epoch 14/100, Train Loss: 0.0458, Val Loss: 0.0613\n",
            "Epoch 15/100, Train Loss: 0.0480, Val Loss: 0.0606\n",
            "Epoch 16/100, Train Loss: 0.0384, Val Loss: 0.0603\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0438, Val Loss: 0.0603\n",
            "Epoch 18/100, Train Loss: 0.0427, Val Loss: 0.0603\n",
            "Epoch 19/100, Train Loss: 0.0473, Val Loss: 0.0603\n",
            "Epoch 20/100, Train Loss: 0.0465, Val Loss: 0.0603\n",
            "Epoch 21/100, Train Loss: 0.0427, Val Loss: 0.0603\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0434, Val Loss: 0.0603\n",
            "Epoch 23/100, Train Loss: 0.0463, Val Loss: 0.0603\n",
            "Epoch 24/100, Train Loss: 0.0442, Val Loss: 0.0603\n",
            "Epoch 25/100, Train Loss: 0.0429, Val Loss: 0.0603\n",
            "Epoch 26/100, Train Loss: 0.0430, Val Loss: 0.0603\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0437, Val Loss: 0.0603\n",
            "Epoch 28/100, Train Loss: 0.0483, Val Loss: 0.0603\n",
            "Epoch 29/100, Train Loss: 0.0458, Val Loss: 0.0603\n",
            "Epoch 30/100, Train Loss: 0.0452, Val Loss: 0.0603\n",
            "Epoch 31/100, Train Loss: 0.0459, Val Loss: 0.0603\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0452, Val Loss: 0.0603\n",
            "Epoch 33/100, Train Loss: 0.0464, Val Loss: 0.0603\n",
            "Epoch 34/100, Train Loss: 0.0454, Val Loss: 0.0603\n",
            "Epoch 35/100, Train Loss: 0.0415, Val Loss: 0.0603\n",
            "Epoch 36/100, Train Loss: 0.0486, Val Loss: 0.0603\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0434, Val Loss: 0.0603\n",
            "Epoch 38/100, Train Loss: 0.0408, Val Loss: 0.0603\n",
            "Epoch 39/100, Train Loss: 0.0475, Val Loss: 0.0603\n",
            "Epoch 40/100, Train Loss: 0.0447, Val Loss: 0.0603\n",
            "Epoch 41/100, Train Loss: 0.0472, Val Loss: 0.0603\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0437, Val Loss: 0.0603\n",
            "Epoch 43/100, Train Loss: 0.0440, Val Loss: 0.0603\n",
            "Epoch 44/100, Train Loss: 0.0422, Val Loss: 0.0603\n",
            "Epoch 45/100, Train Loss: 0.0428, Val Loss: 0.0603\n",
            "Epoch 46/100, Train Loss: 0.0415, Val Loss: 0.0603\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0473, Val Loss: 0.0603\n",
            "Epoch 48/100, Train Loss: 0.0440, Val Loss: 0.0603\n",
            "Epoch 49/100, Train Loss: 0.0460, Val Loss: 0.0603\n",
            "Epoch 50/100, Train Loss: 0.0437, Val Loss: 0.0603\n",
            "Epoch 51/100, Train Loss: 0.0442, Val Loss: 0.0603\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0462, Val Loss: 0.0603\n",
            "Epoch 53/100, Train Loss: 0.0442, Val Loss: 0.0603\n",
            "Epoch 54/100, Train Loss: 0.0472, Val Loss: 0.0603\n",
            "Epoch 55/100, Train Loss: 0.0454, Val Loss: 0.0603\n",
            "Epoch 56/100, Train Loss: 0.0466, Val Loss: 0.0603\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0437, Val Loss: 0.0603\n",
            "Epoch 58/100, Train Loss: 0.0418, Val Loss: 0.0603\n",
            "Epoch 59/100, Train Loss: 0.0425, Val Loss: 0.0603\n",
            "Epoch 60/100, Train Loss: 0.0441, Val Loss: 0.0603\n",
            "Epoch 61/100, Train Loss: 0.0468, Val Loss: 0.0603\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0463, Val Loss: 0.0603\n",
            "Epoch 63/100, Train Loss: 0.0514, Val Loss: 0.0603\n",
            "Epoch 64/100, Train Loss: 0.0413, Val Loss: 0.0603\n",
            "Epoch 65/100, Train Loss: 0.0454, Val Loss: 0.0603\n",
            "Epoch 66/100, Train Loss: 0.0490, Val Loss: 0.0603\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0455, Val Loss: 0.0603\n",
            "Epoch 68/100, Train Loss: 0.0473, Val Loss: 0.0603\n",
            "Epoch 69/100, Train Loss: 0.0472, Val Loss: 0.0603\n",
            "Epoch 70/100, Train Loss: 0.0448, Val Loss: 0.0603\n",
            "Epoch 71/100, Train Loss: 0.0458, Val Loss: 0.0603\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0450, Val Loss: 0.0603\n",
            "Epoch 73/100, Train Loss: 0.0433, Val Loss: 0.0603\n",
            "Epoch 74/100, Train Loss: 0.0460, Val Loss: 0.0603\n",
            "Epoch 75/100, Train Loss: 0.0435, Val Loss: 0.0603\n",
            "Epoch 76/100, Train Loss: 0.0472, Val Loss: 0.0603\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0466, Val Loss: 0.0603\n",
            "Epoch 78/100, Train Loss: 0.0527, Val Loss: 0.0603\n",
            "Epoch 79/100, Train Loss: 0.0445, Val Loss: 0.0603\n",
            "Epoch 80/100, Train Loss: 0.0469, Val Loss: 0.0603\n",
            "Epoch 81/100, Train Loss: 0.0471, Val Loss: 0.0603\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0426, Val Loss: 0.0603\n",
            "Epoch 83/100, Train Loss: 0.0446, Val Loss: 0.0603\n",
            "Epoch 84/100, Train Loss: 0.0464, Val Loss: 0.0603\n",
            "Epoch 85/100, Train Loss: 0.0438, Val Loss: 0.0603\n",
            "Epoch 86/100, Train Loss: 0.0482, Val Loss: 0.0603\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0468, Val Loss: 0.0603\n",
            "Epoch 88/100, Train Loss: 0.0471, Val Loss: 0.0603\n",
            "Epoch 89/100, Train Loss: 0.0456, Val Loss: 0.0603\n",
            "Epoch 90/100, Train Loss: 0.0411, Val Loss: 0.0603\n",
            "Epoch 91/100, Train Loss: 0.0432, Val Loss: 0.0603\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0423, Val Loss: 0.0603\n",
            "Epoch 93/100, Train Loss: 0.0475, Val Loss: 0.0603\n",
            "Epoch 94/100, Train Loss: 0.0460, Val Loss: 0.0603\n",
            "Epoch 95/100, Train Loss: 0.0478, Val Loss: 0.0603\n",
            "Epoch 96/100, Train Loss: 0.0425, Val Loss: 0.0603\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0458, Val Loss: 0.0603\n",
            "Epoch 98/100, Train Loss: 0.0459, Val Loss: 0.0603\n",
            "Epoch 99/100, Train Loss: 0.0479, Val Loss: 0.0603\n",
            "Epoch 100/100, Train Loss: 0.0444, Val Loss: 0.0603\n",
            "\n",
            "Test iteration 14/14\n",
            "Current training set size: 126 samples\n",
            "Epoch 1/100, Train Loss: 0.1616, Val Loss: 0.3391\n",
            "Epoch 2/100, Train Loss: 0.1069, Val Loss: 0.1904\n",
            "Epoch 3/100, Train Loss: 0.0852, Val Loss: 0.1701\n",
            "Epoch 4/100, Train Loss: 0.0796, Val Loss: 0.0860\n",
            "Epoch 5/100, Train Loss: 0.0752, Val Loss: 0.0900\n",
            "Epoch 6/100, Train Loss: 0.0706, Val Loss: 0.0498\n",
            "Epoch 7/100, Train Loss: 0.0594, Val Loss: 0.0434\n",
            "Epoch 8/100, Train Loss: 0.0552, Val Loss: 0.0529\n",
            "Epoch 9/100, Train Loss: 0.0536, Val Loss: 0.0681\n",
            "Epoch 10/100, Train Loss: 0.0485, Val Loss: 0.0957\n",
            "Epoch 11/100, Train Loss: 0.0584, Val Loss: 0.0590\n",
            "Epoch 12/100, Train Loss: 0.0603, Val Loss: 0.0488\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 13/100, Train Loss: 0.0427, Val Loss: 0.0488\n",
            "Epoch 14/100, Train Loss: 0.0454, Val Loss: 0.0488\n",
            "Epoch 15/100, Train Loss: 0.0428, Val Loss: 0.0488\n",
            "Epoch 16/100, Train Loss: 0.0435, Val Loss: 0.0488\n",
            "Epoch 17/100, Train Loss: 0.0406, Val Loss: 0.0487\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0411, Val Loss: 0.0487\n",
            "Epoch 19/100, Train Loss: 0.0397, Val Loss: 0.0487\n",
            "Epoch 20/100, Train Loss: 0.0433, Val Loss: 0.0487\n",
            "Epoch 21/100, Train Loss: 0.0456, Val Loss: 0.0488\n",
            "Epoch 22/100, Train Loss: 0.0436, Val Loss: 0.0488\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0499, Val Loss: 0.0488\n",
            "Epoch 24/100, Train Loss: 0.0450, Val Loss: 0.0488\n",
            "Epoch 25/100, Train Loss: 0.0422, Val Loss: 0.0488\n",
            "Epoch 26/100, Train Loss: 0.0414, Val Loss: 0.0488\n",
            "Epoch 27/100, Train Loss: 0.0420, Val Loss: 0.0488\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0438, Val Loss: 0.0488\n",
            "Epoch 29/100, Train Loss: 0.0468, Val Loss: 0.0488\n",
            "Epoch 30/100, Train Loss: 0.0465, Val Loss: 0.0488\n",
            "Epoch 31/100, Train Loss: 0.0422, Val Loss: 0.0488\n",
            "Epoch 32/100, Train Loss: 0.0451, Val Loss: 0.0488\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0395, Val Loss: 0.0488\n",
            "Epoch 34/100, Train Loss: 0.0467, Val Loss: 0.0488\n",
            "Epoch 35/100, Train Loss: 0.0481, Val Loss: 0.0488\n",
            "Epoch 36/100, Train Loss: 0.0427, Val Loss: 0.0488\n",
            "Epoch 37/100, Train Loss: 0.0445, Val Loss: 0.0488\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0425, Val Loss: 0.0488\n",
            "Epoch 39/100, Train Loss: 0.0434, Val Loss: 0.0488\n",
            "Epoch 40/100, Train Loss: 0.0435, Val Loss: 0.0488\n",
            "Epoch 41/100, Train Loss: 0.0412, Val Loss: 0.0488\n",
            "Epoch 42/100, Train Loss: 0.0426, Val Loss: 0.0488\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0440, Val Loss: 0.0488\n",
            "Epoch 44/100, Train Loss: 0.0443, Val Loss: 0.0488\n",
            "Epoch 45/100, Train Loss: 0.0411, Val Loss: 0.0488\n",
            "Epoch 46/100, Train Loss: 0.0423, Val Loss: 0.0488\n",
            "Epoch 47/100, Train Loss: 0.0433, Val Loss: 0.0488\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0461, Val Loss: 0.0488\n",
            "Epoch 49/100, Train Loss: 0.0457, Val Loss: 0.0488\n",
            "Epoch 50/100, Train Loss: 0.0452, Val Loss: 0.0488\n",
            "Epoch 51/100, Train Loss: 0.0445, Val Loss: 0.0488\n",
            "Epoch 52/100, Train Loss: 0.0479, Val Loss: 0.0488\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0424, Val Loss: 0.0488\n",
            "Epoch 54/100, Train Loss: 0.0404, Val Loss: 0.0488\n",
            "Epoch 55/100, Train Loss: 0.0405, Val Loss: 0.0488\n",
            "Epoch 56/100, Train Loss: 0.0409, Val Loss: 0.0488\n",
            "Epoch 57/100, Train Loss: 0.0447, Val Loss: 0.0488\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0428, Val Loss: 0.0488\n",
            "Epoch 59/100, Train Loss: 0.0438, Val Loss: 0.0488\n",
            "Epoch 60/100, Train Loss: 0.0442, Val Loss: 0.0488\n",
            "Epoch 61/100, Train Loss: 0.0418, Val Loss: 0.0488\n",
            "Epoch 62/100, Train Loss: 0.0467, Val Loss: 0.0488\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0417, Val Loss: 0.0488\n",
            "Epoch 64/100, Train Loss: 0.0403, Val Loss: 0.0488\n",
            "Epoch 65/100, Train Loss: 0.0436, Val Loss: 0.0488\n",
            "Epoch 66/100, Train Loss: 0.0433, Val Loss: 0.0488\n",
            "Epoch 67/100, Train Loss: 0.0391, Val Loss: 0.0488\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0429, Val Loss: 0.0488\n",
            "Epoch 69/100, Train Loss: 0.0446, Val Loss: 0.0488\n",
            "Epoch 70/100, Train Loss: 0.0427, Val Loss: 0.0488\n",
            "Epoch 71/100, Train Loss: 0.0392, Val Loss: 0.0488\n",
            "Epoch 72/100, Train Loss: 0.0448, Val Loss: 0.0488\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0443, Val Loss: 0.0488\n",
            "Epoch 74/100, Train Loss: 0.0446, Val Loss: 0.0488\n",
            "Epoch 75/100, Train Loss: 0.0425, Val Loss: 0.0488\n",
            "Epoch 76/100, Train Loss: 0.0428, Val Loss: 0.0488\n",
            "Epoch 77/100, Train Loss: 0.0424, Val Loss: 0.0488\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0411, Val Loss: 0.0488\n",
            "Epoch 79/100, Train Loss: 0.0446, Val Loss: 0.0488\n",
            "Epoch 80/100, Train Loss: 0.0417, Val Loss: 0.0488\n",
            "Epoch 81/100, Train Loss: 0.0421, Val Loss: 0.0488\n",
            "Epoch 82/100, Train Loss: 0.0395, Val Loss: 0.0488\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0422, Val Loss: 0.0488\n",
            "Epoch 84/100, Train Loss: 0.0425, Val Loss: 0.0488\n",
            "Epoch 85/100, Train Loss: 0.0400, Val Loss: 0.0488\n",
            "Epoch 86/100, Train Loss: 0.0445, Val Loss: 0.0488\n",
            "Epoch 87/100, Train Loss: 0.0442, Val Loss: 0.0488\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0422, Val Loss: 0.0488\n",
            "Epoch 89/100, Train Loss: 0.0388, Val Loss: 0.0488\n",
            "Epoch 90/100, Train Loss: 0.0443, Val Loss: 0.0488\n",
            "Epoch 91/100, Train Loss: 0.0409, Val Loss: 0.0488\n",
            "Epoch 92/100, Train Loss: 0.0411, Val Loss: 0.0488\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0450, Val Loss: 0.0488\n",
            "Epoch 94/100, Train Loss: 0.0425, Val Loss: 0.0488\n",
            "Epoch 95/100, Train Loss: 0.0392, Val Loss: 0.0488\n",
            "Epoch 96/100, Train Loss: 0.0416, Val Loss: 0.0488\n",
            "Epoch 97/100, Train Loss: 0.0396, Val Loss: 0.0488\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0381, Val Loss: 0.0488\n",
            "Epoch 99/100, Train Loss: 0.0403, Val Loss: 0.0488\n",
            "Epoch 100/100, Train Loss: 0.0475, Val Loss: 0.0488\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: cnn, Units/Estimators: CNN\n",
            "Dropout: 0.2, Dense Units: 64\n",
            "Batch Size: 4, Epochs: 100\n",
            "Device: cuda\n",
            "Total test samples: 14\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/14\n",
            "Current training set size: 113 samples\n",
            "Epoch 1/100, Train Loss: 0.1255, Val Loss: 0.3152\n",
            "Epoch 2/100, Train Loss: 0.0887, Val Loss: 0.2940\n",
            "Epoch 3/100, Train Loss: 0.0713, Val Loss: 0.2034\n",
            "Epoch 4/100, Train Loss: 0.0611, Val Loss: 0.1865\n",
            "Epoch 5/100, Train Loss: 0.0620, Val Loss: 0.1740\n",
            "Epoch 6/100, Train Loss: 0.0593, Val Loss: 0.1696\n",
            "Epoch 7/100, Train Loss: 0.0510, Val Loss: 0.1494\n",
            "Epoch 8/100, Train Loss: 0.0538, Val Loss: 0.1322\n",
            "Epoch 9/100, Train Loss: 0.0509, Val Loss: 0.1216\n",
            "Epoch 10/100, Train Loss: 0.0441, Val Loss: 0.1102\n",
            "Epoch 11/100, Train Loss: 0.0556, Val Loss: 0.0937\n",
            "Epoch 12/100, Train Loss: 0.0484, Val Loss: 0.1071\n",
            "Epoch 13/100, Train Loss: 0.0451, Val Loss: 0.0976\n",
            "Epoch 14/100, Train Loss: 0.0453, Val Loss: 0.0855\n",
            "Epoch 15/100, Train Loss: 0.0392, Val Loss: 0.0802\n",
            "Epoch 16/100, Train Loss: 0.0534, Val Loss: 0.0800\n",
            "Epoch 17/100, Train Loss: 0.0408, Val Loss: 0.1420\n",
            "Epoch 18/100, Train Loss: 0.0399, Val Loss: 0.0553\n",
            "Epoch 19/100, Train Loss: 0.0369, Val Loss: 0.0579\n",
            "Epoch 20/100, Train Loss: 0.0403, Val Loss: 0.0985\n",
            "Epoch 21/100, Train Loss: 0.0324, Val Loss: 0.1224\n",
            "Epoch 22/100, Train Loss: 0.0337, Val Loss: 0.0885\n",
            "Epoch 23/100, Train Loss: 0.0364, Val Loss: 0.0804\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0381, Val Loss: 0.0799\n",
            "Epoch 25/100, Train Loss: 0.0349, Val Loss: 0.0791\n",
            "Epoch 26/100, Train Loss: 0.0325, Val Loss: 0.0779\n",
            "Epoch 27/100, Train Loss: 0.0326, Val Loss: 0.0773\n",
            "Epoch 28/100, Train Loss: 0.0344, Val Loss: 0.0775\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0426, Val Loss: 0.0775\n",
            "Epoch 30/100, Train Loss: 0.0283, Val Loss: 0.0774\n",
            "Epoch 31/100, Train Loss: 0.0317, Val Loss: 0.0774\n",
            "Epoch 32/100, Train Loss: 0.0315, Val Loss: 0.0774\n",
            "Epoch 33/100, Train Loss: 0.0353, Val Loss: 0.0774\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0311, Val Loss: 0.0774\n",
            "Epoch 35/100, Train Loss: 0.0328, Val Loss: 0.0774\n",
            "Epoch 36/100, Train Loss: 0.0330, Val Loss: 0.0774\n",
            "Epoch 37/100, Train Loss: 0.0308, Val Loss: 0.0774\n",
            "Epoch 38/100, Train Loss: 0.0303, Val Loss: 0.0774\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0347, Val Loss: 0.0774\n",
            "Epoch 40/100, Train Loss: 0.0351, Val Loss: 0.0774\n",
            "Epoch 41/100, Train Loss: 0.0337, Val Loss: 0.0774\n",
            "Epoch 42/100, Train Loss: 0.0335, Val Loss: 0.0774\n",
            "Epoch 43/100, Train Loss: 0.0312, Val Loss: 0.0774\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0322, Val Loss: 0.0774\n",
            "Epoch 45/100, Train Loss: 0.0348, Val Loss: 0.0774\n",
            "Epoch 46/100, Train Loss: 0.0335, Val Loss: 0.0774\n",
            "Epoch 47/100, Train Loss: 0.0342, Val Loss: 0.0774\n",
            "Epoch 48/100, Train Loss: 0.0333, Val Loss: 0.0774\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0404, Val Loss: 0.0774\n",
            "Epoch 50/100, Train Loss: 0.0344, Val Loss: 0.0774\n",
            "Epoch 51/100, Train Loss: 0.0331, Val Loss: 0.0774\n",
            "Epoch 52/100, Train Loss: 0.0361, Val Loss: 0.0774\n",
            "Epoch 53/100, Train Loss: 0.0331, Val Loss: 0.0774\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0368, Val Loss: 0.0774\n",
            "Epoch 55/100, Train Loss: 0.0317, Val Loss: 0.0774\n",
            "Epoch 56/100, Train Loss: 0.0350, Val Loss: 0.0774\n",
            "Epoch 57/100, Train Loss: 0.0333, Val Loss: 0.0774\n",
            "Epoch 58/100, Train Loss: 0.0332, Val Loss: 0.0774\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0349, Val Loss: 0.0774\n",
            "Epoch 60/100, Train Loss: 0.0305, Val Loss: 0.0774\n",
            "Epoch 61/100, Train Loss: 0.0341, Val Loss: 0.0774\n",
            "Epoch 62/100, Train Loss: 0.0317, Val Loss: 0.0774\n",
            "Epoch 63/100, Train Loss: 0.0344, Val Loss: 0.0774\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0365, Val Loss: 0.0774\n",
            "Epoch 65/100, Train Loss: 0.0354, Val Loss: 0.0774\n",
            "Epoch 66/100, Train Loss: 0.0358, Val Loss: 0.0774\n",
            "Epoch 67/100, Train Loss: 0.0345, Val Loss: 0.0774\n",
            "Epoch 68/100, Train Loss: 0.0318, Val Loss: 0.0774\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0320, Val Loss: 0.0774\n",
            "Epoch 70/100, Train Loss: 0.0321, Val Loss: 0.0774\n",
            "Epoch 71/100, Train Loss: 0.0323, Val Loss: 0.0774\n",
            "Epoch 72/100, Train Loss: 0.0350, Val Loss: 0.0774\n",
            "Epoch 73/100, Train Loss: 0.0319, Val Loss: 0.0774\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0328, Val Loss: 0.0774\n",
            "Epoch 75/100, Train Loss: 0.0319, Val Loss: 0.0774\n",
            "Epoch 76/100, Train Loss: 0.0358, Val Loss: 0.0774\n",
            "Epoch 77/100, Train Loss: 0.0372, Val Loss: 0.0774\n",
            "Epoch 78/100, Train Loss: 0.0347, Val Loss: 0.0774\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0339, Val Loss: 0.0774\n",
            "Epoch 80/100, Train Loss: 0.0333, Val Loss: 0.0774\n",
            "Epoch 81/100, Train Loss: 0.0312, Val Loss: 0.0774\n",
            "Epoch 82/100, Train Loss: 0.0355, Val Loss: 0.0774\n",
            "Epoch 83/100, Train Loss: 0.0341, Val Loss: 0.0774\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0352, Val Loss: 0.0774\n",
            "Epoch 85/100, Train Loss: 0.0343, Val Loss: 0.0774\n",
            "Epoch 86/100, Train Loss: 0.0339, Val Loss: 0.0774\n",
            "Epoch 87/100, Train Loss: 0.0358, Val Loss: 0.0774\n",
            "Epoch 88/100, Train Loss: 0.0316, Val Loss: 0.0774\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0373, Val Loss: 0.0774\n",
            "Epoch 90/100, Train Loss: 0.0351, Val Loss: 0.0774\n",
            "Epoch 91/100, Train Loss: 0.0330, Val Loss: 0.0774\n",
            "Epoch 92/100, Train Loss: 0.0329, Val Loss: 0.0774\n",
            "Epoch 93/100, Train Loss: 0.0353, Val Loss: 0.0774\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0318, Val Loss: 0.0774\n",
            "Epoch 95/100, Train Loss: 0.0329, Val Loss: 0.0774\n",
            "Epoch 96/100, Train Loss: 0.0337, Val Loss: 0.0774\n",
            "Epoch 97/100, Train Loss: 0.0327, Val Loss: 0.0774\n",
            "Epoch 98/100, Train Loss: 0.0332, Val Loss: 0.0774\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0351, Val Loss: 0.0774\n",
            "Epoch 100/100, Train Loss: 0.0334, Val Loss: 0.0774\n",
            "\n",
            "Test iteration 2/14\n",
            "Current training set size: 114 samples\n",
            "Epoch 1/100, Train Loss: 0.1326, Val Loss: 0.2837\n",
            "Epoch 2/100, Train Loss: 0.1027, Val Loss: 0.2232\n",
            "Epoch 3/100, Train Loss: 0.0754, Val Loss: 0.1926\n",
            "Epoch 4/100, Train Loss: 0.0689, Val Loss: 0.1843\n",
            "Epoch 5/100, Train Loss: 0.0669, Val Loss: 0.1732\n",
            "Epoch 6/100, Train Loss: 0.0579, Val Loss: 0.1434\n",
            "Epoch 7/100, Train Loss: 0.0546, Val Loss: 0.1125\n",
            "Epoch 8/100, Train Loss: 0.0569, Val Loss: 0.1051\n",
            "Epoch 9/100, Train Loss: 0.0552, Val Loss: 0.0583\n",
            "Epoch 10/100, Train Loss: 0.0462, Val Loss: 0.0907\n",
            "Epoch 11/100, Train Loss: 0.0437, Val Loss: 0.0773\n",
            "Epoch 12/100, Train Loss: 0.0423, Val Loss: 0.0790\n",
            "Epoch 13/100, Train Loss: 0.0488, Val Loss: 0.0877\n",
            "Epoch 14/100, Train Loss: 0.0431, Val Loss: 0.0500\n",
            "Epoch 15/100, Train Loss: 0.0428, Val Loss: 0.0683\n",
            "Epoch 16/100, Train Loss: 0.0419, Val Loss: 0.0544\n",
            "Epoch 17/100, Train Loss: 0.0448, Val Loss: 0.1210\n",
            "Epoch 18/100, Train Loss: 0.0440, Val Loss: 0.1045\n",
            "Epoch 19/100, Train Loss: 0.0404, Val Loss: 0.1374\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0406, Val Loss: 0.1358\n",
            "Epoch 21/100, Train Loss: 0.0360, Val Loss: 0.1332\n",
            "Epoch 22/100, Train Loss: 0.0344, Val Loss: 0.1310\n",
            "Epoch 23/100, Train Loss: 0.0338, Val Loss: 0.1295\n",
            "Epoch 24/100, Train Loss: 0.0345, Val Loss: 0.1268\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0374, Val Loss: 0.1268\n",
            "Epoch 26/100, Train Loss: 0.0321, Val Loss: 0.1268\n",
            "Epoch 27/100, Train Loss: 0.0342, Val Loss: 0.1268\n",
            "Epoch 28/100, Train Loss: 0.0384, Val Loss: 0.1268\n",
            "Epoch 29/100, Train Loss: 0.0344, Val Loss: 0.1267\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0346, Val Loss: 0.1267\n",
            "Epoch 31/100, Train Loss: 0.0340, Val Loss: 0.1267\n",
            "Epoch 32/100, Train Loss: 0.0351, Val Loss: 0.1267\n",
            "Epoch 33/100, Train Loss: 0.0361, Val Loss: 0.1267\n",
            "Epoch 34/100, Train Loss: 0.0392, Val Loss: 0.1267\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0331, Val Loss: 0.1267\n",
            "Epoch 36/100, Train Loss: 0.0354, Val Loss: 0.1267\n",
            "Epoch 37/100, Train Loss: 0.0362, Val Loss: 0.1267\n",
            "Epoch 38/100, Train Loss: 0.0367, Val Loss: 0.1267\n",
            "Epoch 39/100, Train Loss: 0.0343, Val Loss: 0.1267\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0346, Val Loss: 0.1267\n",
            "Epoch 41/100, Train Loss: 0.0364, Val Loss: 0.1267\n",
            "Epoch 42/100, Train Loss: 0.0347, Val Loss: 0.1267\n",
            "Epoch 43/100, Train Loss: 0.0357, Val Loss: 0.1267\n",
            "Epoch 44/100, Train Loss: 0.0354, Val Loss: 0.1267\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0400, Val Loss: 0.1267\n",
            "Epoch 46/100, Train Loss: 0.0353, Val Loss: 0.1267\n",
            "Epoch 47/100, Train Loss: 0.0354, Val Loss: 0.1267\n",
            "Epoch 48/100, Train Loss: 0.0366, Val Loss: 0.1267\n",
            "Epoch 49/100, Train Loss: 0.0351, Val Loss: 0.1267\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0353, Val Loss: 0.1267\n",
            "Epoch 51/100, Train Loss: 0.0368, Val Loss: 0.1267\n",
            "Epoch 52/100, Train Loss: 0.0373, Val Loss: 0.1267\n",
            "Epoch 53/100, Train Loss: 0.0338, Val Loss: 0.1267\n",
            "Epoch 54/100, Train Loss: 0.0341, Val Loss: 0.1267\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0358, Val Loss: 0.1267\n",
            "Epoch 56/100, Train Loss: 0.0349, Val Loss: 0.1267\n",
            "Epoch 57/100, Train Loss: 0.0356, Val Loss: 0.1267\n",
            "Epoch 58/100, Train Loss: 0.0330, Val Loss: 0.1267\n",
            "Epoch 59/100, Train Loss: 0.0338, Val Loss: 0.1267\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0375, Val Loss: 0.1267\n",
            "Epoch 61/100, Train Loss: 0.0360, Val Loss: 0.1267\n",
            "Epoch 62/100, Train Loss: 0.0317, Val Loss: 0.1267\n",
            "Epoch 63/100, Train Loss: 0.0339, Val Loss: 0.1267\n",
            "Epoch 64/100, Train Loss: 0.0364, Val Loss: 0.1267\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0371, Val Loss: 0.1267\n",
            "Epoch 66/100, Train Loss: 0.0364, Val Loss: 0.1267\n",
            "Epoch 67/100, Train Loss: 0.0350, Val Loss: 0.1267\n",
            "Epoch 68/100, Train Loss: 0.0343, Val Loss: 0.1267\n",
            "Epoch 69/100, Train Loss: 0.0353, Val Loss: 0.1267\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0350, Val Loss: 0.1267\n",
            "Epoch 71/100, Train Loss: 0.0363, Val Loss: 0.1267\n",
            "Epoch 72/100, Train Loss: 0.0329, Val Loss: 0.1267\n",
            "Epoch 73/100, Train Loss: 0.0345, Val Loss: 0.1267\n",
            "Epoch 74/100, Train Loss: 0.0389, Val Loss: 0.1267\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0336, Val Loss: 0.1267\n",
            "Epoch 76/100, Train Loss: 0.0380, Val Loss: 0.1267\n",
            "Epoch 77/100, Train Loss: 0.0372, Val Loss: 0.1267\n",
            "Epoch 78/100, Train Loss: 0.0329, Val Loss: 0.1267\n",
            "Epoch 79/100, Train Loss: 0.0356, Val Loss: 0.1267\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0365, Val Loss: 0.1267\n",
            "Epoch 81/100, Train Loss: 0.0370, Val Loss: 0.1267\n",
            "Epoch 82/100, Train Loss: 0.0340, Val Loss: 0.1267\n",
            "Epoch 83/100, Train Loss: 0.0328, Val Loss: 0.1267\n",
            "Epoch 84/100, Train Loss: 0.0353, Val Loss: 0.1267\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0347, Val Loss: 0.1267\n",
            "Epoch 86/100, Train Loss: 0.0354, Val Loss: 0.1267\n",
            "Epoch 87/100, Train Loss: 0.0383, Val Loss: 0.1267\n",
            "Epoch 88/100, Train Loss: 0.0371, Val Loss: 0.1267\n",
            "Epoch 89/100, Train Loss: 0.0364, Val Loss: 0.1267\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0342, Val Loss: 0.1267\n",
            "Epoch 91/100, Train Loss: 0.0374, Val Loss: 0.1267\n",
            "Epoch 92/100, Train Loss: 0.0342, Val Loss: 0.1267\n",
            "Epoch 93/100, Train Loss: 0.0355, Val Loss: 0.1267\n",
            "Epoch 94/100, Train Loss: 0.0346, Val Loss: 0.1267\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0349, Val Loss: 0.1267\n",
            "Epoch 96/100, Train Loss: 0.0347, Val Loss: 0.1267\n",
            "Epoch 97/100, Train Loss: 0.0338, Val Loss: 0.1267\n",
            "Epoch 98/100, Train Loss: 0.0376, Val Loss: 0.1267\n",
            "Epoch 99/100, Train Loss: 0.0371, Val Loss: 0.1267\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0350, Val Loss: 0.1267\n",
            "\n",
            "Test iteration 3/14\n",
            "Current training set size: 115 samples\n",
            "Epoch 1/100, Train Loss: 0.1300, Val Loss: 0.3150\n",
            "Epoch 2/100, Train Loss: 0.0877, Val Loss: 0.2019\n",
            "Epoch 3/100, Train Loss: 0.0708, Val Loss: 0.1973\n",
            "Epoch 4/100, Train Loss: 0.0629, Val Loss: 0.1747\n",
            "Epoch 5/100, Train Loss: 0.0601, Val Loss: 0.1669\n",
            "Epoch 6/100, Train Loss: 0.0584, Val Loss: 0.1361\n",
            "Epoch 7/100, Train Loss: 0.0571, Val Loss: 0.1239\n",
            "Epoch 8/100, Train Loss: 0.0579, Val Loss: 0.0948\n",
            "Epoch 9/100, Train Loss: 0.0575, Val Loss: 0.2583\n",
            "Epoch 10/100, Train Loss: 0.0529, Val Loss: 0.1127\n",
            "Epoch 11/100, Train Loss: 0.0544, Val Loss: 0.0823\n",
            "Epoch 12/100, Train Loss: 0.0470, Val Loss: 0.0713\n",
            "Epoch 13/100, Train Loss: 0.0487, Val Loss: 0.0487\n",
            "Epoch 14/100, Train Loss: 0.0431, Val Loss: 0.1000\n",
            "Epoch 15/100, Train Loss: 0.0438, Val Loss: 0.0482\n",
            "Epoch 16/100, Train Loss: 0.0414, Val Loss: 0.0454\n",
            "Epoch 17/100, Train Loss: 0.0404, Val Loss: 0.0516\n",
            "Epoch 18/100, Train Loss: 0.0458, Val Loss: 0.0476\n",
            "Epoch 19/100, Train Loss: 0.0409, Val Loss: 0.0591\n",
            "Epoch 20/100, Train Loss: 0.0363, Val Loss: 0.0455\n",
            "Epoch 21/100, Train Loss: 0.0389, Val Loss: 0.0533\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0376, Val Loss: 0.0531\n",
            "Epoch 23/100, Train Loss: 0.0351, Val Loss: 0.0528\n",
            "Epoch 24/100, Train Loss: 0.0335, Val Loss: 0.0523\n",
            "Epoch 25/100, Train Loss: 0.0290, Val Loss: 0.0525\n",
            "Epoch 26/100, Train Loss: 0.0320, Val Loss: 0.0522\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0346, Val Loss: 0.0522\n",
            "Epoch 28/100, Train Loss: 0.0299, Val Loss: 0.0522\n",
            "Epoch 29/100, Train Loss: 0.0323, Val Loss: 0.0522\n",
            "Epoch 30/100, Train Loss: 0.0314, Val Loss: 0.0522\n",
            "Epoch 31/100, Train Loss: 0.0341, Val Loss: 0.0522\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0333, Val Loss: 0.0522\n",
            "Epoch 33/100, Train Loss: 0.0317, Val Loss: 0.0522\n",
            "Epoch 34/100, Train Loss: 0.0301, Val Loss: 0.0522\n",
            "Epoch 35/100, Train Loss: 0.0323, Val Loss: 0.0522\n",
            "Epoch 36/100, Train Loss: 0.0345, Val Loss: 0.0522\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0329, Val Loss: 0.0522\n",
            "Epoch 38/100, Train Loss: 0.0320, Val Loss: 0.0522\n",
            "Epoch 39/100, Train Loss: 0.0307, Val Loss: 0.0522\n",
            "Epoch 40/100, Train Loss: 0.0353, Val Loss: 0.0522\n",
            "Epoch 41/100, Train Loss: 0.0325, Val Loss: 0.0522\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0323, Val Loss: 0.0522\n",
            "Epoch 43/100, Train Loss: 0.0316, Val Loss: 0.0522\n",
            "Epoch 44/100, Train Loss: 0.0292, Val Loss: 0.0522\n",
            "Epoch 45/100, Train Loss: 0.0338, Val Loss: 0.0522\n",
            "Epoch 46/100, Train Loss: 0.0339, Val Loss: 0.0522\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0351, Val Loss: 0.0522\n",
            "Epoch 48/100, Train Loss: 0.0331, Val Loss: 0.0522\n",
            "Epoch 49/100, Train Loss: 0.0314, Val Loss: 0.0522\n",
            "Epoch 50/100, Train Loss: 0.0328, Val Loss: 0.0522\n",
            "Epoch 51/100, Train Loss: 0.0320, Val Loss: 0.0522\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0268, Val Loss: 0.0522\n",
            "Epoch 53/100, Train Loss: 0.0320, Val Loss: 0.0522\n",
            "Epoch 54/100, Train Loss: 0.0330, Val Loss: 0.0522\n",
            "Epoch 55/100, Train Loss: 0.0312, Val Loss: 0.0522\n",
            "Epoch 56/100, Train Loss: 0.0350, Val Loss: 0.0522\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0350, Val Loss: 0.0522\n",
            "Epoch 58/100, Train Loss: 0.0339, Val Loss: 0.0522\n",
            "Epoch 59/100, Train Loss: 0.0367, Val Loss: 0.0522\n",
            "Epoch 60/100, Train Loss: 0.0330, Val Loss: 0.0522\n",
            "Epoch 61/100, Train Loss: 0.0386, Val Loss: 0.0522\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0349, Val Loss: 0.0522\n",
            "Epoch 63/100, Train Loss: 0.0333, Val Loss: 0.0522\n",
            "Epoch 64/100, Train Loss: 0.0369, Val Loss: 0.0522\n",
            "Epoch 65/100, Train Loss: 0.0319, Val Loss: 0.0522\n",
            "Epoch 66/100, Train Loss: 0.0341, Val Loss: 0.0522\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0328, Val Loss: 0.0522\n",
            "Epoch 68/100, Train Loss: 0.0323, Val Loss: 0.0522\n",
            "Epoch 69/100, Train Loss: 0.0289, Val Loss: 0.0522\n",
            "Epoch 70/100, Train Loss: 0.0336, Val Loss: 0.0522\n",
            "Epoch 71/100, Train Loss: 0.0315, Val Loss: 0.0522\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0341, Val Loss: 0.0522\n",
            "Epoch 73/100, Train Loss: 0.0318, Val Loss: 0.0522\n",
            "Epoch 74/100, Train Loss: 0.0366, Val Loss: 0.0522\n",
            "Epoch 75/100, Train Loss: 0.0341, Val Loss: 0.0522\n",
            "Epoch 76/100, Train Loss: 0.0373, Val Loss: 0.0522\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0345, Val Loss: 0.0522\n",
            "Epoch 78/100, Train Loss: 0.0323, Val Loss: 0.0522\n",
            "Epoch 79/100, Train Loss: 0.0323, Val Loss: 0.0522\n",
            "Epoch 80/100, Train Loss: 0.0287, Val Loss: 0.0522\n",
            "Epoch 81/100, Train Loss: 0.0311, Val Loss: 0.0522\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0318, Val Loss: 0.0522\n",
            "Epoch 83/100, Train Loss: 0.0341, Val Loss: 0.0522\n",
            "Epoch 84/100, Train Loss: 0.0329, Val Loss: 0.0522\n",
            "Epoch 85/100, Train Loss: 0.0328, Val Loss: 0.0522\n",
            "Epoch 86/100, Train Loss: 0.0358, Val Loss: 0.0522\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0357, Val Loss: 0.0522\n",
            "Epoch 88/100, Train Loss: 0.0318, Val Loss: 0.0522\n",
            "Epoch 89/100, Train Loss: 0.0332, Val Loss: 0.0522\n",
            "Epoch 90/100, Train Loss: 0.0351, Val Loss: 0.0522\n",
            "Epoch 91/100, Train Loss: 0.0337, Val Loss: 0.0522\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0323, Val Loss: 0.0522\n",
            "Epoch 93/100, Train Loss: 0.0342, Val Loss: 0.0522\n",
            "Epoch 94/100, Train Loss: 0.0357, Val Loss: 0.0522\n",
            "Epoch 95/100, Train Loss: 0.0355, Val Loss: 0.0522\n",
            "Epoch 96/100, Train Loss: 0.0318, Val Loss: 0.0522\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0330, Val Loss: 0.0522\n",
            "Epoch 98/100, Train Loss: 0.0308, Val Loss: 0.0522\n",
            "Epoch 99/100, Train Loss: 0.0309, Val Loss: 0.0522\n",
            "Epoch 100/100, Train Loss: 0.0331, Val Loss: 0.0522\n",
            "\n",
            "Test iteration 4/14\n",
            "Current training set size: 116 samples\n",
            "Epoch 1/100, Train Loss: 0.1519, Val Loss: 0.2797\n",
            "Epoch 2/100, Train Loss: 0.0947, Val Loss: 0.2382\n",
            "Epoch 3/100, Train Loss: 0.0732, Val Loss: 0.1890\n",
            "Epoch 4/100, Train Loss: 0.0697, Val Loss: 0.1716\n",
            "Epoch 5/100, Train Loss: 0.0592, Val Loss: 0.1564\n",
            "Epoch 6/100, Train Loss: 0.0622, Val Loss: 0.1245\n",
            "Epoch 7/100, Train Loss: 0.0579, Val Loss: 0.0898\n",
            "Epoch 8/100, Train Loss: 0.0537, Val Loss: 0.0936\n",
            "Epoch 9/100, Train Loss: 0.0474, Val Loss: 0.1092\n",
            "Epoch 10/100, Train Loss: 0.0463, Val Loss: 0.0753\n",
            "Epoch 11/100, Train Loss: 0.0460, Val Loss: 0.0790\n",
            "Epoch 12/100, Train Loss: 0.0454, Val Loss: 0.0539\n",
            "Epoch 13/100, Train Loss: 0.0487, Val Loss: 0.1317\n",
            "Epoch 14/100, Train Loss: 0.0421, Val Loss: 0.0504\n",
            "Epoch 15/100, Train Loss: 0.0400, Val Loss: 0.0490\n",
            "Epoch 16/100, Train Loss: 0.0404, Val Loss: 0.0557\n",
            "Epoch 17/100, Train Loss: 0.0524, Val Loss: 0.0597\n",
            "Epoch 18/100, Train Loss: 0.0401, Val Loss: 0.0391\n",
            "Epoch 19/100, Train Loss: 0.0404, Val Loss: 0.0635\n",
            "Epoch 20/100, Train Loss: 0.0399, Val Loss: 0.0382\n",
            "Epoch 21/100, Train Loss: 0.0394, Val Loss: 0.0737\n",
            "Epoch 22/100, Train Loss: 0.0458, Val Loss: 0.0870\n",
            "Epoch 23/100, Train Loss: 0.0422, Val Loss: 0.0437\n",
            "Epoch 24/100, Train Loss: 0.0405, Val Loss: 0.0434\n",
            "Epoch 25/100, Train Loss: 0.0463, Val Loss: 0.0643\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0340, Val Loss: 0.0639\n",
            "Epoch 27/100, Train Loss: 0.0345, Val Loss: 0.0631\n",
            "Epoch 28/100, Train Loss: 0.0327, Val Loss: 0.0627\n",
            "Epoch 29/100, Train Loss: 0.0341, Val Loss: 0.0618\n",
            "Epoch 30/100, Train Loss: 0.0323, Val Loss: 0.0623\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0309, Val Loss: 0.0623\n",
            "Epoch 32/100, Train Loss: 0.0338, Val Loss: 0.0623\n",
            "Epoch 33/100, Train Loss: 0.0306, Val Loss: 0.0623\n",
            "Epoch 34/100, Train Loss: 0.0361, Val Loss: 0.0623\n",
            "Epoch 35/100, Train Loss: 0.0344, Val Loss: 0.0623\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0321, Val Loss: 0.0623\n",
            "Epoch 37/100, Train Loss: 0.0337, Val Loss: 0.0623\n",
            "Epoch 38/100, Train Loss: 0.0362, Val Loss: 0.0623\n",
            "Epoch 39/100, Train Loss: 0.0357, Val Loss: 0.0623\n",
            "Epoch 40/100, Train Loss: 0.0365, Val Loss: 0.0623\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0338, Val Loss: 0.0623\n",
            "Epoch 42/100, Train Loss: 0.0360, Val Loss: 0.0623\n",
            "Epoch 43/100, Train Loss: 0.0346, Val Loss: 0.0623\n",
            "Epoch 44/100, Train Loss: 0.0341, Val Loss: 0.0623\n",
            "Epoch 45/100, Train Loss: 0.0323, Val Loss: 0.0623\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0354, Val Loss: 0.0623\n",
            "Epoch 47/100, Train Loss: 0.0327, Val Loss: 0.0623\n",
            "Epoch 48/100, Train Loss: 0.0373, Val Loss: 0.0623\n",
            "Epoch 49/100, Train Loss: 0.0329, Val Loss: 0.0623\n",
            "Epoch 50/100, Train Loss: 0.0330, Val Loss: 0.0623\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0342, Val Loss: 0.0623\n",
            "Epoch 52/100, Train Loss: 0.0357, Val Loss: 0.0623\n",
            "Epoch 53/100, Train Loss: 0.0319, Val Loss: 0.0623\n",
            "Epoch 54/100, Train Loss: 0.0318, Val Loss: 0.0623\n",
            "Epoch 55/100, Train Loss: 0.0300, Val Loss: 0.0623\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0314, Val Loss: 0.0623\n",
            "Epoch 57/100, Train Loss: 0.0332, Val Loss: 0.0623\n",
            "Epoch 58/100, Train Loss: 0.0334, Val Loss: 0.0623\n",
            "Epoch 59/100, Train Loss: 0.0333, Val Loss: 0.0623\n",
            "Epoch 60/100, Train Loss: 0.0327, Val Loss: 0.0623\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0360, Val Loss: 0.0623\n",
            "Epoch 62/100, Train Loss: 0.0325, Val Loss: 0.0623\n",
            "Epoch 63/100, Train Loss: 0.0332, Val Loss: 0.0623\n",
            "Epoch 64/100, Train Loss: 0.0348, Val Loss: 0.0623\n",
            "Epoch 65/100, Train Loss: 0.0318, Val Loss: 0.0623\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0366, Val Loss: 0.0623\n",
            "Epoch 67/100, Train Loss: 0.0332, Val Loss: 0.0623\n",
            "Epoch 68/100, Train Loss: 0.0326, Val Loss: 0.0623\n",
            "Epoch 69/100, Train Loss: 0.0312, Val Loss: 0.0623\n",
            "Epoch 70/100, Train Loss: 0.0329, Val Loss: 0.0623\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0321, Val Loss: 0.0623\n",
            "Epoch 72/100, Train Loss: 0.0321, Val Loss: 0.0623\n",
            "Epoch 73/100, Train Loss: 0.0365, Val Loss: 0.0623\n",
            "Epoch 74/100, Train Loss: 0.0340, Val Loss: 0.0623\n",
            "Epoch 75/100, Train Loss: 0.0333, Val Loss: 0.0623\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0349, Val Loss: 0.0623\n",
            "Epoch 77/100, Train Loss: 0.0323, Val Loss: 0.0623\n",
            "Epoch 78/100, Train Loss: 0.0327, Val Loss: 0.0623\n",
            "Epoch 79/100, Train Loss: 0.0362, Val Loss: 0.0623\n",
            "Epoch 80/100, Train Loss: 0.0324, Val Loss: 0.0623\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0347, Val Loss: 0.0623\n",
            "Epoch 82/100, Train Loss: 0.0356, Val Loss: 0.0623\n",
            "Epoch 83/100, Train Loss: 0.0331, Val Loss: 0.0623\n",
            "Epoch 84/100, Train Loss: 0.0366, Val Loss: 0.0623\n",
            "Epoch 85/100, Train Loss: 0.0316, Val Loss: 0.0623\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0339, Val Loss: 0.0623\n",
            "Epoch 87/100, Train Loss: 0.0338, Val Loss: 0.0623\n",
            "Epoch 88/100, Train Loss: 0.0346, Val Loss: 0.0623\n",
            "Epoch 89/100, Train Loss: 0.0355, Val Loss: 0.0623\n",
            "Epoch 90/100, Train Loss: 0.0356, Val Loss: 0.0623\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0322, Val Loss: 0.0623\n",
            "Epoch 92/100, Train Loss: 0.0332, Val Loss: 0.0623\n",
            "Epoch 93/100, Train Loss: 0.0348, Val Loss: 0.0623\n",
            "Epoch 94/100, Train Loss: 0.0346, Val Loss: 0.0623\n",
            "Epoch 95/100, Train Loss: 0.0332, Val Loss: 0.0623\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0340, Val Loss: 0.0623\n",
            "Epoch 97/100, Train Loss: 0.0347, Val Loss: 0.0623\n",
            "Epoch 98/100, Train Loss: 0.0347, Val Loss: 0.0623\n",
            "Epoch 99/100, Train Loss: 0.0325, Val Loss: 0.0623\n",
            "Epoch 100/100, Train Loss: 0.0351, Val Loss: 0.0623\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 5/14\n",
            "Current training set size: 117 samples\n",
            "Epoch 1/100, Train Loss: 0.1250, Val Loss: 0.3255\n",
            "Epoch 2/100, Train Loss: 0.0803, Val Loss: 0.2065\n",
            "Epoch 3/100, Train Loss: 0.0736, Val Loss: 0.1724\n",
            "Epoch 4/100, Train Loss: 0.0673, Val Loss: 0.1489\n",
            "Epoch 5/100, Train Loss: 0.0622, Val Loss: 0.1268\n",
            "Epoch 6/100, Train Loss: 0.0597, Val Loss: 0.0900\n",
            "Epoch 7/100, Train Loss: 0.0592, Val Loss: 0.0621\n",
            "Epoch 8/100, Train Loss: 0.0577, Val Loss: 0.0474\n",
            "Epoch 9/100, Train Loss: 0.0568, Val Loss: 0.2015\n",
            "Epoch 10/100, Train Loss: 0.0571, Val Loss: 0.0494\n",
            "Epoch 11/100, Train Loss: 0.0460, Val Loss: 0.0605\n",
            "Epoch 12/100, Train Loss: 0.0514, Val Loss: 0.0869\n",
            "Epoch 13/100, Train Loss: 0.0476, Val Loss: 0.0495\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 14/100, Train Loss: 0.0439, Val Loss: 0.0495\n",
            "Epoch 15/100, Train Loss: 0.0424, Val Loss: 0.0496\n",
            "Epoch 16/100, Train Loss: 0.0394, Val Loss: 0.0497\n",
            "Epoch 17/100, Train Loss: 0.0402, Val Loss: 0.0497\n",
            "Epoch 18/100, Train Loss: 0.0412, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0388, Val Loss: 0.0498\n",
            "Epoch 20/100, Train Loss: 0.0383, Val Loss: 0.0498\n",
            "Epoch 21/100, Train Loss: 0.0412, Val Loss: 0.0498\n",
            "Epoch 22/100, Train Loss: 0.0414, Val Loss: 0.0498\n",
            "Epoch 23/100, Train Loss: 0.0402, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0426, Val Loss: 0.0498\n",
            "Epoch 25/100, Train Loss: 0.0397, Val Loss: 0.0498\n",
            "Epoch 26/100, Train Loss: 0.0384, Val Loss: 0.0498\n",
            "Epoch 27/100, Train Loss: 0.0410, Val Loss: 0.0498\n",
            "Epoch 28/100, Train Loss: 0.0387, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0466, Val Loss: 0.0498\n",
            "Epoch 30/100, Train Loss: 0.0393, Val Loss: 0.0498\n",
            "Epoch 31/100, Train Loss: 0.0413, Val Loss: 0.0498\n",
            "Epoch 32/100, Train Loss: 0.0415, Val Loss: 0.0498\n",
            "Epoch 33/100, Train Loss: 0.0389, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0427, Val Loss: 0.0498\n",
            "Epoch 35/100, Train Loss: 0.0408, Val Loss: 0.0498\n",
            "Epoch 36/100, Train Loss: 0.0411, Val Loss: 0.0498\n",
            "Epoch 37/100, Train Loss: 0.0410, Val Loss: 0.0498\n",
            "Epoch 38/100, Train Loss: 0.0399, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0399, Val Loss: 0.0498\n",
            "Epoch 40/100, Train Loss: 0.0402, Val Loss: 0.0498\n",
            "Epoch 41/100, Train Loss: 0.0396, Val Loss: 0.0498\n",
            "Epoch 42/100, Train Loss: 0.0387, Val Loss: 0.0498\n",
            "Epoch 43/100, Train Loss: 0.0440, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0414, Val Loss: 0.0498\n",
            "Epoch 45/100, Train Loss: 0.0399, Val Loss: 0.0498\n",
            "Epoch 46/100, Train Loss: 0.0394, Val Loss: 0.0498\n",
            "Epoch 47/100, Train Loss: 0.0425, Val Loss: 0.0498\n",
            "Epoch 48/100, Train Loss: 0.0420, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0426, Val Loss: 0.0498\n",
            "Epoch 50/100, Train Loss: 0.0431, Val Loss: 0.0498\n",
            "Epoch 51/100, Train Loss: 0.0492, Val Loss: 0.0498\n",
            "Epoch 52/100, Train Loss: 0.0408, Val Loss: 0.0498\n",
            "Epoch 53/100, Train Loss: 0.0379, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0382, Val Loss: 0.0498\n",
            "Epoch 55/100, Train Loss: 0.0374, Val Loss: 0.0498\n",
            "Epoch 56/100, Train Loss: 0.0378, Val Loss: 0.0498\n",
            "Epoch 57/100, Train Loss: 0.0420, Val Loss: 0.0498\n",
            "Epoch 58/100, Train Loss: 0.0426, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0420, Val Loss: 0.0498\n",
            "Epoch 60/100, Train Loss: 0.0397, Val Loss: 0.0498\n",
            "Epoch 61/100, Train Loss: 0.0398, Val Loss: 0.0498\n",
            "Epoch 62/100, Train Loss: 0.0375, Val Loss: 0.0498\n",
            "Epoch 63/100, Train Loss: 0.0403, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0415, Val Loss: 0.0498\n",
            "Epoch 65/100, Train Loss: 0.0392, Val Loss: 0.0498\n",
            "Epoch 66/100, Train Loss: 0.0407, Val Loss: 0.0498\n",
            "Epoch 67/100, Train Loss: 0.0375, Val Loss: 0.0498\n",
            "Epoch 68/100, Train Loss: 0.0394, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0416, Val Loss: 0.0498\n",
            "Epoch 70/100, Train Loss: 0.0416, Val Loss: 0.0498\n",
            "Epoch 71/100, Train Loss: 0.0414, Val Loss: 0.0498\n",
            "Epoch 72/100, Train Loss: 0.0383, Val Loss: 0.0498\n",
            "Epoch 73/100, Train Loss: 0.0411, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0397, Val Loss: 0.0498\n",
            "Epoch 75/100, Train Loss: 0.0389, Val Loss: 0.0498\n",
            "Epoch 76/100, Train Loss: 0.0384, Val Loss: 0.0498\n",
            "Epoch 77/100, Train Loss: 0.0401, Val Loss: 0.0498\n",
            "Epoch 78/100, Train Loss: 0.0399, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0400, Val Loss: 0.0498\n",
            "Epoch 80/100, Train Loss: 0.0405, Val Loss: 0.0498\n",
            "Epoch 81/100, Train Loss: 0.0365, Val Loss: 0.0498\n",
            "Epoch 82/100, Train Loss: 0.0406, Val Loss: 0.0498\n",
            "Epoch 83/100, Train Loss: 0.0399, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0416, Val Loss: 0.0498\n",
            "Epoch 85/100, Train Loss: 0.0402, Val Loss: 0.0498\n",
            "Epoch 86/100, Train Loss: 0.0398, Val Loss: 0.0498\n",
            "Epoch 87/100, Train Loss: 0.0405, Val Loss: 0.0498\n",
            "Epoch 88/100, Train Loss: 0.0395, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0397, Val Loss: 0.0498\n",
            "Epoch 90/100, Train Loss: 0.0424, Val Loss: 0.0498\n",
            "Epoch 91/100, Train Loss: 0.0413, Val Loss: 0.0498\n",
            "Epoch 92/100, Train Loss: 0.0423, Val Loss: 0.0498\n",
            "Epoch 93/100, Train Loss: 0.0400, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0383, Val Loss: 0.0498\n",
            "Epoch 95/100, Train Loss: 0.0431, Val Loss: 0.0498\n",
            "Epoch 96/100, Train Loss: 0.0431, Val Loss: 0.0498\n",
            "Epoch 97/100, Train Loss: 0.0424, Val Loss: 0.0498\n",
            "Epoch 98/100, Train Loss: 0.0410, Val Loss: 0.0498\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0419, Val Loss: 0.0498\n",
            "Epoch 100/100, Train Loss: 0.0384, Val Loss: 0.0498\n",
            "\n",
            "Test iteration 6/14\n",
            "Current training set size: 118 samples\n",
            "Epoch 1/100, Train Loss: 0.1411, Val Loss: 0.2669\n",
            "Epoch 2/100, Train Loss: 0.0911, Val Loss: 0.1967\n",
            "Epoch 3/100, Train Loss: 0.0781, Val Loss: 0.1801\n",
            "Epoch 4/100, Train Loss: 0.0721, Val Loss: 0.1609\n",
            "Epoch 5/100, Train Loss: 0.0703, Val Loss: 0.1303\n",
            "Epoch 6/100, Train Loss: 0.0693, Val Loss: 0.1015\n",
            "Epoch 7/100, Train Loss: 0.0562, Val Loss: 0.0496\n",
            "Epoch 8/100, Train Loss: 0.0540, Val Loss: 0.0473\n",
            "Epoch 9/100, Train Loss: 0.0497, Val Loss: 0.0616\n",
            "Epoch 10/100, Train Loss: 0.0548, Val Loss: 0.0503\n",
            "Epoch 11/100, Train Loss: 0.0535, Val Loss: 0.0437\n",
            "Epoch 12/100, Train Loss: 0.0488, Val Loss: 0.0536\n",
            "Epoch 13/100, Train Loss: 0.0485, Val Loss: 0.0457\n",
            "Epoch 14/100, Train Loss: 0.0428, Val Loss: 0.0518\n",
            "Epoch 15/100, Train Loss: 0.0455, Val Loss: 0.0629\n",
            "Epoch 16/100, Train Loss: 0.0460, Val Loss: 0.0455\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0396, Val Loss: 0.0452\n",
            "Epoch 18/100, Train Loss: 0.0409, Val Loss: 0.0451\n",
            "Epoch 19/100, Train Loss: 0.0413, Val Loss: 0.0451\n",
            "Epoch 20/100, Train Loss: 0.0375, Val Loss: 0.0448\n",
            "Epoch 21/100, Train Loss: 0.0368, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0371, Val Loss: 0.0446\n",
            "Epoch 23/100, Train Loss: 0.0359, Val Loss: 0.0446\n",
            "Epoch 24/100, Train Loss: 0.0382, Val Loss: 0.0446\n",
            "Epoch 25/100, Train Loss: 0.0377, Val Loss: 0.0446\n",
            "Epoch 26/100, Train Loss: 0.0340, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0401, Val Loss: 0.0446\n",
            "Epoch 28/100, Train Loss: 0.0369, Val Loss: 0.0446\n",
            "Epoch 29/100, Train Loss: 0.0361, Val Loss: 0.0446\n",
            "Epoch 30/100, Train Loss: 0.0376, Val Loss: 0.0446\n",
            "Epoch 31/100, Train Loss: 0.0370, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0364, Val Loss: 0.0446\n",
            "Epoch 33/100, Train Loss: 0.0362, Val Loss: 0.0446\n",
            "Epoch 34/100, Train Loss: 0.0370, Val Loss: 0.0446\n",
            "Epoch 35/100, Train Loss: 0.0378, Val Loss: 0.0446\n",
            "Epoch 36/100, Train Loss: 0.0368, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0354, Val Loss: 0.0446\n",
            "Epoch 38/100, Train Loss: 0.0370, Val Loss: 0.0446\n",
            "Epoch 39/100, Train Loss: 0.0366, Val Loss: 0.0446\n",
            "Epoch 40/100, Train Loss: 0.0388, Val Loss: 0.0446\n",
            "Epoch 41/100, Train Loss: 0.0405, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0371, Val Loss: 0.0446\n",
            "Epoch 43/100, Train Loss: 0.0393, Val Loss: 0.0446\n",
            "Epoch 44/100, Train Loss: 0.0391, Val Loss: 0.0446\n",
            "Epoch 45/100, Train Loss: 0.0372, Val Loss: 0.0446\n",
            "Epoch 46/100, Train Loss: 0.0352, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0403, Val Loss: 0.0446\n",
            "Epoch 48/100, Train Loss: 0.0375, Val Loss: 0.0446\n",
            "Epoch 49/100, Train Loss: 0.0399, Val Loss: 0.0446\n",
            "Epoch 50/100, Train Loss: 0.0414, Val Loss: 0.0446\n",
            "Epoch 51/100, Train Loss: 0.0386, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0379, Val Loss: 0.0446\n",
            "Epoch 53/100, Train Loss: 0.0366, Val Loss: 0.0446\n",
            "Epoch 54/100, Train Loss: 0.0374, Val Loss: 0.0446\n",
            "Epoch 55/100, Train Loss: 0.0370, Val Loss: 0.0446\n",
            "Epoch 56/100, Train Loss: 0.0396, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0352, Val Loss: 0.0446\n",
            "Epoch 58/100, Train Loss: 0.0384, Val Loss: 0.0446\n",
            "Epoch 59/100, Train Loss: 0.0394, Val Loss: 0.0446\n",
            "Epoch 60/100, Train Loss: 0.0397, Val Loss: 0.0446\n",
            "Epoch 61/100, Train Loss: 0.0376, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0417, Val Loss: 0.0446\n",
            "Epoch 63/100, Train Loss: 0.0373, Val Loss: 0.0446\n",
            "Epoch 64/100, Train Loss: 0.0387, Val Loss: 0.0446\n",
            "Epoch 65/100, Train Loss: 0.0360, Val Loss: 0.0446\n",
            "Epoch 66/100, Train Loss: 0.0364, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0375, Val Loss: 0.0446\n",
            "Epoch 68/100, Train Loss: 0.0355, Val Loss: 0.0446\n",
            "Epoch 69/100, Train Loss: 0.0379, Val Loss: 0.0446\n",
            "Epoch 70/100, Train Loss: 0.0343, Val Loss: 0.0446\n",
            "Epoch 71/100, Train Loss: 0.0353, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0398, Val Loss: 0.0446\n",
            "Epoch 73/100, Train Loss: 0.0381, Val Loss: 0.0446\n",
            "Epoch 74/100, Train Loss: 0.0366, Val Loss: 0.0446\n",
            "Epoch 75/100, Train Loss: 0.0351, Val Loss: 0.0446\n",
            "Epoch 76/100, Train Loss: 0.0374, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0377, Val Loss: 0.0446\n",
            "Epoch 78/100, Train Loss: 0.0370, Val Loss: 0.0446\n",
            "Epoch 79/100, Train Loss: 0.0358, Val Loss: 0.0446\n",
            "Epoch 80/100, Train Loss: 0.0400, Val Loss: 0.0446\n",
            "Epoch 81/100, Train Loss: 0.0375, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0377, Val Loss: 0.0446\n",
            "Epoch 83/100, Train Loss: 0.0372, Val Loss: 0.0446\n",
            "Epoch 84/100, Train Loss: 0.0361, Val Loss: 0.0446\n",
            "Epoch 85/100, Train Loss: 0.0407, Val Loss: 0.0446\n",
            "Epoch 86/100, Train Loss: 0.0383, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0377, Val Loss: 0.0446\n",
            "Epoch 88/100, Train Loss: 0.0363, Val Loss: 0.0446\n",
            "Epoch 89/100, Train Loss: 0.0371, Val Loss: 0.0446\n",
            "Epoch 90/100, Train Loss: 0.0384, Val Loss: 0.0446\n",
            "Epoch 91/100, Train Loss: 0.0362, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0385, Val Loss: 0.0446\n",
            "Epoch 93/100, Train Loss: 0.0399, Val Loss: 0.0446\n",
            "Epoch 94/100, Train Loss: 0.0385, Val Loss: 0.0446\n",
            "Epoch 95/100, Train Loss: 0.0347, Val Loss: 0.0446\n",
            "Epoch 96/100, Train Loss: 0.0335, Val Loss: 0.0446\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0376, Val Loss: 0.0446\n",
            "Epoch 98/100, Train Loss: 0.0398, Val Loss: 0.0446\n",
            "Epoch 99/100, Train Loss: 0.0378, Val Loss: 0.0446\n",
            "Epoch 100/100, Train Loss: 0.0406, Val Loss: 0.0446\n",
            "\n",
            "Test iteration 7/14\n",
            "Current training set size: 119 samples\n",
            "Epoch 1/100, Train Loss: 0.1489, Val Loss: 0.2853\n",
            "Epoch 2/100, Train Loss: 0.0882, Val Loss: 0.2063\n",
            "Epoch 3/100, Train Loss: 0.0758, Val Loss: 0.1770\n",
            "Epoch 4/100, Train Loss: 0.0693, Val Loss: 0.1515\n",
            "Epoch 5/100, Train Loss: 0.0685, Val Loss: 0.1357\n",
            "Epoch 6/100, Train Loss: 0.0673, Val Loss: 0.0565\n",
            "Epoch 7/100, Train Loss: 0.0534, Val Loss: 0.0475\n",
            "Epoch 8/100, Train Loss: 0.0566, Val Loss: 0.0599\n",
            "Epoch 9/100, Train Loss: 0.0497, Val Loss: 0.0465\n",
            "Epoch 10/100, Train Loss: 0.0503, Val Loss: 0.0478\n",
            "Epoch 11/100, Train Loss: 0.0550, Val Loss: 0.0641\n",
            "Epoch 12/100, Train Loss: 0.0418, Val Loss: 0.0849\n",
            "Epoch 13/100, Train Loss: 0.0496, Val Loss: 0.0505\n",
            "Epoch 14/100, Train Loss: 0.0446, Val Loss: 0.0475\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 15/100, Train Loss: 0.0422, Val Loss: 0.0475\n",
            "Epoch 16/100, Train Loss: 0.0378, Val Loss: 0.0475\n",
            "Epoch 17/100, Train Loss: 0.0373, Val Loss: 0.0475\n",
            "Epoch 18/100, Train Loss: 0.0392, Val Loss: 0.0475\n",
            "Epoch 19/100, Train Loss: 0.0396, Val Loss: 0.0475\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0410, Val Loss: 0.0475\n",
            "Epoch 21/100, Train Loss: 0.0396, Val Loss: 0.0475\n",
            "Epoch 22/100, Train Loss: 0.0423, Val Loss: 0.0475\n",
            "Epoch 23/100, Train Loss: 0.0374, Val Loss: 0.0475\n",
            "Epoch 24/100, Train Loss: 0.0414, Val Loss: 0.0475\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0420, Val Loss: 0.0475\n",
            "Epoch 26/100, Train Loss: 0.0394, Val Loss: 0.0475\n",
            "Epoch 27/100, Train Loss: 0.0403, Val Loss: 0.0475\n",
            "Epoch 28/100, Train Loss: 0.0383, Val Loss: 0.0475\n",
            "Epoch 29/100, Train Loss: 0.0368, Val Loss: 0.0475\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0406, Val Loss: 0.0475\n",
            "Epoch 31/100, Train Loss: 0.0368, Val Loss: 0.0475\n",
            "Epoch 32/100, Train Loss: 0.0409, Val Loss: 0.0475\n",
            "Epoch 33/100, Train Loss: 0.0367, Val Loss: 0.0475\n",
            "Epoch 34/100, Train Loss: 0.0382, Val Loss: 0.0475\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0396, Val Loss: 0.0475\n",
            "Epoch 36/100, Train Loss: 0.0392, Val Loss: 0.0475\n",
            "Epoch 37/100, Train Loss: 0.0416, Val Loss: 0.0475\n",
            "Epoch 38/100, Train Loss: 0.0377, Val Loss: 0.0475\n",
            "Epoch 39/100, Train Loss: 0.0433, Val Loss: 0.0475\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0419, Val Loss: 0.0475\n",
            "Epoch 41/100, Train Loss: 0.0410, Val Loss: 0.0475\n",
            "Epoch 42/100, Train Loss: 0.0411, Val Loss: 0.0475\n",
            "Epoch 43/100, Train Loss: 0.0422, Val Loss: 0.0475\n",
            "Epoch 44/100, Train Loss: 0.0372, Val Loss: 0.0475\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0380, Val Loss: 0.0475\n",
            "Epoch 46/100, Train Loss: 0.0416, Val Loss: 0.0475\n",
            "Epoch 47/100, Train Loss: 0.0393, Val Loss: 0.0475\n",
            "Epoch 48/100, Train Loss: 0.0400, Val Loss: 0.0475\n",
            "Epoch 49/100, Train Loss: 0.0423, Val Loss: 0.0475\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0442, Val Loss: 0.0475\n",
            "Epoch 51/100, Train Loss: 0.0373, Val Loss: 0.0475\n",
            "Epoch 52/100, Train Loss: 0.0393, Val Loss: 0.0475\n",
            "Epoch 53/100, Train Loss: 0.0366, Val Loss: 0.0475\n",
            "Epoch 54/100, Train Loss: 0.0425, Val Loss: 0.0475\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0406, Val Loss: 0.0475\n",
            "Epoch 56/100, Train Loss: 0.0364, Val Loss: 0.0475\n",
            "Epoch 57/100, Train Loss: 0.0408, Val Loss: 0.0475\n",
            "Epoch 58/100, Train Loss: 0.0407, Val Loss: 0.0475\n",
            "Epoch 59/100, Train Loss: 0.0407, Val Loss: 0.0475\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0424, Val Loss: 0.0475\n",
            "Epoch 61/100, Train Loss: 0.0394, Val Loss: 0.0475\n",
            "Epoch 62/100, Train Loss: 0.0371, Val Loss: 0.0475\n",
            "Epoch 63/100, Train Loss: 0.0381, Val Loss: 0.0475\n",
            "Epoch 64/100, Train Loss: 0.0398, Val Loss: 0.0475\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0390, Val Loss: 0.0475\n",
            "Epoch 66/100, Train Loss: 0.0416, Val Loss: 0.0475\n",
            "Epoch 67/100, Train Loss: 0.0376, Val Loss: 0.0475\n",
            "Epoch 68/100, Train Loss: 0.0436, Val Loss: 0.0475\n",
            "Epoch 69/100, Train Loss: 0.0414, Val Loss: 0.0475\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0410, Val Loss: 0.0475\n",
            "Epoch 71/100, Train Loss: 0.0393, Val Loss: 0.0475\n",
            "Epoch 72/100, Train Loss: 0.0386, Val Loss: 0.0475\n",
            "Epoch 73/100, Train Loss: 0.0401, Val Loss: 0.0475\n",
            "Epoch 74/100, Train Loss: 0.0404, Val Loss: 0.0475\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0378, Val Loss: 0.0475\n",
            "Epoch 76/100, Train Loss: 0.0396, Val Loss: 0.0475\n",
            "Epoch 77/100, Train Loss: 0.0438, Val Loss: 0.0475\n",
            "Epoch 78/100, Train Loss: 0.0414, Val Loss: 0.0475\n",
            "Epoch 79/100, Train Loss: 0.0398, Val Loss: 0.0475\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0366, Val Loss: 0.0475\n",
            "Epoch 81/100, Train Loss: 0.0372, Val Loss: 0.0475\n",
            "Epoch 82/100, Train Loss: 0.0415, Val Loss: 0.0475\n",
            "Epoch 83/100, Train Loss: 0.0380, Val Loss: 0.0475\n",
            "Epoch 84/100, Train Loss: 0.0398, Val Loss: 0.0475\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0403, Val Loss: 0.0475\n",
            "Epoch 86/100, Train Loss: 0.0382, Val Loss: 0.0475\n",
            "Epoch 87/100, Train Loss: 0.0378, Val Loss: 0.0475\n",
            "Epoch 88/100, Train Loss: 0.0390, Val Loss: 0.0475\n",
            "Epoch 89/100, Train Loss: 0.0388, Val Loss: 0.0475\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0413, Val Loss: 0.0475\n",
            "Epoch 91/100, Train Loss: 0.0390, Val Loss: 0.0475\n",
            "Epoch 92/100, Train Loss: 0.0418, Val Loss: 0.0475\n",
            "Epoch 93/100, Train Loss: 0.0392, Val Loss: 0.0475\n",
            "Epoch 94/100, Train Loss: 0.0385, Val Loss: 0.0475\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0418, Val Loss: 0.0475\n",
            "Epoch 96/100, Train Loss: 0.0389, Val Loss: 0.0475\n",
            "Epoch 97/100, Train Loss: 0.0372, Val Loss: 0.0475\n",
            "Epoch 98/100, Train Loss: 0.0418, Val Loss: 0.0475\n",
            "Epoch 99/100, Train Loss: 0.0405, Val Loss: 0.0475\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0424, Val Loss: 0.0475\n",
            "\n",
            "Test iteration 8/14\n",
            "Current training set size: 120 samples\n",
            "Epoch 1/100, Train Loss: 0.1574, Val Loss: 0.2229\n",
            "Epoch 2/100, Train Loss: 0.1004, Val Loss: 0.1920\n",
            "Epoch 3/100, Train Loss: 0.0757, Val Loss: 0.1804\n",
            "Epoch 4/100, Train Loss: 0.0728, Val Loss: 0.1515\n",
            "Epoch 5/100, Train Loss: 0.0662, Val Loss: 0.1341\n",
            "Epoch 6/100, Train Loss: 0.0620, Val Loss: 0.0880\n",
            "Epoch 7/100, Train Loss: 0.0554, Val Loss: 0.0453\n",
            "Epoch 8/100, Train Loss: 0.0554, Val Loss: 0.0456\n",
            "Epoch 9/100, Train Loss: 0.0478, Val Loss: 0.0546\n",
            "Epoch 10/100, Train Loss: 0.0468, Val Loss: 0.1890\n",
            "Epoch 11/100, Train Loss: 0.0517, Val Loss: 0.0495\n",
            "Epoch 12/100, Train Loss: 0.0485, Val Loss: 0.0511\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 13/100, Train Loss: 0.0388, Val Loss: 0.0510\n",
            "Epoch 14/100, Train Loss: 0.0387, Val Loss: 0.0510\n",
            "Epoch 15/100, Train Loss: 0.0420, Val Loss: 0.0509\n",
            "Epoch 16/100, Train Loss: 0.0371, Val Loss: 0.0509\n",
            "Epoch 17/100, Train Loss: 0.0391, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0404, Val Loss: 0.0508\n",
            "Epoch 19/100, Train Loss: 0.0423, Val Loss: 0.0508\n",
            "Epoch 20/100, Train Loss: 0.0410, Val Loss: 0.0508\n",
            "Epoch 21/100, Train Loss: 0.0371, Val Loss: 0.0508\n",
            "Epoch 22/100, Train Loss: 0.0415, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0405, Val Loss: 0.0508\n",
            "Epoch 24/100, Train Loss: 0.0442, Val Loss: 0.0508\n",
            "Epoch 25/100, Train Loss: 0.0431, Val Loss: 0.0508\n",
            "Epoch 26/100, Train Loss: 0.0391, Val Loss: 0.0508\n",
            "Epoch 27/100, Train Loss: 0.0433, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0430, Val Loss: 0.0508\n",
            "Epoch 29/100, Train Loss: 0.0384, Val Loss: 0.0508\n",
            "Epoch 30/100, Train Loss: 0.0440, Val Loss: 0.0508\n",
            "Epoch 31/100, Train Loss: 0.0416, Val Loss: 0.0508\n",
            "Epoch 32/100, Train Loss: 0.0425, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0399, Val Loss: 0.0508\n",
            "Epoch 34/100, Train Loss: 0.0404, Val Loss: 0.0508\n",
            "Epoch 35/100, Train Loss: 0.0398, Val Loss: 0.0508\n",
            "Epoch 36/100, Train Loss: 0.0401, Val Loss: 0.0508\n",
            "Epoch 37/100, Train Loss: 0.0418, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0412, Val Loss: 0.0508\n",
            "Epoch 39/100, Train Loss: 0.0408, Val Loss: 0.0508\n",
            "Epoch 40/100, Train Loss: 0.0424, Val Loss: 0.0508\n",
            "Epoch 41/100, Train Loss: 0.0390, Val Loss: 0.0508\n",
            "Epoch 42/100, Train Loss: 0.0402, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0415, Val Loss: 0.0508\n",
            "Epoch 44/100, Train Loss: 0.0421, Val Loss: 0.0508\n",
            "Epoch 45/100, Train Loss: 0.0416, Val Loss: 0.0508\n",
            "Epoch 46/100, Train Loss: 0.0396, Val Loss: 0.0508\n",
            "Epoch 47/100, Train Loss: 0.0394, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0412, Val Loss: 0.0508\n",
            "Epoch 49/100, Train Loss: 0.0393, Val Loss: 0.0508\n",
            "Epoch 50/100, Train Loss: 0.0413, Val Loss: 0.0508\n",
            "Epoch 51/100, Train Loss: 0.0379, Val Loss: 0.0508\n",
            "Epoch 52/100, Train Loss: 0.0419, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0400, Val Loss: 0.0508\n",
            "Epoch 54/100, Train Loss: 0.0413, Val Loss: 0.0508\n",
            "Epoch 55/100, Train Loss: 0.0425, Val Loss: 0.0508\n",
            "Epoch 56/100, Train Loss: 0.0417, Val Loss: 0.0508\n",
            "Epoch 57/100, Train Loss: 0.0400, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0423, Val Loss: 0.0508\n",
            "Epoch 59/100, Train Loss: 0.0386, Val Loss: 0.0508\n",
            "Epoch 60/100, Train Loss: 0.0386, Val Loss: 0.0508\n",
            "Epoch 61/100, Train Loss: 0.0387, Val Loss: 0.0508\n",
            "Epoch 62/100, Train Loss: 0.0412, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0416, Val Loss: 0.0508\n",
            "Epoch 64/100, Train Loss: 0.0397, Val Loss: 0.0508\n",
            "Epoch 65/100, Train Loss: 0.0381, Val Loss: 0.0508\n",
            "Epoch 66/100, Train Loss: 0.0402, Val Loss: 0.0508\n",
            "Epoch 67/100, Train Loss: 0.0389, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0414, Val Loss: 0.0508\n",
            "Epoch 69/100, Train Loss: 0.0412, Val Loss: 0.0508\n",
            "Epoch 70/100, Train Loss: 0.0381, Val Loss: 0.0508\n",
            "Epoch 71/100, Train Loss: 0.0381, Val Loss: 0.0508\n",
            "Epoch 72/100, Train Loss: 0.0394, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0395, Val Loss: 0.0508\n",
            "Epoch 74/100, Train Loss: 0.0367, Val Loss: 0.0508\n",
            "Epoch 75/100, Train Loss: 0.0435, Val Loss: 0.0508\n",
            "Epoch 76/100, Train Loss: 0.0394, Val Loss: 0.0508\n",
            "Epoch 77/100, Train Loss: 0.0417, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0416, Val Loss: 0.0508\n",
            "Epoch 79/100, Train Loss: 0.0366, Val Loss: 0.0508\n",
            "Epoch 80/100, Train Loss: 0.0395, Val Loss: 0.0508\n",
            "Epoch 81/100, Train Loss: 0.0394, Val Loss: 0.0508\n",
            "Epoch 82/100, Train Loss: 0.0398, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0398, Val Loss: 0.0508\n",
            "Epoch 84/100, Train Loss: 0.0383, Val Loss: 0.0508\n",
            "Epoch 85/100, Train Loss: 0.0427, Val Loss: 0.0508\n",
            "Epoch 86/100, Train Loss: 0.0399, Val Loss: 0.0508\n",
            "Epoch 87/100, Train Loss: 0.0456, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0371, Val Loss: 0.0508\n",
            "Epoch 89/100, Train Loss: 0.0398, Val Loss: 0.0508\n",
            "Epoch 90/100, Train Loss: 0.0387, Val Loss: 0.0508\n",
            "Epoch 91/100, Train Loss: 0.0387, Val Loss: 0.0508\n",
            "Epoch 92/100, Train Loss: 0.0425, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0377, Val Loss: 0.0508\n",
            "Epoch 94/100, Train Loss: 0.0403, Val Loss: 0.0508\n",
            "Epoch 95/100, Train Loss: 0.0431, Val Loss: 0.0508\n",
            "Epoch 96/100, Train Loss: 0.0376, Val Loss: 0.0508\n",
            "Epoch 97/100, Train Loss: 0.0404, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0407, Val Loss: 0.0508\n",
            "Epoch 99/100, Train Loss: 0.0398, Val Loss: 0.0508\n",
            "Epoch 100/100, Train Loss: 0.0391, Val Loss: 0.0508\n",
            "\n",
            "Test iteration 9/14\n",
            "Current training set size: 121 samples\n",
            "Epoch 1/100, Train Loss: 0.1468, Val Loss: 0.2584\n",
            "Epoch 2/100, Train Loss: 0.0857, Val Loss: 0.1832\n",
            "Epoch 3/100, Train Loss: 0.0823, Val Loss: 0.1690\n",
            "Epoch 4/100, Train Loss: 0.0726, Val Loss: 0.1431\n",
            "Epoch 5/100, Train Loss: 0.0703, Val Loss: 0.1131\n",
            "Epoch 6/100, Train Loss: 0.0590, Val Loss: 0.1018\n",
            "Epoch 7/100, Train Loss: 0.0572, Val Loss: 0.0496\n",
            "Epoch 8/100, Train Loss: 0.0520, Val Loss: 0.0527\n",
            "Epoch 9/100, Train Loss: 0.0527, Val Loss: 0.0502\n",
            "Epoch 10/100, Train Loss: 0.0550, Val Loss: 0.0730\n",
            "Epoch 11/100, Train Loss: 0.0490, Val Loss: 0.0477\n",
            "Epoch 12/100, Train Loss: 0.0540, Val Loss: 0.0689\n",
            "Epoch 13/100, Train Loss: 0.0542, Val Loss: 0.0491\n",
            "Epoch 14/100, Train Loss: 0.0477, Val Loss: 0.0498\n",
            "Epoch 15/100, Train Loss: 0.0432, Val Loss: 0.0570\n",
            "Epoch 16/100, Train Loss: 0.0441, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0403, Val Loss: 0.0508\n",
            "Epoch 18/100, Train Loss: 0.0393, Val Loss: 0.0507\n",
            "Epoch 19/100, Train Loss: 0.0392, Val Loss: 0.0506\n",
            "Epoch 20/100, Train Loss: 0.0378, Val Loss: 0.0504\n",
            "Epoch 21/100, Train Loss: 0.0366, Val Loss: 0.0504\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0392, Val Loss: 0.0504\n",
            "Epoch 23/100, Train Loss: 0.0365, Val Loss: 0.0503\n",
            "Epoch 24/100, Train Loss: 0.0400, Val Loss: 0.0503\n",
            "Epoch 25/100, Train Loss: 0.0405, Val Loss: 0.0503\n",
            "Epoch 26/100, Train Loss: 0.0392, Val Loss: 0.0503\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0382, Val Loss: 0.0503\n",
            "Epoch 28/100, Train Loss: 0.0339, Val Loss: 0.0503\n",
            "Epoch 29/100, Train Loss: 0.0421, Val Loss: 0.0503\n",
            "Epoch 30/100, Train Loss: 0.0369, Val Loss: 0.0503\n",
            "Epoch 31/100, Train Loss: 0.0356, Val Loss: 0.0503\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0401, Val Loss: 0.0503\n",
            "Epoch 33/100, Train Loss: 0.0383, Val Loss: 0.0503\n",
            "Epoch 34/100, Train Loss: 0.0388, Val Loss: 0.0503\n",
            "Epoch 35/100, Train Loss: 0.0371, Val Loss: 0.0503\n",
            "Epoch 36/100, Train Loss: 0.0398, Val Loss: 0.0503\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0406, Val Loss: 0.0503\n",
            "Epoch 38/100, Train Loss: 0.0388, Val Loss: 0.0503\n",
            "Epoch 39/100, Train Loss: 0.0395, Val Loss: 0.0503\n",
            "Epoch 40/100, Train Loss: 0.0378, Val Loss: 0.0503\n",
            "Epoch 41/100, Train Loss: 0.0353, Val Loss: 0.0503\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0411, Val Loss: 0.0503\n",
            "Epoch 43/100, Train Loss: 0.0376, Val Loss: 0.0503\n",
            "Epoch 44/100, Train Loss: 0.0417, Val Loss: 0.0503\n",
            "Epoch 45/100, Train Loss: 0.0378, Val Loss: 0.0503\n",
            "Epoch 46/100, Train Loss: 0.0396, Val Loss: 0.0503\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0406, Val Loss: 0.0503\n",
            "Epoch 48/100, Train Loss: 0.0404, Val Loss: 0.0503\n",
            "Epoch 49/100, Train Loss: 0.0400, Val Loss: 0.0503\n",
            "Epoch 50/100, Train Loss: 0.0400, Val Loss: 0.0503\n",
            "Epoch 51/100, Train Loss: 0.0357, Val Loss: 0.0503\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0416, Val Loss: 0.0503\n",
            "Epoch 53/100, Train Loss: 0.0397, Val Loss: 0.0503\n",
            "Epoch 54/100, Train Loss: 0.0420, Val Loss: 0.0503\n",
            "Epoch 55/100, Train Loss: 0.0388, Val Loss: 0.0503\n",
            "Epoch 56/100, Train Loss: 0.0394, Val Loss: 0.0503\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0429, Val Loss: 0.0503\n",
            "Epoch 58/100, Train Loss: 0.0396, Val Loss: 0.0503\n",
            "Epoch 59/100, Train Loss: 0.0397, Val Loss: 0.0503\n",
            "Epoch 60/100, Train Loss: 0.0394, Val Loss: 0.0503\n",
            "Epoch 61/100, Train Loss: 0.0387, Val Loss: 0.0503\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0374, Val Loss: 0.0503\n",
            "Epoch 63/100, Train Loss: 0.0393, Val Loss: 0.0503\n",
            "Epoch 64/100, Train Loss: 0.0422, Val Loss: 0.0503\n",
            "Epoch 65/100, Train Loss: 0.0366, Val Loss: 0.0503\n",
            "Epoch 66/100, Train Loss: 0.0398, Val Loss: 0.0503\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0384, Val Loss: 0.0503\n",
            "Epoch 68/100, Train Loss: 0.0388, Val Loss: 0.0503\n",
            "Epoch 69/100, Train Loss: 0.0401, Val Loss: 0.0503\n",
            "Epoch 70/100, Train Loss: 0.0411, Val Loss: 0.0503\n",
            "Epoch 71/100, Train Loss: 0.0393, Val Loss: 0.0503\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0391, Val Loss: 0.0503\n",
            "Epoch 73/100, Train Loss: 0.0387, Val Loss: 0.0503\n",
            "Epoch 74/100, Train Loss: 0.0363, Val Loss: 0.0503\n",
            "Epoch 75/100, Train Loss: 0.0390, Val Loss: 0.0503\n",
            "Epoch 76/100, Train Loss: 0.0390, Val Loss: 0.0503\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0393, Val Loss: 0.0503\n",
            "Epoch 78/100, Train Loss: 0.0413, Val Loss: 0.0503\n",
            "Epoch 79/100, Train Loss: 0.0425, Val Loss: 0.0503\n",
            "Epoch 80/100, Train Loss: 0.0398, Val Loss: 0.0503\n",
            "Epoch 81/100, Train Loss: 0.0381, Val Loss: 0.0503\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0388, Val Loss: 0.0503\n",
            "Epoch 83/100, Train Loss: 0.0392, Val Loss: 0.0503\n",
            "Epoch 84/100, Train Loss: 0.0403, Val Loss: 0.0503\n",
            "Epoch 85/100, Train Loss: 0.0394, Val Loss: 0.0503\n",
            "Epoch 86/100, Train Loss: 0.0361, Val Loss: 0.0503\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0353, Val Loss: 0.0503\n",
            "Epoch 88/100, Train Loss: 0.0380, Val Loss: 0.0503\n",
            "Epoch 89/100, Train Loss: 0.0394, Val Loss: 0.0503\n",
            "Epoch 90/100, Train Loss: 0.0376, Val Loss: 0.0503\n",
            "Epoch 91/100, Train Loss: 0.0391, Val Loss: 0.0503\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0413, Val Loss: 0.0503\n",
            "Epoch 93/100, Train Loss: 0.0387, Val Loss: 0.0503\n",
            "Epoch 94/100, Train Loss: 0.0363, Val Loss: 0.0503\n",
            "Epoch 95/100, Train Loss: 0.0387, Val Loss: 0.0503\n",
            "Epoch 96/100, Train Loss: 0.0383, Val Loss: 0.0503\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0403, Val Loss: 0.0503\n",
            "Epoch 98/100, Train Loss: 0.0390, Val Loss: 0.0503\n",
            "Epoch 99/100, Train Loss: 0.0368, Val Loss: 0.0503\n",
            "Epoch 100/100, Train Loss: 0.0408, Val Loss: 0.0503\n",
            "\n",
            "Test iteration 10/14\n",
            "Current training set size: 122 samples\n",
            "Epoch 1/100, Train Loss: 0.1513, Val Loss: 0.2331\n",
            "Epoch 2/100, Train Loss: 0.0980, Val Loss: 0.1886\n",
            "Epoch 3/100, Train Loss: 0.0737, Val Loss: 0.1637\n",
            "Epoch 4/100, Train Loss: 0.0704, Val Loss: 0.1210\n",
            "Epoch 5/100, Train Loss: 0.0666, Val Loss: 0.0568\n",
            "Epoch 6/100, Train Loss: 0.0618, Val Loss: 0.0640\n",
            "Epoch 7/100, Train Loss: 0.0624, Val Loss: 0.0497\n",
            "Epoch 8/100, Train Loss: 0.0669, Val Loss: 0.0476\n",
            "Epoch 9/100, Train Loss: 0.0536, Val Loss: 0.0475\n",
            "Epoch 10/100, Train Loss: 0.0507, Val Loss: 0.0496\n",
            "Epoch 11/100, Train Loss: 0.0504, Val Loss: 0.0511\n",
            "Epoch 12/100, Train Loss: 0.0510, Val Loss: 0.0572\n",
            "Epoch 13/100, Train Loss: 0.0508, Val Loss: 0.0515\n",
            "Epoch 14/100, Train Loss: 0.0483, Val Loss: 0.0587\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 15/100, Train Loss: 0.0416, Val Loss: 0.0587\n",
            "Epoch 16/100, Train Loss: 0.0421, Val Loss: 0.0579\n",
            "Epoch 17/100, Train Loss: 0.0423, Val Loss: 0.0574\n",
            "Epoch 18/100, Train Loss: 0.0416, Val Loss: 0.0564\n",
            "Epoch 19/100, Train Loss: 0.0407, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0442, Val Loss: 0.0559\n",
            "Epoch 21/100, Train Loss: 0.0397, Val Loss: 0.0559\n",
            "Epoch 22/100, Train Loss: 0.0438, Val Loss: 0.0559\n",
            "Epoch 23/100, Train Loss: 0.0436, Val Loss: 0.0559\n",
            "Epoch 24/100, Train Loss: 0.0410, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0405, Val Loss: 0.0559\n",
            "Epoch 26/100, Train Loss: 0.0393, Val Loss: 0.0559\n",
            "Epoch 27/100, Train Loss: 0.0407, Val Loss: 0.0559\n",
            "Epoch 28/100, Train Loss: 0.0399, Val Loss: 0.0559\n",
            "Epoch 29/100, Train Loss: 0.0422, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0393, Val Loss: 0.0559\n",
            "Epoch 31/100, Train Loss: 0.0390, Val Loss: 0.0559\n",
            "Epoch 32/100, Train Loss: 0.0418, Val Loss: 0.0559\n",
            "Epoch 33/100, Train Loss: 0.0408, Val Loss: 0.0559\n",
            "Epoch 34/100, Train Loss: 0.0430, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0389, Val Loss: 0.0559\n",
            "Epoch 36/100, Train Loss: 0.0439, Val Loss: 0.0559\n",
            "Epoch 37/100, Train Loss: 0.0404, Val Loss: 0.0559\n",
            "Epoch 38/100, Train Loss: 0.0403, Val Loss: 0.0559\n",
            "Epoch 39/100, Train Loss: 0.0404, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0388, Val Loss: 0.0559\n",
            "Epoch 41/100, Train Loss: 0.0407, Val Loss: 0.0559\n",
            "Epoch 42/100, Train Loss: 0.0420, Val Loss: 0.0559\n",
            "Epoch 43/100, Train Loss: 0.0427, Val Loss: 0.0559\n",
            "Epoch 44/100, Train Loss: 0.0396, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0448, Val Loss: 0.0559\n",
            "Epoch 46/100, Train Loss: 0.0423, Val Loss: 0.0559\n",
            "Epoch 47/100, Train Loss: 0.0391, Val Loss: 0.0559\n",
            "Epoch 48/100, Train Loss: 0.0437, Val Loss: 0.0559\n",
            "Epoch 49/100, Train Loss: 0.0402, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0386, Val Loss: 0.0559\n",
            "Epoch 51/100, Train Loss: 0.0402, Val Loss: 0.0559\n",
            "Epoch 52/100, Train Loss: 0.0406, Val Loss: 0.0559\n",
            "Epoch 53/100, Train Loss: 0.0416, Val Loss: 0.0559\n",
            "Epoch 54/100, Train Loss: 0.0406, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0428, Val Loss: 0.0559\n",
            "Epoch 56/100, Train Loss: 0.0404, Val Loss: 0.0559\n",
            "Epoch 57/100, Train Loss: 0.0399, Val Loss: 0.0559\n",
            "Epoch 58/100, Train Loss: 0.0410, Val Loss: 0.0559\n",
            "Epoch 59/100, Train Loss: 0.0404, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0425, Val Loss: 0.0559\n",
            "Epoch 61/100, Train Loss: 0.0419, Val Loss: 0.0559\n",
            "Epoch 62/100, Train Loss: 0.0391, Val Loss: 0.0559\n",
            "Epoch 63/100, Train Loss: 0.0434, Val Loss: 0.0559\n",
            "Epoch 64/100, Train Loss: 0.0452, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0394, Val Loss: 0.0559\n",
            "Epoch 66/100, Train Loss: 0.0403, Val Loss: 0.0559\n",
            "Epoch 67/100, Train Loss: 0.0421, Val Loss: 0.0559\n",
            "Epoch 68/100, Train Loss: 0.0399, Val Loss: 0.0559\n",
            "Epoch 69/100, Train Loss: 0.0402, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0412, Val Loss: 0.0559\n",
            "Epoch 71/100, Train Loss: 0.0400, Val Loss: 0.0559\n",
            "Epoch 72/100, Train Loss: 0.0384, Val Loss: 0.0559\n",
            "Epoch 73/100, Train Loss: 0.0420, Val Loss: 0.0559\n",
            "Epoch 74/100, Train Loss: 0.0397, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0396, Val Loss: 0.0559\n",
            "Epoch 76/100, Train Loss: 0.0407, Val Loss: 0.0559\n",
            "Epoch 77/100, Train Loss: 0.0423, Val Loss: 0.0559\n",
            "Epoch 78/100, Train Loss: 0.0401, Val Loss: 0.0559\n",
            "Epoch 79/100, Train Loss: 0.0415, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0406, Val Loss: 0.0559\n",
            "Epoch 81/100, Train Loss: 0.0411, Val Loss: 0.0559\n",
            "Epoch 82/100, Train Loss: 0.0394, Val Loss: 0.0559\n",
            "Epoch 83/100, Train Loss: 0.0406, Val Loss: 0.0559\n",
            "Epoch 84/100, Train Loss: 0.0408, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0411, Val Loss: 0.0559\n",
            "Epoch 86/100, Train Loss: 0.0422, Val Loss: 0.0559\n",
            "Epoch 87/100, Train Loss: 0.0423, Val Loss: 0.0559\n",
            "Epoch 88/100, Train Loss: 0.0408, Val Loss: 0.0559\n",
            "Epoch 89/100, Train Loss: 0.0453, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0389, Val Loss: 0.0559\n",
            "Epoch 91/100, Train Loss: 0.0427, Val Loss: 0.0559\n",
            "Epoch 92/100, Train Loss: 0.0414, Val Loss: 0.0559\n",
            "Epoch 93/100, Train Loss: 0.0385, Val Loss: 0.0559\n",
            "Epoch 94/100, Train Loss: 0.0400, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0428, Val Loss: 0.0559\n",
            "Epoch 96/100, Train Loss: 0.0410, Val Loss: 0.0559\n",
            "Epoch 97/100, Train Loss: 0.0421, Val Loss: 0.0559\n",
            "Epoch 98/100, Train Loss: 0.0422, Val Loss: 0.0559\n",
            "Epoch 99/100, Train Loss: 0.0408, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0403, Val Loss: 0.0559\n",
            "\n",
            "Test iteration 11/14\n",
            "Current training set size: 123 samples\n",
            "Epoch 1/100, Train Loss: 0.1449, Val Loss: 0.2369\n",
            "Epoch 2/100, Train Loss: 0.0866, Val Loss: 0.1725\n",
            "Epoch 3/100, Train Loss: 0.0761, Val Loss: 0.1078\n",
            "Epoch 4/100, Train Loss: 0.0693, Val Loss: 0.1324\n",
            "Epoch 5/100, Train Loss: 0.0696, Val Loss: 0.0495\n",
            "Epoch 6/100, Train Loss: 0.0627, Val Loss: 0.0494\n",
            "Epoch 7/100, Train Loss: 0.0590, Val Loss: 0.0584\n",
            "Epoch 8/100, Train Loss: 0.0521, Val Loss: 0.0800\n",
            "Epoch 9/100, Train Loss: 0.0518, Val Loss: 0.0544\n",
            "Epoch 10/100, Train Loss: 0.0521, Val Loss: 0.0533\n",
            "Epoch 11/100, Train Loss: 0.0512, Val Loss: 0.0948\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 12/100, Train Loss: 0.0511, Val Loss: 0.0890\n",
            "Epoch 13/100, Train Loss: 0.0484, Val Loss: 0.0790\n",
            "Epoch 14/100, Train Loss: 0.0471, Val Loss: 0.0706\n",
            "Epoch 15/100, Train Loss: 0.0419, Val Loss: 0.0641\n",
            "Epoch 16/100, Train Loss: 0.0471, Val Loss: 0.0577\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0439, Val Loss: 0.0577\n",
            "Epoch 18/100, Train Loss: 0.0447, Val Loss: 0.0576\n",
            "Epoch 19/100, Train Loss: 0.0448, Val Loss: 0.0575\n",
            "Epoch 20/100, Train Loss: 0.0415, Val Loss: 0.0575\n",
            "Epoch 21/100, Train Loss: 0.0450, Val Loss: 0.0575\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0497, Val Loss: 0.0575\n",
            "Epoch 23/100, Train Loss: 0.0446, Val Loss: 0.0575\n",
            "Epoch 24/100, Train Loss: 0.0451, Val Loss: 0.0575\n",
            "Epoch 25/100, Train Loss: 0.0472, Val Loss: 0.0575\n",
            "Epoch 26/100, Train Loss: 0.0479, Val Loss: 0.0575\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0439, Val Loss: 0.0575\n",
            "Epoch 28/100, Train Loss: 0.0414, Val Loss: 0.0575\n",
            "Epoch 29/100, Train Loss: 0.0435, Val Loss: 0.0575\n",
            "Epoch 30/100, Train Loss: 0.0480, Val Loss: 0.0575\n",
            "Epoch 31/100, Train Loss: 0.0424, Val Loss: 0.0575\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0417, Val Loss: 0.0575\n",
            "Epoch 33/100, Train Loss: 0.0458, Val Loss: 0.0575\n",
            "Epoch 34/100, Train Loss: 0.0466, Val Loss: 0.0575\n",
            "Epoch 35/100, Train Loss: 0.0433, Val Loss: 0.0575\n",
            "Epoch 36/100, Train Loss: 0.0436, Val Loss: 0.0575\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0476, Val Loss: 0.0575\n",
            "Epoch 38/100, Train Loss: 0.0428, Val Loss: 0.0575\n",
            "Epoch 39/100, Train Loss: 0.0454, Val Loss: 0.0575\n",
            "Epoch 40/100, Train Loss: 0.0435, Val Loss: 0.0575\n",
            "Epoch 41/100, Train Loss: 0.0445, Val Loss: 0.0575\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0447, Val Loss: 0.0575\n",
            "Epoch 43/100, Train Loss: 0.0466, Val Loss: 0.0575\n",
            "Epoch 44/100, Train Loss: 0.0450, Val Loss: 0.0575\n",
            "Epoch 45/100, Train Loss: 0.0411, Val Loss: 0.0575\n",
            "Epoch 46/100, Train Loss: 0.0454, Val Loss: 0.0575\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0456, Val Loss: 0.0575\n",
            "Epoch 48/100, Train Loss: 0.0444, Val Loss: 0.0575\n",
            "Epoch 49/100, Train Loss: 0.0469, Val Loss: 0.0575\n",
            "Epoch 50/100, Train Loss: 0.0470, Val Loss: 0.0575\n",
            "Epoch 51/100, Train Loss: 0.0451, Val Loss: 0.0575\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0450, Val Loss: 0.0575\n",
            "Epoch 53/100, Train Loss: 0.0434, Val Loss: 0.0575\n",
            "Epoch 54/100, Train Loss: 0.0470, Val Loss: 0.0575\n",
            "Epoch 55/100, Train Loss: 0.0430, Val Loss: 0.0575\n",
            "Epoch 56/100, Train Loss: 0.0440, Val Loss: 0.0575\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0463, Val Loss: 0.0575\n",
            "Epoch 58/100, Train Loss: 0.0483, Val Loss: 0.0575\n",
            "Epoch 59/100, Train Loss: 0.0481, Val Loss: 0.0575\n",
            "Epoch 60/100, Train Loss: 0.0421, Val Loss: 0.0575\n",
            "Epoch 61/100, Train Loss: 0.0443, Val Loss: 0.0575\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0433, Val Loss: 0.0575\n",
            "Epoch 63/100, Train Loss: 0.0472, Val Loss: 0.0575\n",
            "Epoch 64/100, Train Loss: 0.0437, Val Loss: 0.0575\n",
            "Epoch 65/100, Train Loss: 0.0439, Val Loss: 0.0575\n",
            "Epoch 66/100, Train Loss: 0.0430, Val Loss: 0.0575\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0500, Val Loss: 0.0575\n",
            "Epoch 68/100, Train Loss: 0.0469, Val Loss: 0.0575\n",
            "Epoch 69/100, Train Loss: 0.0452, Val Loss: 0.0575\n",
            "Epoch 70/100, Train Loss: 0.0459, Val Loss: 0.0575\n",
            "Epoch 71/100, Train Loss: 0.0425, Val Loss: 0.0575\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0419, Val Loss: 0.0575\n",
            "Epoch 73/100, Train Loss: 0.0446, Val Loss: 0.0575\n",
            "Epoch 74/100, Train Loss: 0.0472, Val Loss: 0.0575\n",
            "Epoch 75/100, Train Loss: 0.0465, Val Loss: 0.0575\n",
            "Epoch 76/100, Train Loss: 0.0438, Val Loss: 0.0575\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0427, Val Loss: 0.0575\n",
            "Epoch 78/100, Train Loss: 0.0475, Val Loss: 0.0575\n",
            "Epoch 79/100, Train Loss: 0.0448, Val Loss: 0.0575\n",
            "Epoch 80/100, Train Loss: 0.0444, Val Loss: 0.0575\n",
            "Epoch 81/100, Train Loss: 0.0441, Val Loss: 0.0575\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0472, Val Loss: 0.0575\n",
            "Epoch 83/100, Train Loss: 0.0416, Val Loss: 0.0575\n",
            "Epoch 84/100, Train Loss: 0.0459, Val Loss: 0.0575\n",
            "Epoch 85/100, Train Loss: 0.0433, Val Loss: 0.0575\n",
            "Epoch 86/100, Train Loss: 0.0438, Val Loss: 0.0575\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0471, Val Loss: 0.0575\n",
            "Epoch 88/100, Train Loss: 0.0423, Val Loss: 0.0575\n",
            "Epoch 89/100, Train Loss: 0.0417, Val Loss: 0.0575\n",
            "Epoch 90/100, Train Loss: 0.0439, Val Loss: 0.0575\n",
            "Epoch 91/100, Train Loss: 0.0482, Val Loss: 0.0575\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0441, Val Loss: 0.0575\n",
            "Epoch 93/100, Train Loss: 0.0459, Val Loss: 0.0575\n",
            "Epoch 94/100, Train Loss: 0.0429, Val Loss: 0.0575\n",
            "Epoch 95/100, Train Loss: 0.0455, Val Loss: 0.0575\n",
            "Epoch 96/100, Train Loss: 0.0437, Val Loss: 0.0575\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0450, Val Loss: 0.0575\n",
            "Epoch 98/100, Train Loss: 0.0446, Val Loss: 0.0575\n",
            "Epoch 99/100, Train Loss: 0.0437, Val Loss: 0.0575\n",
            "Epoch 100/100, Train Loss: 0.0432, Val Loss: 0.0575\n",
            "\n",
            "Test iteration 12/14\n",
            "Current training set size: 124 samples\n",
            "Epoch 1/100, Train Loss: 0.1497, Val Loss: 0.2856\n",
            "Epoch 2/100, Train Loss: 0.1030, Val Loss: 0.1760\n",
            "Epoch 3/100, Train Loss: 0.0788, Val Loss: 0.1365\n",
            "Epoch 4/100, Train Loss: 0.0720, Val Loss: 0.0517\n",
            "Epoch 5/100, Train Loss: 0.0650, Val Loss: 0.0500\n",
            "Epoch 6/100, Train Loss: 0.0670, Val Loss: 0.0468\n",
            "Epoch 7/100, Train Loss: 0.0555, Val Loss: 0.0795\n",
            "Epoch 8/100, Train Loss: 0.0532, Val Loss: 0.0465\n",
            "Epoch 9/100, Train Loss: 0.0588, Val Loss: 0.0658\n",
            "Epoch 10/100, Train Loss: 0.0593, Val Loss: 0.0917\n",
            "Epoch 11/100, Train Loss: 0.0570, Val Loss: 0.0592\n",
            "Epoch 12/100, Train Loss: 0.0469, Val Loss: 0.0468\n",
            "Epoch 13/100, Train Loss: 0.0465, Val Loss: 0.0472\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 14/100, Train Loss: 0.0388, Val Loss: 0.0470\n",
            "Epoch 15/100, Train Loss: 0.0415, Val Loss: 0.0473\n",
            "Epoch 16/100, Train Loss: 0.0389, Val Loss: 0.0478\n",
            "Epoch 17/100, Train Loss: 0.0418, Val Loss: 0.0481\n",
            "Epoch 18/100, Train Loss: 0.0384, Val Loss: 0.0477\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0426, Val Loss: 0.0477\n",
            "Epoch 20/100, Train Loss: 0.0428, Val Loss: 0.0477\n",
            "Epoch 21/100, Train Loss: 0.0388, Val Loss: 0.0477\n",
            "Epoch 22/100, Train Loss: 0.0418, Val Loss: 0.0477\n",
            "Epoch 23/100, Train Loss: 0.0392, Val Loss: 0.0477\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0381, Val Loss: 0.0477\n",
            "Epoch 25/100, Train Loss: 0.0417, Val Loss: 0.0477\n",
            "Epoch 26/100, Train Loss: 0.0392, Val Loss: 0.0477\n",
            "Epoch 27/100, Train Loss: 0.0402, Val Loss: 0.0477\n",
            "Epoch 28/100, Train Loss: 0.0387, Val Loss: 0.0477\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0407, Val Loss: 0.0477\n",
            "Epoch 30/100, Train Loss: 0.0393, Val Loss: 0.0477\n",
            "Epoch 31/100, Train Loss: 0.0404, Val Loss: 0.0477\n",
            "Epoch 32/100, Train Loss: 0.0406, Val Loss: 0.0477\n",
            "Epoch 33/100, Train Loss: 0.0407, Val Loss: 0.0477\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0418, Val Loss: 0.0477\n",
            "Epoch 35/100, Train Loss: 0.0440, Val Loss: 0.0477\n",
            "Epoch 36/100, Train Loss: 0.0405, Val Loss: 0.0477\n",
            "Epoch 37/100, Train Loss: 0.0394, Val Loss: 0.0477\n",
            "Epoch 38/100, Train Loss: 0.0444, Val Loss: 0.0477\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0401, Val Loss: 0.0477\n",
            "Epoch 40/100, Train Loss: 0.0415, Val Loss: 0.0477\n",
            "Epoch 41/100, Train Loss: 0.0388, Val Loss: 0.0477\n",
            "Epoch 42/100, Train Loss: 0.0409, Val Loss: 0.0477\n",
            "Epoch 43/100, Train Loss: 0.0421, Val Loss: 0.0477\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0442, Val Loss: 0.0477\n",
            "Epoch 45/100, Train Loss: 0.0408, Val Loss: 0.0477\n",
            "Epoch 46/100, Train Loss: 0.0411, Val Loss: 0.0477\n",
            "Epoch 47/100, Train Loss: 0.0421, Val Loss: 0.0477\n",
            "Epoch 48/100, Train Loss: 0.0430, Val Loss: 0.0477\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0424, Val Loss: 0.0477\n",
            "Epoch 50/100, Train Loss: 0.0423, Val Loss: 0.0477\n",
            "Epoch 51/100, Train Loss: 0.0429, Val Loss: 0.0477\n",
            "Epoch 52/100, Train Loss: 0.0413, Val Loss: 0.0477\n",
            "Epoch 53/100, Train Loss: 0.0448, Val Loss: 0.0477\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0418, Val Loss: 0.0477\n",
            "Epoch 55/100, Train Loss: 0.0400, Val Loss: 0.0477\n",
            "Epoch 56/100, Train Loss: 0.0420, Val Loss: 0.0477\n",
            "Epoch 57/100, Train Loss: 0.0427, Val Loss: 0.0477\n",
            "Epoch 58/100, Train Loss: 0.0425, Val Loss: 0.0477\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0410, Val Loss: 0.0477\n",
            "Epoch 60/100, Train Loss: 0.0427, Val Loss: 0.0477\n",
            "Epoch 61/100, Train Loss: 0.0419, Val Loss: 0.0477\n",
            "Epoch 62/100, Train Loss: 0.0402, Val Loss: 0.0477\n",
            "Epoch 63/100, Train Loss: 0.0383, Val Loss: 0.0477\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0410, Val Loss: 0.0477\n",
            "Epoch 65/100, Train Loss: 0.0427, Val Loss: 0.0477\n",
            "Epoch 66/100, Train Loss: 0.0404, Val Loss: 0.0477\n",
            "Epoch 67/100, Train Loss: 0.0383, Val Loss: 0.0477\n",
            "Epoch 68/100, Train Loss: 0.0433, Val Loss: 0.0477\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0429, Val Loss: 0.0477\n",
            "Epoch 70/100, Train Loss: 0.0413, Val Loss: 0.0477\n",
            "Epoch 71/100, Train Loss: 0.0408, Val Loss: 0.0477\n",
            "Epoch 72/100, Train Loss: 0.0393, Val Loss: 0.0477\n",
            "Epoch 73/100, Train Loss: 0.0391, Val Loss: 0.0477\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0426, Val Loss: 0.0477\n",
            "Epoch 75/100, Train Loss: 0.0432, Val Loss: 0.0477\n",
            "Epoch 76/100, Train Loss: 0.0392, Val Loss: 0.0477\n",
            "Epoch 77/100, Train Loss: 0.0405, Val Loss: 0.0477\n",
            "Epoch 78/100, Train Loss: 0.0422, Val Loss: 0.0477\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0404, Val Loss: 0.0477\n",
            "Epoch 80/100, Train Loss: 0.0423, Val Loss: 0.0477\n",
            "Epoch 81/100, Train Loss: 0.0398, Val Loss: 0.0477\n",
            "Epoch 82/100, Train Loss: 0.0449, Val Loss: 0.0477\n",
            "Epoch 83/100, Train Loss: 0.0438, Val Loss: 0.0477\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0439, Val Loss: 0.0477\n",
            "Epoch 85/100, Train Loss: 0.0408, Val Loss: 0.0477\n",
            "Epoch 86/100, Train Loss: 0.0385, Val Loss: 0.0477\n",
            "Epoch 87/100, Train Loss: 0.0400, Val Loss: 0.0477\n",
            "Epoch 88/100, Train Loss: 0.0388, Val Loss: 0.0477\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0397, Val Loss: 0.0477\n",
            "Epoch 90/100, Train Loss: 0.0368, Val Loss: 0.0477\n",
            "Epoch 91/100, Train Loss: 0.0440, Val Loss: 0.0477\n",
            "Epoch 92/100, Train Loss: 0.0418, Val Loss: 0.0477\n",
            "Epoch 93/100, Train Loss: 0.0423, Val Loss: 0.0477\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0423, Val Loss: 0.0477\n",
            "Epoch 95/100, Train Loss: 0.0392, Val Loss: 0.0477\n",
            "Epoch 96/100, Train Loss: 0.0434, Val Loss: 0.0477\n",
            "Epoch 97/100, Train Loss: 0.0426, Val Loss: 0.0477\n",
            "Epoch 98/100, Train Loss: 0.0387, Val Loss: 0.0477\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0435, Val Loss: 0.0477\n",
            "Epoch 100/100, Train Loss: 0.0425, Val Loss: 0.0477\n",
            "\n",
            "Test iteration 13/14\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1507, Val Loss: 0.2485\n",
            "Epoch 2/100, Train Loss: 0.0906, Val Loss: 0.1727\n",
            "Epoch 3/100, Train Loss: 0.0846, Val Loss: 0.1464\n",
            "Epoch 4/100, Train Loss: 0.0786, Val Loss: 0.0757\n",
            "Epoch 5/100, Train Loss: 0.0711, Val Loss: 0.0854\n",
            "Epoch 6/100, Train Loss: 0.0685, Val Loss: 0.0527\n",
            "Epoch 7/100, Train Loss: 0.0587, Val Loss: 0.0482\n",
            "Epoch 8/100, Train Loss: 0.0494, Val Loss: 0.0461\n",
            "Epoch 9/100, Train Loss: 0.0626, Val Loss: 0.1081\n",
            "Epoch 10/100, Train Loss: 0.0516, Val Loss: 0.0554\n",
            "Epoch 11/100, Train Loss: 0.0506, Val Loss: 0.0837\n",
            "Epoch 12/100, Train Loss: 0.0516, Val Loss: 0.0546\n",
            "Epoch 13/100, Train Loss: 0.0496, Val Loss: 0.0608\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 14/100, Train Loss: 0.0385, Val Loss: 0.0614\n",
            "Epoch 15/100, Train Loss: 0.0428, Val Loss: 0.0616\n",
            "Epoch 16/100, Train Loss: 0.0403, Val Loss: 0.0612\n",
            "Epoch 17/100, Train Loss: 0.0416, Val Loss: 0.0616\n",
            "Epoch 18/100, Train Loss: 0.0382, Val Loss: 0.0617\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0387, Val Loss: 0.0617\n",
            "Epoch 20/100, Train Loss: 0.0390, Val Loss: 0.0617\n",
            "Epoch 21/100, Train Loss: 0.0419, Val Loss: 0.0617\n",
            "Epoch 22/100, Train Loss: 0.0383, Val Loss: 0.0617\n",
            "Epoch 23/100, Train Loss: 0.0434, Val Loss: 0.0617\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0397, Val Loss: 0.0617\n",
            "Epoch 25/100, Train Loss: 0.0379, Val Loss: 0.0617\n",
            "Epoch 26/100, Train Loss: 0.0373, Val Loss: 0.0617\n",
            "Epoch 27/100, Train Loss: 0.0394, Val Loss: 0.0617\n",
            "Epoch 28/100, Train Loss: 0.0443, Val Loss: 0.0617\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0380, Val Loss: 0.0617\n",
            "Epoch 30/100, Train Loss: 0.0391, Val Loss: 0.0617\n",
            "Epoch 31/100, Train Loss: 0.0411, Val Loss: 0.0617\n",
            "Epoch 32/100, Train Loss: 0.0374, Val Loss: 0.0617\n",
            "Epoch 33/100, Train Loss: 0.0414, Val Loss: 0.0617\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0413, Val Loss: 0.0617\n",
            "Epoch 35/100, Train Loss: 0.0394, Val Loss: 0.0617\n",
            "Epoch 36/100, Train Loss: 0.0449, Val Loss: 0.0617\n",
            "Epoch 37/100, Train Loss: 0.0439, Val Loss: 0.0617\n",
            "Epoch 38/100, Train Loss: 0.0384, Val Loss: 0.0617\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0381, Val Loss: 0.0617\n",
            "Epoch 40/100, Train Loss: 0.0405, Val Loss: 0.0617\n",
            "Epoch 41/100, Train Loss: 0.0452, Val Loss: 0.0617\n",
            "Epoch 42/100, Train Loss: 0.0402, Val Loss: 0.0617\n",
            "Epoch 43/100, Train Loss: 0.0402, Val Loss: 0.0617\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0380, Val Loss: 0.0617\n",
            "Epoch 45/100, Train Loss: 0.0416, Val Loss: 0.0617\n",
            "Epoch 46/100, Train Loss: 0.0373, Val Loss: 0.0617\n",
            "Epoch 47/100, Train Loss: 0.0389, Val Loss: 0.0617\n",
            "Epoch 48/100, Train Loss: 0.0406, Val Loss: 0.0617\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0372, Val Loss: 0.0617\n",
            "Epoch 50/100, Train Loss: 0.0405, Val Loss: 0.0617\n",
            "Epoch 51/100, Train Loss: 0.0396, Val Loss: 0.0617\n",
            "Epoch 52/100, Train Loss: 0.0411, Val Loss: 0.0617\n",
            "Epoch 53/100, Train Loss: 0.0399, Val Loss: 0.0617\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0421, Val Loss: 0.0617\n",
            "Epoch 55/100, Train Loss: 0.0383, Val Loss: 0.0617\n",
            "Epoch 56/100, Train Loss: 0.0445, Val Loss: 0.0617\n",
            "Epoch 57/100, Train Loss: 0.0384, Val Loss: 0.0617\n",
            "Epoch 58/100, Train Loss: 0.0427, Val Loss: 0.0617\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0412, Val Loss: 0.0617\n",
            "Epoch 60/100, Train Loss: 0.0425, Val Loss: 0.0617\n",
            "Epoch 61/100, Train Loss: 0.0456, Val Loss: 0.0617\n",
            "Epoch 62/100, Train Loss: 0.0393, Val Loss: 0.0617\n",
            "Epoch 63/100, Train Loss: 0.0436, Val Loss: 0.0617\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0399, Val Loss: 0.0617\n",
            "Epoch 65/100, Train Loss: 0.0389, Val Loss: 0.0617\n",
            "Epoch 66/100, Train Loss: 0.0411, Val Loss: 0.0617\n",
            "Epoch 67/100, Train Loss: 0.0425, Val Loss: 0.0617\n",
            "Epoch 68/100, Train Loss: 0.0396, Val Loss: 0.0617\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0428, Val Loss: 0.0617\n",
            "Epoch 70/100, Train Loss: 0.0378, Val Loss: 0.0617\n",
            "Epoch 71/100, Train Loss: 0.0411, Val Loss: 0.0617\n",
            "Epoch 72/100, Train Loss: 0.0404, Val Loss: 0.0617\n",
            "Epoch 73/100, Train Loss: 0.0413, Val Loss: 0.0617\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0397, Val Loss: 0.0617\n",
            "Epoch 75/100, Train Loss: 0.0426, Val Loss: 0.0617\n",
            "Epoch 76/100, Train Loss: 0.0415, Val Loss: 0.0617\n",
            "Epoch 77/100, Train Loss: 0.0387, Val Loss: 0.0617\n",
            "Epoch 78/100, Train Loss: 0.0533, Val Loss: 0.0617\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0403, Val Loss: 0.0617\n",
            "Epoch 80/100, Train Loss: 0.0398, Val Loss: 0.0617\n",
            "Epoch 81/100, Train Loss: 0.0398, Val Loss: 0.0617\n",
            "Epoch 82/100, Train Loss: 0.0416, Val Loss: 0.0617\n",
            "Epoch 83/100, Train Loss: 0.0402, Val Loss: 0.0617\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0445, Val Loss: 0.0617\n",
            "Epoch 85/100, Train Loss: 0.0430, Val Loss: 0.0617\n",
            "Epoch 86/100, Train Loss: 0.0421, Val Loss: 0.0617\n",
            "Epoch 87/100, Train Loss: 0.0376, Val Loss: 0.0617\n",
            "Epoch 88/100, Train Loss: 0.0443, Val Loss: 0.0617\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0455, Val Loss: 0.0617\n",
            "Epoch 90/100, Train Loss: 0.0403, Val Loss: 0.0617\n",
            "Epoch 91/100, Train Loss: 0.0392, Val Loss: 0.0617\n",
            "Epoch 92/100, Train Loss: 0.0396, Val Loss: 0.0617\n",
            "Epoch 93/100, Train Loss: 0.0422, Val Loss: 0.0617\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0381, Val Loss: 0.0617\n",
            "Epoch 95/100, Train Loss: 0.0420, Val Loss: 0.0617\n",
            "Epoch 96/100, Train Loss: 0.0423, Val Loss: 0.0617\n",
            "Epoch 97/100, Train Loss: 0.0446, Val Loss: 0.0617\n",
            "Epoch 98/100, Train Loss: 0.0419, Val Loss: 0.0617\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0427, Val Loss: 0.0617\n",
            "Epoch 100/100, Train Loss: 0.0411, Val Loss: 0.0617\n",
            "\n",
            "Test iteration 14/14\n",
            "Current training set size: 126 samples\n",
            "Epoch 1/100, Train Loss: 0.1530, Val Loss: 0.2690\n",
            "Epoch 2/100, Train Loss: 0.1119, Val Loss: 0.1752\n",
            "Epoch 3/100, Train Loss: 0.0955, Val Loss: 0.1677\n",
            "Epoch 4/100, Train Loss: 0.0867, Val Loss: 0.0720\n",
            "Epoch 5/100, Train Loss: 0.0751, Val Loss: 0.1100\n",
            "Epoch 6/100, Train Loss: 0.0680, Val Loss: 0.0440\n",
            "Epoch 7/100, Train Loss: 0.0555, Val Loss: 0.0558\n",
            "Epoch 8/100, Train Loss: 0.0596, Val Loss: 0.0459\n",
            "Epoch 9/100, Train Loss: 0.0580, Val Loss: 0.0496\n",
            "Epoch 10/100, Train Loss: 0.0528, Val Loss: 0.0539\n",
            "Epoch 11/100, Train Loss: 0.0455, Val Loss: 0.0490\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 12/100, Train Loss: 0.0559, Val Loss: 0.0493\n",
            "Epoch 13/100, Train Loss: 0.0473, Val Loss: 0.0510\n",
            "Epoch 14/100, Train Loss: 0.0497, Val Loss: 0.0513\n",
            "Epoch 15/100, Train Loss: 0.0426, Val Loss: 0.0524\n",
            "Epoch 16/100, Train Loss: 0.0416, Val Loss: 0.0533\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0395, Val Loss: 0.0533\n",
            "Epoch 18/100, Train Loss: 0.0415, Val Loss: 0.0533\n",
            "Epoch 19/100, Train Loss: 0.0374, Val Loss: 0.0533\n",
            "Epoch 20/100, Train Loss: 0.0421, Val Loss: 0.0533\n",
            "Epoch 21/100, Train Loss: 0.0378, Val Loss: 0.0533\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0400, Val Loss: 0.0533\n",
            "Epoch 23/100, Train Loss: 0.0409, Val Loss: 0.0533\n",
            "Epoch 24/100, Train Loss: 0.0436, Val Loss: 0.0533\n",
            "Epoch 25/100, Train Loss: 0.0446, Val Loss: 0.0533\n",
            "Epoch 26/100, Train Loss: 0.0427, Val Loss: 0.0533\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0405, Val Loss: 0.0533\n",
            "Epoch 28/100, Train Loss: 0.0435, Val Loss: 0.0533\n",
            "Epoch 29/100, Train Loss: 0.0416, Val Loss: 0.0533\n",
            "Epoch 30/100, Train Loss: 0.0477, Val Loss: 0.0533\n",
            "Epoch 31/100, Train Loss: 0.0431, Val Loss: 0.0533\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0452, Val Loss: 0.0533\n",
            "Epoch 33/100, Train Loss: 0.0410, Val Loss: 0.0533\n",
            "Epoch 34/100, Train Loss: 0.0458, Val Loss: 0.0533\n",
            "Epoch 35/100, Train Loss: 0.0429, Val Loss: 0.0533\n",
            "Epoch 36/100, Train Loss: 0.0437, Val Loss: 0.0533\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0441, Val Loss: 0.0533\n",
            "Epoch 38/100, Train Loss: 0.0401, Val Loss: 0.0533\n",
            "Epoch 39/100, Train Loss: 0.0407, Val Loss: 0.0533\n",
            "Epoch 40/100, Train Loss: 0.0443, Val Loss: 0.0533\n",
            "Epoch 41/100, Train Loss: 0.0405, Val Loss: 0.0533\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0452, Val Loss: 0.0533\n",
            "Epoch 43/100, Train Loss: 0.0410, Val Loss: 0.0533\n",
            "Epoch 44/100, Train Loss: 0.0388, Val Loss: 0.0533\n",
            "Epoch 45/100, Train Loss: 0.0411, Val Loss: 0.0533\n",
            "Epoch 46/100, Train Loss: 0.0430, Val Loss: 0.0533\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0460, Val Loss: 0.0533\n",
            "Epoch 48/100, Train Loss: 0.0436, Val Loss: 0.0533\n",
            "Epoch 49/100, Train Loss: 0.0424, Val Loss: 0.0533\n",
            "Epoch 50/100, Train Loss: 0.0413, Val Loss: 0.0533\n",
            "Epoch 51/100, Train Loss: 0.0405, Val Loss: 0.0533\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0436, Val Loss: 0.0533\n",
            "Epoch 53/100, Train Loss: 0.0395, Val Loss: 0.0533\n",
            "Epoch 54/100, Train Loss: 0.0396, Val Loss: 0.0533\n",
            "Epoch 55/100, Train Loss: 0.0377, Val Loss: 0.0533\n",
            "Epoch 56/100, Train Loss: 0.0419, Val Loss: 0.0533\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0415, Val Loss: 0.0533\n",
            "Epoch 58/100, Train Loss: 0.0415, Val Loss: 0.0533\n",
            "Epoch 59/100, Train Loss: 0.0424, Val Loss: 0.0533\n",
            "Epoch 60/100, Train Loss: 0.0429, Val Loss: 0.0533\n",
            "Epoch 61/100, Train Loss: 0.0425, Val Loss: 0.0533\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0443, Val Loss: 0.0533\n",
            "Epoch 63/100, Train Loss: 0.0401, Val Loss: 0.0533\n",
            "Epoch 64/100, Train Loss: 0.0409, Val Loss: 0.0533\n",
            "Epoch 65/100, Train Loss: 0.0492, Val Loss: 0.0533\n",
            "Epoch 66/100, Train Loss: 0.0411, Val Loss: 0.0533\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0444, Val Loss: 0.0533\n",
            "Epoch 68/100, Train Loss: 0.0400, Val Loss: 0.0533\n",
            "Epoch 69/100, Train Loss: 0.0392, Val Loss: 0.0533\n",
            "Epoch 70/100, Train Loss: 0.0398, Val Loss: 0.0533\n",
            "Epoch 71/100, Train Loss: 0.0387, Val Loss: 0.0533\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0441, Val Loss: 0.0533\n",
            "Epoch 73/100, Train Loss: 0.0412, Val Loss: 0.0533\n",
            "Epoch 74/100, Train Loss: 0.0442, Val Loss: 0.0533\n",
            "Epoch 75/100, Train Loss: 0.0401, Val Loss: 0.0533\n",
            "Epoch 76/100, Train Loss: 0.0394, Val Loss: 0.0533\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0395, Val Loss: 0.0533\n",
            "Epoch 78/100, Train Loss: 0.0393, Val Loss: 0.0533\n",
            "Epoch 79/100, Train Loss: 0.0444, Val Loss: 0.0533\n",
            "Epoch 80/100, Train Loss: 0.0406, Val Loss: 0.0533\n",
            "Epoch 81/100, Train Loss: 0.0425, Val Loss: 0.0533\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0391, Val Loss: 0.0533\n",
            "Epoch 83/100, Train Loss: 0.0453, Val Loss: 0.0533\n",
            "Epoch 84/100, Train Loss: 0.0456, Val Loss: 0.0533\n",
            "Epoch 85/100, Train Loss: 0.0434, Val Loss: 0.0533\n",
            "Epoch 86/100, Train Loss: 0.0445, Val Loss: 0.0533\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0378, Val Loss: 0.0533\n",
            "Epoch 88/100, Train Loss: 0.0418, Val Loss: 0.0533\n",
            "Epoch 89/100, Train Loss: 0.0425, Val Loss: 0.0533\n",
            "Epoch 90/100, Train Loss: 0.0438, Val Loss: 0.0533\n",
            "Epoch 91/100, Train Loss: 0.0410, Val Loss: 0.0533\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0399, Val Loss: 0.0533\n",
            "Epoch 93/100, Train Loss: 0.0440, Val Loss: 0.0533\n",
            "Epoch 94/100, Train Loss: 0.0432, Val Loss: 0.0533\n",
            "Epoch 95/100, Train Loss: 0.0382, Val Loss: 0.0533\n",
            "Epoch 96/100, Train Loss: 0.0443, Val Loss: 0.0533\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0381, Val Loss: 0.0533\n",
            "Epoch 98/100, Train Loss: 0.0419, Val Loss: 0.0533\n",
            "Epoch 99/100, Train Loss: 0.0421, Val Loss: 0.0533\n",
            "Epoch 100/100, Train Loss: 0.0474, Val Loss: 0.0533\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: rf, Units/Estimators: Est100\n",
            "n_estimators: 100\n",
            "Device: cuda\n",
            "Total test samples: 14\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/14\n",
            "Current training set size: 113 samples\n",
            "\n",
            "Test iteration 2/14\n",
            "Current training set size: 114 samples\n",
            "\n",
            "Test iteration 3/14\n",
            "Current training set size: 115 samples\n",
            "\n",
            "Test iteration 4/14\n",
            "Current training set size: 116 samples\n",
            "\n",
            "Test iteration 5/14\n",
            "Current training set size: 117 samples\n",
            "\n",
            "Test iteration 6/14\n",
            "Current training set size: 118 samples\n",
            "\n",
            "Test iteration 7/14\n",
            "Current training set size: 119 samples\n",
            "\n",
            "Test iteration 8/14\n",
            "Current training set size: 120 samples\n",
            "\n",
            "Test iteration 9/14\n",
            "Current training set size: 121 samples\n",
            "\n",
            "Test iteration 10/14\n",
            "Current training set size: 122 samples\n",
            "\n",
            "Test iteration 11/14\n",
            "Current training set size: 123 samples\n",
            "\n",
            "Test iteration 12/14\n",
            "Current training set size: 124 samples\n",
            "\n",
            "Test iteration 13/14\n",
            "Current training set size: 125 samples\n",
            "\n",
            "Test iteration 14/14\n",
            "Current training set size: 126 samples\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: rf, Units/Estimators: Est200\n",
            "n_estimators: 200\n",
            "Device: cuda\n",
            "Total test samples: 14\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/14\n",
            "Current training set size: 113 samples\n",
            "\n",
            "Test iteration 2/14\n",
            "Current training set size: 114 samples\n",
            "\n",
            "Test iteration 3/14\n",
            "Current training set size: 115 samples\n",
            "\n",
            "Test iteration 4/14\n",
            "Current training set size: 116 samples\n",
            "\n",
            "Test iteration 5/14\n",
            "Current training set size: 117 samples\n",
            "\n",
            "Test iteration 6/14\n",
            "Current training set size: 118 samples\n",
            "\n",
            "Test iteration 7/14\n",
            "Current training set size: 119 samples\n",
            "\n",
            "Test iteration 8/14\n",
            "Current training set size: 120 samples\n",
            "\n",
            "Test iteration 9/14\n",
            "Current training set size: 121 samples\n",
            "\n",
            "Test iteration 10/14\n",
            "Current training set size: 122 samples\n",
            "\n",
            "Test iteration 11/14\n",
            "Current training set size: 123 samples\n",
            "\n",
            "Test iteration 12/14\n",
            "Current training set size: 124 samples\n",
            "\n",
            "Test iteration 13/14\n",
            "Current training set size: 125 samples\n",
            "\n",
            "Test iteration 14/14\n",
            "Current training set size: 126 samples\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: xgb, Units/Estimators: Est100\n",
            "n_estimators: 100\n",
            "Device: cuda\n",
            "Total test samples: 14\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/14\n",
            "Current training set size: 113 samples\n",
            "\n",
            "Test iteration 2/14\n",
            "Current training set size: 114 samples\n",
            "\n",
            "Test iteration 3/14\n",
            "Current training set size: 115 samples\n",
            "\n",
            "Test iteration 4/14\n",
            "Current training set size: 116 samples\n",
            "\n",
            "Test iteration 5/14\n",
            "Current training set size: 117 samples\n",
            "\n",
            "Test iteration 6/14\n",
            "Current training set size: 118 samples\n",
            "\n",
            "Test iteration 7/14\n",
            "Current training set size: 119 samples\n",
            "\n",
            "Test iteration 8/14\n",
            "Current training set size: 120 samples\n",
            "\n",
            "Test iteration 9/14\n",
            "Current training set size: 121 samples\n",
            "\n",
            "Test iteration 10/14\n",
            "Current training set size: 122 samples\n",
            "\n",
            "Test iteration 11/14\n",
            "Current training set size: 123 samples\n",
            "\n",
            "Test iteration 12/14\n",
            "Current training set size: 124 samples\n",
            "\n",
            "Test iteration 13/14\n",
            "Current training set size: 125 samples\n",
            "\n",
            "Test iteration 14/14\n",
            "Current training set size: 126 samples\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: xgb, Units/Estimators: Est200\n",
            "n_estimators: 200\n",
            "Device: cuda\n",
            "Total test samples: 14\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/14\n",
            "Current training set size: 113 samples\n",
            "\n",
            "Test iteration 2/14\n",
            "Current training set size: 114 samples\n",
            "\n",
            "Test iteration 3/14\n",
            "Current training set size: 115 samples\n",
            "\n",
            "Test iteration 4/14\n",
            "Current training set size: 116 samples\n",
            "\n",
            "Test iteration 5/14\n",
            "Current training set size: 117 samples\n",
            "\n",
            "Test iteration 6/14\n",
            "Current training set size: 118 samples\n",
            "\n",
            "Test iteration 7/14\n",
            "Current training set size: 119 samples\n",
            "\n",
            "Test iteration 8/14\n",
            "Current training set size: 120 samples\n",
            "\n",
            "Test iteration 9/14\n",
            "Current training set size: 121 samples\n",
            "\n",
            "Test iteration 10/14\n",
            "Current training set size: 122 samples\n",
            "\n",
            "Test iteration 11/14\n",
            "Current training set size: 123 samples\n",
            "\n",
            "Test iteration 12/14\n",
            "Current training set size: 124 samples\n",
            "\n",
            "Test iteration 13/14\n",
            "Current training set size: 125 samples\n",
            "\n",
            "Test iteration 14/14\n",
            "Current training set size: 126 samples\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: gbm, Units/Estimators: Est100\n",
            "n_estimators: 100\n",
            "Device: cuda\n",
            "Total test samples: 14\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/14\n",
            "Current training set size: 113 samples\n",
            "\n",
            "Test iteration 2/14\n",
            "Current training set size: 114 samples\n",
            "\n",
            "Test iteration 3/14\n",
            "Current training set size: 115 samples\n",
            "\n",
            "Test iteration 4/14\n",
            "Current training set size: 116 samples\n",
            "\n",
            "Test iteration 5/14\n",
            "Current training set size: 117 samples\n",
            "\n",
            "Test iteration 6/14\n",
            "Current training set size: 118 samples\n",
            "\n",
            "Test iteration 7/14\n",
            "Current training set size: 119 samples\n",
            "\n",
            "Test iteration 8/14\n",
            "Current training set size: 120 samples\n",
            "\n",
            "Test iteration 9/14\n",
            "Current training set size: 121 samples\n",
            "\n",
            "Test iteration 10/14\n",
            "Current training set size: 122 samples\n",
            "\n",
            "Test iteration 11/14\n",
            "Current training set size: 123 samples\n",
            "\n",
            "Test iteration 12/14\n",
            "Current training set size: 124 samples\n",
            "\n",
            "Test iteration 13/14\n",
            "Current training set size: 125 samples\n",
            "\n",
            "Test iteration 14/14\n",
            "Current training set size: 126 samples\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: gbm, Units/Estimators: Est200\n",
            "n_estimators: 200\n",
            "Device: cuda\n",
            "Total test samples: 14\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/14\n",
            "Current training set size: 113 samples\n",
            "\n",
            "Test iteration 2/14\n",
            "Current training set size: 114 samples\n",
            "\n",
            "Test iteration 3/14\n",
            "Current training set size: 115 samples\n",
            "\n",
            "Test iteration 4/14\n",
            "Current training set size: 116 samples\n",
            "\n",
            "Test iteration 5/14\n",
            "Current training set size: 117 samples\n",
            "\n",
            "Test iteration 6/14\n",
            "Current training set size: 118 samples\n",
            "\n",
            "Test iteration 7/14\n",
            "Current training set size: 119 samples\n",
            "\n",
            "Test iteration 8/14\n",
            "Current training set size: 120 samples\n",
            "\n",
            "Test iteration 9/14\n",
            "Current training set size: 121 samples\n",
            "\n",
            "Test iteration 10/14\n",
            "Current training set size: 122 samples\n",
            "\n",
            "Test iteration 11/14\n",
            "Current training set size: 123 samples\n",
            "\n",
            "Test iteration 12/14\n",
            "Current training set size: 124 samples\n",
            "\n",
            "Test iteration 13/14\n",
            "Current training set size: 125 samples\n",
            "\n",
            "Test iteration 14/14\n",
            "Current training set size: 126 samples\n",
            "Results:\n",
            "            RMSE            MAE       MAPE model_type units drop_rate  \\\n",
            "0  202699.558875  156665.140625  11.801397        cnn   N/A       0.1   \n",
            "1  221476.833931  182660.078125  13.987510        cnn   N/A       0.1   \n",
            "2  251784.256092  203497.125000  15.600215        cnn   N/A       0.2   \n",
            "3  270652.859760  224290.750000  17.002007        cnn   N/A       0.2   \n",
            "4  194188.329735  161490.877588  12.694763         rf   N/A       N/A   \n",
            "5  186477.945227  158849.395778  12.524625         rf   N/A       N/A   \n",
            "6  248289.969318  192064.531250  14.761143        xgb   N/A       N/A   \n",
            "7  248114.372595  191814.859375  14.742420        xgb   N/A       N/A   \n",
            "8  227934.337842  156580.566887  12.578239        gbm   N/A       N/A   \n",
            "9  227933.225207  156570.663444  12.577589        gbm   N/A       N/A   \n",
            "\n",
            "  dense_unit batch_size epochs  n_estimators  \n",
            "0         32          4    100           NaN  \n",
            "1         64          4    100           NaN  \n",
            "2         32          4    100           NaN  \n",
            "3         64          4    100           NaN  \n",
            "4        N/A        N/A    N/A         100.0  \n",
            "5        N/A        N/A    N/A         200.0  \n",
            "6        N/A        N/A    N/A         100.0  \n",
            "7        N/A        N/A    N/A         200.0  \n",
            "8        N/A        N/A    N/A         100.0  \n",
            "9        N/A        N/A    N/A         200.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adjusted_valuelist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "SbmDEJz_NIc_",
        "outputId": "4207de1e-90ca-4dde-dd72-70c1ddd1a530"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            RMSE            MAE       MAPE model_type units drop_rate  \\\n",
              "0  202699.558875  156665.140625  11.801397        cnn   N/A       0.1   \n",
              "1  221476.833931  182660.078125  13.987510        cnn   N/A       0.1   \n",
              "2  251784.256092  203497.125000  15.600215        cnn   N/A       0.2   \n",
              "3  270652.859760  224290.750000  17.002007        cnn   N/A       0.2   \n",
              "4  194188.329735  161490.877588  12.694763         rf   N/A       N/A   \n",
              "5  186477.945227  158849.395778  12.524625         rf   N/A       N/A   \n",
              "6  248289.969318  192064.531250  14.761143        xgb   N/A       N/A   \n",
              "7  248114.372595  191814.859375  14.742420        xgb   N/A       N/A   \n",
              "8  227934.337842  156580.566887  12.578239        gbm   N/A       N/A   \n",
              "9  227933.225207  156570.663444  12.577589        gbm   N/A       N/A   \n",
              "\n",
              "  dense_unit batch_size epochs  n_estimators  \n",
              "0         32          4    100           NaN  \n",
              "1         64          4    100           NaN  \n",
              "2         32          4    100           NaN  \n",
              "3         64          4    100           NaN  \n",
              "4        N/A        N/A    N/A         100.0  \n",
              "5        N/A        N/A    N/A         200.0  \n",
              "6        N/A        N/A    N/A         100.0  \n",
              "7        N/A        N/A    N/A         200.0  \n",
              "8        N/A        N/A    N/A         100.0  \n",
              "9        N/A        N/A    N/A         200.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-090715ba-4829-4c38-b9af-667906e95fa5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAE</th>\n",
              "      <th>MAPE</th>\n",
              "      <th>model_type</th>\n",
              "      <th>units</th>\n",
              "      <th>drop_rate</th>\n",
              "      <th>dense_unit</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>epochs</th>\n",
              "      <th>n_estimators</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>202699.558875</td>\n",
              "      <td>156665.140625</td>\n",
              "      <td>11.801397</td>\n",
              "      <td>cnn</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>221476.833931</td>\n",
              "      <td>182660.078125</td>\n",
              "      <td>13.987510</td>\n",
              "      <td>cnn</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>251784.256092</td>\n",
              "      <td>203497.125000</td>\n",
              "      <td>15.600215</td>\n",
              "      <td>cnn</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>270652.859760</td>\n",
              "      <td>224290.750000</td>\n",
              "      <td>17.002007</td>\n",
              "      <td>cnn</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>194188.329735</td>\n",
              "      <td>161490.877588</td>\n",
              "      <td>12.694763</td>\n",
              "      <td>rf</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>186477.945227</td>\n",
              "      <td>158849.395778</td>\n",
              "      <td>12.524625</td>\n",
              "      <td>rf</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>248289.969318</td>\n",
              "      <td>192064.531250</td>\n",
              "      <td>14.761143</td>\n",
              "      <td>xgb</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>248114.372595</td>\n",
              "      <td>191814.859375</td>\n",
              "      <td>14.742420</td>\n",
              "      <td>xgb</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>227934.337842</td>\n",
              "      <td>156580.566887</td>\n",
              "      <td>12.578239</td>\n",
              "      <td>gbm</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>227933.225207</td>\n",
              "      <td>156570.663444</td>\n",
              "      <td>12.577589</td>\n",
              "      <td>gbm</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>200.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-090715ba-4829-4c38-b9af-667906e95fa5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-090715ba-4829-4c38-b9af-667906e95fa5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-090715ba-4829-4c38-b9af-667906e95fa5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a5a43bc2-5c65-4b25-9039-4f8882d0b769\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a5a43bc2-5c65-4b25-9039-4f8882d0b769')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a5a43bc2-5c65-4b25-9039-4f8882d0b769 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_592c1880-4b18-4532-b535-0d639a87248c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('adjusted_valuelist')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_592c1880-4b18-4532-b535-0d639a87248c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('adjusted_valuelist');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "adjusted_valuelist",
              "summary": "{\n  \"name\": \"adjusted_valuelist\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 27388.543612223188,\n        \"min\": 186477.94522673223,\n        \"max\": 270652.85975950817,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          227934.33784203677,\n          221476.8339307748,\n          186477.94522673223\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 24068.849348001553,\n        \"min\": 156570.66344438356,\n        \"max\": 224290.75,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          156580.56688657185,\n          182660.078125,\n          158849.3957778449\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.6720904337340374,\n        \"min\": 11.801397323608398,\n        \"max\": 17.00200653076172,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          12.578239053674498,\n          13.987509727478027,\n          12.524624518255074\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"rf\",\n          \"gbm\",\n          \"cnn\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"units\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"N/A\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"drop_rate\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dense_unit\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"N/A\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"epochs\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"N/A\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_estimators\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 54.772255750516614,\n        \"min\": 100.0,\n        \"max\": 200.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          200.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RANDOM"
      ],
      "metadata": {
        "id": "TIrhITst1CAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Giảm số lượng/GRU unit, dense unit, epochs và sử dụng batch size nhỏ hơn để huấn luyện nhanh hơn.\n",
        "# model_types = ['multi-scale']\n",
        "# lstm_unit = [128, 256, 512]\n",
        "# gru_unit = [8, 16, 32]\n",
        "# drop_rate = [0.1, 0.2]\n",
        "# dense_unit = [16, 32, 64]\n",
        "# batch_size_num = [2, 4]\n",
        "# epochs = [100]\n",
        "\n",
        "model_types = ['cnn', 'rf', 'xgb', 'gbm']\n",
        "lstm_unit = [256,512]\n",
        "gru_unit = [8,16]\n",
        "drop_rate = [0.1,0.2]\n",
        "dense_unit = [32,64]\n",
        "batch_size_num = [4]\n",
        "epochs = [100]\n",
        "n_estimators = [100, 200]\n",
        "\n",
        "# # Replace the current parameter definitions\n",
        "# model_types = ['hybrid', 'sequential', 'stacked', 'bidirectional', 'cnn-rnn', 'multi-scale', 'transformer-rnn', 'ensemble', 'lstm', 'gru']\n",
        "# lstm_unit = [128]\n",
        "# gru_unit = [8]\n",
        "# drop_rate = [0.1]\n",
        "# dense_unit = [64]\n",
        "# batch_size_num = [2]\n",
        "# epochs = [100]\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import concatenate\n",
        "import itertools\n",
        "from math import sqrt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, dropout_rate=0.2, dense_units=64):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # CNN layers\n",
        "        self.conv1 = nn.Conv1d(input_dim, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(128, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Calculate the size after conv + pooling layers\n",
        "        cnn_output_size = 64 * (time_steps // 2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(cnn_output_size, dense_units)\n",
        "        self.fc2 = nn.Linear(dense_units, dense_units // 2)\n",
        "        self.fc3 = nn.Linear(dense_units // 2, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: (batch, time_steps, features)\n",
        "        # Reshape for CNN: (batch, features, time_steps)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # Apply CNN layers\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        # Apply fully connected layers\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "class AttentionGRU(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, gru_units, dropout_rate, dense_units):\n",
        "        super(AttentionGRU, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention1 = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # GRU layer\n",
        "        self.gru = nn.GRU(input_dim, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear(gru_units * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, time_steps, input_dim)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.attention1(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)  # (batch_size, time_steps, input_dim)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # GRU layer - output: (batch, seq, hidden_size)\n",
        "        gru_out, _ = self.gru(attention_mul)\n",
        "\n",
        "        # Flatten and Dense layers\n",
        "        flattened = self.flatten(gru_out)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class AttentionLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, dropout_rate, dense_units):\n",
        "        super(AttentionLSTM, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention1 = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear(lstm_units * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, time_steps, input_dim)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.attention1(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)  # (batch_size, time_steps, input_dim)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # LSTM layer - output: (batch, seq, hidden_size)\n",
        "        lstm_out, _ = self.lstm(attention_mul)\n",
        "\n",
        "        # Flatten and Dense layers\n",
        "        flattened = self.flatten(lstm_out)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class HybridLSTM_GRU(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(HybridLSTM_GRU, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention1 = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # LSTM and GRU layers\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "        self.gru = nn.GRU(input_dim, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        # Combined size from both LSTM and GRU\n",
        "        self.dense1 = nn.Linear((lstm_units + gru_units) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, time_steps, input_dim)\n",
        "\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.attention1(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)  # (batch_size, input_dim, time_steps)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)  # (batch_size, time_steps, input_dim)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # LSTM and GRU layers\n",
        "        lstm_out, _ = self.lstm(attention_mul)\n",
        "        gru_out, _ = self.gru(attention_mul)\n",
        "\n",
        "        # Concatenate LSTM and GRU outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Flatten and Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class SequentialHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(SequentialHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # LSTM followed by GRU\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "        self.gru = nn.GRU(lstm_units, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear(gru_units * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)\n",
        "        a = self.attention(a)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # Sequential processing: LSTM then GRU\n",
        "        lstm_out, _ = self.lstm(attention_mul)\n",
        "        gru_out, _ = self.gru(lstm_out)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(gru_out)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class StackedHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(StackedHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention\n",
        "        self.attention = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # Two stacked LSTM layers\n",
        "        self.lstm1 = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(lstm_units, lstm_units//2, batch_first=True)\n",
        "\n",
        "        # Two stacked GRU layers\n",
        "        self.gru1 = nn.GRU(input_dim, gru_units, batch_first=True)\n",
        "        self.gru2 = nn.GRU(gru_units, gru_units//2, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units//2 + gru_units//2) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)\n",
        "        a = self.attention(a)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # Stacked LSTM path\n",
        "        lstm_out1, _ = self.lstm1(attention_mul)\n",
        "        lstm_out2, _ = self.lstm2(lstm_out1)\n",
        "\n",
        "        # Stacked GRU path\n",
        "        gru_out1, _ = self.gru1(attention_mul)\n",
        "        gru_out2, _ = self.gru2(gru_out1)\n",
        "\n",
        "        # Concatenate final outputs\n",
        "        combined = torch.cat((lstm_out2, gru_out2), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class BidirectionalHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(BidirectionalHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # Bidirectional LSTM and GRU\n",
        "        self.bilstm = nn.LSTM(input_dim, lstm_units, batch_first=True, bidirectional=True)\n",
        "        self.bigru = nn.GRU(input_dim, gru_units, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units*2 + gru_units*2) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)\n",
        "        a = self.attention(a)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # Bidirectional networks\n",
        "        lstm_out, _ = self.bilstm(attention_mul)\n",
        "        gru_out, _ = self.bigru(attention_mul)\n",
        "\n",
        "        # Concatenate outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class CNNRNNHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(CNNRNNHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # 1D CNN for feature extraction\n",
        "        self.conv1 = nn.Conv1d(input_dim, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # RNN layers\n",
        "        self.lstm = nn.LSTM(64, lstm_units, batch_first=True)\n",
        "        self.gru = nn.GRU(64, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units + gru_units) * (time_steps-1), dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # CNN feature extraction\n",
        "        x = x.permute(0, 2, 1)  # (batch, features, seq_len)\n",
        "        cnn_out = self.relu(self.conv1(x))\n",
        "        cnn_out = self.maxpool(cnn_out)\n",
        "        cnn_out = self.relu(self.conv2(cnn_out))\n",
        "        cnn_out = cnn_out.permute(0, 2, 1)  # (batch, seq_len, features)\n",
        "\n",
        "        # RNN processing\n",
        "        lstm_out, _ = self.lstm(cnn_out)\n",
        "        gru_out, _ = self.gru(cnn_out)\n",
        "\n",
        "        # Combine outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class MultiScaleHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(MultiScaleHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # LSTM for long-term dependencies\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "\n",
        "        # GRU for shorter-term dependencies (operating on windows)\n",
        "        self.gru = nn.GRU(input_dim, gru_units, batch_first=True)\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units + gru_units) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Full sequence for LSTM (long-term)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Attention mechanism for GRU input\n",
        "        a = x.permute(0, 2, 1)\n",
        "        a = self.attention(a)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)\n",
        "\n",
        "        # Apply attention weights for GRU (short-term focus)\n",
        "        gru_input = torch.mul(x, a)\n",
        "        gru_out, _ = self.gru(gru_input)\n",
        "\n",
        "        # Combine outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class TransformerRNNHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units, nhead=4):\n",
        "        super(TransformerRNNHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Input projection for transformer\n",
        "        self.input_proj = nn.Linear(input_dim, 64)\n",
        "\n",
        "        # Transformer encoder layer\n",
        "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=64, nhead=nhead, dropout=dropout_rate, batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layer, num_layers=2)\n",
        "\n",
        "        # RNN layers\n",
        "        self.lstm = nn.LSTM(64, lstm_units, batch_first=True)\n",
        "        self.gru = nn.GRU(64, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units + gru_units) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Project input to transformer dimension\n",
        "        x_proj = self.input_proj(x)\n",
        "\n",
        "        # Apply transformer encoder\n",
        "        transformer_out = self.transformer_encoder(x_proj)\n",
        "\n",
        "        # Process with RNNs\n",
        "        lstm_out, _ = self.lstm(transformer_out)\n",
        "        gru_out, _ = self.gru(transformer_out)\n",
        "\n",
        "        # Combine outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class EnsembleHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(EnsembleHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Individual models\n",
        "        self.lstm_model = AttentionLSTM(input_dim, time_steps, lstm_units, dropout_rate, dense_units)\n",
        "        self.gru_model = AttentionGRU(input_dim, time_steps, gru_units, dropout_rate, dense_units)\n",
        "\n",
        "        # Combination layer\n",
        "        self.combine = nn.Linear(2, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get predictions from each model\n",
        "        lstm_pred = self.lstm_model(x)\n",
        "        gru_pred = self.gru_model(x)\n",
        "\n",
        "        # Combine predictions (learnable weights)\n",
        "        combined = torch.cat((lstm_pred, gru_pred), dim=1)\n",
        "        output = self.final_activation(self.combine(combined))\n",
        "\n",
        "        return output\n",
        "\n",
        "class StackingEnsemble(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(StackingEnsemble, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # First-level models (weak learners)\n",
        "        self.lstm_model = AttentionLSTM(input_dim, time_steps, lstm_units, dropout_rate, dense_units)\n",
        "        self.gru_model = AttentionGRU(input_dim, time_steps, gru_units, dropout_rate, dense_units)\n",
        "\n",
        "        # Enhanced meta-learner with more context awareness\n",
        "        # Takes base model predictions plus context features from the original input\n",
        "        self.context_extractor = nn.Sequential(\n",
        "            nn.Linear(input_dim * time_steps, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "\n",
        "        # Meta-learner network with enhanced capacity and context awareness\n",
        "        self.meta_learner = nn.Sequential(\n",
        "            nn.Linear(2 + 32, 64),  # 2 base predictions + 32 context features\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.Dropout(dropout_rate * 0.5),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Attention mechanism for base model outputs\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(2, 2),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Get predictions from base models\n",
        "        lstm_pred = self.lstm_model(x)\n",
        "        gru_pred = self.gru_model(x)\n",
        "\n",
        "        # Extract context features from input\n",
        "        flattened_input = x.reshape(batch_size, -1)\n",
        "        context_features = self.context_extractor(flattened_input)\n",
        "\n",
        "        # Combine base model predictions\n",
        "        base_preds = torch.cat((lstm_pred, gru_pred), dim=1)\n",
        "\n",
        "        # Calculate attention weights for base predictions\n",
        "        attention_weights = self.attention(base_preds)\n",
        "\n",
        "        # Apply attention to base predictions\n",
        "        weighted_preds = base_preds * attention_weights\n",
        "\n",
        "        # Combine everything for the meta-learner\n",
        "        meta_features = torch.cat((weighted_preds, context_features), dim=1)\n",
        "\n",
        "        # Final prediction\n",
        "        final_pred = self.meta_learner(meta_features)\n",
        "\n",
        "        return final_pred\n",
        "\n",
        "    def train_weak_learners(self, train_loader, val_loader, epochs=10, device='cpu'):\n",
        "        \"\"\"\n",
        "        Pre-train the weak learners separately before training the full ensemble\n",
        "        \"\"\"\n",
        "        print(\"Pre-training weak learners...\")\n",
        "        criterion = nn.L1Loss()\n",
        "\n",
        "        # Train LSTM model\n",
        "        print(\"Pre-training LSTM model...\")\n",
        "        optimizer_lstm = optim.Adam(self.lstm_model.parameters(), lr=0.001)\n",
        "        best_lstm_loss = float('inf')\n",
        "        lstm_patience = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            self.lstm_model.train()\n",
        "            train_loss = 0.0\n",
        "            for inputs, targets in train_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                optimizer_lstm.zero_grad()\n",
        "                outputs = self.lstm_model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                optimizer_lstm.step()\n",
        "                train_loss += loss.item()\n",
        "\n",
        "            # Validation\n",
        "            self.lstm_model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for inputs, targets in val_loader:\n",
        "                    inputs, targets = inputs.to(device), targets.to(device)\n",
        "                    outputs = self.lstm_model(inputs)\n",
        "                    val_loss += criterion(outputs, targets).item()\n",
        "\n",
        "            print(f\"LSTM Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n",
        "\n",
        "            # Early stopping check\n",
        "            if val_loss < best_lstm_loss:\n",
        "                best_lstm_loss = val_loss\n",
        "                lstm_patience = 0\n",
        "            else:\n",
        "                lstm_patience += 1\n",
        "                if lstm_patience >= 3:\n",
        "                    print(\"Early stopping LSTM training\")\n",
        "                    break\n",
        "\n",
        "        # Train GRU model\n",
        "        print(\"Pre-training GRU model...\")\n",
        "        optimizer_gru = optim.Adam(self.gru_model.parameters(), lr=0.001)\n",
        "        best_gru_loss = float('inf')\n",
        "        gru_patience = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            self.gru_model.train()\n",
        "            train_loss = 0.0\n",
        "            for inputs, targets in train_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                optimizer_gru.zero_grad()\n",
        "                outputs = self.gru_model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                optimizer_gru.step()\n",
        "                train_loss += loss.item()\n",
        "\n",
        "            # Validation\n",
        "            self.gru_model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for inputs, targets in val_loader:\n",
        "                    inputs, targets = inputs.to(device), targets.to(device)\n",
        "                    outputs = self.gru_model(inputs)\n",
        "                    val_loss += criterion(outputs, targets).item()\n",
        "\n",
        "            print(f\"GRU Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n",
        "\n",
        "            # Early stopping check\n",
        "            if val_loss < best_gru_loss:\n",
        "                best_gru_loss = val_loss\n",
        "                gru_patience = 0\n",
        "            else:\n",
        "                gru_patience += 1\n",
        "                if gru_patience >= 3:\n",
        "                    print(\"Early stopping GRU training\")\n",
        "                    break\n",
        "\n",
        "        # Generate predictions from trained base models for meta-learner warm-up\n",
        "        print(\"Preparing meta-learner with base model predictions...\")\n",
        "        self.lstm_model.eval()\n",
        "        self.gru_model.eval()\n",
        "\n",
        "        # Freeze the weights of weak learners\n",
        "        for param in self.lstm_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        for param in self.gru_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        print(\"Weak learners trained and frozen.\")\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, dropout_rate=0.2, dense_units=64):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # CNN layers\n",
        "        self.conv1 = nn.Conv1d(input_dim, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(128, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Calculate the size after conv + pooling layers\n",
        "        cnn_output_size = 64 * (time_steps // 2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(cnn_output_size, dense_units)\n",
        "        self.fc2 = nn.Linear(dense_units, dense_units // 2)\n",
        "        self.fc3 = nn.Linear(dense_units // 2, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: (batch, time_steps, features)\n",
        "        # Reshape for CNN: (batch, features, time_steps)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # Apply CNN layers\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        # Apply fully connected layers\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "class SklearnModelWrapper:\n",
        "    \"\"\"Wrapper for traditional ML models to maintain interface consistency with PyTorch models\"\"\"\n",
        "    def __init__(self, model_type='rf', **kwargs):\n",
        "        self.model_type = model_type\n",
        "\n",
        "        if model_type == 'rf':\n",
        "            self.model = RandomForestRegressor(**kwargs)\n",
        "        elif model_type == 'gbm':\n",
        "            self.model = GradientBoostingRegressor(**kwargs)\n",
        "        elif model_type == 'xgb':\n",
        "            self.model = xgb.XGBRegressor(**kwargs)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "        self.device = 'cpu'  # Traditional ML models run on CPU\n",
        "\n",
        "    def to(self, device):\n",
        "        # Dummy method for compatibility with PyTorch interface\n",
        "        self.device = device\n",
        "        return self\n",
        "\n",
        "    def train(self):\n",
        "        # Dummy method for compatibility with PyTorch interface\n",
        "        pass\n",
        "\n",
        "    def eval(self):\n",
        "        # Dummy method for compatibility with PyTorch interface\n",
        "        pass\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # For inference - handle both PyTorch tensors and numpy arrays\n",
        "        if isinstance(x, torch.Tensor):\n",
        "            x_np = x.detach().cpu().numpy()\n",
        "        else:\n",
        "            x_np = x\n",
        "\n",
        "        # Reshape input for traditional ML models: from (batch, time_steps, features) to (batch, time_steps*features)\n",
        "        batch_size = x_np.shape[0]\n",
        "        x_flat = x_np.reshape(batch_size, -1)\n",
        "\n",
        "        # Get predictions\n",
        "        preds = self.model.predict(x_flat)\n",
        "\n",
        "        # Convert back to appropriate format\n",
        "        if isinstance(x, torch.Tensor):\n",
        "            return torch.FloatTensor(preds.reshape(-1, 1))\n",
        "        return preds.reshape(-1, 1)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train the model using scikit-learn's fit method\"\"\"\n",
        "        # Reshape input if necessary\n",
        "        if len(X.shape) == 3:  # (batch, time_steps, features)\n",
        "            X_flat = X.reshape(X.shape[0], -1)\n",
        "        else:\n",
        "            X_flat = X\n",
        "\n",
        "        return self.model.fit(X_flat, y)\n",
        "\n",
        "def build_model(train_X, train_Y, val_X, val_Y, model_type='gru', lstm_units=128, gru_units=128,\n",
        "               drop_rate=0.3, dense_unit=64, batch_size=32, epochs=100, n_estimators=100):\n",
        "    # Print training parameters\n",
        "    train_X_tensor = torch.FloatTensor(train_X)\n",
        "    train_Y_tensor = torch.FloatTensor(train_Y.reshape(-1, 1))\n",
        "    val_X_tensor = torch.FloatTensor(val_X)\n",
        "    val_Y_tensor = torch.FloatTensor(val_Y.reshape(-1, 1))\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
        "    val_dataset = TensorDataset(val_X_tensor, val_Y_tensor)\n",
        "\n",
        "    # Create reproducible DataLoaders with fixed seeds\n",
        "    train_generator = torch.Generator()\n",
        "    train_generator.manual_seed(SEED)\n",
        "    val_generator = torch.Generator()\n",
        "    val_generator.manual_seed(SEED)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=train_generator)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, generator=val_generator)\n",
        "\n",
        "    # Initialize model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    time_steps = train_X.shape[1]\n",
        "    input_dim = train_X.shape[2]\n",
        "\n",
        "    # Initialize model with fixed initial weights\n",
        "    torch.manual_seed(SEED)\n",
        "\n",
        "    # Determine model type and create appropriate model\n",
        "    if model_type == 'gru':\n",
        "        model = AttentionGRU(input_dim, time_steps, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'lstm':\n",
        "        model = AttentionLSTM(input_dim, time_steps, lstm_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'hybrid':\n",
        "        model = HybridLSTM_GRU(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'sequential':\n",
        "        model = SequentialHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'stacked':\n",
        "        model = StackedHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'bidirectional':\n",
        "        model = BidirectionalHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'cnn-rnn':\n",
        "        model = CNNRNNHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'multi-scale':\n",
        "        model = MultiScaleHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'transformer-rnn':\n",
        "        model = TransformerRNNHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'ensemble':\n",
        "        model = EnsembleHybrid(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'stacking':\n",
        "        model = StackingEnsemble(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "        # Pre-train weak learners\n",
        "        model.train_weak_learners(train_loader, val_loader, epochs=10, device=device)\n",
        "    elif model_type == 'cnn':\n",
        "        model = CNNModel(input_dim, time_steps, drop_rate, dense_unit).to(device)\n",
        "    elif model_type in ['rf', 'xgb', 'gbm']:\n",
        "        # Initialize appropriate traditional ML model\n",
        "        if model_type == 'rf':\n",
        "            model = SklearnModelWrapper(model_type='rf', n_estimators=n_estimators, random_state=SEED)\n",
        "        elif model_type == 'xgb':\n",
        "            model = SklearnModelWrapper(model_type='xgb', n_estimators=n_estimators, random_state=SEED,\n",
        "                                       learning_rate=0.1, max_depth=6)\n",
        "        elif model_type == 'gbm':\n",
        "            model = SklearnModelWrapper(model_type='gbm', n_estimators=n_estimators, random_state=SEED,\n",
        "                                       learning_rate=0.1, max_depth=6)\n",
        "\n",
        "        # Directly train traditional ML models (no epochs needed)\n",
        "        train_X_flat = train_X.reshape(train_X.shape[0], -1)\n",
        "        model.fit(train_X_flat, train_Y)\n",
        "        return model\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "    # For deep learning models, continue with the existing training loop\n",
        "    if model_type not in ['rf', 'xgb', 'gbm']:\n",
        "        # Initialize optimizer and loss function\n",
        "        optimizer = optim.Adam(model.parameters())\n",
        "        criterion = nn.L1Loss()  # MAE loss\n",
        "\n",
        "        # Training loop\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "        patience = 5\n",
        "        lr_factor = 0.01\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            for inputs, targets in train_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for inputs, targets in val_loader:\n",
        "                    inputs, targets = inputs.to(device), targets.to(device)\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, targets)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "            # Print progress\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Train Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
        "\n",
        "            # Learning rate schedule based on validation loss\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    for param_group in optimizer.param_groups:\n",
        "                        param_group['lr'] *= lr_factor\n",
        "                    patience_counter = 0\n",
        "                    print(f'Reducing learning rate by factor of {lr_factor}')\n",
        "\n",
        "    return model\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    mask = y_true != 0\n",
        "    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "    return mape\n",
        "\n",
        "def walk_forward(train_X, train_Y, val_X, val_Y, test_X, test_Y, grid_search, scaler):\n",
        "    r, f, c = test_X.shape\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    all_predictions = {}\n",
        "    all_adjusted_predictions = {}\n",
        "    all_ground_truths = {}\n",
        "\n",
        "    # Create lists to store all evaluation results\n",
        "    original_valuelists = []\n",
        "    adjusted_valuelists = []\n",
        "\n",
        "    for x in grid_search:\n",
        "        history_x = np.array([x for x in train_X])\n",
        "        history_y = np.array([y for y in train_Y])\n",
        "        predictions = list()\n",
        "        adjusted_predictions = list()\n",
        "        groundtrue = list()\n",
        "\n",
        "        # Extract model type first to determine how to unpack the rest\n",
        "        model_type = x[0]\n",
        "\n",
        "        # Create the appropriate config_key and extract parameters based on model type\n",
        "        if model_type in ['hybrid', 'sequential', 'stacked', 'bidirectional', 'cnn-rnn', 'multi-scale', 'transformer-rnn', 'ensemble', 'stacking']:\n",
        "            # Hybrid model has 7 parameters\n",
        "            model_type, lstm_unit_val, gru_unit_val, drop, dense, batch, epoch = x\n",
        "            units = f\"L{lstm_unit_val}_G{gru_unit_val}\"  # For logging\n",
        "            config_key = f\"{model_type}_lstmUnit{lstm_unit_val}_gruUnit{gru_unit_val}_drop{drop}_dense{dense}_batch{batch}_epochs{epoch}\"\n",
        "        elif model_type in ['rf', 'xgb', 'gbm']:\n",
        "            # Tree-based models have only 2 parameters\n",
        "            model_type, n_estimators = x\n",
        "            units = f\"Est{n_estimators}\"  # For logging\n",
        "            config_key = f\"{model_type}_estimators{n_estimators}\"\n",
        "            drop = 0\n",
        "            dense = 0\n",
        "            batch = 0\n",
        "            epoch = 0\n",
        "        elif model_type == 'cnn':\n",
        "            # CNN model has 6 parameters but no specific units parameter\n",
        "            model_type, _, drop, dense, batch, epoch = x\n",
        "            units = \"CNN\"  # For logging\n",
        "            config_key = f\"{model_type}_drop{drop}_dense{dense}_batch{batch}_epochs{epoch}\"\n",
        "        else:\n",
        "            # LSTM and GRU models have 6 parameters\n",
        "            model_type, units, drop, dense, batch, epoch = x\n",
        "            config_key = f\"{model_type}_unit{units}_drop{drop}_dense{dense}_batch{batch}_epochs{epoch}\"\n",
        "\n",
        "        print(\"\\n\" + \"*\"*50)\n",
        "        print(f\"Starting walk-forward validation with parameters:\")\n",
        "        print(f\"Model Type: {model_type}, Units/Estimators: {units}\")\n",
        "        if model_type not in ['rf', 'xgb', 'gbm']:\n",
        "            print(f\"Dropout: {drop}, Dense Units: {dense}\")\n",
        "            print(f\"Batch Size: {batch}, Epochs: {epoch}\")\n",
        "        else:\n",
        "            print(f\"n_estimators: {n_estimators}\")\n",
        "        print(f\"Device: {device}\")\n",
        "        print(f\"Total test samples: {len(test_X)}\")\n",
        "        print(\"*\"*50 + \"\\n\")\n",
        "\n",
        "        for i in range(len(test_X)):\n",
        "            print(f\"\\nTest iteration {i+1}/{len(test_X)}\")\n",
        "            print(f\"Current training set size: {history_x.shape[0]} samples\")\n",
        "\n",
        "            # Build model based on model type\n",
        "            if model_type in ['hybrid', 'sequential', 'stacked', 'bidirectional', 'cnn-rnn', 'multi-scale', 'transformer-rnn', 'ensemble', 'stacking']:\n",
        "                model = build_model(history_x, history_y, val_X, val_Y, model_type=model_type,\n",
        "                                lstm_units=lstm_unit_val, gru_units=gru_unit_val, drop_rate=drop,\n",
        "                                dense_unit=dense, batch_size=batch, epochs=epoch)\n",
        "            elif model_type in ['rf', 'xgb', 'gbm']:\n",
        "                model = build_model(history_x, history_y, val_X, val_Y, model_type=model_type,\n",
        "                                n_estimators=n_estimators)\n",
        "            elif model_type == 'cnn':\n",
        "                model = build_model(history_x, history_y, val_X, val_Y, model_type=model_type,\n",
        "                                drop_rate=drop, dense_unit=dense, batch_size=batch, epochs=epoch)\n",
        "            else:\n",
        "                model = build_model(history_x, history_y, val_X, val_Y, model_type=model_type,\n",
        "                                lstm_units=units if model_type == 'lstm' else 128,\n",
        "                                gru_units=units if model_type == 'gru' else 128,\n",
        "                                drop_rate=drop, dense_unit=dense, batch_size=batch, epochs=epoch)\n",
        "\n",
        "            # Set model to eval mode if it's a PyTorch model\n",
        "            if hasattr(model, 'eval'):\n",
        "                model.eval()\n",
        "\n",
        "            # Convert test data to appropriate format\n",
        "            if model_type in ['rf', 'xgb', 'gbm']:\n",
        "                # For traditional ML models, flatten the input\n",
        "                test_data = test_X[i].reshape(1, -1)\n",
        "                yhat = model(test_data)\n",
        "            else:\n",
        "                # For deep learning models, use tensors\n",
        "                test_tensor = torch.FloatTensor(test_X[i].reshape(1, f, c)).to(device)\n",
        "                with torch.no_grad():\n",
        "                    yhat = model(test_tensor).cpu().numpy()\n",
        "\n",
        "            inv_yhat, inv_y = inverscale(yhat, test_X[i], test_Y[i], scaler)\n",
        "            prev_month_lockdown = test_X[i][11][5]\n",
        "            adjusted_inv_yhat = inv_yhat * (1 - prev_month_lockdown)\n",
        "            predictions.append(inv_yhat)\n",
        "            adjusted_predictions.append(adjusted_inv_yhat)\n",
        "            groundtrue.append(inv_y)\n",
        "\n",
        "            # Observation\n",
        "            obs_x = test_X[i]\n",
        "            obs_y = test_Y[i]\n",
        "\n",
        "            history_x = np.append(history_x, [obs_x], axis=0)\n",
        "            history_y = np.append(history_y, obs_y)\n",
        "\n",
        "        # Store predictions and ground truth for this configuration\n",
        "        all_predictions[config_key] = np.array(predictions).flatten()\n",
        "        all_adjusted_predictions[config_key] = np.array(adjusted_predictions).flatten()\n",
        "        all_ground_truths[config_key] = np.array(groundtrue).flatten()\n",
        "\n",
        "        original_valuelist = evalue(predictions, groundtrue)\n",
        "        original_valuelist['model_type'] = model_type\n",
        "        if model_type in ['rf', 'xgb', 'gbm']:\n",
        "            original_valuelist['n_estimators'] = n_estimators\n",
        "            original_valuelist['units'] = \"N/A\"\n",
        "            original_valuelist['drop_rate'] = \"N/A\"\n",
        "            original_valuelist['dense_unit'] = \"N/A\"\n",
        "            original_valuelist['batch_size'] = \"N/A\"\n",
        "            original_valuelist['epochs'] = \"N/A\"\n",
        "        elif model_type == 'cnn':\n",
        "            original_valuelist['units'] = \"N/A\"\n",
        "            original_valuelist['drop_rate'] = drop\n",
        "            original_valuelist['dense_unit'] = dense\n",
        "            original_valuelist['batch_size'] = batch\n",
        "            original_valuelist['epochs'] = epoch\n",
        "        else:\n",
        "            original_valuelist['units'] = units\n",
        "            original_valuelist['drop_rate'] = drop\n",
        "            original_valuelist['dense_unit'] = dense\n",
        "            original_valuelist['batch_size'] = batch\n",
        "            original_valuelist['epochs'] = epoch\n",
        "\n",
        "        # Evaluate with adjusted predictions\n",
        "        adjusted_valuelist = evalue(np.array(adjusted_predictions).flatten(), np.array(groundtrue).flatten())\n",
        "        adjusted_valuelist['model_type'] = model_type\n",
        "        if model_type in ['rf', 'xgb', 'gbm']:\n",
        "            adjusted_valuelist['n_estimators'] = n_estimators\n",
        "            adjusted_valuelist['units'] = \"N/A\"\n",
        "            adjusted_valuelist['drop_rate'] = \"N/A\"\n",
        "            adjusted_valuelist['dense_unit'] = \"N/A\"\n",
        "            adjusted_valuelist['batch_size'] = \"N/A\"\n",
        "            adjusted_valuelist['epochs'] = \"N/A\"\n",
        "        elif model_type == 'cnn':\n",
        "            adjusted_valuelist['units'] = \"N/A\"\n",
        "            adjusted_valuelist['drop_rate'] = drop\n",
        "            adjusted_valuelist['dense_unit'] = dense\n",
        "            adjusted_valuelist['batch_size'] = batch\n",
        "            adjusted_valuelist['epochs'] = epoch\n",
        "        else:\n",
        "            adjusted_valuelist['units'] = units\n",
        "            adjusted_valuelist['drop_rate'] = drop\n",
        "            adjusted_valuelist['dense_unit'] = dense\n",
        "            adjusted_valuelist['batch_size'] = batch\n",
        "            adjusted_valuelist['epochs'] = epoch\n",
        "\n",
        "        # Append to the lists of results\n",
        "        original_valuelists.append(original_valuelist)\n",
        "        adjusted_valuelists.append(adjusted_valuelist)\n",
        "\n",
        "    # Combine all results\n",
        "    all_original_valuelist = pd.concat(original_valuelists, ignore_index=True)\n",
        "    all_adjusted_valuelist = pd.concat(adjusted_valuelists, ignore_index=True)\n",
        "\n",
        "    return all_original_valuelist, all_adjusted_valuelist, all_predictions, all_ground_truths, all_adjusted_predictions\n",
        "\n",
        "def evalue(yhat, inv_y):\n",
        "    valuelist = {}\n",
        "    DLM_rmse = sqrt(mean_squared_error(inv_y, yhat))\n",
        "    valuelist.update({'RMSE': {'DLM': DLM_rmse}})\n",
        "    DLM_mae = mean_absolute_error(inv_y, yhat)\n",
        "    valuelist.update({'MAE': {'DLM': DLM_mae}})\n",
        "    DLM_mape = mean_absolute_percentage_error(inv_y, yhat)\n",
        "    valuelist.update({'MAPE': {'DLM': DLM_mape}})\n",
        "    return pd.DataFrame(valuelist)\n",
        "\n",
        "def inverscale(yhat, test_X, test_Y, scaler):\n",
        "    feature = len(scaler.scale_)\n",
        "    test_Y = np.array(test_Y)\n",
        "    test_X = test_X[1, 0:feature]\n",
        "    test_X = test_X.reshape(1, test_X.shape[0])\n",
        "\n",
        "    if len(yhat.shape) == 1:\n",
        "        yhat = yhat.reshape(len(yhat), 1)\n",
        "\n",
        "    inv_yhat = concatenate((yhat, test_X[:, :-1]), axis=1)\n",
        "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "    inv_yhat = inv_yhat[:, 0]\n",
        "\n",
        "    test_Y = test_Y.reshape(1, 1)\n",
        "    inv_y = concatenate((test_Y, test_X[:, :-1]), axis=1)\n",
        "    inv_y = scaler.inverse_transform(inv_y)\n",
        "    inv_y = inv_y[:, 0]\n",
        "    return inv_yhat, inv_y\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    values = reframed.values\n",
        "    reframed_with_dates_values = reframed_with_dates.values\n",
        "\n",
        "\n",
        "    # Import train_test_split for random splitting\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    # Drop the date column for the splitting but keep track of indices\n",
        "    reframed_without_dates = reframed.copy()\n",
        "\n",
        "    # First split: 80% train+val, 20% test\n",
        "    train_val_indices, test_indices = train_test_split(\n",
        "        np.arange(len(reframed_without_dates)),\n",
        "        test_size=0.2,\n",
        "        random_state=SEED\n",
        "    )\n",
        "\n",
        "    # Second split: From the 80%, use 7/8 for train (70% of total) and 1/8 for val (10% of total)\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        train_val_indices,\n",
        "        test_size=0.125,  # 0.125 * 0.8 = 0.1 (10% of total)\n",
        "        random_state=SEED\n",
        "    )\n",
        "\n",
        "    # Extract values for train, validation, and test sets\n",
        "    train_data = reframed.iloc[train_indices].values\n",
        "    val_data = reframed.iloc[val_indices].values\n",
        "    test_data = reframed.iloc[test_indices].values\n",
        "\n",
        "    # Store the corresponding dates for reference\n",
        "    train_dates = reframed_with_dates.iloc[train_indices]['date']\n",
        "    val_dates = reframed_with_dates.iloc[val_indices]['date']\n",
        "    test_dates = reframed_with_dates.iloc[test_indices]['date']\n",
        "\n",
        "    # Split into X and Y\n",
        "    train_X, train_Y = train_data[:, :-1], train_data[:, -1]\n",
        "    val_X, val_Y = val_data[:, :-1], val_data[:, -1]\n",
        "    test_X, test_Y = test_data[:, :-1], test_data[:, -1]\n",
        "\n",
        "    # Reshape input to be 3D [samples, timesteps, features]\n",
        "    train_X = train_X.reshape(train_X.shape[0], 12, int(train_X.shape[1]/12))\n",
        "    val_X = val_X.reshape(val_X.shape[0], 12, int(val_X.shape[1]/12))\n",
        "    test_X = test_X.reshape(test_X.shape[0], 12, int(test_X.shape[1]/12))\n",
        "\n",
        "    # Modified grid search creation for all model types\n",
        "    grid_search = []\n",
        "    for model_type in model_types:\n",
        "        if model_type == 'lstm':\n",
        "            grid_search.extend(\n",
        "                list(itertools.product([model_type], lstm_unit, drop_rate, dense_unit, batch_size_num, epochs))\n",
        "            )\n",
        "        elif model_type == 'gru':\n",
        "            grid_search.extend(\n",
        "                list(itertools.product([model_type], gru_unit, drop_rate, dense_unit, batch_size_num, epochs))\n",
        "            )\n",
        "        elif model_type == 'cnn':\n",
        "            grid_search.extend(\n",
        "                list(itertools.product([model_type], [None], drop_rate, dense_unit, batch_size_num, epochs))\n",
        "            )\n",
        "        elif model_type in ['rf', 'xgb', 'gbm']:\n",
        "            # For tree-based models, we use n_estimators instead of units\n",
        "            grid_search.extend(\n",
        "                list(itertools.product([model_type], n_estimators))\n",
        "            )\n",
        "        else:\n",
        "            # All other hybrid models need both LSTM and GRU units\n",
        "            grid_search.extend(\n",
        "                list(itertools.product([model_type], lstm_unit, gru_unit, drop_rate, dense_unit, batch_size_num, epochs))\n",
        "            )\n",
        "\n",
        "    original_valuelist, adjusted_valuelist, all_predictions, all_ground_truths, all_adjusted_predictions = walk_forward(train_X, train_Y, val_X, val_Y, test_X, test_Y, grid_search, scaler)\n",
        "\n",
        "    # Group results by model type\n",
        "    # gru_results = adjusted_valuelist[adjusted_valuelist['model_type'] == 'gru']\n",
        "    # lstm_results = adjusted_valuelist[adjusted_valuelist['model_type'] == 'lstm']\n",
        "\n",
        "    print(\"Results:\")\n",
        "    print(adjusted_valuelist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vj2WY__21ETm",
        "outputId": "3ecbfa69-07a3-4f9e-e59d-1a17507ad345"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 18/100, Train Loss: 0.0458, Val Loss: 0.0468\n",
            "Epoch 19/100, Train Loss: 0.0499, Val Loss: 0.0447\n",
            "Epoch 20/100, Train Loss: 0.0493, Val Loss: 0.0443\n",
            "Epoch 21/100, Train Loss: 0.0442, Val Loss: 0.0439\n",
            "Epoch 22/100, Train Loss: 0.0453, Val Loss: 0.0436\n",
            "Epoch 23/100, Train Loss: 0.0483, Val Loss: 0.0437\n",
            "Epoch 24/100, Train Loss: 0.0457, Val Loss: 0.0434\n",
            "Epoch 25/100, Train Loss: 0.0451, Val Loss: 0.0437\n",
            "Epoch 26/100, Train Loss: 0.0487, Val Loss: 0.0436\n",
            "Epoch 27/100, Train Loss: 0.0403, Val Loss: 0.0435\n",
            "Epoch 28/100, Train Loss: 0.0387, Val Loss: 0.0429\n",
            "Epoch 29/100, Train Loss: 0.0407, Val Loss: 0.0432\n",
            "Epoch 30/100, Train Loss: 0.0445, Val Loss: 0.0429\n",
            "Epoch 31/100, Train Loss: 0.0415, Val Loss: 0.0429\n",
            "Epoch 32/100, Train Loss: 0.0423, Val Loss: 0.0429\n",
            "Epoch 33/100, Train Loss: 0.0404, Val Loss: 0.0431\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0457, Val Loss: 0.0431\n",
            "Epoch 35/100, Train Loss: 0.0430, Val Loss: 0.0431\n",
            "Epoch 36/100, Train Loss: 0.0442, Val Loss: 0.0431\n",
            "Epoch 37/100, Train Loss: 0.0415, Val Loss: 0.0431\n",
            "Epoch 38/100, Train Loss: 0.0437, Val Loss: 0.0431\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0408, Val Loss: 0.0431\n",
            "Epoch 40/100, Train Loss: 0.0424, Val Loss: 0.0431\n",
            "Epoch 41/100, Train Loss: 0.0432, Val Loss: 0.0431\n",
            "Epoch 42/100, Train Loss: 0.0465, Val Loss: 0.0431\n",
            "Epoch 43/100, Train Loss: 0.0404, Val Loss: 0.0431\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0434, Val Loss: 0.0431\n",
            "Epoch 45/100, Train Loss: 0.0432, Val Loss: 0.0431\n",
            "Epoch 46/100, Train Loss: 0.0442, Val Loss: 0.0431\n",
            "Epoch 47/100, Train Loss: 0.0433, Val Loss: 0.0431\n",
            "Epoch 48/100, Train Loss: 0.0422, Val Loss: 0.0431\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0414, Val Loss: 0.0431\n",
            "Epoch 50/100, Train Loss: 0.0399, Val Loss: 0.0431\n",
            "Epoch 51/100, Train Loss: 0.0426, Val Loss: 0.0431\n",
            "Epoch 52/100, Train Loss: 0.0434, Val Loss: 0.0431\n",
            "Epoch 53/100, Train Loss: 0.0422, Val Loss: 0.0431\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0464, Val Loss: 0.0431\n",
            "Epoch 55/100, Train Loss: 0.0426, Val Loss: 0.0431\n",
            "Epoch 56/100, Train Loss: 0.0430, Val Loss: 0.0431\n",
            "Epoch 57/100, Train Loss: 0.0432, Val Loss: 0.0431\n",
            "Epoch 58/100, Train Loss: 0.0448, Val Loss: 0.0431\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0447, Val Loss: 0.0431\n",
            "Epoch 60/100, Train Loss: 0.0461, Val Loss: 0.0431\n",
            "Epoch 61/100, Train Loss: 0.0413, Val Loss: 0.0431\n",
            "Epoch 62/100, Train Loss: 0.0427, Val Loss: 0.0431\n",
            "Epoch 63/100, Train Loss: 0.0471, Val Loss: 0.0431\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0441, Val Loss: 0.0431\n",
            "Epoch 65/100, Train Loss: 0.0401, Val Loss: 0.0431\n",
            "Epoch 66/100, Train Loss: 0.0444, Val Loss: 0.0431\n",
            "Epoch 67/100, Train Loss: 0.0420, Val Loss: 0.0431\n",
            "Epoch 68/100, Train Loss: 0.0416, Val Loss: 0.0431\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0398, Val Loss: 0.0431\n",
            "Epoch 70/100, Train Loss: 0.0439, Val Loss: 0.0431\n",
            "Epoch 71/100, Train Loss: 0.0433, Val Loss: 0.0431\n",
            "Epoch 72/100, Train Loss: 0.0464, Val Loss: 0.0431\n",
            "Epoch 73/100, Train Loss: 0.0401, Val Loss: 0.0431\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0440, Val Loss: 0.0431\n",
            "Epoch 75/100, Train Loss: 0.0444, Val Loss: 0.0431\n",
            "Epoch 76/100, Train Loss: 0.0355, Val Loss: 0.0431\n",
            "Epoch 77/100, Train Loss: 0.0404, Val Loss: 0.0431\n",
            "Epoch 78/100, Train Loss: 0.0435, Val Loss: 0.0431\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0432, Val Loss: 0.0431\n",
            "Epoch 80/100, Train Loss: 0.0406, Val Loss: 0.0431\n",
            "Epoch 81/100, Train Loss: 0.0442, Val Loss: 0.0431\n",
            "Epoch 82/100, Train Loss: 0.0411, Val Loss: 0.0431\n",
            "Epoch 83/100, Train Loss: 0.0392, Val Loss: 0.0431\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0432, Val Loss: 0.0431\n",
            "Epoch 85/100, Train Loss: 0.0440, Val Loss: 0.0431\n",
            "Epoch 86/100, Train Loss: 0.0412, Val Loss: 0.0431\n",
            "Epoch 87/100, Train Loss: 0.0416, Val Loss: 0.0431\n",
            "Epoch 88/100, Train Loss: 0.0428, Val Loss: 0.0431\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0421, Val Loss: 0.0431\n",
            "Epoch 90/100, Train Loss: 0.0498, Val Loss: 0.0431\n",
            "Epoch 91/100, Train Loss: 0.0396, Val Loss: 0.0431\n",
            "Epoch 92/100, Train Loss: 0.0431, Val Loss: 0.0431\n",
            "Epoch 93/100, Train Loss: 0.0491, Val Loss: 0.0431\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0409, Val Loss: 0.0431\n",
            "Epoch 95/100, Train Loss: 0.0448, Val Loss: 0.0431\n",
            "Epoch 96/100, Train Loss: 0.0410, Val Loss: 0.0431\n",
            "Epoch 97/100, Train Loss: 0.0379, Val Loss: 0.0431\n",
            "Epoch 98/100, Train Loss: 0.0440, Val Loss: 0.0431\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0462, Val Loss: 0.0431\n",
            "Epoch 100/100, Train Loss: 0.0382, Val Loss: 0.0431\n",
            "\n",
            "Test iteration 21/28\n",
            "Current training set size: 117 samples\n",
            "Epoch 1/100, Train Loss: 0.1781, Val Loss: 0.1807\n",
            "Epoch 2/100, Train Loss: 0.1178, Val Loss: 0.0987\n",
            "Epoch 3/100, Train Loss: 0.0874, Val Loss: 0.0851\n",
            "Epoch 4/100, Train Loss: 0.0754, Val Loss: 0.0665\n",
            "Epoch 5/100, Train Loss: 0.0622, Val Loss: 0.0445\n",
            "Epoch 6/100, Train Loss: 0.0614, Val Loss: 0.0568\n",
            "Epoch 7/100, Train Loss: 0.0832, Val Loss: 0.0553\n",
            "Epoch 8/100, Train Loss: 0.0557, Val Loss: 0.0474\n",
            "Epoch 9/100, Train Loss: 0.0518, Val Loss: 0.0696\n",
            "Epoch 10/100, Train Loss: 0.0599, Val Loss: 0.0601\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 11/100, Train Loss: 0.0537, Val Loss: 0.0576\n",
            "Epoch 12/100, Train Loss: 0.0540, Val Loss: 0.0559\n",
            "Epoch 13/100, Train Loss: 0.0519, Val Loss: 0.0544\n",
            "Epoch 14/100, Train Loss: 0.0500, Val Loss: 0.0528\n",
            "Epoch 15/100, Train Loss: 0.0499, Val Loss: 0.0511\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0551, Val Loss: 0.0511\n",
            "Epoch 17/100, Train Loss: 0.0447, Val Loss: 0.0511\n",
            "Epoch 18/100, Train Loss: 0.0414, Val Loss: 0.0511\n",
            "Epoch 19/100, Train Loss: 0.0481, Val Loss: 0.0511\n",
            "Epoch 20/100, Train Loss: 0.0509, Val Loss: 0.0511\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0503, Val Loss: 0.0511\n",
            "Epoch 22/100, Train Loss: 0.0467, Val Loss: 0.0511\n",
            "Epoch 23/100, Train Loss: 0.0489, Val Loss: 0.0511\n",
            "Epoch 24/100, Train Loss: 0.0473, Val Loss: 0.0511\n",
            "Epoch 25/100, Train Loss: 0.0492, Val Loss: 0.0511\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0466, Val Loss: 0.0511\n",
            "Epoch 27/100, Train Loss: 0.0477, Val Loss: 0.0511\n",
            "Epoch 28/100, Train Loss: 0.0509, Val Loss: 0.0511\n",
            "Epoch 29/100, Train Loss: 0.0518, Val Loss: 0.0511\n",
            "Epoch 30/100, Train Loss: 0.0487, Val Loss: 0.0511\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0476, Val Loss: 0.0511\n",
            "Epoch 32/100, Train Loss: 0.0451, Val Loss: 0.0511\n",
            "Epoch 33/100, Train Loss: 0.0510, Val Loss: 0.0511\n",
            "Epoch 34/100, Train Loss: 0.0527, Val Loss: 0.0511\n",
            "Epoch 35/100, Train Loss: 0.0490, Val Loss: 0.0511\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0497, Val Loss: 0.0511\n",
            "Epoch 37/100, Train Loss: 0.0475, Val Loss: 0.0511\n",
            "Epoch 38/100, Train Loss: 0.0497, Val Loss: 0.0511\n",
            "Epoch 39/100, Train Loss: 0.0442, Val Loss: 0.0511\n",
            "Epoch 40/100, Train Loss: 0.0474, Val Loss: 0.0511\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0480, Val Loss: 0.0511\n",
            "Epoch 42/100, Train Loss: 0.0463, Val Loss: 0.0511\n",
            "Epoch 43/100, Train Loss: 0.0522, Val Loss: 0.0511\n",
            "Epoch 44/100, Train Loss: 0.0443, Val Loss: 0.0511\n",
            "Epoch 45/100, Train Loss: 0.0490, Val Loss: 0.0511\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0489, Val Loss: 0.0511\n",
            "Epoch 47/100, Train Loss: 0.0474, Val Loss: 0.0511\n",
            "Epoch 48/100, Train Loss: 0.0441, Val Loss: 0.0511\n",
            "Epoch 49/100, Train Loss: 0.0474, Val Loss: 0.0511\n",
            "Epoch 50/100, Train Loss: 0.0497, Val Loss: 0.0511\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0502, Val Loss: 0.0511\n",
            "Epoch 52/100, Train Loss: 0.0492, Val Loss: 0.0511\n",
            "Epoch 53/100, Train Loss: 0.0482, Val Loss: 0.0511\n",
            "Epoch 54/100, Train Loss: 0.0518, Val Loss: 0.0511\n",
            "Epoch 55/100, Train Loss: 0.0473, Val Loss: 0.0511\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0509, Val Loss: 0.0511\n",
            "Epoch 57/100, Train Loss: 0.0496, Val Loss: 0.0511\n",
            "Epoch 58/100, Train Loss: 0.0487, Val Loss: 0.0511\n",
            "Epoch 59/100, Train Loss: 0.0474, Val Loss: 0.0511\n",
            "Epoch 60/100, Train Loss: 0.0440, Val Loss: 0.0511\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0437, Val Loss: 0.0511\n",
            "Epoch 62/100, Train Loss: 0.0489, Val Loss: 0.0511\n",
            "Epoch 63/100, Train Loss: 0.0492, Val Loss: 0.0511\n",
            "Epoch 64/100, Train Loss: 0.0476, Val Loss: 0.0511\n",
            "Epoch 65/100, Train Loss: 0.0475, Val Loss: 0.0511\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0506, Val Loss: 0.0511\n",
            "Epoch 67/100, Train Loss: 0.0487, Val Loss: 0.0511\n",
            "Epoch 68/100, Train Loss: 0.0452, Val Loss: 0.0511\n",
            "Epoch 69/100, Train Loss: 0.0503, Val Loss: 0.0511\n",
            "Epoch 70/100, Train Loss: 0.0512, Val Loss: 0.0511\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0503, Val Loss: 0.0511\n",
            "Epoch 72/100, Train Loss: 0.0467, Val Loss: 0.0511\n",
            "Epoch 73/100, Train Loss: 0.0426, Val Loss: 0.0511\n",
            "Epoch 74/100, Train Loss: 0.0467, Val Loss: 0.0511\n",
            "Epoch 75/100, Train Loss: 0.0473, Val Loss: 0.0511\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0441, Val Loss: 0.0511\n",
            "Epoch 77/100, Train Loss: 0.0534, Val Loss: 0.0511\n",
            "Epoch 78/100, Train Loss: 0.0492, Val Loss: 0.0511\n",
            "Epoch 79/100, Train Loss: 0.0508, Val Loss: 0.0511\n",
            "Epoch 80/100, Train Loss: 0.0566, Val Loss: 0.0511\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0466, Val Loss: 0.0511\n",
            "Epoch 82/100, Train Loss: 0.0494, Val Loss: 0.0511\n",
            "Epoch 83/100, Train Loss: 0.0569, Val Loss: 0.0511\n",
            "Epoch 84/100, Train Loss: 0.0471, Val Loss: 0.0511\n",
            "Epoch 85/100, Train Loss: 0.0491, Val Loss: 0.0511\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0507, Val Loss: 0.0511\n",
            "Epoch 87/100, Train Loss: 0.0454, Val Loss: 0.0511\n",
            "Epoch 88/100, Train Loss: 0.0515, Val Loss: 0.0511\n",
            "Epoch 89/100, Train Loss: 0.0484, Val Loss: 0.0511\n",
            "Epoch 90/100, Train Loss: 0.0452, Val Loss: 0.0511\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0471, Val Loss: 0.0511\n",
            "Epoch 92/100, Train Loss: 0.0491, Val Loss: 0.0511\n",
            "Epoch 93/100, Train Loss: 0.0487, Val Loss: 0.0511\n",
            "Epoch 94/100, Train Loss: 0.0504, Val Loss: 0.0511\n",
            "Epoch 95/100, Train Loss: 0.0438, Val Loss: 0.0511\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0529, Val Loss: 0.0511\n",
            "Epoch 97/100, Train Loss: 0.0483, Val Loss: 0.0511\n",
            "Epoch 98/100, Train Loss: 0.0490, Val Loss: 0.0511\n",
            "Epoch 99/100, Train Loss: 0.0516, Val Loss: 0.0511\n",
            "Epoch 100/100, Train Loss: 0.0450, Val Loss: 0.0511\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 22/28\n",
            "Current training set size: 118 samples\n",
            "Epoch 1/100, Train Loss: 0.1729, Val Loss: 0.1708\n",
            "Epoch 2/100, Train Loss: 0.1162, Val Loss: 0.1051\n",
            "Epoch 3/100, Train Loss: 0.0977, Val Loss: 0.0949\n",
            "Epoch 4/100, Train Loss: 0.0857, Val Loss: 0.0837\n",
            "Epoch 5/100, Train Loss: 0.0694, Val Loss: 0.0620\n",
            "Epoch 6/100, Train Loss: 0.0573, Val Loss: 0.0750\n",
            "Epoch 7/100, Train Loss: 0.0666, Val Loss: 0.0528\n",
            "Epoch 8/100, Train Loss: 0.0545, Val Loss: 0.0475\n",
            "Epoch 9/100, Train Loss: 0.0549, Val Loss: 0.0430\n",
            "Epoch 10/100, Train Loss: 0.0497, Val Loss: 0.0457\n",
            "Epoch 11/100, Train Loss: 0.0574, Val Loss: 0.0707\n",
            "Epoch 12/100, Train Loss: 0.0567, Val Loss: 0.0408\n",
            "Epoch 13/100, Train Loss: 0.0494, Val Loss: 0.0463\n",
            "Epoch 14/100, Train Loss: 0.0500, Val Loss: 0.0660\n",
            "Epoch 15/100, Train Loss: 0.0506, Val Loss: 0.0480\n",
            "Epoch 16/100, Train Loss: 0.0533, Val Loss: 0.0422\n",
            "Epoch 17/100, Train Loss: 0.0475, Val Loss: 0.0525\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0459, Val Loss: 0.0494\n",
            "Epoch 19/100, Train Loss: 0.0454, Val Loss: 0.0477\n",
            "Epoch 20/100, Train Loss: 0.0412, Val Loss: 0.0472\n",
            "Epoch 21/100, Train Loss: 0.0379, Val Loss: 0.0472\n",
            "Epoch 22/100, Train Loss: 0.0413, Val Loss: 0.0472\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0372, Val Loss: 0.0472\n",
            "Epoch 24/100, Train Loss: 0.0421, Val Loss: 0.0472\n",
            "Epoch 25/100, Train Loss: 0.0428, Val Loss: 0.0472\n",
            "Epoch 26/100, Train Loss: 0.0394, Val Loss: 0.0472\n",
            "Epoch 27/100, Train Loss: 0.0447, Val Loss: 0.0472\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0418, Val Loss: 0.0472\n",
            "Epoch 29/100, Train Loss: 0.0381, Val Loss: 0.0472\n",
            "Epoch 30/100, Train Loss: 0.0401, Val Loss: 0.0472\n",
            "Epoch 31/100, Train Loss: 0.0393, Val Loss: 0.0472\n",
            "Epoch 32/100, Train Loss: 0.0386, Val Loss: 0.0472\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0385, Val Loss: 0.0472\n",
            "Epoch 34/100, Train Loss: 0.0407, Val Loss: 0.0472\n",
            "Epoch 35/100, Train Loss: 0.0466, Val Loss: 0.0472\n",
            "Epoch 36/100, Train Loss: 0.0394, Val Loss: 0.0472\n",
            "Epoch 37/100, Train Loss: 0.0394, Val Loss: 0.0472\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0386, Val Loss: 0.0472\n",
            "Epoch 39/100, Train Loss: 0.0405, Val Loss: 0.0472\n",
            "Epoch 40/100, Train Loss: 0.0440, Val Loss: 0.0472\n",
            "Epoch 41/100, Train Loss: 0.0413, Val Loss: 0.0472\n",
            "Epoch 42/100, Train Loss: 0.0442, Val Loss: 0.0472\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0393, Val Loss: 0.0472\n",
            "Epoch 44/100, Train Loss: 0.0432, Val Loss: 0.0472\n",
            "Epoch 45/100, Train Loss: 0.0427, Val Loss: 0.0472\n",
            "Epoch 46/100, Train Loss: 0.0399, Val Loss: 0.0472\n",
            "Epoch 47/100, Train Loss: 0.0423, Val Loss: 0.0472\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0369, Val Loss: 0.0472\n",
            "Epoch 49/100, Train Loss: 0.0361, Val Loss: 0.0472\n",
            "Epoch 50/100, Train Loss: 0.0420, Val Loss: 0.0472\n",
            "Epoch 51/100, Train Loss: 0.0425, Val Loss: 0.0472\n",
            "Epoch 52/100, Train Loss: 0.0411, Val Loss: 0.0472\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0423, Val Loss: 0.0472\n",
            "Epoch 54/100, Train Loss: 0.0410, Val Loss: 0.0472\n",
            "Epoch 55/100, Train Loss: 0.0438, Val Loss: 0.0472\n",
            "Epoch 56/100, Train Loss: 0.0439, Val Loss: 0.0472\n",
            "Epoch 57/100, Train Loss: 0.0376, Val Loss: 0.0472\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0393, Val Loss: 0.0472\n",
            "Epoch 59/100, Train Loss: 0.0418, Val Loss: 0.0472\n",
            "Epoch 60/100, Train Loss: 0.0435, Val Loss: 0.0472\n",
            "Epoch 61/100, Train Loss: 0.0396, Val Loss: 0.0472\n",
            "Epoch 62/100, Train Loss: 0.0424, Val Loss: 0.0472\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0393, Val Loss: 0.0472\n",
            "Epoch 64/100, Train Loss: 0.0421, Val Loss: 0.0472\n",
            "Epoch 65/100, Train Loss: 0.0383, Val Loss: 0.0472\n",
            "Epoch 66/100, Train Loss: 0.0440, Val Loss: 0.0472\n",
            "Epoch 67/100, Train Loss: 0.0391, Val Loss: 0.0472\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0422, Val Loss: 0.0472\n",
            "Epoch 69/100, Train Loss: 0.0427, Val Loss: 0.0472\n",
            "Epoch 70/100, Train Loss: 0.0411, Val Loss: 0.0472\n",
            "Epoch 71/100, Train Loss: 0.0419, Val Loss: 0.0472\n",
            "Epoch 72/100, Train Loss: 0.0412, Val Loss: 0.0472\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0405, Val Loss: 0.0472\n",
            "Epoch 74/100, Train Loss: 0.0396, Val Loss: 0.0472\n",
            "Epoch 75/100, Train Loss: 0.0403, Val Loss: 0.0472\n",
            "Epoch 76/100, Train Loss: 0.0417, Val Loss: 0.0472\n",
            "Epoch 77/100, Train Loss: 0.0436, Val Loss: 0.0472\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0415, Val Loss: 0.0472\n",
            "Epoch 79/100, Train Loss: 0.0432, Val Loss: 0.0472\n",
            "Epoch 80/100, Train Loss: 0.0414, Val Loss: 0.0472\n",
            "Epoch 81/100, Train Loss: 0.0423, Val Loss: 0.0472\n",
            "Epoch 82/100, Train Loss: 0.0408, Val Loss: 0.0472\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0426, Val Loss: 0.0472\n",
            "Epoch 84/100, Train Loss: 0.0406, Val Loss: 0.0472\n",
            "Epoch 85/100, Train Loss: 0.0421, Val Loss: 0.0472\n",
            "Epoch 86/100, Train Loss: 0.0406, Val Loss: 0.0472\n",
            "Epoch 87/100, Train Loss: 0.0387, Val Loss: 0.0472\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0411, Val Loss: 0.0472\n",
            "Epoch 89/100, Train Loss: 0.0419, Val Loss: 0.0472\n",
            "Epoch 90/100, Train Loss: 0.0411, Val Loss: 0.0472\n",
            "Epoch 91/100, Train Loss: 0.0414, Val Loss: 0.0472\n",
            "Epoch 92/100, Train Loss: 0.0392, Val Loss: 0.0472\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0425, Val Loss: 0.0472\n",
            "Epoch 94/100, Train Loss: 0.0396, Val Loss: 0.0472\n",
            "Epoch 95/100, Train Loss: 0.0399, Val Loss: 0.0472\n",
            "Epoch 96/100, Train Loss: 0.0414, Val Loss: 0.0472\n",
            "Epoch 97/100, Train Loss: 0.0401, Val Loss: 0.0472\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0420, Val Loss: 0.0472\n",
            "Epoch 99/100, Train Loss: 0.0412, Val Loss: 0.0472\n",
            "Epoch 100/100, Train Loss: 0.0409, Val Loss: 0.0472\n",
            "\n",
            "Test iteration 23/28\n",
            "Current training set size: 119 samples\n",
            "Epoch 1/100, Train Loss: 0.1776, Val Loss: 0.1889\n",
            "Epoch 2/100, Train Loss: 0.1344, Val Loss: 0.1026\n",
            "Epoch 3/100, Train Loss: 0.0932, Val Loss: 0.0806\n",
            "Epoch 4/100, Train Loss: 0.0746, Val Loss: 0.0558\n",
            "Epoch 5/100, Train Loss: 0.0642, Val Loss: 0.0455\n",
            "Epoch 6/100, Train Loss: 0.0706, Val Loss: 0.0724\n",
            "Epoch 7/100, Train Loss: 0.0584, Val Loss: 0.0538\n",
            "Epoch 8/100, Train Loss: 0.0571, Val Loss: 0.0650\n",
            "Epoch 9/100, Train Loss: 0.0483, Val Loss: 0.0463\n",
            "Epoch 10/100, Train Loss: 0.0479, Val Loss: 0.0481\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 11/100, Train Loss: 0.0526, Val Loss: 0.0474\n",
            "Epoch 12/100, Train Loss: 0.0447, Val Loss: 0.0473\n",
            "Epoch 13/100, Train Loss: 0.0476, Val Loss: 0.0473\n",
            "Epoch 14/100, Train Loss: 0.0426, Val Loss: 0.0473\n",
            "Epoch 15/100, Train Loss: 0.0441, Val Loss: 0.0469\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0422, Val Loss: 0.0469\n",
            "Epoch 17/100, Train Loss: 0.0458, Val Loss: 0.0469\n",
            "Epoch 18/100, Train Loss: 0.0462, Val Loss: 0.0469\n",
            "Epoch 19/100, Train Loss: 0.0456, Val Loss: 0.0469\n",
            "Epoch 20/100, Train Loss: 0.0441, Val Loss: 0.0469\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0452, Val Loss: 0.0469\n",
            "Epoch 22/100, Train Loss: 0.0475, Val Loss: 0.0469\n",
            "Epoch 23/100, Train Loss: 0.0397, Val Loss: 0.0469\n",
            "Epoch 24/100, Train Loss: 0.0440, Val Loss: 0.0469\n",
            "Epoch 25/100, Train Loss: 0.0443, Val Loss: 0.0469\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0404, Val Loss: 0.0469\n",
            "Epoch 27/100, Train Loss: 0.0478, Val Loss: 0.0469\n",
            "Epoch 28/100, Train Loss: 0.0459, Val Loss: 0.0469\n",
            "Epoch 29/100, Train Loss: 0.0425, Val Loss: 0.0469\n",
            "Epoch 30/100, Train Loss: 0.0415, Val Loss: 0.0469\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0474, Val Loss: 0.0469\n",
            "Epoch 32/100, Train Loss: 0.0451, Val Loss: 0.0469\n",
            "Epoch 33/100, Train Loss: 0.0432, Val Loss: 0.0469\n",
            "Epoch 34/100, Train Loss: 0.0436, Val Loss: 0.0469\n",
            "Epoch 35/100, Train Loss: 0.0483, Val Loss: 0.0469\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0421, Val Loss: 0.0469\n",
            "Epoch 37/100, Train Loss: 0.0418, Val Loss: 0.0469\n",
            "Epoch 38/100, Train Loss: 0.0403, Val Loss: 0.0469\n",
            "Epoch 39/100, Train Loss: 0.0435, Val Loss: 0.0469\n",
            "Epoch 40/100, Train Loss: 0.0435, Val Loss: 0.0469\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0461, Val Loss: 0.0469\n",
            "Epoch 42/100, Train Loss: 0.0481, Val Loss: 0.0469\n",
            "Epoch 43/100, Train Loss: 0.0431, Val Loss: 0.0469\n",
            "Epoch 44/100, Train Loss: 0.0455, Val Loss: 0.0469\n",
            "Epoch 45/100, Train Loss: 0.0451, Val Loss: 0.0469\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0435, Val Loss: 0.0469\n",
            "Epoch 47/100, Train Loss: 0.0438, Val Loss: 0.0469\n",
            "Epoch 48/100, Train Loss: 0.0424, Val Loss: 0.0469\n",
            "Epoch 49/100, Train Loss: 0.0500, Val Loss: 0.0469\n",
            "Epoch 50/100, Train Loss: 0.0454, Val Loss: 0.0469\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0501, Val Loss: 0.0469\n",
            "Epoch 52/100, Train Loss: 0.0459, Val Loss: 0.0469\n",
            "Epoch 53/100, Train Loss: 0.0400, Val Loss: 0.0469\n",
            "Epoch 54/100, Train Loss: 0.0441, Val Loss: 0.0469\n",
            "Epoch 55/100, Train Loss: 0.0442, Val Loss: 0.0469\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0440, Val Loss: 0.0469\n",
            "Epoch 57/100, Train Loss: 0.0459, Val Loss: 0.0469\n",
            "Epoch 58/100, Train Loss: 0.0436, Val Loss: 0.0469\n",
            "Epoch 59/100, Train Loss: 0.0441, Val Loss: 0.0469\n",
            "Epoch 60/100, Train Loss: 0.0477, Val Loss: 0.0469\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0449, Val Loss: 0.0469\n",
            "Epoch 62/100, Train Loss: 0.0439, Val Loss: 0.0469\n",
            "Epoch 63/100, Train Loss: 0.0472, Val Loss: 0.0469\n",
            "Epoch 64/100, Train Loss: 0.0459, Val Loss: 0.0469\n",
            "Epoch 65/100, Train Loss: 0.0424, Val Loss: 0.0469\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0502, Val Loss: 0.0469\n",
            "Epoch 67/100, Train Loss: 0.0417, Val Loss: 0.0469\n",
            "Epoch 68/100, Train Loss: 0.0426, Val Loss: 0.0469\n",
            "Epoch 69/100, Train Loss: 0.0432, Val Loss: 0.0469\n",
            "Epoch 70/100, Train Loss: 0.0429, Val Loss: 0.0469\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0464, Val Loss: 0.0469\n",
            "Epoch 72/100, Train Loss: 0.0436, Val Loss: 0.0469\n",
            "Epoch 73/100, Train Loss: 0.0507, Val Loss: 0.0469\n",
            "Epoch 74/100, Train Loss: 0.0479, Val Loss: 0.0469\n",
            "Epoch 75/100, Train Loss: 0.0431, Val Loss: 0.0469\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0412, Val Loss: 0.0469\n",
            "Epoch 77/100, Train Loss: 0.0405, Val Loss: 0.0469\n",
            "Epoch 78/100, Train Loss: 0.0438, Val Loss: 0.0469\n",
            "Epoch 79/100, Train Loss: 0.0430, Val Loss: 0.0469\n",
            "Epoch 80/100, Train Loss: 0.0431, Val Loss: 0.0469\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0405, Val Loss: 0.0469\n",
            "Epoch 82/100, Train Loss: 0.0450, Val Loss: 0.0469\n",
            "Epoch 83/100, Train Loss: 0.0442, Val Loss: 0.0469\n",
            "Epoch 84/100, Train Loss: 0.0482, Val Loss: 0.0469\n",
            "Epoch 85/100, Train Loss: 0.0486, Val Loss: 0.0469\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0446, Val Loss: 0.0469\n",
            "Epoch 87/100, Train Loss: 0.0454, Val Loss: 0.0469\n",
            "Epoch 88/100, Train Loss: 0.0435, Val Loss: 0.0469\n",
            "Epoch 89/100, Train Loss: 0.0422, Val Loss: 0.0469\n",
            "Epoch 90/100, Train Loss: 0.0441, Val Loss: 0.0469\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0457, Val Loss: 0.0469\n",
            "Epoch 92/100, Train Loss: 0.0473, Val Loss: 0.0469\n",
            "Epoch 93/100, Train Loss: 0.0464, Val Loss: 0.0469\n",
            "Epoch 94/100, Train Loss: 0.0423, Val Loss: 0.0469\n",
            "Epoch 95/100, Train Loss: 0.0398, Val Loss: 0.0469\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0477, Val Loss: 0.0469\n",
            "Epoch 97/100, Train Loss: 0.0478, Val Loss: 0.0469\n",
            "Epoch 98/100, Train Loss: 0.0447, Val Loss: 0.0469\n",
            "Epoch 99/100, Train Loss: 0.0493, Val Loss: 0.0469\n",
            "Epoch 100/100, Train Loss: 0.0458, Val Loss: 0.0469\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 24/28\n",
            "Current training set size: 120 samples\n",
            "Epoch 1/100, Train Loss: 0.1731, Val Loss: 0.1768\n",
            "Epoch 2/100, Train Loss: 0.1150, Val Loss: 0.1039\n",
            "Epoch 3/100, Train Loss: 0.1000, Val Loss: 0.0950\n",
            "Epoch 4/100, Train Loss: 0.0875, Val Loss: 0.0785\n",
            "Epoch 5/100, Train Loss: 0.0749, Val Loss: 0.0611\n",
            "Epoch 6/100, Train Loss: 0.0623, Val Loss: 0.0762\n",
            "Epoch 7/100, Train Loss: 0.0605, Val Loss: 0.0539\n",
            "Epoch 8/100, Train Loss: 0.0488, Val Loss: 0.0441\n",
            "Epoch 9/100, Train Loss: 0.0544, Val Loss: 0.0561\n",
            "Epoch 10/100, Train Loss: 0.0590, Val Loss: 0.0437\n",
            "Epoch 11/100, Train Loss: 0.0532, Val Loss: 0.0546\n",
            "Epoch 12/100, Train Loss: 0.0510, Val Loss: 0.0536\n",
            "Epoch 13/100, Train Loss: 0.0454, Val Loss: 0.0453\n",
            "Epoch 14/100, Train Loss: 0.0479, Val Loss: 0.0515\n",
            "Epoch 15/100, Train Loss: 0.0472, Val Loss: 0.0537\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0481, Val Loss: 0.0533\n",
            "Epoch 17/100, Train Loss: 0.0447, Val Loss: 0.0531\n",
            "Epoch 18/100, Train Loss: 0.0405, Val Loss: 0.0528\n",
            "Epoch 19/100, Train Loss: 0.0451, Val Loss: 0.0522\n",
            "Epoch 20/100, Train Loss: 0.0383, Val Loss: 0.0517\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0400, Val Loss: 0.0517\n",
            "Epoch 22/100, Train Loss: 0.0401, Val Loss: 0.0517\n",
            "Epoch 23/100, Train Loss: 0.0435, Val Loss: 0.0517\n",
            "Epoch 24/100, Train Loss: 0.0407, Val Loss: 0.0516\n",
            "Epoch 25/100, Train Loss: 0.0455, Val Loss: 0.0516\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0427, Val Loss: 0.0516\n",
            "Epoch 27/100, Train Loss: 0.0424, Val Loss: 0.0516\n",
            "Epoch 28/100, Train Loss: 0.0382, Val Loss: 0.0516\n",
            "Epoch 29/100, Train Loss: 0.0416, Val Loss: 0.0516\n",
            "Epoch 30/100, Train Loss: 0.0441, Val Loss: 0.0516\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0450, Val Loss: 0.0516\n",
            "Epoch 32/100, Train Loss: 0.0410, Val Loss: 0.0516\n",
            "Epoch 33/100, Train Loss: 0.0460, Val Loss: 0.0516\n",
            "Epoch 34/100, Train Loss: 0.0455, Val Loss: 0.0516\n",
            "Epoch 35/100, Train Loss: 0.0409, Val Loss: 0.0516\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0400, Val Loss: 0.0516\n",
            "Epoch 37/100, Train Loss: 0.0442, Val Loss: 0.0516\n",
            "Epoch 38/100, Train Loss: 0.0418, Val Loss: 0.0516\n",
            "Epoch 39/100, Train Loss: 0.0430, Val Loss: 0.0516\n",
            "Epoch 40/100, Train Loss: 0.0420, Val Loss: 0.0516\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0409, Val Loss: 0.0516\n",
            "Epoch 42/100, Train Loss: 0.0444, Val Loss: 0.0516\n",
            "Epoch 43/100, Train Loss: 0.0423, Val Loss: 0.0516\n",
            "Epoch 44/100, Train Loss: 0.0419, Val Loss: 0.0516\n",
            "Epoch 45/100, Train Loss: 0.0438, Val Loss: 0.0516\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0417, Val Loss: 0.0516\n",
            "Epoch 47/100, Train Loss: 0.0400, Val Loss: 0.0516\n",
            "Epoch 48/100, Train Loss: 0.0410, Val Loss: 0.0516\n",
            "Epoch 49/100, Train Loss: 0.0395, Val Loss: 0.0516\n",
            "Epoch 50/100, Train Loss: 0.0421, Val Loss: 0.0516\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0439, Val Loss: 0.0516\n",
            "Epoch 52/100, Train Loss: 0.0465, Val Loss: 0.0516\n",
            "Epoch 53/100, Train Loss: 0.0444, Val Loss: 0.0516\n",
            "Epoch 54/100, Train Loss: 0.0424, Val Loss: 0.0516\n",
            "Epoch 55/100, Train Loss: 0.0416, Val Loss: 0.0516\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0407, Val Loss: 0.0516\n",
            "Epoch 57/100, Train Loss: 0.0422, Val Loss: 0.0516\n",
            "Epoch 58/100, Train Loss: 0.0402, Val Loss: 0.0516\n",
            "Epoch 59/100, Train Loss: 0.0410, Val Loss: 0.0516\n",
            "Epoch 60/100, Train Loss: 0.0419, Val Loss: 0.0516\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0415, Val Loss: 0.0516\n",
            "Epoch 62/100, Train Loss: 0.0409, Val Loss: 0.0516\n",
            "Epoch 63/100, Train Loss: 0.0432, Val Loss: 0.0516\n",
            "Epoch 64/100, Train Loss: 0.0431, Val Loss: 0.0516\n",
            "Epoch 65/100, Train Loss: 0.0428, Val Loss: 0.0516\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0431, Val Loss: 0.0516\n",
            "Epoch 67/100, Train Loss: 0.0394, Val Loss: 0.0516\n",
            "Epoch 68/100, Train Loss: 0.0422, Val Loss: 0.0516\n",
            "Epoch 69/100, Train Loss: 0.0448, Val Loss: 0.0516\n",
            "Epoch 70/100, Train Loss: 0.0433, Val Loss: 0.0516\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0423, Val Loss: 0.0516\n",
            "Epoch 72/100, Train Loss: 0.0367, Val Loss: 0.0516\n",
            "Epoch 73/100, Train Loss: 0.0438, Val Loss: 0.0516\n",
            "Epoch 74/100, Train Loss: 0.0424, Val Loss: 0.0516\n",
            "Epoch 75/100, Train Loss: 0.0434, Val Loss: 0.0516\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0425, Val Loss: 0.0516\n",
            "Epoch 77/100, Train Loss: 0.0442, Val Loss: 0.0516\n",
            "Epoch 78/100, Train Loss: 0.0422, Val Loss: 0.0516\n",
            "Epoch 79/100, Train Loss: 0.0417, Val Loss: 0.0516\n",
            "Epoch 80/100, Train Loss: 0.0417, Val Loss: 0.0516\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0392, Val Loss: 0.0516\n",
            "Epoch 82/100, Train Loss: 0.0445, Val Loss: 0.0516\n",
            "Epoch 83/100, Train Loss: 0.0402, Val Loss: 0.0516\n",
            "Epoch 84/100, Train Loss: 0.0414, Val Loss: 0.0516\n",
            "Epoch 85/100, Train Loss: 0.0386, Val Loss: 0.0516\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0446, Val Loss: 0.0516\n",
            "Epoch 87/100, Train Loss: 0.0459, Val Loss: 0.0516\n",
            "Epoch 88/100, Train Loss: 0.0401, Val Loss: 0.0516\n",
            "Epoch 89/100, Train Loss: 0.0392, Val Loss: 0.0516\n",
            "Epoch 90/100, Train Loss: 0.0436, Val Loss: 0.0516\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0380, Val Loss: 0.0516\n",
            "Epoch 92/100, Train Loss: 0.0427, Val Loss: 0.0516\n",
            "Epoch 93/100, Train Loss: 0.0404, Val Loss: 0.0516\n",
            "Epoch 94/100, Train Loss: 0.0401, Val Loss: 0.0516\n",
            "Epoch 95/100, Train Loss: 0.0397, Val Loss: 0.0516\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0390, Val Loss: 0.0516\n",
            "Epoch 97/100, Train Loss: 0.0428, Val Loss: 0.0516\n",
            "Epoch 98/100, Train Loss: 0.0404, Val Loss: 0.0516\n",
            "Epoch 99/100, Train Loss: 0.0443, Val Loss: 0.0516\n",
            "Epoch 100/100, Train Loss: 0.0448, Val Loss: 0.0516\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 25/28\n",
            "Current training set size: 121 samples\n",
            "Epoch 1/100, Train Loss: 0.1761, Val Loss: 0.1758\n",
            "Epoch 2/100, Train Loss: 0.1359, Val Loss: 0.1099\n",
            "Epoch 3/100, Train Loss: 0.0985, Val Loss: 0.0975\n",
            "Epoch 4/100, Train Loss: 0.0772, Val Loss: 0.0707\n",
            "Epoch 5/100, Train Loss: 0.0728, Val Loss: 0.0513\n",
            "Epoch 6/100, Train Loss: 0.0618, Val Loss: 0.0539\n",
            "Epoch 7/100, Train Loss: 0.0623, Val Loss: 0.0526\n",
            "Epoch 8/100, Train Loss: 0.0516, Val Loss: 0.0435\n",
            "Epoch 9/100, Train Loss: 0.0548, Val Loss: 0.0543\n",
            "Epoch 10/100, Train Loss: 0.0498, Val Loss: 0.0433\n",
            "Epoch 11/100, Train Loss: 0.0527, Val Loss: 0.0489\n",
            "Epoch 12/100, Train Loss: 0.0460, Val Loss: 0.0533\n",
            "Epoch 13/100, Train Loss: 0.0516, Val Loss: 0.0549\n",
            "Epoch 14/100, Train Loss: 0.0652, Val Loss: 0.0508\n",
            "Epoch 15/100, Train Loss: 0.0495, Val Loss: 0.0547\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0430, Val Loss: 0.0536\n",
            "Epoch 17/100, Train Loss: 0.0452, Val Loss: 0.0532\n",
            "Epoch 18/100, Train Loss: 0.0422, Val Loss: 0.0520\n",
            "Epoch 19/100, Train Loss: 0.0411, Val Loss: 0.0510\n",
            "Epoch 20/100, Train Loss: 0.0386, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0430, Val Loss: 0.0506\n",
            "Epoch 22/100, Train Loss: 0.0434, Val Loss: 0.0506\n",
            "Epoch 23/100, Train Loss: 0.0383, Val Loss: 0.0506\n",
            "Epoch 24/100, Train Loss: 0.0410, Val Loss: 0.0506\n",
            "Epoch 25/100, Train Loss: 0.0406, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0413, Val Loss: 0.0506\n",
            "Epoch 27/100, Train Loss: 0.0410, Val Loss: 0.0506\n",
            "Epoch 28/100, Train Loss: 0.0406, Val Loss: 0.0506\n",
            "Epoch 29/100, Train Loss: 0.0431, Val Loss: 0.0506\n",
            "Epoch 30/100, Train Loss: 0.0433, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0384, Val Loss: 0.0506\n",
            "Epoch 32/100, Train Loss: 0.0398, Val Loss: 0.0506\n",
            "Epoch 33/100, Train Loss: 0.0437, Val Loss: 0.0506\n",
            "Epoch 34/100, Train Loss: 0.0418, Val Loss: 0.0506\n",
            "Epoch 35/100, Train Loss: 0.0424, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0408, Val Loss: 0.0506\n",
            "Epoch 37/100, Train Loss: 0.0422, Val Loss: 0.0506\n",
            "Epoch 38/100, Train Loss: 0.0445, Val Loss: 0.0506\n",
            "Epoch 39/100, Train Loss: 0.0424, Val Loss: 0.0506\n",
            "Epoch 40/100, Train Loss: 0.0427, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0435, Val Loss: 0.0506\n",
            "Epoch 42/100, Train Loss: 0.0435, Val Loss: 0.0506\n",
            "Epoch 43/100, Train Loss: 0.0382, Val Loss: 0.0506\n",
            "Epoch 44/100, Train Loss: 0.0397, Val Loss: 0.0506\n",
            "Epoch 45/100, Train Loss: 0.0434, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0420, Val Loss: 0.0506\n",
            "Epoch 47/100, Train Loss: 0.0427, Val Loss: 0.0506\n",
            "Epoch 48/100, Train Loss: 0.0409, Val Loss: 0.0506\n",
            "Epoch 49/100, Train Loss: 0.0384, Val Loss: 0.0506\n",
            "Epoch 50/100, Train Loss: 0.0389, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0395, Val Loss: 0.0506\n",
            "Epoch 52/100, Train Loss: 0.0425, Val Loss: 0.0506\n",
            "Epoch 53/100, Train Loss: 0.0418, Val Loss: 0.0506\n",
            "Epoch 54/100, Train Loss: 0.0439, Val Loss: 0.0506\n",
            "Epoch 55/100, Train Loss: 0.0385, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0411, Val Loss: 0.0506\n",
            "Epoch 57/100, Train Loss: 0.0521, Val Loss: 0.0506\n",
            "Epoch 58/100, Train Loss: 0.0420, Val Loss: 0.0506\n",
            "Epoch 59/100, Train Loss: 0.0417, Val Loss: 0.0506\n",
            "Epoch 60/100, Train Loss: 0.0446, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0415, Val Loss: 0.0506\n",
            "Epoch 62/100, Train Loss: 0.0405, Val Loss: 0.0506\n",
            "Epoch 63/100, Train Loss: 0.0398, Val Loss: 0.0506\n",
            "Epoch 64/100, Train Loss: 0.0430, Val Loss: 0.0506\n",
            "Epoch 65/100, Train Loss: 0.0384, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0445, Val Loss: 0.0506\n",
            "Epoch 67/100, Train Loss: 0.0413, Val Loss: 0.0506\n",
            "Epoch 68/100, Train Loss: 0.0426, Val Loss: 0.0506\n",
            "Epoch 69/100, Train Loss: 0.0363, Val Loss: 0.0506\n",
            "Epoch 70/100, Train Loss: 0.0445, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0420, Val Loss: 0.0506\n",
            "Epoch 72/100, Train Loss: 0.0403, Val Loss: 0.0506\n",
            "Epoch 73/100, Train Loss: 0.0413, Val Loss: 0.0506\n",
            "Epoch 74/100, Train Loss: 0.0406, Val Loss: 0.0506\n",
            "Epoch 75/100, Train Loss: 0.0419, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0404, Val Loss: 0.0506\n",
            "Epoch 77/100, Train Loss: 0.0424, Val Loss: 0.0506\n",
            "Epoch 78/100, Train Loss: 0.0515, Val Loss: 0.0506\n",
            "Epoch 79/100, Train Loss: 0.0419, Val Loss: 0.0506\n",
            "Epoch 80/100, Train Loss: 0.0412, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0435, Val Loss: 0.0506\n",
            "Epoch 82/100, Train Loss: 0.0417, Val Loss: 0.0506\n",
            "Epoch 83/100, Train Loss: 0.0442, Val Loss: 0.0506\n",
            "Epoch 84/100, Train Loss: 0.0404, Val Loss: 0.0506\n",
            "Epoch 85/100, Train Loss: 0.0395, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0424, Val Loss: 0.0506\n",
            "Epoch 87/100, Train Loss: 0.0428, Val Loss: 0.0506\n",
            "Epoch 88/100, Train Loss: 0.0431, Val Loss: 0.0506\n",
            "Epoch 89/100, Train Loss: 0.0420, Val Loss: 0.0506\n",
            "Epoch 90/100, Train Loss: 0.0411, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0435, Val Loss: 0.0506\n",
            "Epoch 92/100, Train Loss: 0.0423, Val Loss: 0.0506\n",
            "Epoch 93/100, Train Loss: 0.0410, Val Loss: 0.0506\n",
            "Epoch 94/100, Train Loss: 0.0414, Val Loss: 0.0506\n",
            "Epoch 95/100, Train Loss: 0.0434, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0415, Val Loss: 0.0506\n",
            "Epoch 97/100, Train Loss: 0.0411, Val Loss: 0.0506\n",
            "Epoch 98/100, Train Loss: 0.0393, Val Loss: 0.0506\n",
            "Epoch 99/100, Train Loss: 0.0437, Val Loss: 0.0506\n",
            "Epoch 100/100, Train Loss: 0.0394, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 26/28\n",
            "Current training set size: 122 samples\n",
            "Epoch 1/100, Train Loss: 0.1611, Val Loss: 0.1793\n",
            "Epoch 2/100, Train Loss: 0.1243, Val Loss: 0.1049\n",
            "Epoch 3/100, Train Loss: 0.0950, Val Loss: 0.0914\n",
            "Epoch 4/100, Train Loss: 0.0719, Val Loss: 0.0587\n",
            "Epoch 5/100, Train Loss: 0.0782, Val Loss: 0.0717\n",
            "Epoch 6/100, Train Loss: 0.0579, Val Loss: 0.0499\n",
            "Epoch 7/100, Train Loss: 0.0608, Val Loss: 0.0442\n",
            "Epoch 8/100, Train Loss: 0.0649, Val Loss: 0.0581\n",
            "Epoch 9/100, Train Loss: 0.0589, Val Loss: 0.0435\n",
            "Epoch 10/100, Train Loss: 0.0497, Val Loss: 0.0496\n",
            "Epoch 11/100, Train Loss: 0.0511, Val Loss: 0.0483\n",
            "Epoch 12/100, Train Loss: 0.0533, Val Loss: 0.0474\n",
            "Epoch 13/100, Train Loss: 0.0458, Val Loss: 0.0412\n",
            "Epoch 14/100, Train Loss: 0.0472, Val Loss: 0.0565\n",
            "Epoch 15/100, Train Loss: 0.0446, Val Loss: 0.0482\n",
            "Epoch 16/100, Train Loss: 0.0432, Val Loss: 0.0396\n",
            "Epoch 17/100, Train Loss: 0.0482, Val Loss: 0.0465\n",
            "Epoch 18/100, Train Loss: 0.0464, Val Loss: 0.0389\n",
            "Epoch 19/100, Train Loss: 0.0443, Val Loss: 0.0435\n",
            "Epoch 20/100, Train Loss: 0.0435, Val Loss: 0.0529\n",
            "Epoch 21/100, Train Loss: 0.0461, Val Loss: 0.0481\n",
            "Epoch 22/100, Train Loss: 0.0450, Val Loss: 0.0450\n",
            "Epoch 23/100, Train Loss: 0.0401, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0430, Val Loss: 0.0440\n",
            "Epoch 25/100, Train Loss: 0.0449, Val Loss: 0.0424\n",
            "Epoch 26/100, Train Loss: 0.0369, Val Loss: 0.0403\n",
            "Epoch 27/100, Train Loss: 0.0378, Val Loss: 0.0394\n",
            "Epoch 28/100, Train Loss: 0.0382, Val Loss: 0.0380\n",
            "Epoch 29/100, Train Loss: 0.0401, Val Loss: 0.0376\n",
            "Epoch 30/100, Train Loss: 0.0371, Val Loss: 0.0372\n",
            "Epoch 31/100, Train Loss: 0.0366, Val Loss: 0.0372\n",
            "Epoch 32/100, Train Loss: 0.0374, Val Loss: 0.0374\n",
            "Epoch 33/100, Train Loss: 0.0366, Val Loss: 0.0375\n",
            "Epoch 34/100, Train Loss: 0.0405, Val Loss: 0.0375\n",
            "Epoch 35/100, Train Loss: 0.0403, Val Loss: 0.0377\n",
            "Epoch 36/100, Train Loss: 0.0402, Val Loss: 0.0373\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0392, Val Loss: 0.0373\n",
            "Epoch 38/100, Train Loss: 0.0370, Val Loss: 0.0373\n",
            "Epoch 39/100, Train Loss: 0.0374, Val Loss: 0.0373\n",
            "Epoch 40/100, Train Loss: 0.0385, Val Loss: 0.0373\n",
            "Epoch 41/100, Train Loss: 0.0340, Val Loss: 0.0373\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0393, Val Loss: 0.0373\n",
            "Epoch 43/100, Train Loss: 0.0376, Val Loss: 0.0373\n",
            "Epoch 44/100, Train Loss: 0.0399, Val Loss: 0.0373\n",
            "Epoch 45/100, Train Loss: 0.0332, Val Loss: 0.0373\n",
            "Epoch 46/100, Train Loss: 0.0391, Val Loss: 0.0373\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0356, Val Loss: 0.0373\n",
            "Epoch 48/100, Train Loss: 0.0356, Val Loss: 0.0373\n",
            "Epoch 49/100, Train Loss: 0.0386, Val Loss: 0.0373\n",
            "Epoch 50/100, Train Loss: 0.0360, Val Loss: 0.0373\n",
            "Epoch 51/100, Train Loss: 0.0374, Val Loss: 0.0373\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0394, Val Loss: 0.0373\n",
            "Epoch 53/100, Train Loss: 0.0322, Val Loss: 0.0373\n",
            "Epoch 54/100, Train Loss: 0.0427, Val Loss: 0.0373\n",
            "Epoch 55/100, Train Loss: 0.0375, Val Loss: 0.0373\n",
            "Epoch 56/100, Train Loss: 0.0385, Val Loss: 0.0373\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0323, Val Loss: 0.0373\n",
            "Epoch 58/100, Train Loss: 0.0367, Val Loss: 0.0373\n",
            "Epoch 59/100, Train Loss: 0.0363, Val Loss: 0.0373\n",
            "Epoch 60/100, Train Loss: 0.0376, Val Loss: 0.0373\n",
            "Epoch 61/100, Train Loss: 0.0362, Val Loss: 0.0373\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0378, Val Loss: 0.0373\n",
            "Epoch 63/100, Train Loss: 0.0392, Val Loss: 0.0373\n",
            "Epoch 64/100, Train Loss: 0.0383, Val Loss: 0.0373\n",
            "Epoch 65/100, Train Loss: 0.0356, Val Loss: 0.0373\n",
            "Epoch 66/100, Train Loss: 0.0378, Val Loss: 0.0373\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0365, Val Loss: 0.0373\n",
            "Epoch 68/100, Train Loss: 0.0380, Val Loss: 0.0373\n",
            "Epoch 69/100, Train Loss: 0.0361, Val Loss: 0.0373\n",
            "Epoch 70/100, Train Loss: 0.0396, Val Loss: 0.0373\n",
            "Epoch 71/100, Train Loss: 0.0342, Val Loss: 0.0373\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0369, Val Loss: 0.0373\n",
            "Epoch 73/100, Train Loss: 0.0379, Val Loss: 0.0373\n",
            "Epoch 74/100, Train Loss: 0.0396, Val Loss: 0.0373\n",
            "Epoch 75/100, Train Loss: 0.0373, Val Loss: 0.0373\n",
            "Epoch 76/100, Train Loss: 0.0357, Val Loss: 0.0373\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0377, Val Loss: 0.0373\n",
            "Epoch 78/100, Train Loss: 0.0355, Val Loss: 0.0373\n",
            "Epoch 79/100, Train Loss: 0.0386, Val Loss: 0.0373\n",
            "Epoch 80/100, Train Loss: 0.0363, Val Loss: 0.0373\n",
            "Epoch 81/100, Train Loss: 0.0366, Val Loss: 0.0373\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0385, Val Loss: 0.0373\n",
            "Epoch 83/100, Train Loss: 0.0357, Val Loss: 0.0373\n",
            "Epoch 84/100, Train Loss: 0.0371, Val Loss: 0.0373\n",
            "Epoch 85/100, Train Loss: 0.0354, Val Loss: 0.0373\n",
            "Epoch 86/100, Train Loss: 0.0412, Val Loss: 0.0373\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0414, Val Loss: 0.0373\n",
            "Epoch 88/100, Train Loss: 0.0363, Val Loss: 0.0373\n",
            "Epoch 89/100, Train Loss: 0.0395, Val Loss: 0.0373\n",
            "Epoch 90/100, Train Loss: 0.0380, Val Loss: 0.0373\n",
            "Epoch 91/100, Train Loss: 0.0399, Val Loss: 0.0373\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0350, Val Loss: 0.0373\n",
            "Epoch 93/100, Train Loss: 0.0354, Val Loss: 0.0373\n",
            "Epoch 94/100, Train Loss: 0.0343, Val Loss: 0.0373\n",
            "Epoch 95/100, Train Loss: 0.0356, Val Loss: 0.0373\n",
            "Epoch 96/100, Train Loss: 0.0371, Val Loss: 0.0373\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0364, Val Loss: 0.0373\n",
            "Epoch 98/100, Train Loss: 0.0414, Val Loss: 0.0373\n",
            "Epoch 99/100, Train Loss: 0.0397, Val Loss: 0.0373\n",
            "Epoch 100/100, Train Loss: 0.0365, Val Loss: 0.0373\n",
            "\n",
            "Test iteration 27/28\n",
            "Current training set size: 123 samples\n",
            "Epoch 1/100, Train Loss: 0.1678, Val Loss: 0.1931\n",
            "Epoch 2/100, Train Loss: 0.1157, Val Loss: 0.1043\n",
            "Epoch 3/100, Train Loss: 0.0997, Val Loss: 0.0962\n",
            "Epoch 4/100, Train Loss: 0.0719, Val Loss: 0.0644\n",
            "Epoch 5/100, Train Loss: 0.0617, Val Loss: 0.0488\n",
            "Epoch 6/100, Train Loss: 0.0599, Val Loss: 0.0486\n",
            "Epoch 7/100, Train Loss: 0.0540, Val Loss: 0.0515\n",
            "Epoch 8/100, Train Loss: 0.0530, Val Loss: 0.0538\n",
            "Epoch 9/100, Train Loss: 0.0564, Val Loss: 0.0685\n",
            "Epoch 10/100, Train Loss: 0.0565, Val Loss: 0.0413\n",
            "Epoch 11/100, Train Loss: 0.0479, Val Loss: 0.0376\n",
            "Epoch 12/100, Train Loss: 0.0512, Val Loss: 0.0570\n",
            "Epoch 13/100, Train Loss: 0.0485, Val Loss: 0.0462\n",
            "Epoch 14/100, Train Loss: 0.0483, Val Loss: 0.0528\n",
            "Epoch 15/100, Train Loss: 0.0496, Val Loss: 0.0465\n",
            "Epoch 16/100, Train Loss: 0.0422, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0399, Val Loss: 0.0419\n",
            "Epoch 18/100, Train Loss: 0.0443, Val Loss: 0.0416\n",
            "Epoch 19/100, Train Loss: 0.0375, Val Loss: 0.0416\n",
            "Epoch 20/100, Train Loss: 0.0401, Val Loss: 0.0417\n",
            "Epoch 21/100, Train Loss: 0.0390, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0435, Val Loss: 0.0413\n",
            "Epoch 23/100, Train Loss: 0.0393, Val Loss: 0.0413\n",
            "Epoch 24/100, Train Loss: 0.0433, Val Loss: 0.0413\n",
            "Epoch 25/100, Train Loss: 0.0452, Val Loss: 0.0413\n",
            "Epoch 26/100, Train Loss: 0.0434, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0425, Val Loss: 0.0413\n",
            "Epoch 28/100, Train Loss: 0.0384, Val Loss: 0.0413\n",
            "Epoch 29/100, Train Loss: 0.0386, Val Loss: 0.0413\n",
            "Epoch 30/100, Train Loss: 0.0368, Val Loss: 0.0413\n",
            "Epoch 31/100, Train Loss: 0.0381, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0405, Val Loss: 0.0413\n",
            "Epoch 33/100, Train Loss: 0.0384, Val Loss: 0.0413\n",
            "Epoch 34/100, Train Loss: 0.0472, Val Loss: 0.0413\n",
            "Epoch 35/100, Train Loss: 0.0384, Val Loss: 0.0413\n",
            "Epoch 36/100, Train Loss: 0.0418, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0421, Val Loss: 0.0413\n",
            "Epoch 38/100, Train Loss: 0.0398, Val Loss: 0.0413\n",
            "Epoch 39/100, Train Loss: 0.0419, Val Loss: 0.0413\n",
            "Epoch 40/100, Train Loss: 0.0438, Val Loss: 0.0413\n",
            "Epoch 41/100, Train Loss: 0.0428, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0390, Val Loss: 0.0413\n",
            "Epoch 43/100, Train Loss: 0.0423, Val Loss: 0.0413\n",
            "Epoch 44/100, Train Loss: 0.0429, Val Loss: 0.0413\n",
            "Epoch 45/100, Train Loss: 0.0382, Val Loss: 0.0413\n",
            "Epoch 46/100, Train Loss: 0.0396, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0400, Val Loss: 0.0413\n",
            "Epoch 48/100, Train Loss: 0.0406, Val Loss: 0.0413\n",
            "Epoch 49/100, Train Loss: 0.0395, Val Loss: 0.0413\n",
            "Epoch 50/100, Train Loss: 0.0415, Val Loss: 0.0413\n",
            "Epoch 51/100, Train Loss: 0.0413, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0388, Val Loss: 0.0413\n",
            "Epoch 53/100, Train Loss: 0.0400, Val Loss: 0.0413\n",
            "Epoch 54/100, Train Loss: 0.0448, Val Loss: 0.0413\n",
            "Epoch 55/100, Train Loss: 0.0412, Val Loss: 0.0413\n",
            "Epoch 56/100, Train Loss: 0.0410, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0370, Val Loss: 0.0413\n",
            "Epoch 58/100, Train Loss: 0.0408, Val Loss: 0.0413\n",
            "Epoch 59/100, Train Loss: 0.0413, Val Loss: 0.0413\n",
            "Epoch 60/100, Train Loss: 0.0358, Val Loss: 0.0413\n",
            "Epoch 61/100, Train Loss: 0.0416, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0385, Val Loss: 0.0413\n",
            "Epoch 63/100, Train Loss: 0.0398, Val Loss: 0.0413\n",
            "Epoch 64/100, Train Loss: 0.0462, Val Loss: 0.0413\n",
            "Epoch 65/100, Train Loss: 0.0399, Val Loss: 0.0413\n",
            "Epoch 66/100, Train Loss: 0.0386, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0391, Val Loss: 0.0413\n",
            "Epoch 68/100, Train Loss: 0.0414, Val Loss: 0.0413\n",
            "Epoch 69/100, Train Loss: 0.0433, Val Loss: 0.0413\n",
            "Epoch 70/100, Train Loss: 0.0345, Val Loss: 0.0413\n",
            "Epoch 71/100, Train Loss: 0.0433, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0417, Val Loss: 0.0413\n",
            "Epoch 73/100, Train Loss: 0.0357, Val Loss: 0.0413\n",
            "Epoch 74/100, Train Loss: 0.0412, Val Loss: 0.0413\n",
            "Epoch 75/100, Train Loss: 0.0399, Val Loss: 0.0413\n",
            "Epoch 76/100, Train Loss: 0.0391, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0431, Val Loss: 0.0413\n",
            "Epoch 78/100, Train Loss: 0.0403, Val Loss: 0.0413\n",
            "Epoch 79/100, Train Loss: 0.0424, Val Loss: 0.0413\n",
            "Epoch 80/100, Train Loss: 0.0434, Val Loss: 0.0413\n",
            "Epoch 81/100, Train Loss: 0.0464, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0479, Val Loss: 0.0413\n",
            "Epoch 83/100, Train Loss: 0.0430, Val Loss: 0.0413\n",
            "Epoch 84/100, Train Loss: 0.0383, Val Loss: 0.0413\n",
            "Epoch 85/100, Train Loss: 0.0398, Val Loss: 0.0413\n",
            "Epoch 86/100, Train Loss: 0.0406, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0401, Val Loss: 0.0413\n",
            "Epoch 88/100, Train Loss: 0.0384, Val Loss: 0.0413\n",
            "Epoch 89/100, Train Loss: 0.0428, Val Loss: 0.0413\n",
            "Epoch 90/100, Train Loss: 0.0432, Val Loss: 0.0413\n",
            "Epoch 91/100, Train Loss: 0.0414, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0401, Val Loss: 0.0413\n",
            "Epoch 93/100, Train Loss: 0.0439, Val Loss: 0.0413\n",
            "Epoch 94/100, Train Loss: 0.0447, Val Loss: 0.0413\n",
            "Epoch 95/100, Train Loss: 0.0409, Val Loss: 0.0413\n",
            "Epoch 96/100, Train Loss: 0.0444, Val Loss: 0.0413\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0403, Val Loss: 0.0413\n",
            "Epoch 98/100, Train Loss: 0.0406, Val Loss: 0.0413\n",
            "Epoch 99/100, Train Loss: 0.0423, Val Loss: 0.0413\n",
            "Epoch 100/100, Train Loss: 0.0398, Val Loss: 0.0413\n",
            "\n",
            "Test iteration 28/28\n",
            "Current training set size: 124 samples\n",
            "Epoch 1/100, Train Loss: 0.1691, Val Loss: 0.1704\n",
            "Epoch 2/100, Train Loss: 0.1092, Val Loss: 0.1047\n",
            "Epoch 3/100, Train Loss: 0.0942, Val Loss: 0.0795\n",
            "Epoch 4/100, Train Loss: 0.0735, Val Loss: 0.0516\n",
            "Epoch 5/100, Train Loss: 0.0649, Val Loss: 0.0491\n",
            "Epoch 6/100, Train Loss: 0.0605, Val Loss: 0.0526\n",
            "Epoch 7/100, Train Loss: 0.0598, Val Loss: 0.0461\n",
            "Epoch 8/100, Train Loss: 0.0517, Val Loss: 0.0422\n",
            "Epoch 9/100, Train Loss: 0.0515, Val Loss: 0.0512\n",
            "Epoch 10/100, Train Loss: 0.0722, Val Loss: 0.0628\n",
            "Epoch 11/100, Train Loss: 0.0555, Val Loss: 0.0720\n",
            "Epoch 12/100, Train Loss: 0.0557, Val Loss: 0.0506\n",
            "Epoch 13/100, Train Loss: 0.0557, Val Loss: 0.0646\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 14/100, Train Loss: 0.0489, Val Loss: 0.0631\n",
            "Epoch 15/100, Train Loss: 0.0496, Val Loss: 0.0608\n",
            "Epoch 16/100, Train Loss: 0.0461, Val Loss: 0.0588\n",
            "Epoch 17/100, Train Loss: 0.0471, Val Loss: 0.0572\n",
            "Epoch 18/100, Train Loss: 0.0469, Val Loss: 0.0561\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0467, Val Loss: 0.0561\n",
            "Epoch 20/100, Train Loss: 0.0445, Val Loss: 0.0561\n",
            "Epoch 21/100, Train Loss: 0.0457, Val Loss: 0.0561\n",
            "Epoch 22/100, Train Loss: 0.0479, Val Loss: 0.0561\n",
            "Epoch 23/100, Train Loss: 0.0475, Val Loss: 0.0561\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0458, Val Loss: 0.0561\n",
            "Epoch 25/100, Train Loss: 0.0451, Val Loss: 0.0561\n",
            "Epoch 26/100, Train Loss: 0.0455, Val Loss: 0.0561\n",
            "Epoch 27/100, Train Loss: 0.0435, Val Loss: 0.0561\n",
            "Epoch 28/100, Train Loss: 0.0487, Val Loss: 0.0561\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0487, Val Loss: 0.0561\n",
            "Epoch 30/100, Train Loss: 0.0471, Val Loss: 0.0561\n",
            "Epoch 31/100, Train Loss: 0.0455, Val Loss: 0.0561\n",
            "Epoch 32/100, Train Loss: 0.0414, Val Loss: 0.0561\n",
            "Epoch 33/100, Train Loss: 0.0473, Val Loss: 0.0561\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0479, Val Loss: 0.0561\n",
            "Epoch 35/100, Train Loss: 0.0446, Val Loss: 0.0561\n",
            "Epoch 36/100, Train Loss: 0.0452, Val Loss: 0.0561\n",
            "Epoch 37/100, Train Loss: 0.0465, Val Loss: 0.0561\n",
            "Epoch 38/100, Train Loss: 0.0491, Val Loss: 0.0561\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0454, Val Loss: 0.0561\n",
            "Epoch 40/100, Train Loss: 0.0448, Val Loss: 0.0561\n",
            "Epoch 41/100, Train Loss: 0.0498, Val Loss: 0.0561\n",
            "Epoch 42/100, Train Loss: 0.0470, Val Loss: 0.0561\n",
            "Epoch 43/100, Train Loss: 0.0483, Val Loss: 0.0561\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0504, Val Loss: 0.0561\n",
            "Epoch 45/100, Train Loss: 0.0477, Val Loss: 0.0561\n",
            "Epoch 46/100, Train Loss: 0.0471, Val Loss: 0.0561\n",
            "Epoch 47/100, Train Loss: 0.0457, Val Loss: 0.0561\n",
            "Epoch 48/100, Train Loss: 0.0459, Val Loss: 0.0561\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0447, Val Loss: 0.0561\n",
            "Epoch 50/100, Train Loss: 0.0450, Val Loss: 0.0561\n",
            "Epoch 51/100, Train Loss: 0.0458, Val Loss: 0.0561\n",
            "Epoch 52/100, Train Loss: 0.0473, Val Loss: 0.0561\n",
            "Epoch 53/100, Train Loss: 0.0452, Val Loss: 0.0561\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0488, Val Loss: 0.0561\n",
            "Epoch 55/100, Train Loss: 0.0423, Val Loss: 0.0561\n",
            "Epoch 56/100, Train Loss: 0.0450, Val Loss: 0.0561\n",
            "Epoch 57/100, Train Loss: 0.0439, Val Loss: 0.0561\n",
            "Epoch 58/100, Train Loss: 0.0451, Val Loss: 0.0561\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0441, Val Loss: 0.0561\n",
            "Epoch 60/100, Train Loss: 0.0492, Val Loss: 0.0561\n",
            "Epoch 61/100, Train Loss: 0.0478, Val Loss: 0.0561\n",
            "Epoch 62/100, Train Loss: 0.0472, Val Loss: 0.0561\n",
            "Epoch 63/100, Train Loss: 0.0442, Val Loss: 0.0561\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0477, Val Loss: 0.0561\n",
            "Epoch 65/100, Train Loss: 0.0473, Val Loss: 0.0561\n",
            "Epoch 66/100, Train Loss: 0.0418, Val Loss: 0.0561\n",
            "Epoch 67/100, Train Loss: 0.0446, Val Loss: 0.0561\n",
            "Epoch 68/100, Train Loss: 0.0486, Val Loss: 0.0561\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0442, Val Loss: 0.0561\n",
            "Epoch 70/100, Train Loss: 0.0461, Val Loss: 0.0561\n",
            "Epoch 71/100, Train Loss: 0.0450, Val Loss: 0.0561\n",
            "Epoch 72/100, Train Loss: 0.0452, Val Loss: 0.0561\n",
            "Epoch 73/100, Train Loss: 0.0481, Val Loss: 0.0561\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0423, Val Loss: 0.0561\n",
            "Epoch 75/100, Train Loss: 0.0460, Val Loss: 0.0561\n",
            "Epoch 76/100, Train Loss: 0.0483, Val Loss: 0.0561\n",
            "Epoch 77/100, Train Loss: 0.0495, Val Loss: 0.0561\n",
            "Epoch 78/100, Train Loss: 0.0486, Val Loss: 0.0561\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0500, Val Loss: 0.0561\n",
            "Epoch 80/100, Train Loss: 0.0427, Val Loss: 0.0561\n",
            "Epoch 81/100, Train Loss: 0.0462, Val Loss: 0.0561\n",
            "Epoch 82/100, Train Loss: 0.0450, Val Loss: 0.0561\n",
            "Epoch 83/100, Train Loss: 0.0483, Val Loss: 0.0561\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0472, Val Loss: 0.0561\n",
            "Epoch 85/100, Train Loss: 0.0476, Val Loss: 0.0561\n",
            "Epoch 86/100, Train Loss: 0.0487, Val Loss: 0.0561\n",
            "Epoch 87/100, Train Loss: 0.0473, Val Loss: 0.0561\n",
            "Epoch 88/100, Train Loss: 0.0460, Val Loss: 0.0561\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0454, Val Loss: 0.0561\n",
            "Epoch 90/100, Train Loss: 0.0456, Val Loss: 0.0561\n",
            "Epoch 91/100, Train Loss: 0.0487, Val Loss: 0.0561\n",
            "Epoch 92/100, Train Loss: 0.0474, Val Loss: 0.0561\n",
            "Epoch 93/100, Train Loss: 0.0436, Val Loss: 0.0561\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0439, Val Loss: 0.0561\n",
            "Epoch 95/100, Train Loss: 0.0454, Val Loss: 0.0561\n",
            "Epoch 96/100, Train Loss: 0.0471, Val Loss: 0.0561\n",
            "Epoch 97/100, Train Loss: 0.0455, Val Loss: 0.0561\n",
            "Epoch 98/100, Train Loss: 0.0443, Val Loss: 0.0561\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0490, Val Loss: 0.0561\n",
            "Epoch 100/100, Train Loss: 0.0443, Val Loss: 0.0561\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: cnn, Units/Estimators: CNN\n",
            "Dropout: 0.2, Dense Units: 64\n",
            "Batch Size: 4, Epochs: 100\n",
            "Device: cuda\n",
            "Total test samples: 28\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/28\n",
            "Current training set size: 97 samples\n",
            "Epoch 1/100, Train Loss: 0.1558, Val Loss: 0.1441\n",
            "Epoch 2/100, Train Loss: 0.0959, Val Loss: 0.0944\n",
            "Epoch 3/100, Train Loss: 0.0901, Val Loss: 0.0802\n",
            "Epoch 4/100, Train Loss: 0.0699, Val Loss: 0.0677\n",
            "Epoch 5/100, Train Loss: 0.0721, Val Loss: 0.0489\n",
            "Epoch 6/100, Train Loss: 0.0487, Val Loss: 0.0411\n",
            "Epoch 7/100, Train Loss: 0.0584, Val Loss: 0.0533\n",
            "Epoch 8/100, Train Loss: 0.0502, Val Loss: 0.0548\n",
            "Epoch 9/100, Train Loss: 0.0465, Val Loss: 0.0415\n",
            "Epoch 10/100, Train Loss: 0.0438, Val Loss: 0.0493\n",
            "Epoch 11/100, Train Loss: 0.0463, Val Loss: 0.0403\n",
            "Epoch 12/100, Train Loss: 0.0472, Val Loss: 0.0501\n",
            "Epoch 13/100, Train Loss: 0.0476, Val Loss: 0.0406\n",
            "Epoch 14/100, Train Loss: 0.0392, Val Loss: 0.0366\n",
            "Epoch 15/100, Train Loss: 0.0421, Val Loss: 0.0539\n",
            "Epoch 16/100, Train Loss: 0.0423, Val Loss: 0.0454\n",
            "Epoch 17/100, Train Loss: 0.0393, Val Loss: 0.0428\n",
            "Epoch 18/100, Train Loss: 0.0379, Val Loss: 0.0500\n",
            "Epoch 19/100, Train Loss: 0.0413, Val Loss: 0.0675\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0590, Val Loss: 0.0642\n",
            "Epoch 21/100, Train Loss: 0.0514, Val Loss: 0.0599\n",
            "Epoch 22/100, Train Loss: 0.0515, Val Loss: 0.0564\n",
            "Epoch 23/100, Train Loss: 0.0431, Val Loss: 0.0526\n",
            "Epoch 24/100, Train Loss: 0.0401, Val Loss: 0.0494\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0477, Val Loss: 0.0494\n",
            "Epoch 26/100, Train Loss: 0.0438, Val Loss: 0.0494\n",
            "Epoch 27/100, Train Loss: 0.0387, Val Loss: 0.0493\n",
            "Epoch 28/100, Train Loss: 0.0426, Val Loss: 0.0493\n",
            "Epoch 29/100, Train Loss: 0.0477, Val Loss: 0.0493\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0421, Val Loss: 0.0493\n",
            "Epoch 31/100, Train Loss: 0.0445, Val Loss: 0.0493\n",
            "Epoch 32/100, Train Loss: 0.0450, Val Loss: 0.0493\n",
            "Epoch 33/100, Train Loss: 0.0422, Val Loss: 0.0493\n",
            "Epoch 34/100, Train Loss: 0.0407, Val Loss: 0.0493\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0448, Val Loss: 0.0493\n",
            "Epoch 36/100, Train Loss: 0.0439, Val Loss: 0.0493\n",
            "Epoch 37/100, Train Loss: 0.0414, Val Loss: 0.0493\n",
            "Epoch 38/100, Train Loss: 0.0401, Val Loss: 0.0493\n",
            "Epoch 39/100, Train Loss: 0.0407, Val Loss: 0.0493\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0430, Val Loss: 0.0493\n",
            "Epoch 41/100, Train Loss: 0.0445, Val Loss: 0.0493\n",
            "Epoch 42/100, Train Loss: 0.0466, Val Loss: 0.0493\n",
            "Epoch 43/100, Train Loss: 0.0421, Val Loss: 0.0493\n",
            "Epoch 44/100, Train Loss: 0.0436, Val Loss: 0.0493\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0403, Val Loss: 0.0493\n",
            "Epoch 46/100, Train Loss: 0.0415, Val Loss: 0.0493\n",
            "Epoch 47/100, Train Loss: 0.0455, Val Loss: 0.0493\n",
            "Epoch 48/100, Train Loss: 0.0423, Val Loss: 0.0493\n",
            "Epoch 49/100, Train Loss: 0.0484, Val Loss: 0.0493\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0449, Val Loss: 0.0493\n",
            "Epoch 51/100, Train Loss: 0.0453, Val Loss: 0.0493\n",
            "Epoch 52/100, Train Loss: 0.0433, Val Loss: 0.0493\n",
            "Epoch 53/100, Train Loss: 0.0430, Val Loss: 0.0493\n",
            "Epoch 54/100, Train Loss: 0.0415, Val Loss: 0.0493\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0466, Val Loss: 0.0493\n",
            "Epoch 56/100, Train Loss: 0.0433, Val Loss: 0.0493\n",
            "Epoch 57/100, Train Loss: 0.0447, Val Loss: 0.0493\n",
            "Epoch 58/100, Train Loss: 0.0430, Val Loss: 0.0493\n",
            "Epoch 59/100, Train Loss: 0.0433, Val Loss: 0.0493\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0533, Val Loss: 0.0493\n",
            "Epoch 61/100, Train Loss: 0.0416, Val Loss: 0.0493\n",
            "Epoch 62/100, Train Loss: 0.0435, Val Loss: 0.0493\n",
            "Epoch 63/100, Train Loss: 0.0479, Val Loss: 0.0493\n",
            "Epoch 64/100, Train Loss: 0.0403, Val Loss: 0.0493\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0404, Val Loss: 0.0493\n",
            "Epoch 66/100, Train Loss: 0.0400, Val Loss: 0.0493\n",
            "Epoch 67/100, Train Loss: 0.0381, Val Loss: 0.0493\n",
            "Epoch 68/100, Train Loss: 0.0392, Val Loss: 0.0493\n",
            "Epoch 69/100, Train Loss: 0.0452, Val Loss: 0.0493\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0386, Val Loss: 0.0493\n",
            "Epoch 71/100, Train Loss: 0.0464, Val Loss: 0.0493\n",
            "Epoch 72/100, Train Loss: 0.0442, Val Loss: 0.0493\n",
            "Epoch 73/100, Train Loss: 0.0417, Val Loss: 0.0493\n",
            "Epoch 74/100, Train Loss: 0.0453, Val Loss: 0.0493\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0430, Val Loss: 0.0493\n",
            "Epoch 76/100, Train Loss: 0.0425, Val Loss: 0.0493\n",
            "Epoch 77/100, Train Loss: 0.0415, Val Loss: 0.0493\n",
            "Epoch 78/100, Train Loss: 0.0438, Val Loss: 0.0493\n",
            "Epoch 79/100, Train Loss: 0.0470, Val Loss: 0.0493\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0417, Val Loss: 0.0493\n",
            "Epoch 81/100, Train Loss: 0.0467, Val Loss: 0.0493\n",
            "Epoch 82/100, Train Loss: 0.0447, Val Loss: 0.0493\n",
            "Epoch 83/100, Train Loss: 0.0463, Val Loss: 0.0493\n",
            "Epoch 84/100, Train Loss: 0.0395, Val Loss: 0.0493\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0457, Val Loss: 0.0493\n",
            "Epoch 86/100, Train Loss: 0.0404, Val Loss: 0.0493\n",
            "Epoch 87/100, Train Loss: 0.0439, Val Loss: 0.0493\n",
            "Epoch 88/100, Train Loss: 0.0451, Val Loss: 0.0493\n",
            "Epoch 89/100, Train Loss: 0.0439, Val Loss: 0.0493\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0407, Val Loss: 0.0493\n",
            "Epoch 91/100, Train Loss: 0.0451, Val Loss: 0.0493\n",
            "Epoch 92/100, Train Loss: 0.0475, Val Loss: 0.0493\n",
            "Epoch 93/100, Train Loss: 0.0460, Val Loss: 0.0493\n",
            "Epoch 94/100, Train Loss: 0.0415, Val Loss: 0.0493\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0432, Val Loss: 0.0493\n",
            "Epoch 96/100, Train Loss: 0.0379, Val Loss: 0.0493\n",
            "Epoch 97/100, Train Loss: 0.0502, Val Loss: 0.0493\n",
            "Epoch 98/100, Train Loss: 0.0406, Val Loss: 0.0493\n",
            "Epoch 99/100, Train Loss: 0.0512, Val Loss: 0.0493\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0421, Val Loss: 0.0493\n",
            "\n",
            "Test iteration 2/28\n",
            "Current training set size: 98 samples\n",
            "Epoch 1/100, Train Loss: 0.1698, Val Loss: 0.1758\n",
            "Epoch 2/100, Train Loss: 0.1091, Val Loss: 0.0974\n",
            "Epoch 3/100, Train Loss: 0.0935, Val Loss: 0.0949\n",
            "Epoch 4/100, Train Loss: 0.0794, Val Loss: 0.0752\n",
            "Epoch 5/100, Train Loss: 0.0717, Val Loss: 0.0742\n",
            "Epoch 6/100, Train Loss: 0.0557, Val Loss: 0.0602\n",
            "Epoch 7/100, Train Loss: 0.0546, Val Loss: 0.0434\n",
            "Epoch 8/100, Train Loss: 0.0518, Val Loss: 0.0582\n",
            "Epoch 9/100, Train Loss: 0.0493, Val Loss: 0.0438\n",
            "Epoch 10/100, Train Loss: 0.0544, Val Loss: 0.0747\n",
            "Epoch 11/100, Train Loss: 0.0565, Val Loss: 0.0414\n",
            "Epoch 12/100, Train Loss: 0.0509, Val Loss: 0.0475\n",
            "Epoch 13/100, Train Loss: 0.0451, Val Loss: 0.0403\n",
            "Epoch 14/100, Train Loss: 0.0431, Val Loss: 0.0482\n",
            "Epoch 15/100, Train Loss: 0.0539, Val Loss: 0.0825\n",
            "Epoch 16/100, Train Loss: 0.0525, Val Loss: 0.0468\n",
            "Epoch 17/100, Train Loss: 0.0412, Val Loss: 0.0405\n",
            "Epoch 18/100, Train Loss: 0.0378, Val Loss: 0.0354\n",
            "Epoch 19/100, Train Loss: 0.0446, Val Loss: 0.0476\n",
            "Epoch 20/100, Train Loss: 0.0408, Val Loss: 0.0462\n",
            "Epoch 21/100, Train Loss: 0.0329, Val Loss: 0.0397\n",
            "Epoch 22/100, Train Loss: 0.0345, Val Loss: 0.0451\n",
            "Epoch 23/100, Train Loss: 0.0418, Val Loss: 0.0542\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0363, Val Loss: 0.0524\n",
            "Epoch 25/100, Train Loss: 0.0390, Val Loss: 0.0504\n",
            "Epoch 26/100, Train Loss: 0.0353, Val Loss: 0.0485\n",
            "Epoch 27/100, Train Loss: 0.0342, Val Loss: 0.0469\n",
            "Epoch 28/100, Train Loss: 0.0349, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0366, Val Loss: 0.0458\n",
            "Epoch 30/100, Train Loss: 0.0375, Val Loss: 0.0458\n",
            "Epoch 31/100, Train Loss: 0.0352, Val Loss: 0.0458\n",
            "Epoch 32/100, Train Loss: 0.0391, Val Loss: 0.0458\n",
            "Epoch 33/100, Train Loss: 0.0331, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0396, Val Loss: 0.0458\n",
            "Epoch 35/100, Train Loss: 0.0319, Val Loss: 0.0458\n",
            "Epoch 36/100, Train Loss: 0.0388, Val Loss: 0.0458\n",
            "Epoch 37/100, Train Loss: 0.0319, Val Loss: 0.0458\n",
            "Epoch 38/100, Train Loss: 0.0359, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0336, Val Loss: 0.0458\n",
            "Epoch 40/100, Train Loss: 0.0324, Val Loss: 0.0458\n",
            "Epoch 41/100, Train Loss: 0.0383, Val Loss: 0.0458\n",
            "Epoch 42/100, Train Loss: 0.0378, Val Loss: 0.0458\n",
            "Epoch 43/100, Train Loss: 0.0360, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0345, Val Loss: 0.0458\n",
            "Epoch 45/100, Train Loss: 0.0352, Val Loss: 0.0458\n",
            "Epoch 46/100, Train Loss: 0.0340, Val Loss: 0.0458\n",
            "Epoch 47/100, Train Loss: 0.0340, Val Loss: 0.0458\n",
            "Epoch 48/100, Train Loss: 0.0350, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0346, Val Loss: 0.0458\n",
            "Epoch 50/100, Train Loss: 0.0297, Val Loss: 0.0458\n",
            "Epoch 51/100, Train Loss: 0.0358, Val Loss: 0.0458\n",
            "Epoch 52/100, Train Loss: 0.0350, Val Loss: 0.0458\n",
            "Epoch 53/100, Train Loss: 0.0367, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0352, Val Loss: 0.0458\n",
            "Epoch 55/100, Train Loss: 0.0379, Val Loss: 0.0458\n",
            "Epoch 56/100, Train Loss: 0.0369, Val Loss: 0.0458\n",
            "Epoch 57/100, Train Loss: 0.0359, Val Loss: 0.0458\n",
            "Epoch 58/100, Train Loss: 0.0353, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0351, Val Loss: 0.0458\n",
            "Epoch 60/100, Train Loss: 0.0358, Val Loss: 0.0458\n",
            "Epoch 61/100, Train Loss: 0.0334, Val Loss: 0.0458\n",
            "Epoch 62/100, Train Loss: 0.0337, Val Loss: 0.0458\n",
            "Epoch 63/100, Train Loss: 0.0341, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0339, Val Loss: 0.0458\n",
            "Epoch 65/100, Train Loss: 0.0334, Val Loss: 0.0458\n",
            "Epoch 66/100, Train Loss: 0.0339, Val Loss: 0.0458\n",
            "Epoch 67/100, Train Loss: 0.0353, Val Loss: 0.0458\n",
            "Epoch 68/100, Train Loss: 0.0330, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0339, Val Loss: 0.0458\n",
            "Epoch 70/100, Train Loss: 0.0374, Val Loss: 0.0458\n",
            "Epoch 71/100, Train Loss: 0.0303, Val Loss: 0.0458\n",
            "Epoch 72/100, Train Loss: 0.0367, Val Loss: 0.0458\n",
            "Epoch 73/100, Train Loss: 0.0342, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0346, Val Loss: 0.0458\n",
            "Epoch 75/100, Train Loss: 0.0380, Val Loss: 0.0458\n",
            "Epoch 76/100, Train Loss: 0.0373, Val Loss: 0.0458\n",
            "Epoch 77/100, Train Loss: 0.0365, Val Loss: 0.0458\n",
            "Epoch 78/100, Train Loss: 0.0332, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0338, Val Loss: 0.0458\n",
            "Epoch 80/100, Train Loss: 0.0340, Val Loss: 0.0458\n",
            "Epoch 81/100, Train Loss: 0.0343, Val Loss: 0.0458\n",
            "Epoch 82/100, Train Loss: 0.0366, Val Loss: 0.0458\n",
            "Epoch 83/100, Train Loss: 0.0353, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0351, Val Loss: 0.0458\n",
            "Epoch 85/100, Train Loss: 0.0366, Val Loss: 0.0458\n",
            "Epoch 86/100, Train Loss: 0.0352, Val Loss: 0.0458\n",
            "Epoch 87/100, Train Loss: 0.0367, Val Loss: 0.0458\n",
            "Epoch 88/100, Train Loss: 0.0306, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0336, Val Loss: 0.0458\n",
            "Epoch 90/100, Train Loss: 0.0345, Val Loss: 0.0458\n",
            "Epoch 91/100, Train Loss: 0.0332, Val Loss: 0.0458\n",
            "Epoch 92/100, Train Loss: 0.0319, Val Loss: 0.0458\n",
            "Epoch 93/100, Train Loss: 0.0359, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0351, Val Loss: 0.0458\n",
            "Epoch 95/100, Train Loss: 0.0310, Val Loss: 0.0458\n",
            "Epoch 96/100, Train Loss: 0.0362, Val Loss: 0.0458\n",
            "Epoch 97/100, Train Loss: 0.0353, Val Loss: 0.0458\n",
            "Epoch 98/100, Train Loss: 0.0362, Val Loss: 0.0458\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0367, Val Loss: 0.0458\n",
            "Epoch 100/100, Train Loss: 0.0308, Val Loss: 0.0458\n",
            "\n",
            "Test iteration 3/28\n",
            "Current training set size: 99 samples\n",
            "Epoch 1/100, Train Loss: 0.1651, Val Loss: 0.1717\n",
            "Epoch 2/100, Train Loss: 0.1053, Val Loss: 0.1147\n",
            "Epoch 3/100, Train Loss: 0.0867, Val Loss: 0.0887\n",
            "Epoch 4/100, Train Loss: 0.0786, Val Loss: 0.0609\n",
            "Epoch 5/100, Train Loss: 0.0580, Val Loss: 0.0710\n",
            "Epoch 6/100, Train Loss: 0.0522, Val Loss: 0.0506\n",
            "Epoch 7/100, Train Loss: 0.0496, Val Loss: 0.0572\n",
            "Epoch 8/100, Train Loss: 0.0493, Val Loss: 0.0473\n",
            "Epoch 9/100, Train Loss: 0.0491, Val Loss: 0.0600\n",
            "Epoch 10/100, Train Loss: 0.0459, Val Loss: 0.0494\n",
            "Epoch 11/100, Train Loss: 0.0420, Val Loss: 0.0418\n",
            "Epoch 12/100, Train Loss: 0.0473, Val Loss: 0.0447\n",
            "Epoch 13/100, Train Loss: 0.0430, Val Loss: 0.0385\n",
            "Epoch 14/100, Train Loss: 0.0479, Val Loss: 0.0493\n",
            "Epoch 15/100, Train Loss: 0.0414, Val Loss: 0.0425\n",
            "Epoch 16/100, Train Loss: 0.0411, Val Loss: 0.0387\n",
            "Epoch 17/100, Train Loss: 0.0417, Val Loss: 0.0418\n",
            "Epoch 18/100, Train Loss: 0.0408, Val Loss: 0.0461\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0323, Val Loss: 0.0454\n",
            "Epoch 20/100, Train Loss: 0.0325, Val Loss: 0.0446\n",
            "Epoch 21/100, Train Loss: 0.0377, Val Loss: 0.0439\n",
            "Epoch 22/100, Train Loss: 0.0369, Val Loss: 0.0431\n",
            "Epoch 23/100, Train Loss: 0.0358, Val Loss: 0.0424\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0334, Val Loss: 0.0424\n",
            "Epoch 25/100, Train Loss: 0.0371, Val Loss: 0.0424\n",
            "Epoch 26/100, Train Loss: 0.0339, Val Loss: 0.0424\n",
            "Epoch 27/100, Train Loss: 0.0325, Val Loss: 0.0424\n",
            "Epoch 28/100, Train Loss: 0.0373, Val Loss: 0.0424\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0354, Val Loss: 0.0424\n",
            "Epoch 30/100, Train Loss: 0.0366, Val Loss: 0.0424\n",
            "Epoch 31/100, Train Loss: 0.0367, Val Loss: 0.0424\n",
            "Epoch 32/100, Train Loss: 0.0347, Val Loss: 0.0424\n",
            "Epoch 33/100, Train Loss: 0.0350, Val Loss: 0.0424\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0309, Val Loss: 0.0424\n",
            "Epoch 35/100, Train Loss: 0.0357, Val Loss: 0.0424\n",
            "Epoch 36/100, Train Loss: 0.0356, Val Loss: 0.0424\n",
            "Epoch 37/100, Train Loss: 0.0325, Val Loss: 0.0424\n",
            "Epoch 38/100, Train Loss: 0.0336, Val Loss: 0.0424\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0361, Val Loss: 0.0424\n",
            "Epoch 40/100, Train Loss: 0.0307, Val Loss: 0.0424\n",
            "Epoch 41/100, Train Loss: 0.0359, Val Loss: 0.0424\n",
            "Epoch 42/100, Train Loss: 0.0365, Val Loss: 0.0424\n",
            "Epoch 43/100, Train Loss: 0.0325, Val Loss: 0.0424\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0375, Val Loss: 0.0424\n",
            "Epoch 45/100, Train Loss: 0.0345, Val Loss: 0.0424\n",
            "Epoch 46/100, Train Loss: 0.0348, Val Loss: 0.0424\n",
            "Epoch 47/100, Train Loss: 0.0343, Val Loss: 0.0424\n",
            "Epoch 48/100, Train Loss: 0.0351, Val Loss: 0.0424\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0372, Val Loss: 0.0424\n",
            "Epoch 50/100, Train Loss: 0.0354, Val Loss: 0.0424\n",
            "Epoch 51/100, Train Loss: 0.0353, Val Loss: 0.0424\n",
            "Epoch 52/100, Train Loss: 0.0385, Val Loss: 0.0424\n",
            "Epoch 53/100, Train Loss: 0.0317, Val Loss: 0.0424\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0345, Val Loss: 0.0424\n",
            "Epoch 55/100, Train Loss: 0.0347, Val Loss: 0.0424\n",
            "Epoch 56/100, Train Loss: 0.0357, Val Loss: 0.0424\n",
            "Epoch 57/100, Train Loss: 0.0319, Val Loss: 0.0424\n",
            "Epoch 58/100, Train Loss: 0.0375, Val Loss: 0.0424\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0346, Val Loss: 0.0424\n",
            "Epoch 60/100, Train Loss: 0.0305, Val Loss: 0.0424\n",
            "Epoch 61/100, Train Loss: 0.0349, Val Loss: 0.0424\n",
            "Epoch 62/100, Train Loss: 0.0350, Val Loss: 0.0424\n",
            "Epoch 63/100, Train Loss: 0.0324, Val Loss: 0.0424\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0353, Val Loss: 0.0424\n",
            "Epoch 65/100, Train Loss: 0.0375, Val Loss: 0.0424\n",
            "Epoch 66/100, Train Loss: 0.0400, Val Loss: 0.0424\n",
            "Epoch 67/100, Train Loss: 0.0298, Val Loss: 0.0424\n",
            "Epoch 68/100, Train Loss: 0.0345, Val Loss: 0.0424\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0359, Val Loss: 0.0424\n",
            "Epoch 70/100, Train Loss: 0.0377, Val Loss: 0.0424\n",
            "Epoch 71/100, Train Loss: 0.0328, Val Loss: 0.0424\n",
            "Epoch 72/100, Train Loss: 0.0366, Val Loss: 0.0424\n",
            "Epoch 73/100, Train Loss: 0.0331, Val Loss: 0.0424\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0364, Val Loss: 0.0424\n",
            "Epoch 75/100, Train Loss: 0.0342, Val Loss: 0.0424\n",
            "Epoch 76/100, Train Loss: 0.0328, Val Loss: 0.0424\n",
            "Epoch 77/100, Train Loss: 0.0392, Val Loss: 0.0424\n",
            "Epoch 78/100, Train Loss: 0.0340, Val Loss: 0.0424\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0406, Val Loss: 0.0424\n",
            "Epoch 80/100, Train Loss: 0.0343, Val Loss: 0.0424\n",
            "Epoch 81/100, Train Loss: 0.0323, Val Loss: 0.0424\n",
            "Epoch 82/100, Train Loss: 0.0340, Val Loss: 0.0424\n",
            "Epoch 83/100, Train Loss: 0.0319, Val Loss: 0.0424\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0365, Val Loss: 0.0424\n",
            "Epoch 85/100, Train Loss: 0.0336, Val Loss: 0.0424\n",
            "Epoch 86/100, Train Loss: 0.0351, Val Loss: 0.0424\n",
            "Epoch 87/100, Train Loss: 0.0392, Val Loss: 0.0424\n",
            "Epoch 88/100, Train Loss: 0.0324, Val Loss: 0.0424\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0350, Val Loss: 0.0424\n",
            "Epoch 90/100, Train Loss: 0.0336, Val Loss: 0.0424\n",
            "Epoch 91/100, Train Loss: 0.0356, Val Loss: 0.0424\n",
            "Epoch 92/100, Train Loss: 0.0358, Val Loss: 0.0424\n",
            "Epoch 93/100, Train Loss: 0.0291, Val Loss: 0.0424\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0349, Val Loss: 0.0424\n",
            "Epoch 95/100, Train Loss: 0.0346, Val Loss: 0.0424\n",
            "Epoch 96/100, Train Loss: 0.0352, Val Loss: 0.0424\n",
            "Epoch 97/100, Train Loss: 0.0363, Val Loss: 0.0424\n",
            "Epoch 98/100, Train Loss: 0.0349, Val Loss: 0.0424\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0344, Val Loss: 0.0424\n",
            "Epoch 100/100, Train Loss: 0.0349, Val Loss: 0.0424\n",
            "\n",
            "Test iteration 4/28\n",
            "Current training set size: 100 samples\n",
            "Epoch 1/100, Train Loss: 0.1661, Val Loss: 0.1684\n",
            "Epoch 2/100, Train Loss: 0.1053, Val Loss: 0.0978\n",
            "Epoch 3/100, Train Loss: 0.0884, Val Loss: 0.0935\n",
            "Epoch 4/100, Train Loss: 0.0656, Val Loss: 0.0667\n",
            "Epoch 5/100, Train Loss: 0.0597, Val Loss: 0.0449\n",
            "Epoch 6/100, Train Loss: 0.0540, Val Loss: 0.0644\n",
            "Epoch 7/100, Train Loss: 0.0556, Val Loss: 0.0564\n",
            "Epoch 8/100, Train Loss: 0.0503, Val Loss: 0.0457\n",
            "Epoch 9/100, Train Loss: 0.0468, Val Loss: 0.0681\n",
            "Epoch 10/100, Train Loss: 0.0604, Val Loss: 0.0518\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 11/100, Train Loss: 0.0497, Val Loss: 0.0489\n",
            "Epoch 12/100, Train Loss: 0.0488, Val Loss: 0.0457\n",
            "Epoch 13/100, Train Loss: 0.0400, Val Loss: 0.0432\n",
            "Epoch 14/100, Train Loss: 0.0414, Val Loss: 0.0412\n",
            "Epoch 15/100, Train Loss: 0.0441, Val Loss: 0.0407\n",
            "Epoch 16/100, Train Loss: 0.0378, Val Loss: 0.0403\n",
            "Epoch 17/100, Train Loss: 0.0414, Val Loss: 0.0400\n",
            "Epoch 18/100, Train Loss: 0.0373, Val Loss: 0.0400\n",
            "Epoch 19/100, Train Loss: 0.0384, Val Loss: 0.0402\n",
            "Epoch 20/100, Train Loss: 0.0402, Val Loss: 0.0403\n",
            "Epoch 21/100, Train Loss: 0.0395, Val Loss: 0.0404\n",
            "Epoch 22/100, Train Loss: 0.0404, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0370, Val Loss: 0.0403\n",
            "Epoch 24/100, Train Loss: 0.0364, Val Loss: 0.0403\n",
            "Epoch 25/100, Train Loss: 0.0386, Val Loss: 0.0403\n",
            "Epoch 26/100, Train Loss: 0.0378, Val Loss: 0.0403\n",
            "Epoch 27/100, Train Loss: 0.0366, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0366, Val Loss: 0.0403\n",
            "Epoch 29/100, Train Loss: 0.0361, Val Loss: 0.0403\n",
            "Epoch 30/100, Train Loss: 0.0418, Val Loss: 0.0403\n",
            "Epoch 31/100, Train Loss: 0.0371, Val Loss: 0.0403\n",
            "Epoch 32/100, Train Loss: 0.0402, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0397, Val Loss: 0.0403\n",
            "Epoch 34/100, Train Loss: 0.0362, Val Loss: 0.0403\n",
            "Epoch 35/100, Train Loss: 0.0365, Val Loss: 0.0403\n",
            "Epoch 36/100, Train Loss: 0.0375, Val Loss: 0.0403\n",
            "Epoch 37/100, Train Loss: 0.0380, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0365, Val Loss: 0.0403\n",
            "Epoch 39/100, Train Loss: 0.0321, Val Loss: 0.0403\n",
            "Epoch 40/100, Train Loss: 0.0368, Val Loss: 0.0403\n",
            "Epoch 41/100, Train Loss: 0.0377, Val Loss: 0.0403\n",
            "Epoch 42/100, Train Loss: 0.0401, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0385, Val Loss: 0.0403\n",
            "Epoch 44/100, Train Loss: 0.0394, Val Loss: 0.0403\n",
            "Epoch 45/100, Train Loss: 0.0372, Val Loss: 0.0403\n",
            "Epoch 46/100, Train Loss: 0.0429, Val Loss: 0.0403\n",
            "Epoch 47/100, Train Loss: 0.0389, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0428, Val Loss: 0.0403\n",
            "Epoch 49/100, Train Loss: 0.0388, Val Loss: 0.0403\n",
            "Epoch 50/100, Train Loss: 0.0395, Val Loss: 0.0403\n",
            "Epoch 51/100, Train Loss: 0.0348, Val Loss: 0.0403\n",
            "Epoch 52/100, Train Loss: 0.0374, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0400, Val Loss: 0.0403\n",
            "Epoch 54/100, Train Loss: 0.0382, Val Loss: 0.0403\n",
            "Epoch 55/100, Train Loss: 0.0403, Val Loss: 0.0403\n",
            "Epoch 56/100, Train Loss: 0.0447, Val Loss: 0.0403\n",
            "Epoch 57/100, Train Loss: 0.0370, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0376, Val Loss: 0.0403\n",
            "Epoch 59/100, Train Loss: 0.0417, Val Loss: 0.0403\n",
            "Epoch 60/100, Train Loss: 0.0382, Val Loss: 0.0403\n",
            "Epoch 61/100, Train Loss: 0.0405, Val Loss: 0.0403\n",
            "Epoch 62/100, Train Loss: 0.0399, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0452, Val Loss: 0.0403\n",
            "Epoch 64/100, Train Loss: 0.0394, Val Loss: 0.0403\n",
            "Epoch 65/100, Train Loss: 0.0372, Val Loss: 0.0403\n",
            "Epoch 66/100, Train Loss: 0.0359, Val Loss: 0.0403\n",
            "Epoch 67/100, Train Loss: 0.0401, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0412, Val Loss: 0.0403\n",
            "Epoch 69/100, Train Loss: 0.0380, Val Loss: 0.0403\n",
            "Epoch 70/100, Train Loss: 0.0406, Val Loss: 0.0403\n",
            "Epoch 71/100, Train Loss: 0.0364, Val Loss: 0.0403\n",
            "Epoch 72/100, Train Loss: 0.0364, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0363, Val Loss: 0.0403\n",
            "Epoch 74/100, Train Loss: 0.0402, Val Loss: 0.0403\n",
            "Epoch 75/100, Train Loss: 0.0387, Val Loss: 0.0403\n",
            "Epoch 76/100, Train Loss: 0.0373, Val Loss: 0.0403\n",
            "Epoch 77/100, Train Loss: 0.0378, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0369, Val Loss: 0.0403\n",
            "Epoch 79/100, Train Loss: 0.0415, Val Loss: 0.0403\n",
            "Epoch 80/100, Train Loss: 0.0395, Val Loss: 0.0403\n",
            "Epoch 81/100, Train Loss: 0.0374, Val Loss: 0.0403\n",
            "Epoch 82/100, Train Loss: 0.0389, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0415, Val Loss: 0.0403\n",
            "Epoch 84/100, Train Loss: 0.0398, Val Loss: 0.0403\n",
            "Epoch 85/100, Train Loss: 0.0416, Val Loss: 0.0403\n",
            "Epoch 86/100, Train Loss: 0.0427, Val Loss: 0.0403\n",
            "Epoch 87/100, Train Loss: 0.0398, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0365, Val Loss: 0.0403\n",
            "Epoch 89/100, Train Loss: 0.0416, Val Loss: 0.0403\n",
            "Epoch 90/100, Train Loss: 0.0387, Val Loss: 0.0403\n",
            "Epoch 91/100, Train Loss: 0.0403, Val Loss: 0.0403\n",
            "Epoch 92/100, Train Loss: 0.0404, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0345, Val Loss: 0.0403\n",
            "Epoch 94/100, Train Loss: 0.0379, Val Loss: 0.0403\n",
            "Epoch 95/100, Train Loss: 0.0392, Val Loss: 0.0403\n",
            "Epoch 96/100, Train Loss: 0.0380, Val Loss: 0.0403\n",
            "Epoch 97/100, Train Loss: 0.0400, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0412, Val Loss: 0.0403\n",
            "Epoch 99/100, Train Loss: 0.0390, Val Loss: 0.0403\n",
            "Epoch 100/100, Train Loss: 0.0366, Val Loss: 0.0403\n",
            "\n",
            "Test iteration 5/28\n",
            "Current training set size: 101 samples\n",
            "Epoch 1/100, Train Loss: 0.1601, Val Loss: 0.1562\n",
            "Epoch 2/100, Train Loss: 0.1133, Val Loss: 0.1093\n",
            "Epoch 3/100, Train Loss: 0.0942, Val Loss: 0.0970\n",
            "Epoch 4/100, Train Loss: 0.0869, Val Loss: 0.0880\n",
            "Epoch 5/100, Train Loss: 0.0700, Val Loss: 0.0556\n",
            "Epoch 6/100, Train Loss: 0.0623, Val Loss: 0.0675\n",
            "Epoch 7/100, Train Loss: 0.0474, Val Loss: 0.0466\n",
            "Epoch 8/100, Train Loss: 0.0605, Val Loss: 0.0446\n",
            "Epoch 9/100, Train Loss: 0.0504, Val Loss: 0.0542\n",
            "Epoch 10/100, Train Loss: 0.0494, Val Loss: 0.0429\n",
            "Epoch 11/100, Train Loss: 0.0410, Val Loss: 0.0529\n",
            "Epoch 12/100, Train Loss: 0.0616, Val Loss: 0.0420\n",
            "Epoch 13/100, Train Loss: 0.0437, Val Loss: 0.0398\n",
            "Epoch 14/100, Train Loss: 0.0426, Val Loss: 0.0382\n",
            "Epoch 15/100, Train Loss: 0.0413, Val Loss: 0.0392\n",
            "Epoch 16/100, Train Loss: 0.0473, Val Loss: 0.0579\n",
            "Epoch 17/100, Train Loss: 0.0433, Val Loss: 0.0550\n",
            "Epoch 18/100, Train Loss: 0.0439, Val Loss: 0.0584\n",
            "Epoch 19/100, Train Loss: 0.0513, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0388, Val Loss: 0.0403\n",
            "Epoch 21/100, Train Loss: 0.0387, Val Loss: 0.0403\n",
            "Epoch 22/100, Train Loss: 0.0366, Val Loss: 0.0403\n",
            "Epoch 23/100, Train Loss: 0.0342, Val Loss: 0.0403\n",
            "Epoch 24/100, Train Loss: 0.0383, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0380, Val Loss: 0.0404\n",
            "Epoch 26/100, Train Loss: 0.0406, Val Loss: 0.0404\n",
            "Epoch 27/100, Train Loss: 0.0338, Val Loss: 0.0404\n",
            "Epoch 28/100, Train Loss: 0.0354, Val Loss: 0.0404\n",
            "Epoch 29/100, Train Loss: 0.0360, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0388, Val Loss: 0.0404\n",
            "Epoch 31/100, Train Loss: 0.0366, Val Loss: 0.0404\n",
            "Epoch 32/100, Train Loss: 0.0422, Val Loss: 0.0404\n",
            "Epoch 33/100, Train Loss: 0.0354, Val Loss: 0.0404\n",
            "Epoch 34/100, Train Loss: 0.0366, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0387, Val Loss: 0.0404\n",
            "Epoch 36/100, Train Loss: 0.0374, Val Loss: 0.0404\n",
            "Epoch 37/100, Train Loss: 0.0372, Val Loss: 0.0404\n",
            "Epoch 38/100, Train Loss: 0.0353, Val Loss: 0.0404\n",
            "Epoch 39/100, Train Loss: 0.0407, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0395, Val Loss: 0.0404\n",
            "Epoch 41/100, Train Loss: 0.0383, Val Loss: 0.0404\n",
            "Epoch 42/100, Train Loss: 0.0389, Val Loss: 0.0404\n",
            "Epoch 43/100, Train Loss: 0.0375, Val Loss: 0.0404\n",
            "Epoch 44/100, Train Loss: 0.0383, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0379, Val Loss: 0.0404\n",
            "Epoch 46/100, Train Loss: 0.0346, Val Loss: 0.0404\n",
            "Epoch 47/100, Train Loss: 0.0404, Val Loss: 0.0404\n",
            "Epoch 48/100, Train Loss: 0.0370, Val Loss: 0.0404\n",
            "Epoch 49/100, Train Loss: 0.0387, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0389, Val Loss: 0.0404\n",
            "Epoch 51/100, Train Loss: 0.0412, Val Loss: 0.0404\n",
            "Epoch 52/100, Train Loss: 0.0345, Val Loss: 0.0404\n",
            "Epoch 53/100, Train Loss: 0.0366, Val Loss: 0.0404\n",
            "Epoch 54/100, Train Loss: 0.0395, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0359, Val Loss: 0.0404\n",
            "Epoch 56/100, Train Loss: 0.0318, Val Loss: 0.0404\n",
            "Epoch 57/100, Train Loss: 0.0348, Val Loss: 0.0404\n",
            "Epoch 58/100, Train Loss: 0.0374, Val Loss: 0.0404\n",
            "Epoch 59/100, Train Loss: 0.0389, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0364, Val Loss: 0.0404\n",
            "Epoch 61/100, Train Loss: 0.0378, Val Loss: 0.0404\n",
            "Epoch 62/100, Train Loss: 0.0397, Val Loss: 0.0404\n",
            "Epoch 63/100, Train Loss: 0.0373, Val Loss: 0.0404\n",
            "Epoch 64/100, Train Loss: 0.0390, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0404, Val Loss: 0.0404\n",
            "Epoch 66/100, Train Loss: 0.0349, Val Loss: 0.0404\n",
            "Epoch 67/100, Train Loss: 0.0404, Val Loss: 0.0404\n",
            "Epoch 68/100, Train Loss: 0.0402, Val Loss: 0.0404\n",
            "Epoch 69/100, Train Loss: 0.0374, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0352, Val Loss: 0.0404\n",
            "Epoch 71/100, Train Loss: 0.0351, Val Loss: 0.0404\n",
            "Epoch 72/100, Train Loss: 0.0380, Val Loss: 0.0404\n",
            "Epoch 73/100, Train Loss: 0.0391, Val Loss: 0.0404\n",
            "Epoch 74/100, Train Loss: 0.0376, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0383, Val Loss: 0.0404\n",
            "Epoch 76/100, Train Loss: 0.0366, Val Loss: 0.0404\n",
            "Epoch 77/100, Train Loss: 0.0389, Val Loss: 0.0404\n",
            "Epoch 78/100, Train Loss: 0.0425, Val Loss: 0.0404\n",
            "Epoch 79/100, Train Loss: 0.0325, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0316, Val Loss: 0.0404\n",
            "Epoch 81/100, Train Loss: 0.0351, Val Loss: 0.0404\n",
            "Epoch 82/100, Train Loss: 0.0371, Val Loss: 0.0404\n",
            "Epoch 83/100, Train Loss: 0.0379, Val Loss: 0.0404\n",
            "Epoch 84/100, Train Loss: 0.0423, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0352, Val Loss: 0.0404\n",
            "Epoch 86/100, Train Loss: 0.0360, Val Loss: 0.0404\n",
            "Epoch 87/100, Train Loss: 0.0341, Val Loss: 0.0404\n",
            "Epoch 88/100, Train Loss: 0.0394, Val Loss: 0.0404\n",
            "Epoch 89/100, Train Loss: 0.0380, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0352, Val Loss: 0.0404\n",
            "Epoch 91/100, Train Loss: 0.0350, Val Loss: 0.0404\n",
            "Epoch 92/100, Train Loss: 0.0347, Val Loss: 0.0404\n",
            "Epoch 93/100, Train Loss: 0.0367, Val Loss: 0.0404\n",
            "Epoch 94/100, Train Loss: 0.0356, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0404, Val Loss: 0.0404\n",
            "Epoch 96/100, Train Loss: 0.0340, Val Loss: 0.0404\n",
            "Epoch 97/100, Train Loss: 0.0412, Val Loss: 0.0404\n",
            "Epoch 98/100, Train Loss: 0.0367, Val Loss: 0.0404\n",
            "Epoch 99/100, Train Loss: 0.0390, Val Loss: 0.0404\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0369, Val Loss: 0.0404\n",
            "\n",
            "Test iteration 6/28\n",
            "Current training set size: 102 samples\n",
            "Epoch 1/100, Train Loss: 0.1562, Val Loss: 0.1657\n",
            "Epoch 2/100, Train Loss: 0.1302, Val Loss: 0.1011\n",
            "Epoch 3/100, Train Loss: 0.1014, Val Loss: 0.0966\n",
            "Epoch 4/100, Train Loss: 0.0815, Val Loss: 0.1002\n",
            "Epoch 5/100, Train Loss: 0.0618, Val Loss: 0.0598\n",
            "Epoch 6/100, Train Loss: 0.0608, Val Loss: 0.0497\n",
            "Epoch 7/100, Train Loss: 0.0498, Val Loss: 0.0586\n",
            "Epoch 8/100, Train Loss: 0.0458, Val Loss: 0.0407\n",
            "Epoch 9/100, Train Loss: 0.0448, Val Loss: 0.0487\n",
            "Epoch 10/100, Train Loss: 0.0452, Val Loss: 0.0401\n",
            "Epoch 11/100, Train Loss: 0.0417, Val Loss: 0.0507\n",
            "Epoch 12/100, Train Loss: 0.0437, Val Loss: 0.0429\n",
            "Epoch 13/100, Train Loss: 0.0479, Val Loss: 0.0526\n",
            "Epoch 14/100, Train Loss: 0.0426, Val Loss: 0.0430\n",
            "Epoch 15/100, Train Loss: 0.0399, Val Loss: 0.0427\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Epoch 17/100, Train Loss: 0.0406, Val Loss: 0.0422\n",
            "Epoch 18/100, Train Loss: 0.0394, Val Loss: 0.0417\n",
            "Epoch 19/100, Train Loss: 0.0376, Val Loss: 0.0412\n",
            "Epoch 20/100, Train Loss: 0.0336, Val Loss: 0.0411\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0363, Val Loss: 0.0411\n",
            "Epoch 22/100, Train Loss: 0.0342, Val Loss: 0.0411\n",
            "Epoch 23/100, Train Loss: 0.0330, Val Loss: 0.0411\n",
            "Epoch 24/100, Train Loss: 0.0358, Val Loss: 0.0410\n",
            "Epoch 25/100, Train Loss: 0.0322, Val Loss: 0.0410\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0320, Val Loss: 0.0410\n",
            "Epoch 27/100, Train Loss: 0.0354, Val Loss: 0.0410\n",
            "Epoch 28/100, Train Loss: 0.0386, Val Loss: 0.0410\n",
            "Epoch 29/100, Train Loss: 0.0335, Val Loss: 0.0410\n",
            "Epoch 30/100, Train Loss: 0.0377, Val Loss: 0.0410\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0394, Val Loss: 0.0410\n",
            "Epoch 32/100, Train Loss: 0.0329, Val Loss: 0.0410\n",
            "Epoch 33/100, Train Loss: 0.0371, Val Loss: 0.0410\n",
            "Epoch 34/100, Train Loss: 0.0371, Val Loss: 0.0410\n",
            "Epoch 35/100, Train Loss: 0.0368, Val Loss: 0.0410\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0346, Val Loss: 0.0410\n",
            "Epoch 37/100, Train Loss: 0.0397, Val Loss: 0.0410\n",
            "Epoch 38/100, Train Loss: 0.0364, Val Loss: 0.0410\n",
            "Epoch 39/100, Train Loss: 0.0328, Val Loss: 0.0410\n",
            "Epoch 40/100, Train Loss: 0.0401, Val Loss: 0.0410\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0349, Val Loss: 0.0410\n",
            "Epoch 42/100, Train Loss: 0.0374, Val Loss: 0.0410\n",
            "Epoch 43/100, Train Loss: 0.0349, Val Loss: 0.0410\n",
            "Epoch 44/100, Train Loss: 0.0352, Val Loss: 0.0410\n",
            "Epoch 45/100, Train Loss: 0.0370, Val Loss: 0.0410\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0359, Val Loss: 0.0410\n",
            "Epoch 47/100, Train Loss: 0.0364, Val Loss: 0.0410\n",
            "Epoch 48/100, Train Loss: 0.0348, Val Loss: 0.0410\n",
            "Epoch 49/100, Train Loss: 0.0362, Val Loss: 0.0410\n",
            "Epoch 50/100, Train Loss: 0.0360, Val Loss: 0.0410\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0361, Val Loss: 0.0410\n",
            "Epoch 52/100, Train Loss: 0.0379, Val Loss: 0.0410\n",
            "Epoch 53/100, Train Loss: 0.0351, Val Loss: 0.0410\n",
            "Epoch 54/100, Train Loss: 0.0344, Val Loss: 0.0410\n",
            "Epoch 55/100, Train Loss: 0.0306, Val Loss: 0.0410\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0342, Val Loss: 0.0410\n",
            "Epoch 57/100, Train Loss: 0.0388, Val Loss: 0.0410\n",
            "Epoch 58/100, Train Loss: 0.0385, Val Loss: 0.0410\n",
            "Epoch 59/100, Train Loss: 0.0337, Val Loss: 0.0410\n",
            "Epoch 60/100, Train Loss: 0.0340, Val Loss: 0.0410\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0362, Val Loss: 0.0410\n",
            "Epoch 62/100, Train Loss: 0.0338, Val Loss: 0.0410\n",
            "Epoch 63/100, Train Loss: 0.0364, Val Loss: 0.0410\n",
            "Epoch 64/100, Train Loss: 0.0361, Val Loss: 0.0410\n",
            "Epoch 65/100, Train Loss: 0.0322, Val Loss: 0.0410\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0346, Val Loss: 0.0410\n",
            "Epoch 67/100, Train Loss: 0.0346, Val Loss: 0.0410\n",
            "Epoch 68/100, Train Loss: 0.0368, Val Loss: 0.0410\n",
            "Epoch 69/100, Train Loss: 0.0362, Val Loss: 0.0410\n",
            "Epoch 70/100, Train Loss: 0.0372, Val Loss: 0.0410\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0352, Val Loss: 0.0410\n",
            "Epoch 72/100, Train Loss: 0.0395, Val Loss: 0.0410\n",
            "Epoch 73/100, Train Loss: 0.0367, Val Loss: 0.0410\n",
            "Epoch 74/100, Train Loss: 0.0341, Val Loss: 0.0410\n",
            "Epoch 75/100, Train Loss: 0.0364, Val Loss: 0.0410\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0369, Val Loss: 0.0410\n",
            "Epoch 77/100, Train Loss: 0.0385, Val Loss: 0.0410\n",
            "Epoch 78/100, Train Loss: 0.0384, Val Loss: 0.0410\n",
            "Epoch 79/100, Train Loss: 0.0335, Val Loss: 0.0410\n",
            "Epoch 80/100, Train Loss: 0.0337, Val Loss: 0.0410\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0358, Val Loss: 0.0410\n",
            "Epoch 82/100, Train Loss: 0.0375, Val Loss: 0.0410\n",
            "Epoch 83/100, Train Loss: 0.0354, Val Loss: 0.0410\n",
            "Epoch 84/100, Train Loss: 0.0368, Val Loss: 0.0410\n",
            "Epoch 85/100, Train Loss: 0.0319, Val Loss: 0.0410\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0347, Val Loss: 0.0410\n",
            "Epoch 87/100, Train Loss: 0.0348, Val Loss: 0.0410\n",
            "Epoch 88/100, Train Loss: 0.0359, Val Loss: 0.0410\n",
            "Epoch 89/100, Train Loss: 0.0358, Val Loss: 0.0410\n",
            "Epoch 90/100, Train Loss: 0.0324, Val Loss: 0.0410\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0347, Val Loss: 0.0410\n",
            "Epoch 92/100, Train Loss: 0.0349, Val Loss: 0.0410\n",
            "Epoch 93/100, Train Loss: 0.0358, Val Loss: 0.0410\n",
            "Epoch 94/100, Train Loss: 0.0394, Val Loss: 0.0410\n",
            "Epoch 95/100, Train Loss: 0.0356, Val Loss: 0.0410\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0347, Val Loss: 0.0410\n",
            "Epoch 97/100, Train Loss: 0.0332, Val Loss: 0.0410\n",
            "Epoch 98/100, Train Loss: 0.0341, Val Loss: 0.0410\n",
            "Epoch 99/100, Train Loss: 0.0376, Val Loss: 0.0410\n",
            "Epoch 100/100, Train Loss: 0.0404, Val Loss: 0.0410\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 7/28\n",
            "Current training set size: 103 samples\n",
            "Epoch 1/100, Train Loss: 0.1714, Val Loss: 0.1936\n",
            "Epoch 2/100, Train Loss: 0.1303, Val Loss: 0.1079\n",
            "Epoch 3/100, Train Loss: 0.0862, Val Loss: 0.0737\n",
            "Epoch 4/100, Train Loss: 0.0660, Val Loss: 0.0711\n",
            "Epoch 5/100, Train Loss: 0.0550, Val Loss: 0.0587\n",
            "Epoch 6/100, Train Loss: 0.0621, Val Loss: 0.0541\n",
            "Epoch 7/100, Train Loss: 0.0522, Val Loss: 0.0509\n",
            "Epoch 8/100, Train Loss: 0.0411, Val Loss: 0.0441\n",
            "Epoch 9/100, Train Loss: 0.0441, Val Loss: 0.0613\n",
            "Epoch 10/100, Train Loss: 0.0455, Val Loss: 0.0407\n",
            "Epoch 11/100, Train Loss: 0.0605, Val Loss: 0.0478\n",
            "Epoch 12/100, Train Loss: 0.0484, Val Loss: 0.0490\n",
            "Epoch 13/100, Train Loss: 0.0381, Val Loss: 0.0414\n",
            "Epoch 14/100, Train Loss: 0.0457, Val Loss: 0.0413\n",
            "Epoch 15/100, Train Loss: 0.0400, Val Loss: 0.0414\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0382, Val Loss: 0.0414\n",
            "Epoch 17/100, Train Loss: 0.0365, Val Loss: 0.0416\n",
            "Epoch 18/100, Train Loss: 0.0376, Val Loss: 0.0416\n",
            "Epoch 19/100, Train Loss: 0.0383, Val Loss: 0.0417\n",
            "Epoch 20/100, Train Loss: 0.0354, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0389, Val Loss: 0.0417\n",
            "Epoch 22/100, Train Loss: 0.0314, Val Loss: 0.0417\n",
            "Epoch 23/100, Train Loss: 0.0347, Val Loss: 0.0417\n",
            "Epoch 24/100, Train Loss: 0.0335, Val Loss: 0.0417\n",
            "Epoch 25/100, Train Loss: 0.0335, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0317, Val Loss: 0.0417\n",
            "Epoch 27/100, Train Loss: 0.0369, Val Loss: 0.0417\n",
            "Epoch 28/100, Train Loss: 0.0378, Val Loss: 0.0417\n",
            "Epoch 29/100, Train Loss: 0.0357, Val Loss: 0.0417\n",
            "Epoch 30/100, Train Loss: 0.0323, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0353, Val Loss: 0.0417\n",
            "Epoch 32/100, Train Loss: 0.0339, Val Loss: 0.0417\n",
            "Epoch 33/100, Train Loss: 0.0338, Val Loss: 0.0417\n",
            "Epoch 34/100, Train Loss: 0.0356, Val Loss: 0.0417\n",
            "Epoch 35/100, Train Loss: 0.0342, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0384, Val Loss: 0.0417\n",
            "Epoch 37/100, Train Loss: 0.0345, Val Loss: 0.0417\n",
            "Epoch 38/100, Train Loss: 0.0335, Val Loss: 0.0417\n",
            "Epoch 39/100, Train Loss: 0.0339, Val Loss: 0.0417\n",
            "Epoch 40/100, Train Loss: 0.0356, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0326, Val Loss: 0.0417\n",
            "Epoch 42/100, Train Loss: 0.0339, Val Loss: 0.0417\n",
            "Epoch 43/100, Train Loss: 0.0351, Val Loss: 0.0417\n",
            "Epoch 44/100, Train Loss: 0.0347, Val Loss: 0.0417\n",
            "Epoch 45/100, Train Loss: 0.0362, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0349, Val Loss: 0.0417\n",
            "Epoch 47/100, Train Loss: 0.0359, Val Loss: 0.0417\n",
            "Epoch 48/100, Train Loss: 0.0357, Val Loss: 0.0417\n",
            "Epoch 49/100, Train Loss: 0.0327, Val Loss: 0.0417\n",
            "Epoch 50/100, Train Loss: 0.0324, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0396, Val Loss: 0.0417\n",
            "Epoch 52/100, Train Loss: 0.0346, Val Loss: 0.0417\n",
            "Epoch 53/100, Train Loss: 0.0358, Val Loss: 0.0417\n",
            "Epoch 54/100, Train Loss: 0.0370, Val Loss: 0.0417\n",
            "Epoch 55/100, Train Loss: 0.0343, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0355, Val Loss: 0.0417\n",
            "Epoch 57/100, Train Loss: 0.0359, Val Loss: 0.0417\n",
            "Epoch 58/100, Train Loss: 0.0350, Val Loss: 0.0417\n",
            "Epoch 59/100, Train Loss: 0.0326, Val Loss: 0.0417\n",
            "Epoch 60/100, Train Loss: 0.0320, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0353, Val Loss: 0.0417\n",
            "Epoch 62/100, Train Loss: 0.0338, Val Loss: 0.0417\n",
            "Epoch 63/100, Train Loss: 0.0355, Val Loss: 0.0417\n",
            "Epoch 64/100, Train Loss: 0.0330, Val Loss: 0.0417\n",
            "Epoch 65/100, Train Loss: 0.0371, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0344, Val Loss: 0.0417\n",
            "Epoch 67/100, Train Loss: 0.0348, Val Loss: 0.0417\n",
            "Epoch 68/100, Train Loss: 0.0358, Val Loss: 0.0417\n",
            "Epoch 69/100, Train Loss: 0.0337, Val Loss: 0.0417\n",
            "Epoch 70/100, Train Loss: 0.0367, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0358, Val Loss: 0.0417\n",
            "Epoch 72/100, Train Loss: 0.0338, Val Loss: 0.0417\n",
            "Epoch 73/100, Train Loss: 0.0307, Val Loss: 0.0417\n",
            "Epoch 74/100, Train Loss: 0.0356, Val Loss: 0.0417\n",
            "Epoch 75/100, Train Loss: 0.0341, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0328, Val Loss: 0.0417\n",
            "Epoch 77/100, Train Loss: 0.0319, Val Loss: 0.0417\n",
            "Epoch 78/100, Train Loss: 0.0375, Val Loss: 0.0417\n",
            "Epoch 79/100, Train Loss: 0.0311, Val Loss: 0.0417\n",
            "Epoch 80/100, Train Loss: 0.0344, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0360, Val Loss: 0.0417\n",
            "Epoch 82/100, Train Loss: 0.0342, Val Loss: 0.0417\n",
            "Epoch 83/100, Train Loss: 0.0329, Val Loss: 0.0417\n",
            "Epoch 84/100, Train Loss: 0.0384, Val Loss: 0.0417\n",
            "Epoch 85/100, Train Loss: 0.0363, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0337, Val Loss: 0.0417\n",
            "Epoch 87/100, Train Loss: 0.0342, Val Loss: 0.0417\n",
            "Epoch 88/100, Train Loss: 0.0340, Val Loss: 0.0417\n",
            "Epoch 89/100, Train Loss: 0.0357, Val Loss: 0.0417\n",
            "Epoch 90/100, Train Loss: 0.0335, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0347, Val Loss: 0.0417\n",
            "Epoch 92/100, Train Loss: 0.0336, Val Loss: 0.0417\n",
            "Epoch 93/100, Train Loss: 0.0325, Val Loss: 0.0417\n",
            "Epoch 94/100, Train Loss: 0.0331, Val Loss: 0.0417\n",
            "Epoch 95/100, Train Loss: 0.0379, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0334, Val Loss: 0.0417\n",
            "Epoch 97/100, Train Loss: 0.0338, Val Loss: 0.0417\n",
            "Epoch 98/100, Train Loss: 0.0333, Val Loss: 0.0417\n",
            "Epoch 99/100, Train Loss: 0.0354, Val Loss: 0.0417\n",
            "Epoch 100/100, Train Loss: 0.0342, Val Loss: 0.0417\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 8/28\n",
            "Current training set size: 104 samples\n",
            "Epoch 1/100, Train Loss: 0.1623, Val Loss: 0.1677\n",
            "Epoch 2/100, Train Loss: 0.1166, Val Loss: 0.1262\n",
            "Epoch 3/100, Train Loss: 0.0963, Val Loss: 0.0903\n",
            "Epoch 4/100, Train Loss: 0.0785, Val Loss: 0.0961\n",
            "Epoch 5/100, Train Loss: 0.0717, Val Loss: 0.0449\n",
            "Epoch 6/100, Train Loss: 0.0520, Val Loss: 0.0438\n",
            "Epoch 7/100, Train Loss: 0.0552, Val Loss: 0.0491\n",
            "Epoch 8/100, Train Loss: 0.0501, Val Loss: 0.0469\n",
            "Epoch 9/100, Train Loss: 0.0513, Val Loss: 0.0560\n",
            "Epoch 10/100, Train Loss: 0.0430, Val Loss: 0.0450\n",
            "Epoch 11/100, Train Loss: 0.0457, Val Loss: 0.0472\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 12/100, Train Loss: 0.0393, Val Loss: 0.0463\n",
            "Epoch 13/100, Train Loss: 0.0377, Val Loss: 0.0450\n",
            "Epoch 14/100, Train Loss: 0.0384, Val Loss: 0.0440\n",
            "Epoch 15/100, Train Loss: 0.0396, Val Loss: 0.0431\n",
            "Epoch 16/100, Train Loss: 0.0383, Val Loss: 0.0425\n",
            "Epoch 17/100, Train Loss: 0.0402, Val Loss: 0.0419\n",
            "Epoch 18/100, Train Loss: 0.0385, Val Loss: 0.0414\n",
            "Epoch 19/100, Train Loss: 0.0362, Val Loss: 0.0413\n",
            "Epoch 20/100, Train Loss: 0.0366, Val Loss: 0.0408\n",
            "Epoch 21/100, Train Loss: 0.0422, Val Loss: 0.0405\n",
            "Epoch 22/100, Train Loss: 0.0394, Val Loss: 0.0401\n",
            "Epoch 23/100, Train Loss: 0.0394, Val Loss: 0.0398\n",
            "Epoch 24/100, Train Loss: 0.0361, Val Loss: 0.0396\n",
            "Epoch 25/100, Train Loss: 0.0389, Val Loss: 0.0394\n",
            "Epoch 26/100, Train Loss: 0.0353, Val Loss: 0.0391\n",
            "Epoch 27/100, Train Loss: 0.0371, Val Loss: 0.0391\n",
            "Epoch 28/100, Train Loss: 0.0376, Val Loss: 0.0390\n",
            "Epoch 29/100, Train Loss: 0.0346, Val Loss: 0.0392\n",
            "Epoch 30/100, Train Loss: 0.0356, Val Loss: 0.0389\n",
            "Epoch 31/100, Train Loss: 0.0373, Val Loss: 0.0385\n",
            "Epoch 32/100, Train Loss: 0.0313, Val Loss: 0.0385\n",
            "Epoch 33/100, Train Loss: 0.0345, Val Loss: 0.0383\n",
            "Epoch 34/100, Train Loss: 0.0354, Val Loss: 0.0385\n",
            "Epoch 35/100, Train Loss: 0.0370, Val Loss: 0.0383\n",
            "Epoch 36/100, Train Loss: 0.0386, Val Loss: 0.0385\n",
            "Epoch 37/100, Train Loss: 0.0355, Val Loss: 0.0383\n",
            "Epoch 38/100, Train Loss: 0.0359, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0386, Val Loss: 0.0385\n",
            "Epoch 40/100, Train Loss: 0.0368, Val Loss: 0.0385\n",
            "Epoch 41/100, Train Loss: 0.0367, Val Loss: 0.0385\n",
            "Epoch 42/100, Train Loss: 0.0375, Val Loss: 0.0385\n",
            "Epoch 43/100, Train Loss: 0.0335, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0385, Val Loss: 0.0385\n",
            "Epoch 45/100, Train Loss: 0.0348, Val Loss: 0.0385\n",
            "Epoch 46/100, Train Loss: 0.0309, Val Loss: 0.0385\n",
            "Epoch 47/100, Train Loss: 0.0391, Val Loss: 0.0385\n",
            "Epoch 48/100, Train Loss: 0.0385, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0322, Val Loss: 0.0385\n",
            "Epoch 50/100, Train Loss: 0.0427, Val Loss: 0.0385\n",
            "Epoch 51/100, Train Loss: 0.0370, Val Loss: 0.0385\n",
            "Epoch 52/100, Train Loss: 0.0379, Val Loss: 0.0385\n",
            "Epoch 53/100, Train Loss: 0.0349, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0366, Val Loss: 0.0385\n",
            "Epoch 55/100, Train Loss: 0.0369, Val Loss: 0.0385\n",
            "Epoch 56/100, Train Loss: 0.0385, Val Loss: 0.0385\n",
            "Epoch 57/100, Train Loss: 0.0327, Val Loss: 0.0385\n",
            "Epoch 58/100, Train Loss: 0.0396, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0371, Val Loss: 0.0385\n",
            "Epoch 60/100, Train Loss: 0.0342, Val Loss: 0.0385\n",
            "Epoch 61/100, Train Loss: 0.0360, Val Loss: 0.0385\n",
            "Epoch 62/100, Train Loss: 0.0377, Val Loss: 0.0385\n",
            "Epoch 63/100, Train Loss: 0.0333, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0380, Val Loss: 0.0385\n",
            "Epoch 65/100, Train Loss: 0.0361, Val Loss: 0.0385\n",
            "Epoch 66/100, Train Loss: 0.0357, Val Loss: 0.0385\n",
            "Epoch 67/100, Train Loss: 0.0372, Val Loss: 0.0385\n",
            "Epoch 68/100, Train Loss: 0.0340, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0370, Val Loss: 0.0385\n",
            "Epoch 70/100, Train Loss: 0.0395, Val Loss: 0.0385\n",
            "Epoch 71/100, Train Loss: 0.0357, Val Loss: 0.0385\n",
            "Epoch 72/100, Train Loss: 0.0332, Val Loss: 0.0385\n",
            "Epoch 73/100, Train Loss: 0.0385, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0369, Val Loss: 0.0385\n",
            "Epoch 75/100, Train Loss: 0.0348, Val Loss: 0.0385\n",
            "Epoch 76/100, Train Loss: 0.0371, Val Loss: 0.0385\n",
            "Epoch 77/100, Train Loss: 0.0345, Val Loss: 0.0385\n",
            "Epoch 78/100, Train Loss: 0.0374, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0387, Val Loss: 0.0385\n",
            "Epoch 80/100, Train Loss: 0.0378, Val Loss: 0.0385\n",
            "Epoch 81/100, Train Loss: 0.0412, Val Loss: 0.0385\n",
            "Epoch 82/100, Train Loss: 0.0359, Val Loss: 0.0385\n",
            "Epoch 83/100, Train Loss: 0.0354, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0362, Val Loss: 0.0385\n",
            "Epoch 85/100, Train Loss: 0.0350, Val Loss: 0.0385\n",
            "Epoch 86/100, Train Loss: 0.0316, Val Loss: 0.0385\n",
            "Epoch 87/100, Train Loss: 0.0334, Val Loss: 0.0385\n",
            "Epoch 88/100, Train Loss: 0.0407, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0343, Val Loss: 0.0385\n",
            "Epoch 90/100, Train Loss: 0.0291, Val Loss: 0.0385\n",
            "Epoch 91/100, Train Loss: 0.0372, Val Loss: 0.0385\n",
            "Epoch 92/100, Train Loss: 0.0405, Val Loss: 0.0385\n",
            "Epoch 93/100, Train Loss: 0.0342, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0383, Val Loss: 0.0385\n",
            "Epoch 95/100, Train Loss: 0.0366, Val Loss: 0.0385\n",
            "Epoch 96/100, Train Loss: 0.0378, Val Loss: 0.0385\n",
            "Epoch 97/100, Train Loss: 0.0363, Val Loss: 0.0385\n",
            "Epoch 98/100, Train Loss: 0.0391, Val Loss: 0.0385\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0337, Val Loss: 0.0385\n",
            "Epoch 100/100, Train Loss: 0.0345, Val Loss: 0.0385\n",
            "\n",
            "Test iteration 9/28\n",
            "Current training set size: 105 samples\n",
            "Epoch 1/100, Train Loss: 0.1594, Val Loss: 0.1618\n",
            "Epoch 2/100, Train Loss: 0.1059, Val Loss: 0.1018\n",
            "Epoch 3/100, Train Loss: 0.0953, Val Loss: 0.0920\n",
            "Epoch 4/100, Train Loss: 0.0659, Val Loss: 0.0673\n",
            "Epoch 5/100, Train Loss: 0.0650, Val Loss: 0.0482\n",
            "Epoch 6/100, Train Loss: 0.0508, Val Loss: 0.0509\n",
            "Epoch 7/100, Train Loss: 0.0454, Val Loss: 0.0471\n",
            "Epoch 8/100, Train Loss: 0.0543, Val Loss: 0.0605\n",
            "Epoch 9/100, Train Loss: 0.0445, Val Loss: 0.0463\n",
            "Epoch 10/100, Train Loss: 0.0434, Val Loss: 0.0454\n",
            "Epoch 11/100, Train Loss: 0.0436, Val Loss: 0.0478\n",
            "Epoch 12/100, Train Loss: 0.0481, Val Loss: 0.0447\n",
            "Epoch 13/100, Train Loss: 0.0450, Val Loss: 0.0438\n",
            "Epoch 14/100, Train Loss: 0.0491, Val Loss: 0.0468\n",
            "Epoch 15/100, Train Loss: 0.0409, Val Loss: 0.0405\n",
            "Epoch 16/100, Train Loss: 0.0480, Val Loss: 0.0505\n",
            "Epoch 17/100, Train Loss: 0.0432, Val Loss: 0.0513\n",
            "Epoch 18/100, Train Loss: 0.0390, Val Loss: 0.0393\n",
            "Epoch 19/100, Train Loss: 0.0358, Val Loss: 0.0532\n",
            "Epoch 20/100, Train Loss: 0.0378, Val Loss: 0.0439\n",
            "Epoch 21/100, Train Loss: 0.0391, Val Loss: 0.0402\n",
            "Epoch 22/100, Train Loss: 0.0418, Val Loss: 0.0450\n",
            "Epoch 23/100, Train Loss: 0.0486, Val Loss: 0.0625\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0457, Val Loss: 0.0615\n",
            "Epoch 25/100, Train Loss: 0.0425, Val Loss: 0.0602\n",
            "Epoch 26/100, Train Loss: 0.0402, Val Loss: 0.0590\n",
            "Epoch 27/100, Train Loss: 0.0405, Val Loss: 0.0577\n",
            "Epoch 28/100, Train Loss: 0.0386, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0414, Val Loss: 0.0564\n",
            "Epoch 30/100, Train Loss: 0.0395, Val Loss: 0.0564\n",
            "Epoch 31/100, Train Loss: 0.0396, Val Loss: 0.0564\n",
            "Epoch 32/100, Train Loss: 0.0388, Val Loss: 0.0564\n",
            "Epoch 33/100, Train Loss: 0.0402, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0384, Val Loss: 0.0564\n",
            "Epoch 35/100, Train Loss: 0.0428, Val Loss: 0.0564\n",
            "Epoch 36/100, Train Loss: 0.0426, Val Loss: 0.0564\n",
            "Epoch 37/100, Train Loss: 0.0411, Val Loss: 0.0564\n",
            "Epoch 38/100, Train Loss: 0.0406, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0433, Val Loss: 0.0564\n",
            "Epoch 40/100, Train Loss: 0.0370, Val Loss: 0.0564\n",
            "Epoch 41/100, Train Loss: 0.0391, Val Loss: 0.0564\n",
            "Epoch 42/100, Train Loss: 0.0388, Val Loss: 0.0564\n",
            "Epoch 43/100, Train Loss: 0.0422, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0408, Val Loss: 0.0564\n",
            "Epoch 45/100, Train Loss: 0.0422, Val Loss: 0.0564\n",
            "Epoch 46/100, Train Loss: 0.0396, Val Loss: 0.0564\n",
            "Epoch 47/100, Train Loss: 0.0399, Val Loss: 0.0564\n",
            "Epoch 48/100, Train Loss: 0.0426, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0435, Val Loss: 0.0564\n",
            "Epoch 50/100, Train Loss: 0.0399, Val Loss: 0.0564\n",
            "Epoch 51/100, Train Loss: 0.0431, Val Loss: 0.0564\n",
            "Epoch 52/100, Train Loss: 0.0408, Val Loss: 0.0564\n",
            "Epoch 53/100, Train Loss: 0.0407, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0416, Val Loss: 0.0564\n",
            "Epoch 55/100, Train Loss: 0.0386, Val Loss: 0.0564\n",
            "Epoch 56/100, Train Loss: 0.0400, Val Loss: 0.0564\n",
            "Epoch 57/100, Train Loss: 0.0390, Val Loss: 0.0564\n",
            "Epoch 58/100, Train Loss: 0.0365, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0440, Val Loss: 0.0564\n",
            "Epoch 60/100, Train Loss: 0.0395, Val Loss: 0.0564\n",
            "Epoch 61/100, Train Loss: 0.0405, Val Loss: 0.0564\n",
            "Epoch 62/100, Train Loss: 0.0412, Val Loss: 0.0564\n",
            "Epoch 63/100, Train Loss: 0.0422, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0411, Val Loss: 0.0564\n",
            "Epoch 65/100, Train Loss: 0.0439, Val Loss: 0.0564\n",
            "Epoch 66/100, Train Loss: 0.0379, Val Loss: 0.0564\n",
            "Epoch 67/100, Train Loss: 0.0388, Val Loss: 0.0564\n",
            "Epoch 68/100, Train Loss: 0.0402, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0437, Val Loss: 0.0564\n",
            "Epoch 70/100, Train Loss: 0.0409, Val Loss: 0.0564\n",
            "Epoch 71/100, Train Loss: 0.0407, Val Loss: 0.0564\n",
            "Epoch 72/100, Train Loss: 0.0377, Val Loss: 0.0564\n",
            "Epoch 73/100, Train Loss: 0.0432, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0357, Val Loss: 0.0564\n",
            "Epoch 75/100, Train Loss: 0.0378, Val Loss: 0.0564\n",
            "Epoch 76/100, Train Loss: 0.0393, Val Loss: 0.0564\n",
            "Epoch 77/100, Train Loss: 0.0401, Val Loss: 0.0564\n",
            "Epoch 78/100, Train Loss: 0.0424, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0406, Val Loss: 0.0564\n",
            "Epoch 80/100, Train Loss: 0.0422, Val Loss: 0.0564\n",
            "Epoch 81/100, Train Loss: 0.0394, Val Loss: 0.0564\n",
            "Epoch 82/100, Train Loss: 0.0418, Val Loss: 0.0564\n",
            "Epoch 83/100, Train Loss: 0.0381, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0403, Val Loss: 0.0564\n",
            "Epoch 85/100, Train Loss: 0.0386, Val Loss: 0.0564\n",
            "Epoch 86/100, Train Loss: 0.0415, Val Loss: 0.0564\n",
            "Epoch 87/100, Train Loss: 0.0402, Val Loss: 0.0564\n",
            "Epoch 88/100, Train Loss: 0.0384, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0387, Val Loss: 0.0564\n",
            "Epoch 90/100, Train Loss: 0.0401, Val Loss: 0.0564\n",
            "Epoch 91/100, Train Loss: 0.0377, Val Loss: 0.0564\n",
            "Epoch 92/100, Train Loss: 0.0445, Val Loss: 0.0564\n",
            "Epoch 93/100, Train Loss: 0.0402, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0374, Val Loss: 0.0564\n",
            "Epoch 95/100, Train Loss: 0.0410, Val Loss: 0.0564\n",
            "Epoch 96/100, Train Loss: 0.0406, Val Loss: 0.0564\n",
            "Epoch 97/100, Train Loss: 0.0381, Val Loss: 0.0564\n",
            "Epoch 98/100, Train Loss: 0.0393, Val Loss: 0.0564\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0434, Val Loss: 0.0564\n",
            "Epoch 100/100, Train Loss: 0.0381, Val Loss: 0.0564\n",
            "\n",
            "Test iteration 10/28\n",
            "Current training set size: 106 samples\n",
            "Epoch 1/100, Train Loss: 0.1804, Val Loss: 0.2067\n",
            "Epoch 2/100, Train Loss: 0.1192, Val Loss: 0.1107\n",
            "Epoch 3/100, Train Loss: 0.1053, Val Loss: 0.0876\n",
            "Epoch 4/100, Train Loss: 0.0704, Val Loss: 0.0556\n",
            "Epoch 5/100, Train Loss: 0.0662, Val Loss: 0.0536\n",
            "Epoch 6/100, Train Loss: 0.0547, Val Loss: 0.0548\n",
            "Epoch 7/100, Train Loss: 0.0451, Val Loss: 0.0466\n",
            "Epoch 8/100, Train Loss: 0.0478, Val Loss: 0.0405\n",
            "Epoch 9/100, Train Loss: 0.0461, Val Loss: 0.0461\n",
            "Epoch 10/100, Train Loss: 0.0468, Val Loss: 0.0530\n",
            "Epoch 11/100, Train Loss: 0.0460, Val Loss: 0.0450\n",
            "Epoch 12/100, Train Loss: 0.0465, Val Loss: 0.0411\n",
            "Epoch 13/100, Train Loss: 0.0385, Val Loss: 0.0424\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 14/100, Train Loss: 0.0479, Val Loss: 0.0418\n",
            "Epoch 15/100, Train Loss: 0.0416, Val Loss: 0.0399\n",
            "Epoch 16/100, Train Loss: 0.0398, Val Loss: 0.0389\n",
            "Epoch 17/100, Train Loss: 0.0375, Val Loss: 0.0383\n",
            "Epoch 18/100, Train Loss: 0.0395, Val Loss: 0.0369\n",
            "Epoch 19/100, Train Loss: 0.0385, Val Loss: 0.0359\n",
            "Epoch 20/100, Train Loss: 0.0390, Val Loss: 0.0360\n",
            "Epoch 21/100, Train Loss: 0.0365, Val Loss: 0.0361\n",
            "Epoch 22/100, Train Loss: 0.0359, Val Loss: 0.0362\n",
            "Epoch 23/100, Train Loss: 0.0365, Val Loss: 0.0363\n",
            "Epoch 24/100, Train Loss: 0.0424, Val Loss: 0.0366\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0363, Val Loss: 0.0366\n",
            "Epoch 26/100, Train Loss: 0.0370, Val Loss: 0.0366\n",
            "Epoch 27/100, Train Loss: 0.0397, Val Loss: 0.0366\n",
            "Epoch 28/100, Train Loss: 0.0373, Val Loss: 0.0366\n",
            "Epoch 29/100, Train Loss: 0.0407, Val Loss: 0.0366\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0367, Val Loss: 0.0366\n",
            "Epoch 31/100, Train Loss: 0.0371, Val Loss: 0.0366\n",
            "Epoch 32/100, Train Loss: 0.0371, Val Loss: 0.0366\n",
            "Epoch 33/100, Train Loss: 0.0386, Val Loss: 0.0366\n",
            "Epoch 34/100, Train Loss: 0.0420, Val Loss: 0.0366\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0396, Val Loss: 0.0366\n",
            "Epoch 36/100, Train Loss: 0.0347, Val Loss: 0.0366\n",
            "Epoch 37/100, Train Loss: 0.0389, Val Loss: 0.0366\n",
            "Epoch 38/100, Train Loss: 0.0382, Val Loss: 0.0366\n",
            "Epoch 39/100, Train Loss: 0.0377, Val Loss: 0.0366\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0409, Val Loss: 0.0366\n",
            "Epoch 41/100, Train Loss: 0.0398, Val Loss: 0.0366\n",
            "Epoch 42/100, Train Loss: 0.0386, Val Loss: 0.0366\n",
            "Epoch 43/100, Train Loss: 0.0372, Val Loss: 0.0366\n",
            "Epoch 44/100, Train Loss: 0.0376, Val Loss: 0.0366\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0346, Val Loss: 0.0366\n",
            "Epoch 46/100, Train Loss: 0.0363, Val Loss: 0.0366\n",
            "Epoch 47/100, Train Loss: 0.0364, Val Loss: 0.0366\n",
            "Epoch 48/100, Train Loss: 0.0375, Val Loss: 0.0366\n",
            "Epoch 49/100, Train Loss: 0.0403, Val Loss: 0.0366\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0378, Val Loss: 0.0366\n",
            "Epoch 51/100, Train Loss: 0.0403, Val Loss: 0.0366\n",
            "Epoch 52/100, Train Loss: 0.0369, Val Loss: 0.0366\n",
            "Epoch 53/100, Train Loss: 0.0386, Val Loss: 0.0366\n",
            "Epoch 54/100, Train Loss: 0.0399, Val Loss: 0.0366\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0362, Val Loss: 0.0366\n",
            "Epoch 56/100, Train Loss: 0.0360, Val Loss: 0.0366\n",
            "Epoch 57/100, Train Loss: 0.0383, Val Loss: 0.0366\n",
            "Epoch 58/100, Train Loss: 0.0388, Val Loss: 0.0366\n",
            "Epoch 59/100, Train Loss: 0.0354, Val Loss: 0.0366\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0374, Val Loss: 0.0366\n",
            "Epoch 61/100, Train Loss: 0.0367, Val Loss: 0.0366\n",
            "Epoch 62/100, Train Loss: 0.0420, Val Loss: 0.0366\n",
            "Epoch 63/100, Train Loss: 0.0385, Val Loss: 0.0366\n",
            "Epoch 64/100, Train Loss: 0.0358, Val Loss: 0.0366\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0383, Val Loss: 0.0366\n",
            "Epoch 66/100, Train Loss: 0.0366, Val Loss: 0.0366\n",
            "Epoch 67/100, Train Loss: 0.0356, Val Loss: 0.0366\n",
            "Epoch 68/100, Train Loss: 0.0401, Val Loss: 0.0366\n",
            "Epoch 69/100, Train Loss: 0.0415, Val Loss: 0.0366\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0411, Val Loss: 0.0366\n",
            "Epoch 71/100, Train Loss: 0.0391, Val Loss: 0.0366\n",
            "Epoch 72/100, Train Loss: 0.0395, Val Loss: 0.0366\n",
            "Epoch 73/100, Train Loss: 0.0378, Val Loss: 0.0366\n",
            "Epoch 74/100, Train Loss: 0.0355, Val Loss: 0.0366\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0357, Val Loss: 0.0366\n",
            "Epoch 76/100, Train Loss: 0.0378, Val Loss: 0.0366\n",
            "Epoch 77/100, Train Loss: 0.0381, Val Loss: 0.0366\n",
            "Epoch 78/100, Train Loss: 0.0359, Val Loss: 0.0366\n",
            "Epoch 79/100, Train Loss: 0.0360, Val Loss: 0.0366\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0358, Val Loss: 0.0366\n",
            "Epoch 81/100, Train Loss: 0.0385, Val Loss: 0.0366\n",
            "Epoch 82/100, Train Loss: 0.0377, Val Loss: 0.0366\n",
            "Epoch 83/100, Train Loss: 0.0364, Val Loss: 0.0366\n",
            "Epoch 84/100, Train Loss: 0.0345, Val Loss: 0.0366\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0378, Val Loss: 0.0366\n",
            "Epoch 86/100, Train Loss: 0.0377, Val Loss: 0.0366\n",
            "Epoch 87/100, Train Loss: 0.0378, Val Loss: 0.0366\n",
            "Epoch 88/100, Train Loss: 0.0378, Val Loss: 0.0366\n",
            "Epoch 89/100, Train Loss: 0.0381, Val Loss: 0.0366\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0350, Val Loss: 0.0366\n",
            "Epoch 91/100, Train Loss: 0.0365, Val Loss: 0.0366\n",
            "Epoch 92/100, Train Loss: 0.0409, Val Loss: 0.0366\n",
            "Epoch 93/100, Train Loss: 0.0338, Val Loss: 0.0366\n",
            "Epoch 94/100, Train Loss: 0.0355, Val Loss: 0.0366\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0367, Val Loss: 0.0366\n",
            "Epoch 96/100, Train Loss: 0.0336, Val Loss: 0.0366\n",
            "Epoch 97/100, Train Loss: 0.0370, Val Loss: 0.0366\n",
            "Epoch 98/100, Train Loss: 0.0358, Val Loss: 0.0366\n",
            "Epoch 99/100, Train Loss: 0.0365, Val Loss: 0.0366\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0327, Val Loss: 0.0366\n",
            "\n",
            "Test iteration 11/28\n",
            "Current training set size: 107 samples\n",
            "Epoch 1/100, Train Loss: 0.1591, Val Loss: 0.1570\n",
            "Epoch 2/100, Train Loss: 0.1128, Val Loss: 0.1075\n",
            "Epoch 3/100, Train Loss: 0.0912, Val Loss: 0.0970\n",
            "Epoch 4/100, Train Loss: 0.0734, Val Loss: 0.0759\n",
            "Epoch 5/100, Train Loss: 0.0838, Val Loss: 0.0733\n",
            "Epoch 6/100, Train Loss: 0.0499, Val Loss: 0.0458\n",
            "Epoch 7/100, Train Loss: 0.0537, Val Loss: 0.0568\n",
            "Epoch 8/100, Train Loss: 0.0514, Val Loss: 0.0548\n",
            "Epoch 9/100, Train Loss: 0.0462, Val Loss: 0.0438\n",
            "Epoch 10/100, Train Loss: 0.0404, Val Loss: 0.0447\n",
            "Epoch 11/100, Train Loss: 0.0458, Val Loss: 0.0518\n",
            "Epoch 12/100, Train Loss: 0.0406, Val Loss: 0.0432\n",
            "Epoch 13/100, Train Loss: 0.0408, Val Loss: 0.0432\n",
            "Epoch 14/100, Train Loss: 0.0469, Val Loss: 0.0473\n",
            "Epoch 15/100, Train Loss: 0.0378, Val Loss: 0.0464\n",
            "Epoch 16/100, Train Loss: 0.0461, Val Loss: 0.0478\n",
            "Epoch 17/100, Train Loss: 0.0352, Val Loss: 0.0422\n",
            "Epoch 18/100, Train Loss: 0.0446, Val Loss: 0.0471\n",
            "Epoch 19/100, Train Loss: 0.0345, Val Loss: 0.0497\n",
            "Epoch 20/100, Train Loss: 0.0410, Val Loss: 0.0516\n",
            "Epoch 21/100, Train Loss: 0.0430, Val Loss: 0.0505\n",
            "Epoch 22/100, Train Loss: 0.0443, Val Loss: 0.0438\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0395, Val Loss: 0.0407\n",
            "Epoch 24/100, Train Loss: 0.0354, Val Loss: 0.0391\n",
            "Epoch 25/100, Train Loss: 0.0360, Val Loss: 0.0380\n",
            "Epoch 26/100, Train Loss: 0.0353, Val Loss: 0.0374\n",
            "Epoch 27/100, Train Loss: 0.0365, Val Loss: 0.0370\n",
            "Epoch 28/100, Train Loss: 0.0345, Val Loss: 0.0367\n",
            "Epoch 29/100, Train Loss: 0.0349, Val Loss: 0.0363\n",
            "Epoch 30/100, Train Loss: 0.0345, Val Loss: 0.0360\n",
            "Epoch 31/100, Train Loss: 0.0358, Val Loss: 0.0358\n",
            "Epoch 32/100, Train Loss: 0.0349, Val Loss: 0.0357\n",
            "Epoch 33/100, Train Loss: 0.0350, Val Loss: 0.0356\n",
            "Epoch 34/100, Train Loss: 0.0348, Val Loss: 0.0354\n",
            "Epoch 35/100, Train Loss: 0.0389, Val Loss: 0.0352\n",
            "Epoch 36/100, Train Loss: 0.0337, Val Loss: 0.0351\n",
            "Epoch 37/100, Train Loss: 0.0329, Val Loss: 0.0349\n",
            "Epoch 38/100, Train Loss: 0.0316, Val Loss: 0.0349\n",
            "Epoch 39/100, Train Loss: 0.0299, Val Loss: 0.0349\n",
            "Epoch 40/100, Train Loss: 0.0323, Val Loss: 0.0348\n",
            "Epoch 41/100, Train Loss: 0.0324, Val Loss: 0.0349\n",
            "Epoch 42/100, Train Loss: 0.0345, Val Loss: 0.0351\n",
            "Epoch 43/100, Train Loss: 0.0328, Val Loss: 0.0351\n",
            "Epoch 44/100, Train Loss: 0.0337, Val Loss: 0.0352\n",
            "Epoch 45/100, Train Loss: 0.0335, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0339, Val Loss: 0.0352\n",
            "Epoch 47/100, Train Loss: 0.0309, Val Loss: 0.0352\n",
            "Epoch 48/100, Train Loss: 0.0343, Val Loss: 0.0352\n",
            "Epoch 49/100, Train Loss: 0.0354, Val Loss: 0.0352\n",
            "Epoch 50/100, Train Loss: 0.0349, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0329, Val Loss: 0.0352\n",
            "Epoch 52/100, Train Loss: 0.0306, Val Loss: 0.0352\n",
            "Epoch 53/100, Train Loss: 0.0329, Val Loss: 0.0352\n",
            "Epoch 54/100, Train Loss: 0.0325, Val Loss: 0.0352\n",
            "Epoch 55/100, Train Loss: 0.0319, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0334, Val Loss: 0.0352\n",
            "Epoch 57/100, Train Loss: 0.0358, Val Loss: 0.0352\n",
            "Epoch 58/100, Train Loss: 0.0303, Val Loss: 0.0352\n",
            "Epoch 59/100, Train Loss: 0.0309, Val Loss: 0.0352\n",
            "Epoch 60/100, Train Loss: 0.0346, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0307, Val Loss: 0.0352\n",
            "Epoch 62/100, Train Loss: 0.0310, Val Loss: 0.0352\n",
            "Epoch 63/100, Train Loss: 0.0326, Val Loss: 0.0352\n",
            "Epoch 64/100, Train Loss: 0.0352, Val Loss: 0.0352\n",
            "Epoch 65/100, Train Loss: 0.0304, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0326, Val Loss: 0.0352\n",
            "Epoch 67/100, Train Loss: 0.0344, Val Loss: 0.0352\n",
            "Epoch 68/100, Train Loss: 0.0318, Val Loss: 0.0352\n",
            "Epoch 69/100, Train Loss: 0.0320, Val Loss: 0.0352\n",
            "Epoch 70/100, Train Loss: 0.0324, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0316, Val Loss: 0.0352\n",
            "Epoch 72/100, Train Loss: 0.0324, Val Loss: 0.0352\n",
            "Epoch 73/100, Train Loss: 0.0346, Val Loss: 0.0352\n",
            "Epoch 74/100, Train Loss: 0.0315, Val Loss: 0.0352\n",
            "Epoch 75/100, Train Loss: 0.0335, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0299, Val Loss: 0.0352\n",
            "Epoch 77/100, Train Loss: 0.0332, Val Loss: 0.0352\n",
            "Epoch 78/100, Train Loss: 0.0315, Val Loss: 0.0352\n",
            "Epoch 79/100, Train Loss: 0.0281, Val Loss: 0.0352\n",
            "Epoch 80/100, Train Loss: 0.0349, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0333, Val Loss: 0.0352\n",
            "Epoch 82/100, Train Loss: 0.0321, Val Loss: 0.0352\n",
            "Epoch 83/100, Train Loss: 0.0308, Val Loss: 0.0352\n",
            "Epoch 84/100, Train Loss: 0.0323, Val Loss: 0.0352\n",
            "Epoch 85/100, Train Loss: 0.0324, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0319, Val Loss: 0.0352\n",
            "Epoch 87/100, Train Loss: 0.0329, Val Loss: 0.0352\n",
            "Epoch 88/100, Train Loss: 0.0320, Val Loss: 0.0352\n",
            "Epoch 89/100, Train Loss: 0.0320, Val Loss: 0.0352\n",
            "Epoch 90/100, Train Loss: 0.0319, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0313, Val Loss: 0.0352\n",
            "Epoch 92/100, Train Loss: 0.0323, Val Loss: 0.0352\n",
            "Epoch 93/100, Train Loss: 0.0335, Val Loss: 0.0352\n",
            "Epoch 94/100, Train Loss: 0.0296, Val Loss: 0.0352\n",
            "Epoch 95/100, Train Loss: 0.0304, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0321, Val Loss: 0.0352\n",
            "Epoch 97/100, Train Loss: 0.0333, Val Loss: 0.0352\n",
            "Epoch 98/100, Train Loss: 0.0302, Val Loss: 0.0352\n",
            "Epoch 99/100, Train Loss: 0.0335, Val Loss: 0.0352\n",
            "Epoch 100/100, Train Loss: 0.0335, Val Loss: 0.0352\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 12/28\n",
            "Current training set size: 108 samples\n",
            "Epoch 1/100, Train Loss: 0.1664, Val Loss: 0.1830\n",
            "Epoch 2/100, Train Loss: 0.1357, Val Loss: 0.1193\n",
            "Epoch 3/100, Train Loss: 0.0892, Val Loss: 0.0962\n",
            "Epoch 4/100, Train Loss: 0.0674, Val Loss: 0.0559\n",
            "Epoch 5/100, Train Loss: 0.0561, Val Loss: 0.0560\n",
            "Epoch 6/100, Train Loss: 0.0523, Val Loss: 0.0435\n",
            "Epoch 7/100, Train Loss: 0.0485, Val Loss: 0.0496\n",
            "Epoch 8/100, Train Loss: 0.0551, Val Loss: 0.0552\n",
            "Epoch 9/100, Train Loss: 0.0446, Val Loss: 0.0541\n",
            "Epoch 10/100, Train Loss: 0.0421, Val Loss: 0.0598\n",
            "Epoch 11/100, Train Loss: 0.0452, Val Loss: 0.0507\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 12/100, Train Loss: 0.0417, Val Loss: 0.0493\n",
            "Epoch 13/100, Train Loss: 0.0419, Val Loss: 0.0485\n",
            "Epoch 14/100, Train Loss: 0.0383, Val Loss: 0.0481\n",
            "Epoch 15/100, Train Loss: 0.0404, Val Loss: 0.0475\n",
            "Epoch 16/100, Train Loss: 0.0418, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 17/100, Train Loss: 0.0383, Val Loss: 0.0474\n",
            "Epoch 18/100, Train Loss: 0.0362, Val Loss: 0.0474\n",
            "Epoch 19/100, Train Loss: 0.0381, Val Loss: 0.0474\n",
            "Epoch 20/100, Train Loss: 0.0395, Val Loss: 0.0474\n",
            "Epoch 21/100, Train Loss: 0.0343, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0348, Val Loss: 0.0474\n",
            "Epoch 23/100, Train Loss: 0.0380, Val Loss: 0.0474\n",
            "Epoch 24/100, Train Loss: 0.0370, Val Loss: 0.0474\n",
            "Epoch 25/100, Train Loss: 0.0395, Val Loss: 0.0474\n",
            "Epoch 26/100, Train Loss: 0.0379, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0391, Val Loss: 0.0474\n",
            "Epoch 28/100, Train Loss: 0.0377, Val Loss: 0.0474\n",
            "Epoch 29/100, Train Loss: 0.0370, Val Loss: 0.0474\n",
            "Epoch 30/100, Train Loss: 0.0367, Val Loss: 0.0474\n",
            "Epoch 31/100, Train Loss: 0.0394, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0383, Val Loss: 0.0474\n",
            "Epoch 33/100, Train Loss: 0.0370, Val Loss: 0.0474\n",
            "Epoch 34/100, Train Loss: 0.0360, Val Loss: 0.0474\n",
            "Epoch 35/100, Train Loss: 0.0382, Val Loss: 0.0474\n",
            "Epoch 36/100, Train Loss: 0.0380, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0398, Val Loss: 0.0474\n",
            "Epoch 38/100, Train Loss: 0.0395, Val Loss: 0.0474\n",
            "Epoch 39/100, Train Loss: 0.0382, Val Loss: 0.0474\n",
            "Epoch 40/100, Train Loss: 0.0375, Val Loss: 0.0474\n",
            "Epoch 41/100, Train Loss: 0.0395, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0398, Val Loss: 0.0474\n",
            "Epoch 43/100, Train Loss: 0.0400, Val Loss: 0.0474\n",
            "Epoch 44/100, Train Loss: 0.0387, Val Loss: 0.0474\n",
            "Epoch 45/100, Train Loss: 0.0375, Val Loss: 0.0474\n",
            "Epoch 46/100, Train Loss: 0.0362, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0397, Val Loss: 0.0474\n",
            "Epoch 48/100, Train Loss: 0.0387, Val Loss: 0.0474\n",
            "Epoch 49/100, Train Loss: 0.0397, Val Loss: 0.0474\n",
            "Epoch 50/100, Train Loss: 0.0390, Val Loss: 0.0474\n",
            "Epoch 51/100, Train Loss: 0.0385, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0376, Val Loss: 0.0474\n",
            "Epoch 53/100, Train Loss: 0.0378, Val Loss: 0.0474\n",
            "Epoch 54/100, Train Loss: 0.0388, Val Loss: 0.0474\n",
            "Epoch 55/100, Train Loss: 0.0376, Val Loss: 0.0474\n",
            "Epoch 56/100, Train Loss: 0.0389, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0396, Val Loss: 0.0474\n",
            "Epoch 58/100, Train Loss: 0.0383, Val Loss: 0.0474\n",
            "Epoch 59/100, Train Loss: 0.0361, Val Loss: 0.0474\n",
            "Epoch 60/100, Train Loss: 0.0378, Val Loss: 0.0474\n",
            "Epoch 61/100, Train Loss: 0.0369, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0377, Val Loss: 0.0474\n",
            "Epoch 63/100, Train Loss: 0.0382, Val Loss: 0.0474\n",
            "Epoch 64/100, Train Loss: 0.0394, Val Loss: 0.0474\n",
            "Epoch 65/100, Train Loss: 0.0401, Val Loss: 0.0474\n",
            "Epoch 66/100, Train Loss: 0.0359, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0392, Val Loss: 0.0474\n",
            "Epoch 68/100, Train Loss: 0.0385, Val Loss: 0.0474\n",
            "Epoch 69/100, Train Loss: 0.0377, Val Loss: 0.0474\n",
            "Epoch 70/100, Train Loss: 0.0359, Val Loss: 0.0474\n",
            "Epoch 71/100, Train Loss: 0.0369, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0409, Val Loss: 0.0474\n",
            "Epoch 73/100, Train Loss: 0.0375, Val Loss: 0.0474\n",
            "Epoch 74/100, Train Loss: 0.0380, Val Loss: 0.0474\n",
            "Epoch 75/100, Train Loss: 0.0346, Val Loss: 0.0474\n",
            "Epoch 76/100, Train Loss: 0.0381, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0388, Val Loss: 0.0474\n",
            "Epoch 78/100, Train Loss: 0.0419, Val Loss: 0.0474\n",
            "Epoch 79/100, Train Loss: 0.0351, Val Loss: 0.0474\n",
            "Epoch 80/100, Train Loss: 0.0347, Val Loss: 0.0474\n",
            "Epoch 81/100, Train Loss: 0.0389, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0382, Val Loss: 0.0474\n",
            "Epoch 83/100, Train Loss: 0.0362, Val Loss: 0.0474\n",
            "Epoch 84/100, Train Loss: 0.0392, Val Loss: 0.0474\n",
            "Epoch 85/100, Train Loss: 0.0411, Val Loss: 0.0474\n",
            "Epoch 86/100, Train Loss: 0.0398, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0355, Val Loss: 0.0474\n",
            "Epoch 88/100, Train Loss: 0.0371, Val Loss: 0.0474\n",
            "Epoch 89/100, Train Loss: 0.0349, Val Loss: 0.0474\n",
            "Epoch 90/100, Train Loss: 0.0385, Val Loss: 0.0474\n",
            "Epoch 91/100, Train Loss: 0.0381, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0380, Val Loss: 0.0474\n",
            "Epoch 93/100, Train Loss: 0.0404, Val Loss: 0.0474\n",
            "Epoch 94/100, Train Loss: 0.0383, Val Loss: 0.0474\n",
            "Epoch 95/100, Train Loss: 0.0341, Val Loss: 0.0474\n",
            "Epoch 96/100, Train Loss: 0.0387, Val Loss: 0.0474\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0383, Val Loss: 0.0474\n",
            "Epoch 98/100, Train Loss: 0.0357, Val Loss: 0.0474\n",
            "Epoch 99/100, Train Loss: 0.0367, Val Loss: 0.0474\n",
            "Epoch 100/100, Train Loss: 0.0362, Val Loss: 0.0474\n",
            "\n",
            "Test iteration 13/28\n",
            "Current training set size: 109 samples\n",
            "Epoch 1/100, Train Loss: 0.1834, Val Loss: 0.1708\n",
            "Epoch 2/100, Train Loss: 0.1111, Val Loss: 0.0975\n",
            "Epoch 3/100, Train Loss: 0.0869, Val Loss: 0.0901\n",
            "Epoch 4/100, Train Loss: 0.0793, Val Loss: 0.0718\n",
            "Epoch 5/100, Train Loss: 0.0613, Val Loss: 0.0516\n",
            "Epoch 6/100, Train Loss: 0.0500, Val Loss: 0.0437\n",
            "Epoch 7/100, Train Loss: 0.0558, Val Loss: 0.0526\n",
            "Epoch 8/100, Train Loss: 0.0558, Val Loss: 0.0407\n",
            "Epoch 9/100, Train Loss: 0.0440, Val Loss: 0.0416\n",
            "Epoch 10/100, Train Loss: 0.0453, Val Loss: 0.0427\n",
            "Epoch 11/100, Train Loss: 0.0499, Val Loss: 0.0554\n",
            "Epoch 12/100, Train Loss: 0.0460, Val Loss: 0.0514\n",
            "Epoch 13/100, Train Loss: 0.0431, Val Loss: 0.0420\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 14/100, Train Loss: 0.0459, Val Loss: 0.0408\n",
            "Epoch 15/100, Train Loss: 0.0377, Val Loss: 0.0398\n",
            "Epoch 16/100, Train Loss: 0.0388, Val Loss: 0.0395\n",
            "Epoch 17/100, Train Loss: 0.0419, Val Loss: 0.0395\n",
            "Epoch 18/100, Train Loss: 0.0366, Val Loss: 0.0393\n",
            "Epoch 19/100, Train Loss: 0.0425, Val Loss: 0.0391\n",
            "Epoch 20/100, Train Loss: 0.0364, Val Loss: 0.0388\n",
            "Epoch 21/100, Train Loss: 0.0377, Val Loss: 0.0388\n",
            "Epoch 22/100, Train Loss: 0.0340, Val Loss: 0.0384\n",
            "Epoch 23/100, Train Loss: 0.0383, Val Loss: 0.0382\n",
            "Epoch 24/100, Train Loss: 0.0392, Val Loss: 0.0385\n",
            "Epoch 25/100, Train Loss: 0.0393, Val Loss: 0.0384\n",
            "Epoch 26/100, Train Loss: 0.0341, Val Loss: 0.0384\n",
            "Epoch 27/100, Train Loss: 0.0367, Val Loss: 0.0382\n",
            "Epoch 28/100, Train Loss: 0.0381, Val Loss: 0.0378\n",
            "Epoch 29/100, Train Loss: 0.0400, Val Loss: 0.0379\n",
            "Epoch 30/100, Train Loss: 0.0372, Val Loss: 0.0376\n",
            "Epoch 31/100, Train Loss: 0.0333, Val Loss: 0.0376\n",
            "Epoch 32/100, Train Loss: 0.0367, Val Loss: 0.0377\n",
            "Epoch 33/100, Train Loss: 0.0382, Val Loss: 0.0375\n",
            "Epoch 34/100, Train Loss: 0.0342, Val Loss: 0.0375\n",
            "Epoch 35/100, Train Loss: 0.0387, Val Loss: 0.0372\n",
            "Epoch 36/100, Train Loss: 0.0396, Val Loss: 0.0373\n",
            "Epoch 37/100, Train Loss: 0.0352, Val Loss: 0.0374\n",
            "Epoch 38/100, Train Loss: 0.0346, Val Loss: 0.0377\n",
            "Epoch 39/100, Train Loss: 0.0361, Val Loss: 0.0379\n",
            "Epoch 40/100, Train Loss: 0.0415, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0373, Val Loss: 0.0379\n",
            "Epoch 42/100, Train Loss: 0.0392, Val Loss: 0.0379\n",
            "Epoch 43/100, Train Loss: 0.0389, Val Loss: 0.0379\n",
            "Epoch 44/100, Train Loss: 0.0338, Val Loss: 0.0379\n",
            "Epoch 45/100, Train Loss: 0.0338, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0337, Val Loss: 0.0379\n",
            "Epoch 47/100, Train Loss: 0.0346, Val Loss: 0.0379\n",
            "Epoch 48/100, Train Loss: 0.0347, Val Loss: 0.0379\n",
            "Epoch 49/100, Train Loss: 0.0405, Val Loss: 0.0379\n",
            "Epoch 50/100, Train Loss: 0.0401, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0373, Val Loss: 0.0379\n",
            "Epoch 52/100, Train Loss: 0.0436, Val Loss: 0.0379\n",
            "Epoch 53/100, Train Loss: 0.0367, Val Loss: 0.0379\n",
            "Epoch 54/100, Train Loss: 0.0346, Val Loss: 0.0379\n",
            "Epoch 55/100, Train Loss: 0.0364, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0382, Val Loss: 0.0379\n",
            "Epoch 57/100, Train Loss: 0.0374, Val Loss: 0.0379\n",
            "Epoch 58/100, Train Loss: 0.0361, Val Loss: 0.0379\n",
            "Epoch 59/100, Train Loss: 0.0434, Val Loss: 0.0379\n",
            "Epoch 60/100, Train Loss: 0.0341, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0351, Val Loss: 0.0379\n",
            "Epoch 62/100, Train Loss: 0.0345, Val Loss: 0.0379\n",
            "Epoch 63/100, Train Loss: 0.0392, Val Loss: 0.0379\n",
            "Epoch 64/100, Train Loss: 0.0347, Val Loss: 0.0379\n",
            "Epoch 65/100, Train Loss: 0.0386, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0316, Val Loss: 0.0379\n",
            "Epoch 67/100, Train Loss: 0.0391, Val Loss: 0.0379\n",
            "Epoch 68/100, Train Loss: 0.0410, Val Loss: 0.0379\n",
            "Epoch 69/100, Train Loss: 0.0343, Val Loss: 0.0379\n",
            "Epoch 70/100, Train Loss: 0.0367, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0370, Val Loss: 0.0379\n",
            "Epoch 72/100, Train Loss: 0.0374, Val Loss: 0.0379\n",
            "Epoch 73/100, Train Loss: 0.0377, Val Loss: 0.0379\n",
            "Epoch 74/100, Train Loss: 0.0391, Val Loss: 0.0379\n",
            "Epoch 75/100, Train Loss: 0.0380, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0386, Val Loss: 0.0379\n",
            "Epoch 77/100, Train Loss: 0.0388, Val Loss: 0.0379\n",
            "Epoch 78/100, Train Loss: 0.0397, Val Loss: 0.0379\n",
            "Epoch 79/100, Train Loss: 0.0346, Val Loss: 0.0379\n",
            "Epoch 80/100, Train Loss: 0.0366, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0352, Val Loss: 0.0379\n",
            "Epoch 82/100, Train Loss: 0.0416, Val Loss: 0.0379\n",
            "Epoch 83/100, Train Loss: 0.0356, Val Loss: 0.0379\n",
            "Epoch 84/100, Train Loss: 0.0336, Val Loss: 0.0379\n",
            "Epoch 85/100, Train Loss: 0.0387, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0356, Val Loss: 0.0379\n",
            "Epoch 87/100, Train Loss: 0.0378, Val Loss: 0.0379\n",
            "Epoch 88/100, Train Loss: 0.0336, Val Loss: 0.0379\n",
            "Epoch 89/100, Train Loss: 0.0405, Val Loss: 0.0379\n",
            "Epoch 90/100, Train Loss: 0.0352, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0378, Val Loss: 0.0379\n",
            "Epoch 92/100, Train Loss: 0.0411, Val Loss: 0.0379\n",
            "Epoch 93/100, Train Loss: 0.0386, Val Loss: 0.0379\n",
            "Epoch 94/100, Train Loss: 0.0366, Val Loss: 0.0379\n",
            "Epoch 95/100, Train Loss: 0.0342, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0353, Val Loss: 0.0379\n",
            "Epoch 97/100, Train Loss: 0.0367, Val Loss: 0.0379\n",
            "Epoch 98/100, Train Loss: 0.0338, Val Loss: 0.0379\n",
            "Epoch 99/100, Train Loss: 0.0397, Val Loss: 0.0379\n",
            "Epoch 100/100, Train Loss: 0.0384, Val Loss: 0.0379\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 14/28\n",
            "Current training set size: 110 samples\n",
            "Epoch 1/100, Train Loss: 0.1647, Val Loss: 0.1703\n",
            "Epoch 2/100, Train Loss: 0.1261, Val Loss: 0.1012\n",
            "Epoch 3/100, Train Loss: 0.0879, Val Loss: 0.0872\n",
            "Epoch 4/100, Train Loss: 0.0630, Val Loss: 0.0728\n",
            "Epoch 5/100, Train Loss: 0.0682, Val Loss: 0.0737\n",
            "Epoch 6/100, Train Loss: 0.0513, Val Loss: 0.0444\n",
            "Epoch 7/100, Train Loss: 0.0475, Val Loss: 0.0364\n",
            "Epoch 8/100, Train Loss: 0.0476, Val Loss: 0.0590\n",
            "Epoch 9/100, Train Loss: 0.0390, Val Loss: 0.0431\n",
            "Epoch 10/100, Train Loss: 0.0492, Val Loss: 0.0441\n",
            "Epoch 11/100, Train Loss: 0.0428, Val Loss: 0.0534\n",
            "Epoch 12/100, Train Loss: 0.0449, Val Loss: 0.0341\n",
            "Epoch 13/100, Train Loss: 0.0400, Val Loss: 0.0611\n",
            "Epoch 14/100, Train Loss: 0.0525, Val Loss: 0.0668\n",
            "Epoch 15/100, Train Loss: 0.0458, Val Loss: 0.0454\n",
            "Epoch 16/100, Train Loss: 0.0430, Val Loss: 0.0391\n",
            "Epoch 17/100, Train Loss: 0.0364, Val Loss: 0.0488\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0454, Val Loss: 0.0456\n",
            "Epoch 19/100, Train Loss: 0.0428, Val Loss: 0.0415\n",
            "Epoch 20/100, Train Loss: 0.0386, Val Loss: 0.0391\n",
            "Epoch 21/100, Train Loss: 0.0374, Val Loss: 0.0377\n",
            "Epoch 22/100, Train Loss: 0.0369, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0376, Val Loss: 0.0370\n",
            "Epoch 24/100, Train Loss: 0.0362, Val Loss: 0.0370\n",
            "Epoch 25/100, Train Loss: 0.0373, Val Loss: 0.0370\n",
            "Epoch 26/100, Train Loss: 0.0373, Val Loss: 0.0370\n",
            "Epoch 27/100, Train Loss: 0.0390, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0354, Val Loss: 0.0370\n",
            "Epoch 29/100, Train Loss: 0.0355, Val Loss: 0.0370\n",
            "Epoch 30/100, Train Loss: 0.0339, Val Loss: 0.0370\n",
            "Epoch 31/100, Train Loss: 0.0370, Val Loss: 0.0370\n",
            "Epoch 32/100, Train Loss: 0.0369, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0348, Val Loss: 0.0370\n",
            "Epoch 34/100, Train Loss: 0.0359, Val Loss: 0.0370\n",
            "Epoch 35/100, Train Loss: 0.0357, Val Loss: 0.0370\n",
            "Epoch 36/100, Train Loss: 0.0387, Val Loss: 0.0370\n",
            "Epoch 37/100, Train Loss: 0.0408, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0373, Val Loss: 0.0370\n",
            "Epoch 39/100, Train Loss: 0.0372, Val Loss: 0.0370\n",
            "Epoch 40/100, Train Loss: 0.0348, Val Loss: 0.0370\n",
            "Epoch 41/100, Train Loss: 0.0325, Val Loss: 0.0370\n",
            "Epoch 42/100, Train Loss: 0.0383, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0380, Val Loss: 0.0370\n",
            "Epoch 44/100, Train Loss: 0.0386, Val Loss: 0.0370\n",
            "Epoch 45/100, Train Loss: 0.0344, Val Loss: 0.0370\n",
            "Epoch 46/100, Train Loss: 0.0353, Val Loss: 0.0370\n",
            "Epoch 47/100, Train Loss: 0.0365, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0348, Val Loss: 0.0370\n",
            "Epoch 49/100, Train Loss: 0.0378, Val Loss: 0.0370\n",
            "Epoch 50/100, Train Loss: 0.0363, Val Loss: 0.0370\n",
            "Epoch 51/100, Train Loss: 0.0356, Val Loss: 0.0370\n",
            "Epoch 52/100, Train Loss: 0.0362, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0377, Val Loss: 0.0370\n",
            "Epoch 54/100, Train Loss: 0.0340, Val Loss: 0.0370\n",
            "Epoch 55/100, Train Loss: 0.0353, Val Loss: 0.0370\n",
            "Epoch 56/100, Train Loss: 0.0383, Val Loss: 0.0370\n",
            "Epoch 57/100, Train Loss: 0.0339, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0372, Val Loss: 0.0370\n",
            "Epoch 59/100, Train Loss: 0.0368, Val Loss: 0.0370\n",
            "Epoch 60/100, Train Loss: 0.0400, Val Loss: 0.0370\n",
            "Epoch 61/100, Train Loss: 0.0355, Val Loss: 0.0370\n",
            "Epoch 62/100, Train Loss: 0.0368, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0352, Val Loss: 0.0370\n",
            "Epoch 64/100, Train Loss: 0.0328, Val Loss: 0.0370\n",
            "Epoch 65/100, Train Loss: 0.0365, Val Loss: 0.0370\n",
            "Epoch 66/100, Train Loss: 0.0340, Val Loss: 0.0370\n",
            "Epoch 67/100, Train Loss: 0.0375, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0370, Val Loss: 0.0370\n",
            "Epoch 69/100, Train Loss: 0.0355, Val Loss: 0.0370\n",
            "Epoch 70/100, Train Loss: 0.0361, Val Loss: 0.0370\n",
            "Epoch 71/100, Train Loss: 0.0337, Val Loss: 0.0370\n",
            "Epoch 72/100, Train Loss: 0.0361, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0317, Val Loss: 0.0370\n",
            "Epoch 74/100, Train Loss: 0.0361, Val Loss: 0.0370\n",
            "Epoch 75/100, Train Loss: 0.0368, Val Loss: 0.0370\n",
            "Epoch 76/100, Train Loss: 0.0386, Val Loss: 0.0370\n",
            "Epoch 77/100, Train Loss: 0.0372, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0381, Val Loss: 0.0370\n",
            "Epoch 79/100, Train Loss: 0.0376, Val Loss: 0.0370\n",
            "Epoch 80/100, Train Loss: 0.0359, Val Loss: 0.0370\n",
            "Epoch 81/100, Train Loss: 0.0374, Val Loss: 0.0370\n",
            "Epoch 82/100, Train Loss: 0.0402, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0347, Val Loss: 0.0370\n",
            "Epoch 84/100, Train Loss: 0.0398, Val Loss: 0.0370\n",
            "Epoch 85/100, Train Loss: 0.0329, Val Loss: 0.0370\n",
            "Epoch 86/100, Train Loss: 0.0343, Val Loss: 0.0370\n",
            "Epoch 87/100, Train Loss: 0.0366, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0377, Val Loss: 0.0370\n",
            "Epoch 89/100, Train Loss: 0.0400, Val Loss: 0.0370\n",
            "Epoch 90/100, Train Loss: 0.0379, Val Loss: 0.0370\n",
            "Epoch 91/100, Train Loss: 0.0366, Val Loss: 0.0370\n",
            "Epoch 92/100, Train Loss: 0.0388, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0386, Val Loss: 0.0370\n",
            "Epoch 94/100, Train Loss: 0.0299, Val Loss: 0.0370\n",
            "Epoch 95/100, Train Loss: 0.0355, Val Loss: 0.0370\n",
            "Epoch 96/100, Train Loss: 0.0332, Val Loss: 0.0370\n",
            "Epoch 97/100, Train Loss: 0.0358, Val Loss: 0.0370\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0370, Val Loss: 0.0370\n",
            "Epoch 99/100, Train Loss: 0.0380, Val Loss: 0.0370\n",
            "Epoch 100/100, Train Loss: 0.0342, Val Loss: 0.0370\n",
            "\n",
            "Test iteration 15/28\n",
            "Current training set size: 111 samples\n",
            "Epoch 1/100, Train Loss: 0.1628, Val Loss: 0.1496\n",
            "Epoch 2/100, Train Loss: 0.1123, Val Loss: 0.1022\n",
            "Epoch 3/100, Train Loss: 0.0940, Val Loss: 0.0911\n",
            "Epoch 4/100, Train Loss: 0.0731, Val Loss: 0.0533\n",
            "Epoch 5/100, Train Loss: 0.0649, Val Loss: 0.0538\n",
            "Epoch 6/100, Train Loss: 0.0513, Val Loss: 0.0431\n",
            "Epoch 7/100, Train Loss: 0.0463, Val Loss: 0.0439\n",
            "Epoch 8/100, Train Loss: 0.0467, Val Loss: 0.0522\n",
            "Epoch 9/100, Train Loss: 0.0467, Val Loss: 0.0456\n",
            "Epoch 10/100, Train Loss: 0.0503, Val Loss: 0.0465\n",
            "Epoch 11/100, Train Loss: 0.0427, Val Loss: 0.0507\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 12/100, Train Loss: 0.0453, Val Loss: 0.0489\n",
            "Epoch 13/100, Train Loss: 0.0453, Val Loss: 0.0464\n",
            "Epoch 14/100, Train Loss: 0.0435, Val Loss: 0.0452\n",
            "Epoch 15/100, Train Loss: 0.0381, Val Loss: 0.0438\n",
            "Epoch 16/100, Train Loss: 0.0389, Val Loss: 0.0429\n",
            "Epoch 17/100, Train Loss: 0.0405, Val Loss: 0.0426\n",
            "Epoch 18/100, Train Loss: 0.0414, Val Loss: 0.0423\n",
            "Epoch 19/100, Train Loss: 0.0380, Val Loss: 0.0421\n",
            "Epoch 20/100, Train Loss: 0.0369, Val Loss: 0.0421\n",
            "Epoch 21/100, Train Loss: 0.0401, Val Loss: 0.0417\n",
            "Epoch 22/100, Train Loss: 0.0380, Val Loss: 0.0414\n",
            "Epoch 23/100, Train Loss: 0.0371, Val Loss: 0.0414\n",
            "Epoch 24/100, Train Loss: 0.0349, Val Loss: 0.0413\n",
            "Epoch 25/100, Train Loss: 0.0370, Val Loss: 0.0413\n",
            "Epoch 26/100, Train Loss: 0.0386, Val Loss: 0.0413\n",
            "Epoch 27/100, Train Loss: 0.0370, Val Loss: 0.0412\n",
            "Epoch 28/100, Train Loss: 0.0432, Val Loss: 0.0412\n",
            "Epoch 29/100, Train Loss: 0.0413, Val Loss: 0.0412\n",
            "Epoch 30/100, Train Loss: 0.0375, Val Loss: 0.0411\n",
            "Epoch 31/100, Train Loss: 0.0362, Val Loss: 0.0410\n",
            "Epoch 32/100, Train Loss: 0.0345, Val Loss: 0.0411\n",
            "Epoch 33/100, Train Loss: 0.0401, Val Loss: 0.0410\n",
            "Epoch 34/100, Train Loss: 0.0365, Val Loss: 0.0409\n",
            "Epoch 35/100, Train Loss: 0.0357, Val Loss: 0.0410\n",
            "Epoch 36/100, Train Loss: 0.0390, Val Loss: 0.0411\n",
            "Epoch 37/100, Train Loss: 0.0388, Val Loss: 0.0411\n",
            "Epoch 38/100, Train Loss: 0.0359, Val Loss: 0.0412\n",
            "Epoch 39/100, Train Loss: 0.0382, Val Loss: 0.0412\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0417, Val Loss: 0.0412\n",
            "Epoch 41/100, Train Loss: 0.0365, Val Loss: 0.0412\n",
            "Epoch 42/100, Train Loss: 0.0376, Val Loss: 0.0412\n",
            "Epoch 43/100, Train Loss: 0.0346, Val Loss: 0.0412\n",
            "Epoch 44/100, Train Loss: 0.0330, Val Loss: 0.0412\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0387, Val Loss: 0.0412\n",
            "Epoch 46/100, Train Loss: 0.0340, Val Loss: 0.0412\n",
            "Epoch 47/100, Train Loss: 0.0383, Val Loss: 0.0412\n",
            "Epoch 48/100, Train Loss: 0.0371, Val Loss: 0.0412\n",
            "Epoch 49/100, Train Loss: 0.0366, Val Loss: 0.0412\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0370, Val Loss: 0.0412\n",
            "Epoch 51/100, Train Loss: 0.0359, Val Loss: 0.0412\n",
            "Epoch 52/100, Train Loss: 0.0346, Val Loss: 0.0412\n",
            "Epoch 53/100, Train Loss: 0.0413, Val Loss: 0.0412\n",
            "Epoch 54/100, Train Loss: 0.0362, Val Loss: 0.0412\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0356, Val Loss: 0.0412\n",
            "Epoch 56/100, Train Loss: 0.0386, Val Loss: 0.0412\n",
            "Epoch 57/100, Train Loss: 0.0365, Val Loss: 0.0412\n",
            "Epoch 58/100, Train Loss: 0.0369, Val Loss: 0.0412\n",
            "Epoch 59/100, Train Loss: 0.0389, Val Loss: 0.0412\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0368, Val Loss: 0.0412\n",
            "Epoch 61/100, Train Loss: 0.0374, Val Loss: 0.0412\n",
            "Epoch 62/100, Train Loss: 0.0435, Val Loss: 0.0412\n",
            "Epoch 63/100, Train Loss: 0.0367, Val Loss: 0.0412\n",
            "Epoch 64/100, Train Loss: 0.0383, Val Loss: 0.0412\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0372, Val Loss: 0.0412\n",
            "Epoch 66/100, Train Loss: 0.0390, Val Loss: 0.0412\n",
            "Epoch 67/100, Train Loss: 0.0390, Val Loss: 0.0412\n",
            "Epoch 68/100, Train Loss: 0.0420, Val Loss: 0.0412\n",
            "Epoch 69/100, Train Loss: 0.0354, Val Loss: 0.0412\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0360, Val Loss: 0.0412\n",
            "Epoch 71/100, Train Loss: 0.0408, Val Loss: 0.0412\n",
            "Epoch 72/100, Train Loss: 0.0383, Val Loss: 0.0412\n",
            "Epoch 73/100, Train Loss: 0.0384, Val Loss: 0.0412\n",
            "Epoch 74/100, Train Loss: 0.0339, Val Loss: 0.0412\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0356, Val Loss: 0.0412\n",
            "Epoch 76/100, Train Loss: 0.0333, Val Loss: 0.0412\n",
            "Epoch 77/100, Train Loss: 0.0368, Val Loss: 0.0412\n",
            "Epoch 78/100, Train Loss: 0.0370, Val Loss: 0.0412\n",
            "Epoch 79/100, Train Loss: 0.0372, Val Loss: 0.0412\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0357, Val Loss: 0.0412\n",
            "Epoch 81/100, Train Loss: 0.0371, Val Loss: 0.0412\n",
            "Epoch 82/100, Train Loss: 0.0398, Val Loss: 0.0412\n",
            "Epoch 83/100, Train Loss: 0.0376, Val Loss: 0.0412\n",
            "Epoch 84/100, Train Loss: 0.0347, Val Loss: 0.0412\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0374, Val Loss: 0.0412\n",
            "Epoch 86/100, Train Loss: 0.0348, Val Loss: 0.0412\n",
            "Epoch 87/100, Train Loss: 0.0350, Val Loss: 0.0412\n",
            "Epoch 88/100, Train Loss: 0.0372, Val Loss: 0.0412\n",
            "Epoch 89/100, Train Loss: 0.0390, Val Loss: 0.0412\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0350, Val Loss: 0.0412\n",
            "Epoch 91/100, Train Loss: 0.0403, Val Loss: 0.0412\n",
            "Epoch 92/100, Train Loss: 0.0408, Val Loss: 0.0412\n",
            "Epoch 93/100, Train Loss: 0.0357, Val Loss: 0.0412\n",
            "Epoch 94/100, Train Loss: 0.0394, Val Loss: 0.0412\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0405, Val Loss: 0.0412\n",
            "Epoch 96/100, Train Loss: 0.0424, Val Loss: 0.0412\n",
            "Epoch 97/100, Train Loss: 0.0359, Val Loss: 0.0412\n",
            "Epoch 98/100, Train Loss: 0.0342, Val Loss: 0.0412\n",
            "Epoch 99/100, Train Loss: 0.0379, Val Loss: 0.0412\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0357, Val Loss: 0.0412\n",
            "\n",
            "Test iteration 16/28\n",
            "Current training set size: 112 samples\n",
            "Epoch 1/100, Train Loss: 0.1588, Val Loss: 0.1380\n",
            "Epoch 2/100, Train Loss: 0.1035, Val Loss: 0.1030\n",
            "Epoch 3/100, Train Loss: 0.0778, Val Loss: 0.0559\n",
            "Epoch 4/100, Train Loss: 0.0699, Val Loss: 0.0412\n",
            "Epoch 5/100, Train Loss: 0.0657, Val Loss: 0.0444\n",
            "Epoch 6/100, Train Loss: 0.0563, Val Loss: 0.0571\n",
            "Epoch 7/100, Train Loss: 0.0531, Val Loss: 0.0421\n",
            "Epoch 8/100, Train Loss: 0.0477, Val Loss: 0.0540\n",
            "Epoch 9/100, Train Loss: 0.0456, Val Loss: 0.0613\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 10/100, Train Loss: 0.0546, Val Loss: 0.0569\n",
            "Epoch 11/100, Train Loss: 0.0449, Val Loss: 0.0526\n",
            "Epoch 12/100, Train Loss: 0.0446, Val Loss: 0.0502\n",
            "Epoch 13/100, Train Loss: 0.0421, Val Loss: 0.0480\n",
            "Epoch 14/100, Train Loss: 0.0407, Val Loss: 0.0467\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 15/100, Train Loss: 0.0387, Val Loss: 0.0467\n",
            "Epoch 16/100, Train Loss: 0.0412, Val Loss: 0.0467\n",
            "Epoch 17/100, Train Loss: 0.0392, Val Loss: 0.0466\n",
            "Epoch 18/100, Train Loss: 0.0372, Val Loss: 0.0466\n",
            "Epoch 19/100, Train Loss: 0.0414, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0437, Val Loss: 0.0466\n",
            "Epoch 21/100, Train Loss: 0.0411, Val Loss: 0.0466\n",
            "Epoch 22/100, Train Loss: 0.0407, Val Loss: 0.0466\n",
            "Epoch 23/100, Train Loss: 0.0400, Val Loss: 0.0466\n",
            "Epoch 24/100, Train Loss: 0.0378, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0420, Val Loss: 0.0466\n",
            "Epoch 26/100, Train Loss: 0.0397, Val Loss: 0.0466\n",
            "Epoch 27/100, Train Loss: 0.0411, Val Loss: 0.0466\n",
            "Epoch 28/100, Train Loss: 0.0426, Val Loss: 0.0466\n",
            "Epoch 29/100, Train Loss: 0.0380, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0429, Val Loss: 0.0466\n",
            "Epoch 31/100, Train Loss: 0.0418, Val Loss: 0.0466\n",
            "Epoch 32/100, Train Loss: 0.0391, Val Loss: 0.0466\n",
            "Epoch 33/100, Train Loss: 0.0438, Val Loss: 0.0466\n",
            "Epoch 34/100, Train Loss: 0.0355, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0370, Val Loss: 0.0466\n",
            "Epoch 36/100, Train Loss: 0.0413, Val Loss: 0.0466\n",
            "Epoch 37/100, Train Loss: 0.0412, Val Loss: 0.0466\n",
            "Epoch 38/100, Train Loss: 0.0418, Val Loss: 0.0466\n",
            "Epoch 39/100, Train Loss: 0.0469, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0426, Val Loss: 0.0466\n",
            "Epoch 41/100, Train Loss: 0.0404, Val Loss: 0.0466\n",
            "Epoch 42/100, Train Loss: 0.0393, Val Loss: 0.0466\n",
            "Epoch 43/100, Train Loss: 0.0425, Val Loss: 0.0466\n",
            "Epoch 44/100, Train Loss: 0.0422, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0413, Val Loss: 0.0466\n",
            "Epoch 46/100, Train Loss: 0.0437, Val Loss: 0.0466\n",
            "Epoch 47/100, Train Loss: 0.0465, Val Loss: 0.0466\n",
            "Epoch 48/100, Train Loss: 0.0397, Val Loss: 0.0466\n",
            "Epoch 49/100, Train Loss: 0.0427, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0412, Val Loss: 0.0466\n",
            "Epoch 51/100, Train Loss: 0.0403, Val Loss: 0.0466\n",
            "Epoch 52/100, Train Loss: 0.0422, Val Loss: 0.0466\n",
            "Epoch 53/100, Train Loss: 0.0437, Val Loss: 0.0466\n",
            "Epoch 54/100, Train Loss: 0.0395, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0455, Val Loss: 0.0466\n",
            "Epoch 56/100, Train Loss: 0.0457, Val Loss: 0.0466\n",
            "Epoch 57/100, Train Loss: 0.0390, Val Loss: 0.0466\n",
            "Epoch 58/100, Train Loss: 0.0423, Val Loss: 0.0466\n",
            "Epoch 59/100, Train Loss: 0.0398, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0403, Val Loss: 0.0466\n",
            "Epoch 61/100, Train Loss: 0.0420, Val Loss: 0.0466\n",
            "Epoch 62/100, Train Loss: 0.0397, Val Loss: 0.0466\n",
            "Epoch 63/100, Train Loss: 0.0381, Val Loss: 0.0466\n",
            "Epoch 64/100, Train Loss: 0.0411, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0449, Val Loss: 0.0466\n",
            "Epoch 66/100, Train Loss: 0.0375, Val Loss: 0.0466\n",
            "Epoch 67/100, Train Loss: 0.0400, Val Loss: 0.0466\n",
            "Epoch 68/100, Train Loss: 0.0383, Val Loss: 0.0466\n",
            "Epoch 69/100, Train Loss: 0.0389, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0396, Val Loss: 0.0466\n",
            "Epoch 71/100, Train Loss: 0.0421, Val Loss: 0.0466\n",
            "Epoch 72/100, Train Loss: 0.0428, Val Loss: 0.0466\n",
            "Epoch 73/100, Train Loss: 0.0423, Val Loss: 0.0466\n",
            "Epoch 74/100, Train Loss: 0.0419, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0393, Val Loss: 0.0466\n",
            "Epoch 76/100, Train Loss: 0.0419, Val Loss: 0.0466\n",
            "Epoch 77/100, Train Loss: 0.0387, Val Loss: 0.0466\n",
            "Epoch 78/100, Train Loss: 0.0429, Val Loss: 0.0466\n",
            "Epoch 79/100, Train Loss: 0.0389, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0376, Val Loss: 0.0466\n",
            "Epoch 81/100, Train Loss: 0.0373, Val Loss: 0.0466\n",
            "Epoch 82/100, Train Loss: 0.0386, Val Loss: 0.0466\n",
            "Epoch 83/100, Train Loss: 0.0430, Val Loss: 0.0466\n",
            "Epoch 84/100, Train Loss: 0.0428, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0438, Val Loss: 0.0466\n",
            "Epoch 86/100, Train Loss: 0.0409, Val Loss: 0.0466\n",
            "Epoch 87/100, Train Loss: 0.0450, Val Loss: 0.0466\n",
            "Epoch 88/100, Train Loss: 0.0399, Val Loss: 0.0466\n",
            "Epoch 89/100, Train Loss: 0.0426, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0425, Val Loss: 0.0466\n",
            "Epoch 91/100, Train Loss: 0.0410, Val Loss: 0.0466\n",
            "Epoch 92/100, Train Loss: 0.0394, Val Loss: 0.0466\n",
            "Epoch 93/100, Train Loss: 0.0420, Val Loss: 0.0466\n",
            "Epoch 94/100, Train Loss: 0.0419, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0433, Val Loss: 0.0466\n",
            "Epoch 96/100, Train Loss: 0.0407, Val Loss: 0.0466\n",
            "Epoch 97/100, Train Loss: 0.0408, Val Loss: 0.0466\n",
            "Epoch 98/100, Train Loss: 0.0406, Val Loss: 0.0466\n",
            "Epoch 99/100, Train Loss: 0.0408, Val Loss: 0.0466\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0437, Val Loss: 0.0466\n",
            "\n",
            "Test iteration 17/28\n",
            "Current training set size: 113 samples\n",
            "Epoch 1/100, Train Loss: 0.1662, Val Loss: 0.1608\n",
            "Epoch 2/100, Train Loss: 0.1093, Val Loss: 0.0967\n",
            "Epoch 3/100, Train Loss: 0.0901, Val Loss: 0.0782\n",
            "Epoch 4/100, Train Loss: 0.0667, Val Loss: 0.0652\n",
            "Epoch 5/100, Train Loss: 0.0644, Val Loss: 0.0584\n",
            "Epoch 6/100, Train Loss: 0.0541, Val Loss: 0.0484\n",
            "Epoch 7/100, Train Loss: 0.0528, Val Loss: 0.0600\n",
            "Epoch 8/100, Train Loss: 0.0499, Val Loss: 0.0512\n",
            "Epoch 9/100, Train Loss: 0.0413, Val Loss: 0.0520\n",
            "Epoch 10/100, Train Loss: 0.0442, Val Loss: 0.0391\n",
            "Epoch 11/100, Train Loss: 0.0462, Val Loss: 0.0496\n",
            "Epoch 12/100, Train Loss: 0.0408, Val Loss: 0.0649\n",
            "Epoch 13/100, Train Loss: 0.0483, Val Loss: 0.0436\n",
            "Epoch 14/100, Train Loss: 0.0415, Val Loss: 0.0412\n",
            "Epoch 15/100, Train Loss: 0.0430, Val Loss: 0.0375\n",
            "Epoch 16/100, Train Loss: 0.0431, Val Loss: 0.0456\n",
            "Epoch 17/100, Train Loss: 0.0400, Val Loss: 0.0498\n",
            "Epoch 18/100, Train Loss: 0.0315, Val Loss: 0.0444\n",
            "Epoch 19/100, Train Loss: 0.0438, Val Loss: 0.0468\n",
            "Epoch 20/100, Train Loss: 0.0407, Val Loss: 0.0478\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0420, Val Loss: 0.0456\n",
            "Epoch 22/100, Train Loss: 0.0383, Val Loss: 0.0429\n",
            "Epoch 23/100, Train Loss: 0.0379, Val Loss: 0.0423\n",
            "Epoch 24/100, Train Loss: 0.0328, Val Loss: 0.0419\n",
            "Epoch 25/100, Train Loss: 0.0322, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0329, Val Loss: 0.0416\n",
            "Epoch 27/100, Train Loss: 0.0308, Val Loss: 0.0416\n",
            "Epoch 28/100, Train Loss: 0.0344, Val Loss: 0.0416\n",
            "Epoch 29/100, Train Loss: 0.0384, Val Loss: 0.0416\n",
            "Epoch 30/100, Train Loss: 0.0339, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0339, Val Loss: 0.0416\n",
            "Epoch 32/100, Train Loss: 0.0302, Val Loss: 0.0416\n",
            "Epoch 33/100, Train Loss: 0.0357, Val Loss: 0.0416\n",
            "Epoch 34/100, Train Loss: 0.0331, Val Loss: 0.0416\n",
            "Epoch 35/100, Train Loss: 0.0340, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0373, Val Loss: 0.0416\n",
            "Epoch 37/100, Train Loss: 0.0360, Val Loss: 0.0416\n",
            "Epoch 38/100, Train Loss: 0.0326, Val Loss: 0.0416\n",
            "Epoch 39/100, Train Loss: 0.0385, Val Loss: 0.0416\n",
            "Epoch 40/100, Train Loss: 0.0375, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0373, Val Loss: 0.0416\n",
            "Epoch 42/100, Train Loss: 0.0355, Val Loss: 0.0416\n",
            "Epoch 43/100, Train Loss: 0.0355, Val Loss: 0.0416\n",
            "Epoch 44/100, Train Loss: 0.0351, Val Loss: 0.0416\n",
            "Epoch 45/100, Train Loss: 0.0381, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0304, Val Loss: 0.0416\n",
            "Epoch 47/100, Train Loss: 0.0363, Val Loss: 0.0416\n",
            "Epoch 48/100, Train Loss: 0.0369, Val Loss: 0.0416\n",
            "Epoch 49/100, Train Loss: 0.0319, Val Loss: 0.0416\n",
            "Epoch 50/100, Train Loss: 0.0350, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0313, Val Loss: 0.0416\n",
            "Epoch 52/100, Train Loss: 0.0376, Val Loss: 0.0416\n",
            "Epoch 53/100, Train Loss: 0.0361, Val Loss: 0.0416\n",
            "Epoch 54/100, Train Loss: 0.0344, Val Loss: 0.0416\n",
            "Epoch 55/100, Train Loss: 0.0318, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0356, Val Loss: 0.0416\n",
            "Epoch 57/100, Train Loss: 0.0321, Val Loss: 0.0416\n",
            "Epoch 58/100, Train Loss: 0.0351, Val Loss: 0.0416\n",
            "Epoch 59/100, Train Loss: 0.0290, Val Loss: 0.0416\n",
            "Epoch 60/100, Train Loss: 0.0354, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0334, Val Loss: 0.0416\n",
            "Epoch 62/100, Train Loss: 0.0305, Val Loss: 0.0416\n",
            "Epoch 63/100, Train Loss: 0.0374, Val Loss: 0.0416\n",
            "Epoch 64/100, Train Loss: 0.0316, Val Loss: 0.0416\n",
            "Epoch 65/100, Train Loss: 0.0348, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0358, Val Loss: 0.0416\n",
            "Epoch 67/100, Train Loss: 0.0363, Val Loss: 0.0416\n",
            "Epoch 68/100, Train Loss: 0.0363, Val Loss: 0.0416\n",
            "Epoch 69/100, Train Loss: 0.0350, Val Loss: 0.0416\n",
            "Epoch 70/100, Train Loss: 0.0343, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0336, Val Loss: 0.0416\n",
            "Epoch 72/100, Train Loss: 0.0330, Val Loss: 0.0416\n",
            "Epoch 73/100, Train Loss: 0.0348, Val Loss: 0.0416\n",
            "Epoch 74/100, Train Loss: 0.0350, Val Loss: 0.0416\n",
            "Epoch 75/100, Train Loss: 0.0351, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0337, Val Loss: 0.0416\n",
            "Epoch 77/100, Train Loss: 0.0367, Val Loss: 0.0416\n",
            "Epoch 78/100, Train Loss: 0.0343, Val Loss: 0.0416\n",
            "Epoch 79/100, Train Loss: 0.0346, Val Loss: 0.0416\n",
            "Epoch 80/100, Train Loss: 0.0351, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0356, Val Loss: 0.0416\n",
            "Epoch 82/100, Train Loss: 0.0305, Val Loss: 0.0416\n",
            "Epoch 83/100, Train Loss: 0.0349, Val Loss: 0.0416\n",
            "Epoch 84/100, Train Loss: 0.0348, Val Loss: 0.0416\n",
            "Epoch 85/100, Train Loss: 0.0361, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0376, Val Loss: 0.0416\n",
            "Epoch 87/100, Train Loss: 0.0343, Val Loss: 0.0416\n",
            "Epoch 88/100, Train Loss: 0.0342, Val Loss: 0.0416\n",
            "Epoch 89/100, Train Loss: 0.0358, Val Loss: 0.0416\n",
            "Epoch 90/100, Train Loss: 0.0343, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0315, Val Loss: 0.0416\n",
            "Epoch 92/100, Train Loss: 0.0353, Val Loss: 0.0416\n",
            "Epoch 93/100, Train Loss: 0.0382, Val Loss: 0.0416\n",
            "Epoch 94/100, Train Loss: 0.0329, Val Loss: 0.0416\n",
            "Epoch 95/100, Train Loss: 0.0341, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0364, Val Loss: 0.0416\n",
            "Epoch 97/100, Train Loss: 0.0363, Val Loss: 0.0416\n",
            "Epoch 98/100, Train Loss: 0.0371, Val Loss: 0.0416\n",
            "Epoch 99/100, Train Loss: 0.0328, Val Loss: 0.0416\n",
            "Epoch 100/100, Train Loss: 0.0363, Val Loss: 0.0416\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 18/28\n",
            "Current training set size: 114 samples\n",
            "Epoch 1/100, Train Loss: 0.1627, Val Loss: 0.1616\n",
            "Epoch 2/100, Train Loss: 0.0965, Val Loss: 0.0985\n",
            "Epoch 3/100, Train Loss: 0.0842, Val Loss: 0.0937\n",
            "Epoch 4/100, Train Loss: 0.0704, Val Loss: 0.0607\n",
            "Epoch 5/100, Train Loss: 0.0589, Val Loss: 0.0697\n",
            "Epoch 6/100, Train Loss: 0.0553, Val Loss: 0.0733\n",
            "Epoch 7/100, Train Loss: 0.0540, Val Loss: 0.0605\n",
            "Epoch 8/100, Train Loss: 0.0491, Val Loss: 0.0497\n",
            "Epoch 9/100, Train Loss: 0.0484, Val Loss: 0.0531\n",
            "Epoch 10/100, Train Loss: 0.0463, Val Loss: 0.0482\n",
            "Epoch 11/100, Train Loss: 0.0445, Val Loss: 0.0454\n",
            "Epoch 12/100, Train Loss: 0.0429, Val Loss: 0.0494\n",
            "Epoch 13/100, Train Loss: 0.0495, Val Loss: 0.0565\n",
            "Epoch 14/100, Train Loss: 0.0430, Val Loss: 0.0436\n",
            "Epoch 15/100, Train Loss: 0.0405, Val Loss: 0.0403\n",
            "Epoch 16/100, Train Loss: 0.0420, Val Loss: 0.0456\n",
            "Epoch 17/100, Train Loss: 0.0419, Val Loss: 0.0412\n",
            "Epoch 18/100, Train Loss: 0.0382, Val Loss: 0.0484\n",
            "Epoch 19/100, Train Loss: 0.0394, Val Loss: 0.0433\n",
            "Epoch 20/100, Train Loss: 0.0396, Val Loss: 0.0429\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0386, Val Loss: 0.0416\n",
            "Epoch 22/100, Train Loss: 0.0343, Val Loss: 0.0396\n",
            "Epoch 23/100, Train Loss: 0.0350, Val Loss: 0.0394\n",
            "Epoch 24/100, Train Loss: 0.0346, Val Loss: 0.0392\n",
            "Epoch 25/100, Train Loss: 0.0358, Val Loss: 0.0393\n",
            "Epoch 26/100, Train Loss: 0.0358, Val Loss: 0.0394\n",
            "Epoch 27/100, Train Loss: 0.0314, Val Loss: 0.0396\n",
            "Epoch 28/100, Train Loss: 0.0324, Val Loss: 0.0396\n",
            "Epoch 29/100, Train Loss: 0.0320, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0317, Val Loss: 0.0398\n",
            "Epoch 31/100, Train Loss: 0.0324, Val Loss: 0.0398\n",
            "Epoch 32/100, Train Loss: 0.0322, Val Loss: 0.0398\n",
            "Epoch 33/100, Train Loss: 0.0288, Val Loss: 0.0398\n",
            "Epoch 34/100, Train Loss: 0.0294, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0328, Val Loss: 0.0398\n",
            "Epoch 36/100, Train Loss: 0.0348, Val Loss: 0.0398\n",
            "Epoch 37/100, Train Loss: 0.0320, Val Loss: 0.0398\n",
            "Epoch 38/100, Train Loss: 0.0343, Val Loss: 0.0398\n",
            "Epoch 39/100, Train Loss: 0.0323, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0326, Val Loss: 0.0398\n",
            "Epoch 41/100, Train Loss: 0.0328, Val Loss: 0.0398\n",
            "Epoch 42/100, Train Loss: 0.0311, Val Loss: 0.0398\n",
            "Epoch 43/100, Train Loss: 0.0329, Val Loss: 0.0398\n",
            "Epoch 44/100, Train Loss: 0.0311, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0307, Val Loss: 0.0398\n",
            "Epoch 46/100, Train Loss: 0.0335, Val Loss: 0.0398\n",
            "Epoch 47/100, Train Loss: 0.0340, Val Loss: 0.0398\n",
            "Epoch 48/100, Train Loss: 0.0350, Val Loss: 0.0398\n",
            "Epoch 49/100, Train Loss: 0.0324, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0367, Val Loss: 0.0398\n",
            "Epoch 51/100, Train Loss: 0.0295, Val Loss: 0.0398\n",
            "Epoch 52/100, Train Loss: 0.0344, Val Loss: 0.0398\n",
            "Epoch 53/100, Train Loss: 0.0370, Val Loss: 0.0398\n",
            "Epoch 54/100, Train Loss: 0.0330, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0308, Val Loss: 0.0398\n",
            "Epoch 56/100, Train Loss: 0.0326, Val Loss: 0.0398\n",
            "Epoch 57/100, Train Loss: 0.0334, Val Loss: 0.0398\n",
            "Epoch 58/100, Train Loss: 0.0294, Val Loss: 0.0398\n",
            "Epoch 59/100, Train Loss: 0.0337, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0349, Val Loss: 0.0398\n",
            "Epoch 61/100, Train Loss: 0.0349, Val Loss: 0.0398\n",
            "Epoch 62/100, Train Loss: 0.0347, Val Loss: 0.0398\n",
            "Epoch 63/100, Train Loss: 0.0366, Val Loss: 0.0398\n",
            "Epoch 64/100, Train Loss: 0.0310, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0331, Val Loss: 0.0398\n",
            "Epoch 66/100, Train Loss: 0.0318, Val Loss: 0.0398\n",
            "Epoch 67/100, Train Loss: 0.0329, Val Loss: 0.0398\n",
            "Epoch 68/100, Train Loss: 0.0324, Val Loss: 0.0398\n",
            "Epoch 69/100, Train Loss: 0.0336, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0329, Val Loss: 0.0398\n",
            "Epoch 71/100, Train Loss: 0.0288, Val Loss: 0.0398\n",
            "Epoch 72/100, Train Loss: 0.0300, Val Loss: 0.0398\n",
            "Epoch 73/100, Train Loss: 0.0331, Val Loss: 0.0398\n",
            "Epoch 74/100, Train Loss: 0.0347, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0304, Val Loss: 0.0398\n",
            "Epoch 76/100, Train Loss: 0.0303, Val Loss: 0.0398\n",
            "Epoch 77/100, Train Loss: 0.0325, Val Loss: 0.0398\n",
            "Epoch 78/100, Train Loss: 0.0310, Val Loss: 0.0398\n",
            "Epoch 79/100, Train Loss: 0.0319, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0320, Val Loss: 0.0398\n",
            "Epoch 81/100, Train Loss: 0.0323, Val Loss: 0.0398\n",
            "Epoch 82/100, Train Loss: 0.0310, Val Loss: 0.0398\n",
            "Epoch 83/100, Train Loss: 0.0332, Val Loss: 0.0398\n",
            "Epoch 84/100, Train Loss: 0.0348, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0321, Val Loss: 0.0398\n",
            "Epoch 86/100, Train Loss: 0.0314, Val Loss: 0.0398\n",
            "Epoch 87/100, Train Loss: 0.0302, Val Loss: 0.0398\n",
            "Epoch 88/100, Train Loss: 0.0359, Val Loss: 0.0398\n",
            "Epoch 89/100, Train Loss: 0.0328, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0308, Val Loss: 0.0398\n",
            "Epoch 91/100, Train Loss: 0.0349, Val Loss: 0.0398\n",
            "Epoch 92/100, Train Loss: 0.0302, Val Loss: 0.0398\n",
            "Epoch 93/100, Train Loss: 0.0324, Val Loss: 0.0398\n",
            "Epoch 94/100, Train Loss: 0.0309, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0313, Val Loss: 0.0398\n",
            "Epoch 96/100, Train Loss: 0.0296, Val Loss: 0.0398\n",
            "Epoch 97/100, Train Loss: 0.0349, Val Loss: 0.0398\n",
            "Epoch 98/100, Train Loss: 0.0311, Val Loss: 0.0398\n",
            "Epoch 99/100, Train Loss: 0.0317, Val Loss: 0.0398\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0303, Val Loss: 0.0398\n",
            "\n",
            "Test iteration 19/28\n",
            "Current training set size: 115 samples\n",
            "Epoch 1/100, Train Loss: 0.1664, Val Loss: 0.1595\n",
            "Epoch 2/100, Train Loss: 0.1089, Val Loss: 0.0948\n",
            "Epoch 3/100, Train Loss: 0.0832, Val Loss: 0.0801\n",
            "Epoch 4/100, Train Loss: 0.0706, Val Loss: 0.0458\n",
            "Epoch 5/100, Train Loss: 0.0576, Val Loss: 0.0546\n",
            "Epoch 6/100, Train Loss: 0.0571, Val Loss: 0.0438\n",
            "Epoch 7/100, Train Loss: 0.0499, Val Loss: 0.0680\n",
            "Epoch 8/100, Train Loss: 0.0537, Val Loss: 0.0597\n",
            "Epoch 9/100, Train Loss: 0.0574, Val Loss: 0.0448\n",
            "Epoch 10/100, Train Loss: 0.0417, Val Loss: 0.0402\n",
            "Epoch 11/100, Train Loss: 0.0462, Val Loss: 0.0428\n",
            "Epoch 12/100, Train Loss: 0.0479, Val Loss: 0.0413\n",
            "Epoch 13/100, Train Loss: 0.0415, Val Loss: 0.0519\n",
            "Epoch 14/100, Train Loss: 0.0356, Val Loss: 0.0426\n",
            "Epoch 15/100, Train Loss: 0.0441, Val Loss: 0.0389\n",
            "Epoch 16/100, Train Loss: 0.0412, Val Loss: 0.0608\n",
            "Epoch 17/100, Train Loss: 0.0479, Val Loss: 0.0365\n",
            "Epoch 18/100, Train Loss: 0.0403, Val Loss: 0.0486\n",
            "Epoch 19/100, Train Loss: 0.0460, Val Loss: 0.0430\n",
            "Epoch 20/100, Train Loss: 0.0425, Val Loss: 0.0412\n",
            "Epoch 21/100, Train Loss: 0.0432, Val Loss: 0.0362\n",
            "Epoch 22/100, Train Loss: 0.0370, Val Loss: 0.0445\n",
            "Epoch 23/100, Train Loss: 0.0338, Val Loss: 0.0536\n",
            "Epoch 24/100, Train Loss: 0.0436, Val Loss: 0.0479\n",
            "Epoch 25/100, Train Loss: 0.0355, Val Loss: 0.0487\n",
            "Epoch 26/100, Train Loss: 0.0404, Val Loss: 0.0464\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0373, Val Loss: 0.0451\n",
            "Epoch 28/100, Train Loss: 0.0359, Val Loss: 0.0425\n",
            "Epoch 29/100, Train Loss: 0.0326, Val Loss: 0.0410\n",
            "Epoch 30/100, Train Loss: 0.0336, Val Loss: 0.0403\n",
            "Epoch 31/100, Train Loss: 0.0290, Val Loss: 0.0397\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0342, Val Loss: 0.0397\n",
            "Epoch 33/100, Train Loss: 0.0300, Val Loss: 0.0397\n",
            "Epoch 34/100, Train Loss: 0.0287, Val Loss: 0.0396\n",
            "Epoch 35/100, Train Loss: 0.0314, Val Loss: 0.0396\n",
            "Epoch 36/100, Train Loss: 0.0313, Val Loss: 0.0396\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0300, Val Loss: 0.0396\n",
            "Epoch 38/100, Train Loss: 0.0288, Val Loss: 0.0396\n",
            "Epoch 39/100, Train Loss: 0.0282, Val Loss: 0.0396\n",
            "Epoch 40/100, Train Loss: 0.0310, Val Loss: 0.0396\n",
            "Epoch 41/100, Train Loss: 0.0336, Val Loss: 0.0396\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0320, Val Loss: 0.0396\n",
            "Epoch 43/100, Train Loss: 0.0325, Val Loss: 0.0396\n",
            "Epoch 44/100, Train Loss: 0.0275, Val Loss: 0.0396\n",
            "Epoch 45/100, Train Loss: 0.0336, Val Loss: 0.0396\n",
            "Epoch 46/100, Train Loss: 0.0325, Val Loss: 0.0396\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0319, Val Loss: 0.0396\n",
            "Epoch 48/100, Train Loss: 0.0272, Val Loss: 0.0396\n",
            "Epoch 49/100, Train Loss: 0.0313, Val Loss: 0.0396\n",
            "Epoch 50/100, Train Loss: 0.0307, Val Loss: 0.0396\n",
            "Epoch 51/100, Train Loss: 0.0339, Val Loss: 0.0396\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0339, Val Loss: 0.0396\n",
            "Epoch 53/100, Train Loss: 0.0324, Val Loss: 0.0396\n",
            "Epoch 54/100, Train Loss: 0.0309, Val Loss: 0.0396\n",
            "Epoch 55/100, Train Loss: 0.0313, Val Loss: 0.0396\n",
            "Epoch 56/100, Train Loss: 0.0328, Val Loss: 0.0396\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0296, Val Loss: 0.0396\n",
            "Epoch 58/100, Train Loss: 0.0359, Val Loss: 0.0396\n",
            "Epoch 59/100, Train Loss: 0.0341, Val Loss: 0.0396\n",
            "Epoch 60/100, Train Loss: 0.0326, Val Loss: 0.0396\n",
            "Epoch 61/100, Train Loss: 0.0332, Val Loss: 0.0396\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0308, Val Loss: 0.0396\n",
            "Epoch 63/100, Train Loss: 0.0302, Val Loss: 0.0396\n",
            "Epoch 64/100, Train Loss: 0.0325, Val Loss: 0.0396\n",
            "Epoch 65/100, Train Loss: 0.0337, Val Loss: 0.0396\n",
            "Epoch 66/100, Train Loss: 0.0339, Val Loss: 0.0396\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0310, Val Loss: 0.0396\n",
            "Epoch 68/100, Train Loss: 0.0349, Val Loss: 0.0396\n",
            "Epoch 69/100, Train Loss: 0.0292, Val Loss: 0.0396\n",
            "Epoch 70/100, Train Loss: 0.0334, Val Loss: 0.0396\n",
            "Epoch 71/100, Train Loss: 0.0313, Val Loss: 0.0396\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0325, Val Loss: 0.0396\n",
            "Epoch 73/100, Train Loss: 0.0315, Val Loss: 0.0396\n",
            "Epoch 74/100, Train Loss: 0.0323, Val Loss: 0.0396\n",
            "Epoch 75/100, Train Loss: 0.0306, Val Loss: 0.0396\n",
            "Epoch 76/100, Train Loss: 0.0378, Val Loss: 0.0396\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0280, Val Loss: 0.0396\n",
            "Epoch 78/100, Train Loss: 0.0297, Val Loss: 0.0396\n",
            "Epoch 79/100, Train Loss: 0.0326, Val Loss: 0.0396\n",
            "Epoch 80/100, Train Loss: 0.0297, Val Loss: 0.0396\n",
            "Epoch 81/100, Train Loss: 0.0320, Val Loss: 0.0396\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0329, Val Loss: 0.0396\n",
            "Epoch 83/100, Train Loss: 0.0322, Val Loss: 0.0396\n",
            "Epoch 84/100, Train Loss: 0.0330, Val Loss: 0.0396\n",
            "Epoch 85/100, Train Loss: 0.0346, Val Loss: 0.0396\n",
            "Epoch 86/100, Train Loss: 0.0311, Val Loss: 0.0396\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0334, Val Loss: 0.0396\n",
            "Epoch 88/100, Train Loss: 0.0304, Val Loss: 0.0396\n",
            "Epoch 89/100, Train Loss: 0.0300, Val Loss: 0.0396\n",
            "Epoch 90/100, Train Loss: 0.0315, Val Loss: 0.0396\n",
            "Epoch 91/100, Train Loss: 0.0319, Val Loss: 0.0396\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0292, Val Loss: 0.0396\n",
            "Epoch 93/100, Train Loss: 0.0341, Val Loss: 0.0396\n",
            "Epoch 94/100, Train Loss: 0.0340, Val Loss: 0.0396\n",
            "Epoch 95/100, Train Loss: 0.0320, Val Loss: 0.0396\n",
            "Epoch 96/100, Train Loss: 0.0294, Val Loss: 0.0396\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0320, Val Loss: 0.0396\n",
            "Epoch 98/100, Train Loss: 0.0324, Val Loss: 0.0396\n",
            "Epoch 99/100, Train Loss: 0.0311, Val Loss: 0.0396\n",
            "Epoch 100/100, Train Loss: 0.0303, Val Loss: 0.0396\n",
            "\n",
            "Test iteration 20/28\n",
            "Current training set size: 116 samples\n",
            "Epoch 1/100, Train Loss: 0.1682, Val Loss: 0.1582\n",
            "Epoch 2/100, Train Loss: 0.1029, Val Loss: 0.0963\n",
            "Epoch 3/100, Train Loss: 0.0808, Val Loss: 0.0618\n",
            "Epoch 4/100, Train Loss: 0.0678, Val Loss: 0.0697\n",
            "Epoch 5/100, Train Loss: 0.0670, Val Loss: 0.0524\n",
            "Epoch 6/100, Train Loss: 0.0641, Val Loss: 0.0522\n",
            "Epoch 7/100, Train Loss: 0.0557, Val Loss: 0.0494\n",
            "Epoch 8/100, Train Loss: 0.0483, Val Loss: 0.0515\n",
            "Epoch 9/100, Train Loss: 0.0481, Val Loss: 0.0508\n",
            "Epoch 10/100, Train Loss: 0.0466, Val Loss: 0.0446\n",
            "Epoch 11/100, Train Loss: 0.0572, Val Loss: 0.0543\n",
            "Epoch 12/100, Train Loss: 0.0468, Val Loss: 0.0504\n",
            "Epoch 13/100, Train Loss: 0.0455, Val Loss: 0.0448\n",
            "Epoch 14/100, Train Loss: 0.0582, Val Loss: 0.0450\n",
            "Epoch 15/100, Train Loss: 0.0466, Val Loss: 0.0508\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0486, Val Loss: 0.0494\n",
            "Epoch 17/100, Train Loss: 0.0435, Val Loss: 0.0472\n",
            "Epoch 18/100, Train Loss: 0.0400, Val Loss: 0.0456\n",
            "Epoch 19/100, Train Loss: 0.0412, Val Loss: 0.0447\n",
            "Epoch 20/100, Train Loss: 0.0418, Val Loss: 0.0443\n",
            "Epoch 21/100, Train Loss: 0.0407, Val Loss: 0.0438\n",
            "Epoch 22/100, Train Loss: 0.0416, Val Loss: 0.0436\n",
            "Epoch 23/100, Train Loss: 0.0395, Val Loss: 0.0434\n",
            "Epoch 24/100, Train Loss: 0.0424, Val Loss: 0.0440\n",
            "Epoch 25/100, Train Loss: 0.0396, Val Loss: 0.0442\n",
            "Epoch 26/100, Train Loss: 0.0420, Val Loss: 0.0447\n",
            "Epoch 27/100, Train Loss: 0.0409, Val Loss: 0.0445\n",
            "Epoch 28/100, Train Loss: 0.0382, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0374, Val Loss: 0.0449\n",
            "Epoch 30/100, Train Loss: 0.0381, Val Loss: 0.0449\n",
            "Epoch 31/100, Train Loss: 0.0399, Val Loss: 0.0449\n",
            "Epoch 32/100, Train Loss: 0.0424, Val Loss: 0.0449\n",
            "Epoch 33/100, Train Loss: 0.0394, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0398, Val Loss: 0.0449\n",
            "Epoch 35/100, Train Loss: 0.0422, Val Loss: 0.0449\n",
            "Epoch 36/100, Train Loss: 0.0379, Val Loss: 0.0449\n",
            "Epoch 37/100, Train Loss: 0.0373, Val Loss: 0.0449\n",
            "Epoch 38/100, Train Loss: 0.0416, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0398, Val Loss: 0.0449\n",
            "Epoch 40/100, Train Loss: 0.0373, Val Loss: 0.0449\n",
            "Epoch 41/100, Train Loss: 0.0404, Val Loss: 0.0449\n",
            "Epoch 42/100, Train Loss: 0.0422, Val Loss: 0.0449\n",
            "Epoch 43/100, Train Loss: 0.0405, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0394, Val Loss: 0.0449\n",
            "Epoch 45/100, Train Loss: 0.0370, Val Loss: 0.0449\n",
            "Epoch 46/100, Train Loss: 0.0384, Val Loss: 0.0449\n",
            "Epoch 47/100, Train Loss: 0.0427, Val Loss: 0.0449\n",
            "Epoch 48/100, Train Loss: 0.0398, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0388, Val Loss: 0.0449\n",
            "Epoch 50/100, Train Loss: 0.0384, Val Loss: 0.0449\n",
            "Epoch 51/100, Train Loss: 0.0380, Val Loss: 0.0449\n",
            "Epoch 52/100, Train Loss: 0.0413, Val Loss: 0.0449\n",
            "Epoch 53/100, Train Loss: 0.0393, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0407, Val Loss: 0.0449\n",
            "Epoch 55/100, Train Loss: 0.0403, Val Loss: 0.0449\n",
            "Epoch 56/100, Train Loss: 0.0414, Val Loss: 0.0449\n",
            "Epoch 57/100, Train Loss: 0.0373, Val Loss: 0.0449\n",
            "Epoch 58/100, Train Loss: 0.0430, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0403, Val Loss: 0.0449\n",
            "Epoch 60/100, Train Loss: 0.0401, Val Loss: 0.0449\n",
            "Epoch 61/100, Train Loss: 0.0386, Val Loss: 0.0449\n",
            "Epoch 62/100, Train Loss: 0.0406, Val Loss: 0.0449\n",
            "Epoch 63/100, Train Loss: 0.0379, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0444, Val Loss: 0.0449\n",
            "Epoch 65/100, Train Loss: 0.0384, Val Loss: 0.0449\n",
            "Epoch 66/100, Train Loss: 0.0409, Val Loss: 0.0449\n",
            "Epoch 67/100, Train Loss: 0.0365, Val Loss: 0.0449\n",
            "Epoch 68/100, Train Loss: 0.0350, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0335, Val Loss: 0.0449\n",
            "Epoch 70/100, Train Loss: 0.0405, Val Loss: 0.0449\n",
            "Epoch 71/100, Train Loss: 0.0384, Val Loss: 0.0449\n",
            "Epoch 72/100, Train Loss: 0.0387, Val Loss: 0.0449\n",
            "Epoch 73/100, Train Loss: 0.0382, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0368, Val Loss: 0.0449\n",
            "Epoch 75/100, Train Loss: 0.0415, Val Loss: 0.0449\n",
            "Epoch 76/100, Train Loss: 0.0377, Val Loss: 0.0449\n",
            "Epoch 77/100, Train Loss: 0.0393, Val Loss: 0.0449\n",
            "Epoch 78/100, Train Loss: 0.0371, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0380, Val Loss: 0.0449\n",
            "Epoch 80/100, Train Loss: 0.0366, Val Loss: 0.0449\n",
            "Epoch 81/100, Train Loss: 0.0377, Val Loss: 0.0449\n",
            "Epoch 82/100, Train Loss: 0.0382, Val Loss: 0.0449\n",
            "Epoch 83/100, Train Loss: 0.0385, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0418, Val Loss: 0.0449\n",
            "Epoch 85/100, Train Loss: 0.0418, Val Loss: 0.0449\n",
            "Epoch 86/100, Train Loss: 0.0421, Val Loss: 0.0449\n",
            "Epoch 87/100, Train Loss: 0.0438, Val Loss: 0.0449\n",
            "Epoch 88/100, Train Loss: 0.0380, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0399, Val Loss: 0.0449\n",
            "Epoch 90/100, Train Loss: 0.0406, Val Loss: 0.0449\n",
            "Epoch 91/100, Train Loss: 0.0384, Val Loss: 0.0449\n",
            "Epoch 92/100, Train Loss: 0.0408, Val Loss: 0.0449\n",
            "Epoch 93/100, Train Loss: 0.0434, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0410, Val Loss: 0.0449\n",
            "Epoch 95/100, Train Loss: 0.0378, Val Loss: 0.0449\n",
            "Epoch 96/100, Train Loss: 0.0399, Val Loss: 0.0449\n",
            "Epoch 97/100, Train Loss: 0.0371, Val Loss: 0.0449\n",
            "Epoch 98/100, Train Loss: 0.0420, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0378, Val Loss: 0.0449\n",
            "Epoch 100/100, Train Loss: 0.0386, Val Loss: 0.0449\n",
            "\n",
            "Test iteration 21/28\n",
            "Current training set size: 117 samples\n",
            "Epoch 1/100, Train Loss: 0.1716, Val Loss: 0.1718\n",
            "Epoch 2/100, Train Loss: 0.1249, Val Loss: 0.0942\n",
            "Epoch 3/100, Train Loss: 0.0889, Val Loss: 0.0728\n",
            "Epoch 4/100, Train Loss: 0.0729, Val Loss: 0.0823\n",
            "Epoch 5/100, Train Loss: 0.0693, Val Loss: 0.0594\n",
            "Epoch 6/100, Train Loss: 0.0619, Val Loss: 0.0502\n",
            "Epoch 7/100, Train Loss: 0.0628, Val Loss: 0.0569\n",
            "Epoch 8/100, Train Loss: 0.0554, Val Loss: 0.0525\n",
            "Epoch 9/100, Train Loss: 0.0500, Val Loss: 0.0519\n",
            "Epoch 10/100, Train Loss: 0.0457, Val Loss: 0.0422\n",
            "Epoch 11/100, Train Loss: 0.0467, Val Loss: 0.0477\n",
            "Epoch 12/100, Train Loss: 0.0515, Val Loss: 0.0401\n",
            "Epoch 13/100, Train Loss: 0.0426, Val Loss: 0.0395\n",
            "Epoch 14/100, Train Loss: 0.0444, Val Loss: 0.0421\n",
            "Epoch 15/100, Train Loss: 0.0462, Val Loss: 0.0672\n",
            "Epoch 16/100, Train Loss: 0.0515, Val Loss: 0.0415\n",
            "Epoch 17/100, Train Loss: 0.0435, Val Loss: 0.0422\n",
            "Epoch 18/100, Train Loss: 0.0446, Val Loss: 0.0934\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0635, Val Loss: 0.0805\n",
            "Epoch 20/100, Train Loss: 0.0539, Val Loss: 0.0677\n",
            "Epoch 21/100, Train Loss: 0.0450, Val Loss: 0.0616\n",
            "Epoch 22/100, Train Loss: 0.0444, Val Loss: 0.0581\n",
            "Epoch 23/100, Train Loss: 0.0372, Val Loss: 0.0557\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0400, Val Loss: 0.0557\n",
            "Epoch 25/100, Train Loss: 0.0384, Val Loss: 0.0557\n",
            "Epoch 26/100, Train Loss: 0.0373, Val Loss: 0.0557\n",
            "Epoch 27/100, Train Loss: 0.0385, Val Loss: 0.0557\n",
            "Epoch 28/100, Train Loss: 0.0427, Val Loss: 0.0557\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0379, Val Loss: 0.0557\n",
            "Epoch 30/100, Train Loss: 0.0417, Val Loss: 0.0557\n",
            "Epoch 31/100, Train Loss: 0.0432, Val Loss: 0.0557\n",
            "Epoch 32/100, Train Loss: 0.0368, Val Loss: 0.0557\n",
            "Epoch 33/100, Train Loss: 0.0389, Val Loss: 0.0557\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0439, Val Loss: 0.0557\n",
            "Epoch 35/100, Train Loss: 0.0430, Val Loss: 0.0557\n",
            "Epoch 36/100, Train Loss: 0.0426, Val Loss: 0.0557\n",
            "Epoch 37/100, Train Loss: 0.0438, Val Loss: 0.0557\n",
            "Epoch 38/100, Train Loss: 0.0362, Val Loss: 0.0557\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0376, Val Loss: 0.0557\n",
            "Epoch 40/100, Train Loss: 0.0408, Val Loss: 0.0557\n",
            "Epoch 41/100, Train Loss: 0.0413, Val Loss: 0.0557\n",
            "Epoch 42/100, Train Loss: 0.0366, Val Loss: 0.0557\n",
            "Epoch 43/100, Train Loss: 0.0411, Val Loss: 0.0557\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0365, Val Loss: 0.0557\n",
            "Epoch 45/100, Train Loss: 0.0437, Val Loss: 0.0557\n",
            "Epoch 46/100, Train Loss: 0.0372, Val Loss: 0.0557\n",
            "Epoch 47/100, Train Loss: 0.0442, Val Loss: 0.0557\n",
            "Epoch 48/100, Train Loss: 0.0386, Val Loss: 0.0557\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0386, Val Loss: 0.0557\n",
            "Epoch 50/100, Train Loss: 0.0367, Val Loss: 0.0557\n",
            "Epoch 51/100, Train Loss: 0.0412, Val Loss: 0.0557\n",
            "Epoch 52/100, Train Loss: 0.0425, Val Loss: 0.0557\n",
            "Epoch 53/100, Train Loss: 0.0370, Val Loss: 0.0557\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0453, Val Loss: 0.0557\n",
            "Epoch 55/100, Train Loss: 0.0413, Val Loss: 0.0557\n",
            "Epoch 56/100, Train Loss: 0.0462, Val Loss: 0.0557\n",
            "Epoch 57/100, Train Loss: 0.0401, Val Loss: 0.0557\n",
            "Epoch 58/100, Train Loss: 0.0424, Val Loss: 0.0557\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0387, Val Loss: 0.0557\n",
            "Epoch 60/100, Train Loss: 0.0432, Val Loss: 0.0557\n",
            "Epoch 61/100, Train Loss: 0.0420, Val Loss: 0.0557\n",
            "Epoch 62/100, Train Loss: 0.0452, Val Loss: 0.0557\n",
            "Epoch 63/100, Train Loss: 0.0355, Val Loss: 0.0557\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0464, Val Loss: 0.0557\n",
            "Epoch 65/100, Train Loss: 0.0377, Val Loss: 0.0557\n",
            "Epoch 66/100, Train Loss: 0.0380, Val Loss: 0.0557\n",
            "Epoch 67/100, Train Loss: 0.0379, Val Loss: 0.0557\n",
            "Epoch 68/100, Train Loss: 0.0395, Val Loss: 0.0557\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0388, Val Loss: 0.0557\n",
            "Epoch 70/100, Train Loss: 0.0423, Val Loss: 0.0557\n",
            "Epoch 71/100, Train Loss: 0.0440, Val Loss: 0.0557\n",
            "Epoch 72/100, Train Loss: 0.0425, Val Loss: 0.0557\n",
            "Epoch 73/100, Train Loss: 0.0388, Val Loss: 0.0557\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0432, Val Loss: 0.0557\n",
            "Epoch 75/100, Train Loss: 0.0412, Val Loss: 0.0557\n",
            "Epoch 76/100, Train Loss: 0.0440, Val Loss: 0.0557\n",
            "Epoch 77/100, Train Loss: 0.0418, Val Loss: 0.0557\n",
            "Epoch 78/100, Train Loss: 0.0387, Val Loss: 0.0557\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0431, Val Loss: 0.0557\n",
            "Epoch 80/100, Train Loss: 0.0434, Val Loss: 0.0557\n",
            "Epoch 81/100, Train Loss: 0.0379, Val Loss: 0.0557\n",
            "Epoch 82/100, Train Loss: 0.0433, Val Loss: 0.0557\n",
            "Epoch 83/100, Train Loss: 0.0421, Val Loss: 0.0557\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0390, Val Loss: 0.0557\n",
            "Epoch 85/100, Train Loss: 0.0434, Val Loss: 0.0557\n",
            "Epoch 86/100, Train Loss: 0.0412, Val Loss: 0.0557\n",
            "Epoch 87/100, Train Loss: 0.0375, Val Loss: 0.0557\n",
            "Epoch 88/100, Train Loss: 0.0414, Val Loss: 0.0557\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0368, Val Loss: 0.0557\n",
            "Epoch 90/100, Train Loss: 0.0377, Val Loss: 0.0557\n",
            "Epoch 91/100, Train Loss: 0.0433, Val Loss: 0.0557\n",
            "Epoch 92/100, Train Loss: 0.0390, Val Loss: 0.0557\n",
            "Epoch 93/100, Train Loss: 0.0406, Val Loss: 0.0557\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0436, Val Loss: 0.0557\n",
            "Epoch 95/100, Train Loss: 0.0410, Val Loss: 0.0557\n",
            "Epoch 96/100, Train Loss: 0.0473, Val Loss: 0.0557\n",
            "Epoch 97/100, Train Loss: 0.0421, Val Loss: 0.0557\n",
            "Epoch 98/100, Train Loss: 0.0423, Val Loss: 0.0557\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0433, Val Loss: 0.0557\n",
            "Epoch 100/100, Train Loss: 0.0401, Val Loss: 0.0557\n",
            "\n",
            "Test iteration 22/28\n",
            "Current training set size: 118 samples\n",
            "Epoch 1/100, Train Loss: 0.1651, Val Loss: 0.1507\n",
            "Epoch 2/100, Train Loss: 0.1021, Val Loss: 0.1017\n",
            "Epoch 3/100, Train Loss: 0.0864, Val Loss: 0.0923\n",
            "Epoch 4/100, Train Loss: 0.0742, Val Loss: 0.0506\n",
            "Epoch 5/100, Train Loss: 0.0723, Val Loss: 0.0766\n",
            "Epoch 6/100, Train Loss: 0.0673, Val Loss: 0.0612\n",
            "Epoch 7/100, Train Loss: 0.0472, Val Loss: 0.0561\n",
            "Epoch 8/100, Train Loss: 0.0487, Val Loss: 0.0463\n",
            "Epoch 9/100, Train Loss: 0.0480, Val Loss: 0.0424\n",
            "Epoch 10/100, Train Loss: 0.0446, Val Loss: 0.0578\n",
            "Epoch 11/100, Train Loss: 0.0480, Val Loss: 0.0484\n",
            "Epoch 12/100, Train Loss: 0.0453, Val Loss: 0.0548\n",
            "Epoch 13/100, Train Loss: 0.0543, Val Loss: 0.0373\n",
            "Epoch 14/100, Train Loss: 0.0521, Val Loss: 0.0497\n",
            "Epoch 15/100, Train Loss: 0.0479, Val Loss: 0.0406\n",
            "Epoch 16/100, Train Loss: 0.0466, Val Loss: 0.0564\n",
            "Epoch 17/100, Train Loss: 0.0438, Val Loss: 0.0510\n",
            "Epoch 18/100, Train Loss: 0.0428, Val Loss: 0.0369\n",
            "Epoch 19/100, Train Loss: 0.0421, Val Loss: 0.0617\n",
            "Epoch 20/100, Train Loss: 0.0464, Val Loss: 0.0379\n",
            "Epoch 21/100, Train Loss: 0.0389, Val Loss: 0.0555\n",
            "Epoch 22/100, Train Loss: 0.0428, Val Loss: 0.0381\n",
            "Epoch 23/100, Train Loss: 0.0427, Val Loss: 0.0457\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0399, Val Loss: 0.0452\n",
            "Epoch 25/100, Train Loss: 0.0379, Val Loss: 0.0448\n",
            "Epoch 26/100, Train Loss: 0.0377, Val Loss: 0.0445\n",
            "Epoch 27/100, Train Loss: 0.0422, Val Loss: 0.0450\n",
            "Epoch 28/100, Train Loss: 0.0385, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0377, Val Loss: 0.0449\n",
            "Epoch 30/100, Train Loss: 0.0355, Val Loss: 0.0449\n",
            "Epoch 31/100, Train Loss: 0.0344, Val Loss: 0.0449\n",
            "Epoch 32/100, Train Loss: 0.0369, Val Loss: 0.0449\n",
            "Epoch 33/100, Train Loss: 0.0368, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0367, Val Loss: 0.0449\n",
            "Epoch 35/100, Train Loss: 0.0409, Val Loss: 0.0449\n",
            "Epoch 36/100, Train Loss: 0.0358, Val Loss: 0.0449\n",
            "Epoch 37/100, Train Loss: 0.0375, Val Loss: 0.0449\n",
            "Epoch 38/100, Train Loss: 0.0361, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0367, Val Loss: 0.0449\n",
            "Epoch 40/100, Train Loss: 0.0365, Val Loss: 0.0449\n",
            "Epoch 41/100, Train Loss: 0.0368, Val Loss: 0.0449\n",
            "Epoch 42/100, Train Loss: 0.0383, Val Loss: 0.0449\n",
            "Epoch 43/100, Train Loss: 0.0372, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0373, Val Loss: 0.0449\n",
            "Epoch 45/100, Train Loss: 0.0352, Val Loss: 0.0449\n",
            "Epoch 46/100, Train Loss: 0.0351, Val Loss: 0.0449\n",
            "Epoch 47/100, Train Loss: 0.0401, Val Loss: 0.0449\n",
            "Epoch 48/100, Train Loss: 0.0371, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0343, Val Loss: 0.0449\n",
            "Epoch 50/100, Train Loss: 0.0397, Val Loss: 0.0449\n",
            "Epoch 51/100, Train Loss: 0.0380, Val Loss: 0.0449\n",
            "Epoch 52/100, Train Loss: 0.0364, Val Loss: 0.0449\n",
            "Epoch 53/100, Train Loss: 0.0363, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0356, Val Loss: 0.0449\n",
            "Epoch 55/100, Train Loss: 0.0376, Val Loss: 0.0449\n",
            "Epoch 56/100, Train Loss: 0.0385, Val Loss: 0.0449\n",
            "Epoch 57/100, Train Loss: 0.0339, Val Loss: 0.0449\n",
            "Epoch 58/100, Train Loss: 0.0383, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0349, Val Loss: 0.0449\n",
            "Epoch 60/100, Train Loss: 0.0407, Val Loss: 0.0449\n",
            "Epoch 61/100, Train Loss: 0.0379, Val Loss: 0.0449\n",
            "Epoch 62/100, Train Loss: 0.0394, Val Loss: 0.0449\n",
            "Epoch 63/100, Train Loss: 0.0363, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0390, Val Loss: 0.0449\n",
            "Epoch 65/100, Train Loss: 0.0358, Val Loss: 0.0449\n",
            "Epoch 66/100, Train Loss: 0.0377, Val Loss: 0.0449\n",
            "Epoch 67/100, Train Loss: 0.0355, Val Loss: 0.0449\n",
            "Epoch 68/100, Train Loss: 0.0366, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0387, Val Loss: 0.0449\n",
            "Epoch 70/100, Train Loss: 0.0335, Val Loss: 0.0449\n",
            "Epoch 71/100, Train Loss: 0.0357, Val Loss: 0.0449\n",
            "Epoch 72/100, Train Loss: 0.0349, Val Loss: 0.0449\n",
            "Epoch 73/100, Train Loss: 0.0379, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0399, Val Loss: 0.0449\n",
            "Epoch 75/100, Train Loss: 0.0365, Val Loss: 0.0449\n",
            "Epoch 76/100, Train Loss: 0.0357, Val Loss: 0.0449\n",
            "Epoch 77/100, Train Loss: 0.0399, Val Loss: 0.0449\n",
            "Epoch 78/100, Train Loss: 0.0372, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0388, Val Loss: 0.0449\n",
            "Epoch 80/100, Train Loss: 0.0348, Val Loss: 0.0449\n",
            "Epoch 81/100, Train Loss: 0.0381, Val Loss: 0.0449\n",
            "Epoch 82/100, Train Loss: 0.0372, Val Loss: 0.0449\n",
            "Epoch 83/100, Train Loss: 0.0388, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0371, Val Loss: 0.0449\n",
            "Epoch 85/100, Train Loss: 0.0366, Val Loss: 0.0449\n",
            "Epoch 86/100, Train Loss: 0.0377, Val Loss: 0.0449\n",
            "Epoch 87/100, Train Loss: 0.0349, Val Loss: 0.0449\n",
            "Epoch 88/100, Train Loss: 0.0379, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0367, Val Loss: 0.0449\n",
            "Epoch 90/100, Train Loss: 0.0370, Val Loss: 0.0449\n",
            "Epoch 91/100, Train Loss: 0.0366, Val Loss: 0.0449\n",
            "Epoch 92/100, Train Loss: 0.0372, Val Loss: 0.0449\n",
            "Epoch 93/100, Train Loss: 0.0397, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0376, Val Loss: 0.0449\n",
            "Epoch 95/100, Train Loss: 0.0374, Val Loss: 0.0449\n",
            "Epoch 96/100, Train Loss: 0.0382, Val Loss: 0.0449\n",
            "Epoch 97/100, Train Loss: 0.0373, Val Loss: 0.0449\n",
            "Epoch 98/100, Train Loss: 0.0389, Val Loss: 0.0449\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0391, Val Loss: 0.0449\n",
            "Epoch 100/100, Train Loss: 0.0353, Val Loss: 0.0449\n",
            "\n",
            "Test iteration 23/28\n",
            "Current training set size: 119 samples\n",
            "Epoch 1/100, Train Loss: 0.1670, Val Loss: 0.1678\n",
            "Epoch 2/100, Train Loss: 0.1128, Val Loss: 0.0939\n",
            "Epoch 3/100, Train Loss: 0.0815, Val Loss: 0.0616\n",
            "Epoch 4/100, Train Loss: 0.0616, Val Loss: 0.0636\n",
            "Epoch 5/100, Train Loss: 0.0619, Val Loss: 0.0502\n",
            "Epoch 6/100, Train Loss: 0.0621, Val Loss: 0.0485\n",
            "Epoch 7/100, Train Loss: 0.0479, Val Loss: 0.0560\n",
            "Epoch 8/100, Train Loss: 0.0495, Val Loss: 0.0498\n",
            "Epoch 9/100, Train Loss: 0.0512, Val Loss: 0.0468\n",
            "Epoch 10/100, Train Loss: 0.0443, Val Loss: 0.0544\n",
            "Epoch 11/100, Train Loss: 0.0463, Val Loss: 0.0513\n",
            "Epoch 12/100, Train Loss: 0.0481, Val Loss: 0.0429\n",
            "Epoch 13/100, Train Loss: 0.0506, Val Loss: 0.0413\n",
            "Epoch 14/100, Train Loss: 0.0458, Val Loss: 0.0535\n",
            "Epoch 15/100, Train Loss: 0.0416, Val Loss: 0.0582\n",
            "Epoch 16/100, Train Loss: 0.0511, Val Loss: 0.0505\n",
            "Epoch 17/100, Train Loss: 0.0442, Val Loss: 0.0363\n",
            "Epoch 18/100, Train Loss: 0.0425, Val Loss: 0.0437\n",
            "Epoch 19/100, Train Loss: 0.0406, Val Loss: 0.0515\n",
            "Epoch 20/100, Train Loss: 0.0435, Val Loss: 0.0621\n",
            "Epoch 21/100, Train Loss: 0.0433, Val Loss: 0.0450\n",
            "Epoch 22/100, Train Loss: 0.0371, Val Loss: 0.0615\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0384, Val Loss: 0.0594\n",
            "Epoch 24/100, Train Loss: 0.0381, Val Loss: 0.0573\n",
            "Epoch 25/100, Train Loss: 0.0377, Val Loss: 0.0555\n",
            "Epoch 26/100, Train Loss: 0.0373, Val Loss: 0.0540\n",
            "Epoch 27/100, Train Loss: 0.0345, Val Loss: 0.0530\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0366, Val Loss: 0.0530\n",
            "Epoch 29/100, Train Loss: 0.0312, Val Loss: 0.0530\n",
            "Epoch 30/100, Train Loss: 0.0371, Val Loss: 0.0530\n",
            "Epoch 31/100, Train Loss: 0.0365, Val Loss: 0.0530\n",
            "Epoch 32/100, Train Loss: 0.0380, Val Loss: 0.0529\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0365, Val Loss: 0.0529\n",
            "Epoch 34/100, Train Loss: 0.0363, Val Loss: 0.0529\n",
            "Epoch 35/100, Train Loss: 0.0366, Val Loss: 0.0529\n",
            "Epoch 36/100, Train Loss: 0.0339, Val Loss: 0.0529\n",
            "Epoch 37/100, Train Loss: 0.0356, Val Loss: 0.0529\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0344, Val Loss: 0.0529\n",
            "Epoch 39/100, Train Loss: 0.0316, Val Loss: 0.0529\n",
            "Epoch 40/100, Train Loss: 0.0384, Val Loss: 0.0529\n",
            "Epoch 41/100, Train Loss: 0.0340, Val Loss: 0.0529\n",
            "Epoch 42/100, Train Loss: 0.0356, Val Loss: 0.0529\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0364, Val Loss: 0.0529\n",
            "Epoch 44/100, Train Loss: 0.0361, Val Loss: 0.0529\n",
            "Epoch 45/100, Train Loss: 0.0407, Val Loss: 0.0529\n",
            "Epoch 46/100, Train Loss: 0.0345, Val Loss: 0.0529\n",
            "Epoch 47/100, Train Loss: 0.0364, Val Loss: 0.0529\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0399, Val Loss: 0.0529\n",
            "Epoch 49/100, Train Loss: 0.0377, Val Loss: 0.0529\n",
            "Epoch 50/100, Train Loss: 0.0346, Val Loss: 0.0529\n",
            "Epoch 51/100, Train Loss: 0.0393, Val Loss: 0.0529\n",
            "Epoch 52/100, Train Loss: 0.0382, Val Loss: 0.0529\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0384, Val Loss: 0.0529\n",
            "Epoch 54/100, Train Loss: 0.0378, Val Loss: 0.0529\n",
            "Epoch 55/100, Train Loss: 0.0367, Val Loss: 0.0529\n",
            "Epoch 56/100, Train Loss: 0.0365, Val Loss: 0.0529\n",
            "Epoch 57/100, Train Loss: 0.0399, Val Loss: 0.0529\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0376, Val Loss: 0.0529\n",
            "Epoch 59/100, Train Loss: 0.0342, Val Loss: 0.0529\n",
            "Epoch 60/100, Train Loss: 0.0380, Val Loss: 0.0529\n",
            "Epoch 61/100, Train Loss: 0.0351, Val Loss: 0.0529\n",
            "Epoch 62/100, Train Loss: 0.0373, Val Loss: 0.0529\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0393, Val Loss: 0.0529\n",
            "Epoch 64/100, Train Loss: 0.0361, Val Loss: 0.0529\n",
            "Epoch 65/100, Train Loss: 0.0376, Val Loss: 0.0529\n",
            "Epoch 66/100, Train Loss: 0.0400, Val Loss: 0.0529\n",
            "Epoch 67/100, Train Loss: 0.0346, Val Loss: 0.0529\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0354, Val Loss: 0.0529\n",
            "Epoch 69/100, Train Loss: 0.0384, Val Loss: 0.0529\n",
            "Epoch 70/100, Train Loss: 0.0341, Val Loss: 0.0529\n",
            "Epoch 71/100, Train Loss: 0.0390, Val Loss: 0.0529\n",
            "Epoch 72/100, Train Loss: 0.0392, Val Loss: 0.0529\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0339, Val Loss: 0.0529\n",
            "Epoch 74/100, Train Loss: 0.0409, Val Loss: 0.0529\n",
            "Epoch 75/100, Train Loss: 0.0395, Val Loss: 0.0529\n",
            "Epoch 76/100, Train Loss: 0.0372, Val Loss: 0.0529\n",
            "Epoch 77/100, Train Loss: 0.0354, Val Loss: 0.0529\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0339, Val Loss: 0.0529\n",
            "Epoch 79/100, Train Loss: 0.0366, Val Loss: 0.0529\n",
            "Epoch 80/100, Train Loss: 0.0357, Val Loss: 0.0529\n",
            "Epoch 81/100, Train Loss: 0.0351, Val Loss: 0.0529\n",
            "Epoch 82/100, Train Loss: 0.0392, Val Loss: 0.0529\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0421, Val Loss: 0.0529\n",
            "Epoch 84/100, Train Loss: 0.0328, Val Loss: 0.0529\n",
            "Epoch 85/100, Train Loss: 0.0346, Val Loss: 0.0529\n",
            "Epoch 86/100, Train Loss: 0.0361, Val Loss: 0.0529\n",
            "Epoch 87/100, Train Loss: 0.0382, Val Loss: 0.0529\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0345, Val Loss: 0.0529\n",
            "Epoch 89/100, Train Loss: 0.0334, Val Loss: 0.0529\n",
            "Epoch 90/100, Train Loss: 0.0352, Val Loss: 0.0529\n",
            "Epoch 91/100, Train Loss: 0.0371, Val Loss: 0.0529\n",
            "Epoch 92/100, Train Loss: 0.0399, Val Loss: 0.0529\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0346, Val Loss: 0.0529\n",
            "Epoch 94/100, Train Loss: 0.0383, Val Loss: 0.0529\n",
            "Epoch 95/100, Train Loss: 0.0395, Val Loss: 0.0529\n",
            "Epoch 96/100, Train Loss: 0.0360, Val Loss: 0.0529\n",
            "Epoch 97/100, Train Loss: 0.0353, Val Loss: 0.0529\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0402, Val Loss: 0.0529\n",
            "Epoch 99/100, Train Loss: 0.0403, Val Loss: 0.0529\n",
            "Epoch 100/100, Train Loss: 0.0399, Val Loss: 0.0529\n",
            "\n",
            "Test iteration 24/28\n",
            "Current training set size: 120 samples\n",
            "Epoch 1/100, Train Loss: 0.1663, Val Loss: 0.1473\n",
            "Epoch 2/100, Train Loss: 0.1026, Val Loss: 0.0972\n",
            "Epoch 3/100, Train Loss: 0.0920, Val Loss: 0.0845\n",
            "Epoch 4/100, Train Loss: 0.0734, Val Loss: 0.0631\n",
            "Epoch 5/100, Train Loss: 0.0752, Val Loss: 0.0523\n",
            "Epoch 6/100, Train Loss: 0.0553, Val Loss: 0.0689\n",
            "Epoch 7/100, Train Loss: 0.0656, Val Loss: 0.0511\n",
            "Epoch 8/100, Train Loss: 0.0444, Val Loss: 0.0621\n",
            "Epoch 9/100, Train Loss: 0.0527, Val Loss: 0.0526\n",
            "Epoch 10/100, Train Loss: 0.0529, Val Loss: 0.0398\n",
            "Epoch 11/100, Train Loss: 0.0463, Val Loss: 0.0411\n",
            "Epoch 12/100, Train Loss: 0.0462, Val Loss: 0.0474\n",
            "Epoch 13/100, Train Loss: 0.0472, Val Loss: 0.0439\n",
            "Epoch 14/100, Train Loss: 0.0499, Val Loss: 0.0550\n",
            "Epoch 15/100, Train Loss: 0.0475, Val Loss: 0.0550\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0445, Val Loss: 0.0541\n",
            "Epoch 17/100, Train Loss: 0.0446, Val Loss: 0.0534\n",
            "Epoch 18/100, Train Loss: 0.0431, Val Loss: 0.0524\n",
            "Epoch 19/100, Train Loss: 0.0446, Val Loss: 0.0515\n",
            "Epoch 20/100, Train Loss: 0.0400, Val Loss: 0.0509\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0374, Val Loss: 0.0509\n",
            "Epoch 22/100, Train Loss: 0.0422, Val Loss: 0.0509\n",
            "Epoch 23/100, Train Loss: 0.0401, Val Loss: 0.0509\n",
            "Epoch 24/100, Train Loss: 0.0408, Val Loss: 0.0509\n",
            "Epoch 25/100, Train Loss: 0.0438, Val Loss: 0.0509\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0426, Val Loss: 0.0509\n",
            "Epoch 27/100, Train Loss: 0.0445, Val Loss: 0.0509\n",
            "Epoch 28/100, Train Loss: 0.0405, Val Loss: 0.0509\n",
            "Epoch 29/100, Train Loss: 0.0436, Val Loss: 0.0509\n",
            "Epoch 30/100, Train Loss: 0.0431, Val Loss: 0.0509\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0403, Val Loss: 0.0509\n",
            "Epoch 32/100, Train Loss: 0.0441, Val Loss: 0.0509\n",
            "Epoch 33/100, Train Loss: 0.0404, Val Loss: 0.0509\n",
            "Epoch 34/100, Train Loss: 0.0459, Val Loss: 0.0509\n",
            "Epoch 35/100, Train Loss: 0.0424, Val Loss: 0.0509\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0421, Val Loss: 0.0509\n",
            "Epoch 37/100, Train Loss: 0.0437, Val Loss: 0.0509\n",
            "Epoch 38/100, Train Loss: 0.0434, Val Loss: 0.0509\n",
            "Epoch 39/100, Train Loss: 0.0426, Val Loss: 0.0509\n",
            "Epoch 40/100, Train Loss: 0.0408, Val Loss: 0.0509\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0428, Val Loss: 0.0509\n",
            "Epoch 42/100, Train Loss: 0.0394, Val Loss: 0.0509\n",
            "Epoch 43/100, Train Loss: 0.0407, Val Loss: 0.0509\n",
            "Epoch 44/100, Train Loss: 0.0438, Val Loss: 0.0509\n",
            "Epoch 45/100, Train Loss: 0.0443, Val Loss: 0.0509\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0405, Val Loss: 0.0509\n",
            "Epoch 47/100, Train Loss: 0.0426, Val Loss: 0.0509\n",
            "Epoch 48/100, Train Loss: 0.0427, Val Loss: 0.0509\n",
            "Epoch 49/100, Train Loss: 0.0406, Val Loss: 0.0509\n",
            "Epoch 50/100, Train Loss: 0.0424, Val Loss: 0.0509\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0455, Val Loss: 0.0509\n",
            "Epoch 52/100, Train Loss: 0.0420, Val Loss: 0.0509\n",
            "Epoch 53/100, Train Loss: 0.0420, Val Loss: 0.0509\n",
            "Epoch 54/100, Train Loss: 0.0425, Val Loss: 0.0509\n",
            "Epoch 55/100, Train Loss: 0.0417, Val Loss: 0.0509\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0457, Val Loss: 0.0509\n",
            "Epoch 57/100, Train Loss: 0.0441, Val Loss: 0.0509\n",
            "Epoch 58/100, Train Loss: 0.0432, Val Loss: 0.0509\n",
            "Epoch 59/100, Train Loss: 0.0425, Val Loss: 0.0509\n",
            "Epoch 60/100, Train Loss: 0.0414, Val Loss: 0.0509\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0401, Val Loss: 0.0509\n",
            "Epoch 62/100, Train Loss: 0.0448, Val Loss: 0.0509\n",
            "Epoch 63/100, Train Loss: 0.0425, Val Loss: 0.0509\n",
            "Epoch 64/100, Train Loss: 0.0420, Val Loss: 0.0509\n",
            "Epoch 65/100, Train Loss: 0.0426, Val Loss: 0.0509\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0413, Val Loss: 0.0509\n",
            "Epoch 67/100, Train Loss: 0.0401, Val Loss: 0.0509\n",
            "Epoch 68/100, Train Loss: 0.0414, Val Loss: 0.0509\n",
            "Epoch 69/100, Train Loss: 0.0454, Val Loss: 0.0509\n",
            "Epoch 70/100, Train Loss: 0.0405, Val Loss: 0.0509\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0412, Val Loss: 0.0509\n",
            "Epoch 72/100, Train Loss: 0.0396, Val Loss: 0.0509\n",
            "Epoch 73/100, Train Loss: 0.0443, Val Loss: 0.0509\n",
            "Epoch 74/100, Train Loss: 0.0406, Val Loss: 0.0509\n",
            "Epoch 75/100, Train Loss: 0.0428, Val Loss: 0.0509\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0406, Val Loss: 0.0509\n",
            "Epoch 77/100, Train Loss: 0.0435, Val Loss: 0.0509\n",
            "Epoch 78/100, Train Loss: 0.0409, Val Loss: 0.0509\n",
            "Epoch 79/100, Train Loss: 0.0402, Val Loss: 0.0509\n",
            "Epoch 80/100, Train Loss: 0.0400, Val Loss: 0.0509\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0416, Val Loss: 0.0509\n",
            "Epoch 82/100, Train Loss: 0.0412, Val Loss: 0.0509\n",
            "Epoch 83/100, Train Loss: 0.0410, Val Loss: 0.0509\n",
            "Epoch 84/100, Train Loss: 0.0399, Val Loss: 0.0509\n",
            "Epoch 85/100, Train Loss: 0.0416, Val Loss: 0.0509\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0431, Val Loss: 0.0509\n",
            "Epoch 87/100, Train Loss: 0.0441, Val Loss: 0.0509\n",
            "Epoch 88/100, Train Loss: 0.0426, Val Loss: 0.0509\n",
            "Epoch 89/100, Train Loss: 0.0406, Val Loss: 0.0509\n",
            "Epoch 90/100, Train Loss: 0.0404, Val Loss: 0.0509\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0405, Val Loss: 0.0509\n",
            "Epoch 92/100, Train Loss: 0.0446, Val Loss: 0.0509\n",
            "Epoch 93/100, Train Loss: 0.0405, Val Loss: 0.0509\n",
            "Epoch 94/100, Train Loss: 0.0423, Val Loss: 0.0509\n",
            "Epoch 95/100, Train Loss: 0.0425, Val Loss: 0.0509\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0414, Val Loss: 0.0509\n",
            "Epoch 97/100, Train Loss: 0.0447, Val Loss: 0.0509\n",
            "Epoch 98/100, Train Loss: 0.0429, Val Loss: 0.0509\n",
            "Epoch 99/100, Train Loss: 0.0430, Val Loss: 0.0509\n",
            "Epoch 100/100, Train Loss: 0.0402, Val Loss: 0.0509\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "Test iteration 25/28\n",
            "Current training set size: 121 samples\n",
            "Epoch 1/100, Train Loss: 0.1722, Val Loss: 0.1718\n",
            "Epoch 2/100, Train Loss: 0.1273, Val Loss: 0.0951\n",
            "Epoch 3/100, Train Loss: 0.0833, Val Loss: 0.0792\n",
            "Epoch 4/100, Train Loss: 0.0594, Val Loss: 0.0414\n",
            "Epoch 5/100, Train Loss: 0.0623, Val Loss: 0.0454\n",
            "Epoch 6/100, Train Loss: 0.0612, Val Loss: 0.0564\n",
            "Epoch 7/100, Train Loss: 0.0573, Val Loss: 0.0504\n",
            "Epoch 8/100, Train Loss: 0.0488, Val Loss: 0.0468\n",
            "Epoch 9/100, Train Loss: 0.0530, Val Loss: 0.0572\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 10/100, Train Loss: 0.0525, Val Loss: 0.0521\n",
            "Epoch 11/100, Train Loss: 0.0474, Val Loss: 0.0478\n",
            "Epoch 12/100, Train Loss: 0.0494, Val Loss: 0.0446\n",
            "Epoch 13/100, Train Loss: 0.0416, Val Loss: 0.0437\n",
            "Epoch 14/100, Train Loss: 0.0453, Val Loss: 0.0434\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 15/100, Train Loss: 0.0435, Val Loss: 0.0433\n",
            "Epoch 16/100, Train Loss: 0.0429, Val Loss: 0.0433\n",
            "Epoch 17/100, Train Loss: 0.0402, Val Loss: 0.0433\n",
            "Epoch 18/100, Train Loss: 0.0414, Val Loss: 0.0433\n",
            "Epoch 19/100, Train Loss: 0.0410, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 20/100, Train Loss: 0.0397, Val Loss: 0.0433\n",
            "Epoch 21/100, Train Loss: 0.0403, Val Loss: 0.0433\n",
            "Epoch 22/100, Train Loss: 0.0424, Val Loss: 0.0433\n",
            "Epoch 23/100, Train Loss: 0.0476, Val Loss: 0.0433\n",
            "Epoch 24/100, Train Loss: 0.0465, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 25/100, Train Loss: 0.0430, Val Loss: 0.0433\n",
            "Epoch 26/100, Train Loss: 0.0401, Val Loss: 0.0433\n",
            "Epoch 27/100, Train Loss: 0.0447, Val Loss: 0.0433\n",
            "Epoch 28/100, Train Loss: 0.0416, Val Loss: 0.0433\n",
            "Epoch 29/100, Train Loss: 0.0443, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 30/100, Train Loss: 0.0457, Val Loss: 0.0433\n",
            "Epoch 31/100, Train Loss: 0.0429, Val Loss: 0.0433\n",
            "Epoch 32/100, Train Loss: 0.0395, Val Loss: 0.0433\n",
            "Epoch 33/100, Train Loss: 0.0411, Val Loss: 0.0433\n",
            "Epoch 34/100, Train Loss: 0.0448, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 35/100, Train Loss: 0.0406, Val Loss: 0.0433\n",
            "Epoch 36/100, Train Loss: 0.0430, Val Loss: 0.0433\n",
            "Epoch 37/100, Train Loss: 0.0382, Val Loss: 0.0433\n",
            "Epoch 38/100, Train Loss: 0.0422, Val Loss: 0.0433\n",
            "Epoch 39/100, Train Loss: 0.0405, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 40/100, Train Loss: 0.0407, Val Loss: 0.0433\n",
            "Epoch 41/100, Train Loss: 0.0449, Val Loss: 0.0433\n",
            "Epoch 42/100, Train Loss: 0.0441, Val Loss: 0.0433\n",
            "Epoch 43/100, Train Loss: 0.0424, Val Loss: 0.0433\n",
            "Epoch 44/100, Train Loss: 0.0440, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 45/100, Train Loss: 0.0425, Val Loss: 0.0433\n",
            "Epoch 46/100, Train Loss: 0.0457, Val Loss: 0.0433\n",
            "Epoch 47/100, Train Loss: 0.0402, Val Loss: 0.0433\n",
            "Epoch 48/100, Train Loss: 0.0375, Val Loss: 0.0433\n",
            "Epoch 49/100, Train Loss: 0.0386, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0393, Val Loss: 0.0433\n",
            "Epoch 51/100, Train Loss: 0.0413, Val Loss: 0.0433\n",
            "Epoch 52/100, Train Loss: 0.0415, Val Loss: 0.0433\n",
            "Epoch 53/100, Train Loss: 0.0412, Val Loss: 0.0433\n",
            "Epoch 54/100, Train Loss: 0.0438, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0400, Val Loss: 0.0433\n",
            "Epoch 56/100, Train Loss: 0.0445, Val Loss: 0.0433\n",
            "Epoch 57/100, Train Loss: 0.0502, Val Loss: 0.0433\n",
            "Epoch 58/100, Train Loss: 0.0415, Val Loss: 0.0433\n",
            "Epoch 59/100, Train Loss: 0.0470, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0429, Val Loss: 0.0433\n",
            "Epoch 61/100, Train Loss: 0.0422, Val Loss: 0.0433\n",
            "Epoch 62/100, Train Loss: 0.0401, Val Loss: 0.0433\n",
            "Epoch 63/100, Train Loss: 0.0434, Val Loss: 0.0433\n",
            "Epoch 64/100, Train Loss: 0.0442, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0406, Val Loss: 0.0433\n",
            "Epoch 66/100, Train Loss: 0.0447, Val Loss: 0.0433\n",
            "Epoch 67/100, Train Loss: 0.0407, Val Loss: 0.0433\n",
            "Epoch 68/100, Train Loss: 0.0433, Val Loss: 0.0433\n",
            "Epoch 69/100, Train Loss: 0.0409, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0405, Val Loss: 0.0433\n",
            "Epoch 71/100, Train Loss: 0.0436, Val Loss: 0.0433\n",
            "Epoch 72/100, Train Loss: 0.0428, Val Loss: 0.0433\n",
            "Epoch 73/100, Train Loss: 0.0402, Val Loss: 0.0433\n",
            "Epoch 74/100, Train Loss: 0.0421, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0427, Val Loss: 0.0433\n",
            "Epoch 76/100, Train Loss: 0.0436, Val Loss: 0.0433\n",
            "Epoch 77/100, Train Loss: 0.0433, Val Loss: 0.0433\n",
            "Epoch 78/100, Train Loss: 0.0530, Val Loss: 0.0433\n",
            "Epoch 79/100, Train Loss: 0.0453, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0400, Val Loss: 0.0433\n",
            "Epoch 81/100, Train Loss: 0.0420, Val Loss: 0.0433\n",
            "Epoch 82/100, Train Loss: 0.0407, Val Loss: 0.0433\n",
            "Epoch 83/100, Train Loss: 0.0468, Val Loss: 0.0433\n",
            "Epoch 84/100, Train Loss: 0.0434, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0402, Val Loss: 0.0433\n",
            "Epoch 86/100, Train Loss: 0.0407, Val Loss: 0.0433\n",
            "Epoch 87/100, Train Loss: 0.0418, Val Loss: 0.0433\n",
            "Epoch 88/100, Train Loss: 0.0393, Val Loss: 0.0433\n",
            "Epoch 89/100, Train Loss: 0.0424, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0412, Val Loss: 0.0433\n",
            "Epoch 91/100, Train Loss: 0.0446, Val Loss: 0.0433\n",
            "Epoch 92/100, Train Loss: 0.0473, Val Loss: 0.0433\n",
            "Epoch 93/100, Train Loss: 0.0420, Val Loss: 0.0433\n",
            "Epoch 94/100, Train Loss: 0.0416, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0438, Val Loss: 0.0433\n",
            "Epoch 96/100, Train Loss: 0.0408, Val Loss: 0.0433\n",
            "Epoch 97/100, Train Loss: 0.0428, Val Loss: 0.0433\n",
            "Epoch 98/100, Train Loss: 0.0429, Val Loss: 0.0433\n",
            "Epoch 99/100, Train Loss: 0.0458, Val Loss: 0.0433\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0458, Val Loss: 0.0433\n",
            "\n",
            "Test iteration 26/28\n",
            "Current training set size: 122 samples\n",
            "Epoch 1/100, Train Loss: 0.1478, Val Loss: 0.1263\n",
            "Epoch 2/100, Train Loss: 0.1007, Val Loss: 0.1451\n",
            "Epoch 3/100, Train Loss: 0.1031, Val Loss: 0.1058\n",
            "Epoch 4/100, Train Loss: 0.0723, Val Loss: 0.0526\n",
            "Epoch 5/100, Train Loss: 0.0672, Val Loss: 0.0493\n",
            "Epoch 6/100, Train Loss: 0.0532, Val Loss: 0.0516\n",
            "Epoch 7/100, Train Loss: 0.0543, Val Loss: 0.0429\n",
            "Epoch 8/100, Train Loss: 0.0592, Val Loss: 0.0475\n",
            "Epoch 9/100, Train Loss: 0.0444, Val Loss: 0.0419\n",
            "Epoch 10/100, Train Loss: 0.0484, Val Loss: 0.0492\n",
            "Epoch 11/100, Train Loss: 0.0435, Val Loss: 0.0425\n",
            "Epoch 12/100, Train Loss: 0.0515, Val Loss: 0.0489\n",
            "Epoch 13/100, Train Loss: 0.0448, Val Loss: 0.0375\n",
            "Epoch 14/100, Train Loss: 0.0481, Val Loss: 0.0425\n",
            "Epoch 15/100, Train Loss: 0.0417, Val Loss: 0.0452\n",
            "Epoch 16/100, Train Loss: 0.0439, Val Loss: 0.0537\n",
            "Epoch 17/100, Train Loss: 0.0478, Val Loss: 0.0523\n",
            "Epoch 18/100, Train Loss: 0.0489, Val Loss: 0.0339\n",
            "Epoch 19/100, Train Loss: 0.0408, Val Loss: 0.0465\n",
            "Epoch 20/100, Train Loss: 0.0382, Val Loss: 0.0486\n",
            "Epoch 21/100, Train Loss: 0.0355, Val Loss: 0.0478\n",
            "Epoch 22/100, Train Loss: 0.0377, Val Loss: 0.0354\n",
            "Epoch 23/100, Train Loss: 0.0403, Val Loss: 0.0460\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0391, Val Loss: 0.0431\n",
            "Epoch 25/100, Train Loss: 0.0391, Val Loss: 0.0411\n",
            "Epoch 26/100, Train Loss: 0.0365, Val Loss: 0.0390\n",
            "Epoch 27/100, Train Loss: 0.0361, Val Loss: 0.0380\n",
            "Epoch 28/100, Train Loss: 0.0317, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0371, Val Loss: 0.0376\n",
            "Epoch 30/100, Train Loss: 0.0350, Val Loss: 0.0376\n",
            "Epoch 31/100, Train Loss: 0.0332, Val Loss: 0.0376\n",
            "Epoch 32/100, Train Loss: 0.0298, Val Loss: 0.0376\n",
            "Epoch 33/100, Train Loss: 0.0337, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0374, Val Loss: 0.0376\n",
            "Epoch 35/100, Train Loss: 0.0323, Val Loss: 0.0376\n",
            "Epoch 36/100, Train Loss: 0.0362, Val Loss: 0.0376\n",
            "Epoch 37/100, Train Loss: 0.0310, Val Loss: 0.0376\n",
            "Epoch 38/100, Train Loss: 0.0336, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0365, Val Loss: 0.0376\n",
            "Epoch 40/100, Train Loss: 0.0357, Val Loss: 0.0376\n",
            "Epoch 41/100, Train Loss: 0.0334, Val Loss: 0.0376\n",
            "Epoch 42/100, Train Loss: 0.0352, Val Loss: 0.0376\n",
            "Epoch 43/100, Train Loss: 0.0312, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0337, Val Loss: 0.0376\n",
            "Epoch 45/100, Train Loss: 0.0348, Val Loss: 0.0376\n",
            "Epoch 46/100, Train Loss: 0.0327, Val Loss: 0.0376\n",
            "Epoch 47/100, Train Loss: 0.0345, Val Loss: 0.0376\n",
            "Epoch 48/100, Train Loss: 0.0338, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0325, Val Loss: 0.0376\n",
            "Epoch 50/100, Train Loss: 0.0325, Val Loss: 0.0376\n",
            "Epoch 51/100, Train Loss: 0.0318, Val Loss: 0.0376\n",
            "Epoch 52/100, Train Loss: 0.0386, Val Loss: 0.0376\n",
            "Epoch 53/100, Train Loss: 0.0296, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0334, Val Loss: 0.0376\n",
            "Epoch 55/100, Train Loss: 0.0344, Val Loss: 0.0376\n",
            "Epoch 56/100, Train Loss: 0.0351, Val Loss: 0.0376\n",
            "Epoch 57/100, Train Loss: 0.0329, Val Loss: 0.0376\n",
            "Epoch 58/100, Train Loss: 0.0340, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0351, Val Loss: 0.0376\n",
            "Epoch 60/100, Train Loss: 0.0372, Val Loss: 0.0376\n",
            "Epoch 61/100, Train Loss: 0.0353, Val Loss: 0.0376\n",
            "Epoch 62/100, Train Loss: 0.0342, Val Loss: 0.0376\n",
            "Epoch 63/100, Train Loss: 0.0331, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0316, Val Loss: 0.0376\n",
            "Epoch 65/100, Train Loss: 0.0299, Val Loss: 0.0376\n",
            "Epoch 66/100, Train Loss: 0.0348, Val Loss: 0.0376\n",
            "Epoch 67/100, Train Loss: 0.0350, Val Loss: 0.0376\n",
            "Epoch 68/100, Train Loss: 0.0341, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0350, Val Loss: 0.0376\n",
            "Epoch 70/100, Train Loss: 0.0338, Val Loss: 0.0376\n",
            "Epoch 71/100, Train Loss: 0.0344, Val Loss: 0.0376\n",
            "Epoch 72/100, Train Loss: 0.0341, Val Loss: 0.0376\n",
            "Epoch 73/100, Train Loss: 0.0389, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0338, Val Loss: 0.0376\n",
            "Epoch 75/100, Train Loss: 0.0328, Val Loss: 0.0376\n",
            "Epoch 76/100, Train Loss: 0.0348, Val Loss: 0.0376\n",
            "Epoch 77/100, Train Loss: 0.0337, Val Loss: 0.0376\n",
            "Epoch 78/100, Train Loss: 0.0301, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0337, Val Loss: 0.0376\n",
            "Epoch 80/100, Train Loss: 0.0339, Val Loss: 0.0376\n",
            "Epoch 81/100, Train Loss: 0.0347, Val Loss: 0.0376\n",
            "Epoch 82/100, Train Loss: 0.0337, Val Loss: 0.0376\n",
            "Epoch 83/100, Train Loss: 0.0346, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0349, Val Loss: 0.0376\n",
            "Epoch 85/100, Train Loss: 0.0364, Val Loss: 0.0376\n",
            "Epoch 86/100, Train Loss: 0.0356, Val Loss: 0.0376\n",
            "Epoch 87/100, Train Loss: 0.0391, Val Loss: 0.0376\n",
            "Epoch 88/100, Train Loss: 0.0355, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0324, Val Loss: 0.0376\n",
            "Epoch 90/100, Train Loss: 0.0342, Val Loss: 0.0376\n",
            "Epoch 91/100, Train Loss: 0.0337, Val Loss: 0.0376\n",
            "Epoch 92/100, Train Loss: 0.0323, Val Loss: 0.0376\n",
            "Epoch 93/100, Train Loss: 0.0316, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0315, Val Loss: 0.0376\n",
            "Epoch 95/100, Train Loss: 0.0375, Val Loss: 0.0376\n",
            "Epoch 96/100, Train Loss: 0.0313, Val Loss: 0.0376\n",
            "Epoch 97/100, Train Loss: 0.0345, Val Loss: 0.0376\n",
            "Epoch 98/100, Train Loss: 0.0317, Val Loss: 0.0376\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0345, Val Loss: 0.0376\n",
            "Epoch 100/100, Train Loss: 0.0319, Val Loss: 0.0376\n",
            "\n",
            "Test iteration 27/28\n",
            "Current training set size: 123 samples\n",
            "Epoch 1/100, Train Loss: 0.1578, Val Loss: 0.1594\n",
            "Epoch 2/100, Train Loss: 0.1028, Val Loss: 0.0916\n",
            "Epoch 3/100, Train Loss: 0.0862, Val Loss: 0.0745\n",
            "Epoch 4/100, Train Loss: 0.0684, Val Loss: 0.0774\n",
            "Epoch 5/100, Train Loss: 0.0595, Val Loss: 0.0443\n",
            "Epoch 6/100, Train Loss: 0.0557, Val Loss: 0.0567\n",
            "Epoch 7/100, Train Loss: 0.0613, Val Loss: 0.0413\n",
            "Epoch 8/100, Train Loss: 0.0524, Val Loss: 0.0646\n",
            "Epoch 9/100, Train Loss: 0.0598, Val Loss: 0.0522\n",
            "Epoch 10/100, Train Loss: 0.0493, Val Loss: 0.0399\n",
            "Epoch 11/100, Train Loss: 0.0427, Val Loss: 0.0463\n",
            "Epoch 12/100, Train Loss: 0.0480, Val Loss: 0.0492\n",
            "Epoch 13/100, Train Loss: 0.0458, Val Loss: 0.0386\n",
            "Epoch 14/100, Train Loss: 0.0507, Val Loss: 0.0496\n",
            "Epoch 15/100, Train Loss: 0.0430, Val Loss: 0.0449\n",
            "Epoch 16/100, Train Loss: 0.0454, Val Loss: 0.0446\n",
            "Epoch 17/100, Train Loss: 0.0433, Val Loss: 0.0597\n",
            "Epoch 18/100, Train Loss: 0.0513, Val Loss: 0.0559\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0428, Val Loss: 0.0551\n",
            "Epoch 20/100, Train Loss: 0.0461, Val Loss: 0.0540\n",
            "Epoch 21/100, Train Loss: 0.0409, Val Loss: 0.0526\n",
            "Epoch 22/100, Train Loss: 0.0428, Val Loss: 0.0516\n",
            "Epoch 23/100, Train Loss: 0.0418, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0426, Val Loss: 0.0506\n",
            "Epoch 25/100, Train Loss: 0.0436, Val Loss: 0.0506\n",
            "Epoch 26/100, Train Loss: 0.0437, Val Loss: 0.0506\n",
            "Epoch 27/100, Train Loss: 0.0416, Val Loss: 0.0506\n",
            "Epoch 28/100, Train Loss: 0.0411, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0431, Val Loss: 0.0506\n",
            "Epoch 30/100, Train Loss: 0.0428, Val Loss: 0.0506\n",
            "Epoch 31/100, Train Loss: 0.0426, Val Loss: 0.0506\n",
            "Epoch 32/100, Train Loss: 0.0416, Val Loss: 0.0506\n",
            "Epoch 33/100, Train Loss: 0.0425, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0370, Val Loss: 0.0506\n",
            "Epoch 35/100, Train Loss: 0.0415, Val Loss: 0.0506\n",
            "Epoch 36/100, Train Loss: 0.0438, Val Loss: 0.0506\n",
            "Epoch 37/100, Train Loss: 0.0430, Val Loss: 0.0506\n",
            "Epoch 38/100, Train Loss: 0.0416, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0421, Val Loss: 0.0506\n",
            "Epoch 40/100, Train Loss: 0.0410, Val Loss: 0.0506\n",
            "Epoch 41/100, Train Loss: 0.0413, Val Loss: 0.0506\n",
            "Epoch 42/100, Train Loss: 0.0411, Val Loss: 0.0506\n",
            "Epoch 43/100, Train Loss: 0.0434, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0395, Val Loss: 0.0506\n",
            "Epoch 45/100, Train Loss: 0.0455, Val Loss: 0.0506\n",
            "Epoch 46/100, Train Loss: 0.0415, Val Loss: 0.0506\n",
            "Epoch 47/100, Train Loss: 0.0401, Val Loss: 0.0506\n",
            "Epoch 48/100, Train Loss: 0.0407, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0409, Val Loss: 0.0506\n",
            "Epoch 50/100, Train Loss: 0.0411, Val Loss: 0.0506\n",
            "Epoch 51/100, Train Loss: 0.0441, Val Loss: 0.0506\n",
            "Epoch 52/100, Train Loss: 0.0398, Val Loss: 0.0506\n",
            "Epoch 53/100, Train Loss: 0.0425, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0425, Val Loss: 0.0506\n",
            "Epoch 55/100, Train Loss: 0.0399, Val Loss: 0.0506\n",
            "Epoch 56/100, Train Loss: 0.0425, Val Loss: 0.0506\n",
            "Epoch 57/100, Train Loss: 0.0407, Val Loss: 0.0506\n",
            "Epoch 58/100, Train Loss: 0.0420, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0406, Val Loss: 0.0506\n",
            "Epoch 60/100, Train Loss: 0.0406, Val Loss: 0.0506\n",
            "Epoch 61/100, Train Loss: 0.0410, Val Loss: 0.0506\n",
            "Epoch 62/100, Train Loss: 0.0427, Val Loss: 0.0506\n",
            "Epoch 63/100, Train Loss: 0.0410, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0427, Val Loss: 0.0506\n",
            "Epoch 65/100, Train Loss: 0.0386, Val Loss: 0.0506\n",
            "Epoch 66/100, Train Loss: 0.0424, Val Loss: 0.0506\n",
            "Epoch 67/100, Train Loss: 0.0393, Val Loss: 0.0506\n",
            "Epoch 68/100, Train Loss: 0.0417, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0462, Val Loss: 0.0506\n",
            "Epoch 70/100, Train Loss: 0.0425, Val Loss: 0.0506\n",
            "Epoch 71/100, Train Loss: 0.0445, Val Loss: 0.0506\n",
            "Epoch 72/100, Train Loss: 0.0405, Val Loss: 0.0506\n",
            "Epoch 73/100, Train Loss: 0.0443, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0404, Val Loss: 0.0506\n",
            "Epoch 75/100, Train Loss: 0.0421, Val Loss: 0.0506\n",
            "Epoch 76/100, Train Loss: 0.0414, Val Loss: 0.0506\n",
            "Epoch 77/100, Train Loss: 0.0427, Val Loss: 0.0506\n",
            "Epoch 78/100, Train Loss: 0.0399, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0465, Val Loss: 0.0506\n",
            "Epoch 80/100, Train Loss: 0.0447, Val Loss: 0.0506\n",
            "Epoch 81/100, Train Loss: 0.0421, Val Loss: 0.0506\n",
            "Epoch 82/100, Train Loss: 0.0454, Val Loss: 0.0506\n",
            "Epoch 83/100, Train Loss: 0.0420, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0412, Val Loss: 0.0506\n",
            "Epoch 85/100, Train Loss: 0.0427, Val Loss: 0.0506\n",
            "Epoch 86/100, Train Loss: 0.0418, Val Loss: 0.0506\n",
            "Epoch 87/100, Train Loss: 0.0421, Val Loss: 0.0506\n",
            "Epoch 88/100, Train Loss: 0.0381, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0425, Val Loss: 0.0506\n",
            "Epoch 90/100, Train Loss: 0.0410, Val Loss: 0.0506\n",
            "Epoch 91/100, Train Loss: 0.0416, Val Loss: 0.0506\n",
            "Epoch 92/100, Train Loss: 0.0401, Val Loss: 0.0506\n",
            "Epoch 93/100, Train Loss: 0.0441, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0418, Val Loss: 0.0506\n",
            "Epoch 95/100, Train Loss: 0.0417, Val Loss: 0.0506\n",
            "Epoch 96/100, Train Loss: 0.0419, Val Loss: 0.0506\n",
            "Epoch 97/100, Train Loss: 0.0426, Val Loss: 0.0506\n",
            "Epoch 98/100, Train Loss: 0.0426, Val Loss: 0.0506\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0436, Val Loss: 0.0506\n",
            "Epoch 100/100, Train Loss: 0.0420, Val Loss: 0.0506\n",
            "\n",
            "Test iteration 28/28\n",
            "Current training set size: 124 samples\n",
            "Epoch 1/100, Train Loss: 0.1678, Val Loss: 0.1362\n",
            "Epoch 2/100, Train Loss: 0.1034, Val Loss: 0.0976\n",
            "Epoch 3/100, Train Loss: 0.0884, Val Loss: 0.0898\n",
            "Epoch 4/100, Train Loss: 0.0713, Val Loss: 0.0476\n",
            "Epoch 5/100, Train Loss: 0.0595, Val Loss: 0.0474\n",
            "Epoch 6/100, Train Loss: 0.0610, Val Loss: 0.0543\n",
            "Epoch 7/100, Train Loss: 0.0533, Val Loss: 0.0645\n",
            "Epoch 8/100, Train Loss: 0.0561, Val Loss: 0.0455\n",
            "Epoch 9/100, Train Loss: 0.0564, Val Loss: 0.0548\n",
            "Epoch 10/100, Train Loss: 0.0452, Val Loss: 0.0426\n",
            "Epoch 11/100, Train Loss: 0.0441, Val Loss: 0.0489\n",
            "Epoch 12/100, Train Loss: 0.0488, Val Loss: 0.0542\n",
            "Epoch 13/100, Train Loss: 0.0496, Val Loss: 0.0525\n",
            "Epoch 14/100, Train Loss: 0.0483, Val Loss: 0.0612\n",
            "Epoch 15/100, Train Loss: 0.0518, Val Loss: 0.0516\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 16/100, Train Loss: 0.0393, Val Loss: 0.0516\n",
            "Epoch 17/100, Train Loss: 0.0424, Val Loss: 0.0513\n",
            "Epoch 18/100, Train Loss: 0.0379, Val Loss: 0.0509\n",
            "Epoch 19/100, Train Loss: 0.0387, Val Loss: 0.0506\n",
            "Epoch 20/100, Train Loss: 0.0436, Val Loss: 0.0504\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 21/100, Train Loss: 0.0416, Val Loss: 0.0504\n",
            "Epoch 22/100, Train Loss: 0.0420, Val Loss: 0.0504\n",
            "Epoch 23/100, Train Loss: 0.0417, Val Loss: 0.0504\n",
            "Epoch 24/100, Train Loss: 0.0394, Val Loss: 0.0504\n",
            "Epoch 25/100, Train Loss: 0.0413, Val Loss: 0.0504\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 26/100, Train Loss: 0.0392, Val Loss: 0.0504\n",
            "Epoch 27/100, Train Loss: 0.0398, Val Loss: 0.0504\n",
            "Epoch 28/100, Train Loss: 0.0404, Val Loss: 0.0504\n",
            "Epoch 29/100, Train Loss: 0.0455, Val Loss: 0.0504\n",
            "Epoch 30/100, Train Loss: 0.0432, Val Loss: 0.0504\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 31/100, Train Loss: 0.0399, Val Loss: 0.0504\n",
            "Epoch 32/100, Train Loss: 0.0391, Val Loss: 0.0504\n",
            "Epoch 33/100, Train Loss: 0.0389, Val Loss: 0.0504\n",
            "Epoch 34/100, Train Loss: 0.0426, Val Loss: 0.0504\n",
            "Epoch 35/100, Train Loss: 0.0427, Val Loss: 0.0504\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 36/100, Train Loss: 0.0398, Val Loss: 0.0504\n",
            "Epoch 37/100, Train Loss: 0.0426, Val Loss: 0.0504\n",
            "Epoch 38/100, Train Loss: 0.0440, Val Loss: 0.0504\n",
            "Epoch 39/100, Train Loss: 0.0415, Val Loss: 0.0504\n",
            "Epoch 40/100, Train Loss: 0.0395, Val Loss: 0.0504\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 41/100, Train Loss: 0.0418, Val Loss: 0.0504\n",
            "Epoch 42/100, Train Loss: 0.0434, Val Loss: 0.0504\n",
            "Epoch 43/100, Train Loss: 0.0398, Val Loss: 0.0504\n",
            "Epoch 44/100, Train Loss: 0.0450, Val Loss: 0.0504\n",
            "Epoch 45/100, Train Loss: 0.0444, Val Loss: 0.0504\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 46/100, Train Loss: 0.0382, Val Loss: 0.0504\n",
            "Epoch 47/100, Train Loss: 0.0422, Val Loss: 0.0504\n",
            "Epoch 48/100, Train Loss: 0.0413, Val Loss: 0.0504\n",
            "Epoch 49/100, Train Loss: 0.0405, Val Loss: 0.0504\n",
            "Epoch 50/100, Train Loss: 0.0401, Val Loss: 0.0504\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 51/100, Train Loss: 0.0389, Val Loss: 0.0504\n",
            "Epoch 52/100, Train Loss: 0.0439, Val Loss: 0.0504\n",
            "Epoch 53/100, Train Loss: 0.0418, Val Loss: 0.0504\n",
            "Epoch 54/100, Train Loss: 0.0431, Val Loss: 0.0504\n",
            "Epoch 55/100, Train Loss: 0.0356, Val Loss: 0.0504\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 56/100, Train Loss: 0.0394, Val Loss: 0.0504\n",
            "Epoch 57/100, Train Loss: 0.0375, Val Loss: 0.0504\n",
            "Epoch 58/100, Train Loss: 0.0380, Val Loss: 0.0504\n",
            "Epoch 59/100, Train Loss: 0.0394, Val Loss: 0.0504\n",
            "Epoch 60/100, Train Loss: 0.0410, Val Loss: 0.0504\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 61/100, Train Loss: 0.0415, Val Loss: 0.0504\n",
            "Epoch 62/100, Train Loss: 0.0408, Val Loss: 0.0504\n",
            "Epoch 63/100, Train Loss: 0.0382, Val Loss: 0.0504\n",
            "Epoch 64/100, Train Loss: 0.0380, Val Loss: 0.0504\n",
            "Epoch 65/100, Train Loss: 0.0426, Val Loss: 0.0504\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 66/100, Train Loss: 0.0401, Val Loss: 0.0504\n",
            "Epoch 67/100, Train Loss: 0.0408, Val Loss: 0.0504\n",
            "Epoch 68/100, Train Loss: 0.0438, Val Loss: 0.0504\n",
            "Epoch 69/100, Train Loss: 0.0457, Val Loss: 0.0504\n",
            "Epoch 70/100, Train Loss: 0.0429, Val Loss: 0.0504\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 71/100, Train Loss: 0.0416, Val Loss: 0.0504\n",
            "Epoch 72/100, Train Loss: 0.0399, Val Loss: 0.0504\n",
            "Epoch 73/100, Train Loss: 0.0391, Val Loss: 0.0504\n",
            "Epoch 74/100, Train Loss: 0.0392, Val Loss: 0.0504\n",
            "Epoch 75/100, Train Loss: 0.0425, Val Loss: 0.0504\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 76/100, Train Loss: 0.0414, Val Loss: 0.0504\n",
            "Epoch 77/100, Train Loss: 0.0446, Val Loss: 0.0504\n",
            "Epoch 78/100, Train Loss: 0.0430, Val Loss: 0.0504\n",
            "Epoch 79/100, Train Loss: 0.0428, Val Loss: 0.0504\n",
            "Epoch 80/100, Train Loss: 0.0399, Val Loss: 0.0504\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 81/100, Train Loss: 0.0430, Val Loss: 0.0504\n",
            "Epoch 82/100, Train Loss: 0.0440, Val Loss: 0.0504\n",
            "Epoch 83/100, Train Loss: 0.0440, Val Loss: 0.0504\n",
            "Epoch 84/100, Train Loss: 0.0406, Val Loss: 0.0504\n",
            "Epoch 85/100, Train Loss: 0.0424, Val Loss: 0.0504\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 86/100, Train Loss: 0.0424, Val Loss: 0.0504\n",
            "Epoch 87/100, Train Loss: 0.0414, Val Loss: 0.0504\n",
            "Epoch 88/100, Train Loss: 0.0432, Val Loss: 0.0504\n",
            "Epoch 89/100, Train Loss: 0.0374, Val Loss: 0.0504\n",
            "Epoch 90/100, Train Loss: 0.0440, Val Loss: 0.0504\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 91/100, Train Loss: 0.0390, Val Loss: 0.0504\n",
            "Epoch 92/100, Train Loss: 0.0396, Val Loss: 0.0504\n",
            "Epoch 93/100, Train Loss: 0.0406, Val Loss: 0.0504\n",
            "Epoch 94/100, Train Loss: 0.0431, Val Loss: 0.0504\n",
            "Epoch 95/100, Train Loss: 0.0395, Val Loss: 0.0504\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 96/100, Train Loss: 0.0425, Val Loss: 0.0504\n",
            "Epoch 97/100, Train Loss: 0.0407, Val Loss: 0.0504\n",
            "Epoch 98/100, Train Loss: 0.0431, Val Loss: 0.0504\n",
            "Epoch 99/100, Train Loss: 0.0403, Val Loss: 0.0504\n",
            "Epoch 100/100, Train Loss: 0.0417, Val Loss: 0.0504\n",
            "Reducing learning rate by factor of 0.01\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: rf, Units/Estimators: Est100\n",
            "n_estimators: 100\n",
            "Device: cuda\n",
            "Total test samples: 28\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/28\n",
            "Current training set size: 97 samples\n",
            "\n",
            "Test iteration 2/28\n",
            "Current training set size: 98 samples\n",
            "\n",
            "Test iteration 3/28\n",
            "Current training set size: 99 samples\n",
            "\n",
            "Test iteration 4/28\n",
            "Current training set size: 100 samples\n",
            "\n",
            "Test iteration 5/28\n",
            "Current training set size: 101 samples\n",
            "\n",
            "Test iteration 6/28\n",
            "Current training set size: 102 samples\n",
            "\n",
            "Test iteration 7/28\n",
            "Current training set size: 103 samples\n",
            "\n",
            "Test iteration 8/28\n",
            "Current training set size: 104 samples\n",
            "\n",
            "Test iteration 9/28\n",
            "Current training set size: 105 samples\n",
            "\n",
            "Test iteration 10/28\n",
            "Current training set size: 106 samples\n",
            "\n",
            "Test iteration 11/28\n",
            "Current training set size: 107 samples\n",
            "\n",
            "Test iteration 12/28\n",
            "Current training set size: 108 samples\n",
            "\n",
            "Test iteration 13/28\n",
            "Current training set size: 109 samples\n",
            "\n",
            "Test iteration 14/28\n",
            "Current training set size: 110 samples\n",
            "\n",
            "Test iteration 15/28\n",
            "Current training set size: 111 samples\n",
            "\n",
            "Test iteration 16/28\n",
            "Current training set size: 112 samples\n",
            "\n",
            "Test iteration 17/28\n",
            "Current training set size: 113 samples\n",
            "\n",
            "Test iteration 18/28\n",
            "Current training set size: 114 samples\n",
            "\n",
            "Test iteration 19/28\n",
            "Current training set size: 115 samples\n",
            "\n",
            "Test iteration 20/28\n",
            "Current training set size: 116 samples\n",
            "\n",
            "Test iteration 21/28\n",
            "Current training set size: 117 samples\n",
            "\n",
            "Test iteration 22/28\n",
            "Current training set size: 118 samples\n",
            "\n",
            "Test iteration 23/28\n",
            "Current training set size: 119 samples\n",
            "\n",
            "Test iteration 24/28\n",
            "Current training set size: 120 samples\n",
            "\n",
            "Test iteration 25/28\n",
            "Current training set size: 121 samples\n",
            "\n",
            "Test iteration 26/28\n",
            "Current training set size: 122 samples\n",
            "\n",
            "Test iteration 27/28\n",
            "Current training set size: 123 samples\n",
            "\n",
            "Test iteration 28/28\n",
            "Current training set size: 124 samples\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: rf, Units/Estimators: Est200\n",
            "n_estimators: 200\n",
            "Device: cuda\n",
            "Total test samples: 28\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/28\n",
            "Current training set size: 97 samples\n",
            "\n",
            "Test iteration 2/28\n",
            "Current training set size: 98 samples\n",
            "\n",
            "Test iteration 3/28\n",
            "Current training set size: 99 samples\n",
            "\n",
            "Test iteration 4/28\n",
            "Current training set size: 100 samples\n",
            "\n",
            "Test iteration 5/28\n",
            "Current training set size: 101 samples\n",
            "\n",
            "Test iteration 6/28\n",
            "Current training set size: 102 samples\n",
            "\n",
            "Test iteration 7/28\n",
            "Current training set size: 103 samples\n",
            "\n",
            "Test iteration 8/28\n",
            "Current training set size: 104 samples\n",
            "\n",
            "Test iteration 9/28\n",
            "Current training set size: 105 samples\n",
            "\n",
            "Test iteration 10/28\n",
            "Current training set size: 106 samples\n",
            "\n",
            "Test iteration 11/28\n",
            "Current training set size: 107 samples\n",
            "\n",
            "Test iteration 12/28\n",
            "Current training set size: 108 samples\n",
            "\n",
            "Test iteration 13/28\n",
            "Current training set size: 109 samples\n",
            "\n",
            "Test iteration 14/28\n",
            "Current training set size: 110 samples\n",
            "\n",
            "Test iteration 15/28\n",
            "Current training set size: 111 samples\n",
            "\n",
            "Test iteration 16/28\n",
            "Current training set size: 112 samples\n",
            "\n",
            "Test iteration 17/28\n",
            "Current training set size: 113 samples\n",
            "\n",
            "Test iteration 18/28\n",
            "Current training set size: 114 samples\n",
            "\n",
            "Test iteration 19/28\n",
            "Current training set size: 115 samples\n",
            "\n",
            "Test iteration 20/28\n",
            "Current training set size: 116 samples\n",
            "\n",
            "Test iteration 21/28\n",
            "Current training set size: 117 samples\n",
            "\n",
            "Test iteration 22/28\n",
            "Current training set size: 118 samples\n",
            "\n",
            "Test iteration 23/28\n",
            "Current training set size: 119 samples\n",
            "\n",
            "Test iteration 24/28\n",
            "Current training set size: 120 samples\n",
            "\n",
            "Test iteration 25/28\n",
            "Current training set size: 121 samples\n",
            "\n",
            "Test iteration 26/28\n",
            "Current training set size: 122 samples\n",
            "\n",
            "Test iteration 27/28\n",
            "Current training set size: 123 samples\n",
            "\n",
            "Test iteration 28/28\n",
            "Current training set size: 124 samples\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: xgb, Units/Estimators: Est100\n",
            "n_estimators: 100\n",
            "Device: cuda\n",
            "Total test samples: 28\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/28\n",
            "Current training set size: 97 samples\n",
            "\n",
            "Test iteration 2/28\n",
            "Current training set size: 98 samples\n",
            "\n",
            "Test iteration 3/28\n",
            "Current training set size: 99 samples\n",
            "\n",
            "Test iteration 4/28\n",
            "Current training set size: 100 samples\n",
            "\n",
            "Test iteration 5/28\n",
            "Current training set size: 101 samples\n",
            "\n",
            "Test iteration 6/28\n",
            "Current training set size: 102 samples\n",
            "\n",
            "Test iteration 7/28\n",
            "Current training set size: 103 samples\n",
            "\n",
            "Test iteration 8/28\n",
            "Current training set size: 104 samples\n",
            "\n",
            "Test iteration 9/28\n",
            "Current training set size: 105 samples\n",
            "\n",
            "Test iteration 10/28\n",
            "Current training set size: 106 samples\n",
            "\n",
            "Test iteration 11/28\n",
            "Current training set size: 107 samples\n",
            "\n",
            "Test iteration 12/28\n",
            "Current training set size: 108 samples\n",
            "\n",
            "Test iteration 13/28\n",
            "Current training set size: 109 samples\n",
            "\n",
            "Test iteration 14/28\n",
            "Current training set size: 110 samples\n",
            "\n",
            "Test iteration 15/28\n",
            "Current training set size: 111 samples\n",
            "\n",
            "Test iteration 16/28\n",
            "Current training set size: 112 samples\n",
            "\n",
            "Test iteration 17/28\n",
            "Current training set size: 113 samples\n",
            "\n",
            "Test iteration 18/28\n",
            "Current training set size: 114 samples\n",
            "\n",
            "Test iteration 19/28\n",
            "Current training set size: 115 samples\n",
            "\n",
            "Test iteration 20/28\n",
            "Current training set size: 116 samples\n",
            "\n",
            "Test iteration 21/28\n",
            "Current training set size: 117 samples\n",
            "\n",
            "Test iteration 22/28\n",
            "Current training set size: 118 samples\n",
            "\n",
            "Test iteration 23/28\n",
            "Current training set size: 119 samples\n",
            "\n",
            "Test iteration 24/28\n",
            "Current training set size: 120 samples\n",
            "\n",
            "Test iteration 25/28\n",
            "Current training set size: 121 samples\n",
            "\n",
            "Test iteration 26/28\n",
            "Current training set size: 122 samples\n",
            "\n",
            "Test iteration 27/28\n",
            "Current training set size: 123 samples\n",
            "\n",
            "Test iteration 28/28\n",
            "Current training set size: 124 samples\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: xgb, Units/Estimators: Est200\n",
            "n_estimators: 200\n",
            "Device: cuda\n",
            "Total test samples: 28\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/28\n",
            "Current training set size: 97 samples\n",
            "\n",
            "Test iteration 2/28\n",
            "Current training set size: 98 samples\n",
            "\n",
            "Test iteration 3/28\n",
            "Current training set size: 99 samples\n",
            "\n",
            "Test iteration 4/28\n",
            "Current training set size: 100 samples\n",
            "\n",
            "Test iteration 5/28\n",
            "Current training set size: 101 samples\n",
            "\n",
            "Test iteration 6/28\n",
            "Current training set size: 102 samples\n",
            "\n",
            "Test iteration 7/28\n",
            "Current training set size: 103 samples\n",
            "\n",
            "Test iteration 8/28\n",
            "Current training set size: 104 samples\n",
            "\n",
            "Test iteration 9/28\n",
            "Current training set size: 105 samples\n",
            "\n",
            "Test iteration 10/28\n",
            "Current training set size: 106 samples\n",
            "\n",
            "Test iteration 11/28\n",
            "Current training set size: 107 samples\n",
            "\n",
            "Test iteration 12/28\n",
            "Current training set size: 108 samples\n",
            "\n",
            "Test iteration 13/28\n",
            "Current training set size: 109 samples\n",
            "\n",
            "Test iteration 14/28\n",
            "Current training set size: 110 samples\n",
            "\n",
            "Test iteration 15/28\n",
            "Current training set size: 111 samples\n",
            "\n",
            "Test iteration 16/28\n",
            "Current training set size: 112 samples\n",
            "\n",
            "Test iteration 17/28\n",
            "Current training set size: 113 samples\n",
            "\n",
            "Test iteration 18/28\n",
            "Current training set size: 114 samples\n",
            "\n",
            "Test iteration 19/28\n",
            "Current training set size: 115 samples\n",
            "\n",
            "Test iteration 20/28\n",
            "Current training set size: 116 samples\n",
            "\n",
            "Test iteration 21/28\n",
            "Current training set size: 117 samples\n",
            "\n",
            "Test iteration 22/28\n",
            "Current training set size: 118 samples\n",
            "\n",
            "Test iteration 23/28\n",
            "Current training set size: 119 samples\n",
            "\n",
            "Test iteration 24/28\n",
            "Current training set size: 120 samples\n",
            "\n",
            "Test iteration 25/28\n",
            "Current training set size: 121 samples\n",
            "\n",
            "Test iteration 26/28\n",
            "Current training set size: 122 samples\n",
            "\n",
            "Test iteration 27/28\n",
            "Current training set size: 123 samples\n",
            "\n",
            "Test iteration 28/28\n",
            "Current training set size: 124 samples\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: gbm, Units/Estimators: Est100\n",
            "n_estimators: 100\n",
            "Device: cuda\n",
            "Total test samples: 28\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/28\n",
            "Current training set size: 97 samples\n",
            "\n",
            "Test iteration 2/28\n",
            "Current training set size: 98 samples\n",
            "\n",
            "Test iteration 3/28\n",
            "Current training set size: 99 samples\n",
            "\n",
            "Test iteration 4/28\n",
            "Current training set size: 100 samples\n",
            "\n",
            "Test iteration 5/28\n",
            "Current training set size: 101 samples\n",
            "\n",
            "Test iteration 6/28\n",
            "Current training set size: 102 samples\n",
            "\n",
            "Test iteration 7/28\n",
            "Current training set size: 103 samples\n",
            "\n",
            "Test iteration 8/28\n",
            "Current training set size: 104 samples\n",
            "\n",
            "Test iteration 9/28\n",
            "Current training set size: 105 samples\n",
            "\n",
            "Test iteration 10/28\n",
            "Current training set size: 106 samples\n",
            "\n",
            "Test iteration 11/28\n",
            "Current training set size: 107 samples\n",
            "\n",
            "Test iteration 12/28\n",
            "Current training set size: 108 samples\n",
            "\n",
            "Test iteration 13/28\n",
            "Current training set size: 109 samples\n",
            "\n",
            "Test iteration 14/28\n",
            "Current training set size: 110 samples\n",
            "\n",
            "Test iteration 15/28\n",
            "Current training set size: 111 samples\n",
            "\n",
            "Test iteration 16/28\n",
            "Current training set size: 112 samples\n",
            "\n",
            "Test iteration 17/28\n",
            "Current training set size: 113 samples\n",
            "\n",
            "Test iteration 18/28\n",
            "Current training set size: 114 samples\n",
            "\n",
            "Test iteration 19/28\n",
            "Current training set size: 115 samples\n",
            "\n",
            "Test iteration 20/28\n",
            "Current training set size: 116 samples\n",
            "\n",
            "Test iteration 21/28\n",
            "Current training set size: 117 samples\n",
            "\n",
            "Test iteration 22/28\n",
            "Current training set size: 118 samples\n",
            "\n",
            "Test iteration 23/28\n",
            "Current training set size: 119 samples\n",
            "\n",
            "Test iteration 24/28\n",
            "Current training set size: 120 samples\n",
            "\n",
            "Test iteration 25/28\n",
            "Current training set size: 121 samples\n",
            "\n",
            "Test iteration 26/28\n",
            "Current training set size: 122 samples\n",
            "\n",
            "Test iteration 27/28\n",
            "Current training set size: 123 samples\n",
            "\n",
            "Test iteration 28/28\n",
            "Current training set size: 124 samples\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: gbm, Units/Estimators: Est200\n",
            "n_estimators: 200\n",
            "Device: cuda\n",
            "Total test samples: 28\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration 1/28\n",
            "Current training set size: 97 samples\n",
            "\n",
            "Test iteration 2/28\n",
            "Current training set size: 98 samples\n",
            "\n",
            "Test iteration 3/28\n",
            "Current training set size: 99 samples\n",
            "\n",
            "Test iteration 4/28\n",
            "Current training set size: 100 samples\n",
            "\n",
            "Test iteration 5/28\n",
            "Current training set size: 101 samples\n",
            "\n",
            "Test iteration 6/28\n",
            "Current training set size: 102 samples\n",
            "\n",
            "Test iteration 7/28\n",
            "Current training set size: 103 samples\n",
            "\n",
            "Test iteration 8/28\n",
            "Current training set size: 104 samples\n",
            "\n",
            "Test iteration 9/28\n",
            "Current training set size: 105 samples\n",
            "\n",
            "Test iteration 10/28\n",
            "Current training set size: 106 samples\n",
            "\n",
            "Test iteration 11/28\n",
            "Current training set size: 107 samples\n",
            "\n",
            "Test iteration 12/28\n",
            "Current training set size: 108 samples\n",
            "\n",
            "Test iteration 13/28\n",
            "Current training set size: 109 samples\n",
            "\n",
            "Test iteration 14/28\n",
            "Current training set size: 110 samples\n",
            "\n",
            "Test iteration 15/28\n",
            "Current training set size: 111 samples\n",
            "\n",
            "Test iteration 16/28\n",
            "Current training set size: 112 samples\n",
            "\n",
            "Test iteration 17/28\n",
            "Current training set size: 113 samples\n",
            "\n",
            "Test iteration 18/28\n",
            "Current training set size: 114 samples\n",
            "\n",
            "Test iteration 19/28\n",
            "Current training set size: 115 samples\n",
            "\n",
            "Test iteration 20/28\n",
            "Current training set size: 116 samples\n",
            "\n",
            "Test iteration 21/28\n",
            "Current training set size: 117 samples\n",
            "\n",
            "Test iteration 22/28\n",
            "Current training set size: 118 samples\n",
            "\n",
            "Test iteration 23/28\n",
            "Current training set size: 119 samples\n",
            "\n",
            "Test iteration 24/28\n",
            "Current training set size: 120 samples\n",
            "\n",
            "Test iteration 25/28\n",
            "Current training set size: 121 samples\n",
            "\n",
            "Test iteration 26/28\n",
            "Current training set size: 122 samples\n",
            "\n",
            "Test iteration 27/28\n",
            "Current training set size: 123 samples\n",
            "\n",
            "Test iteration 28/28\n",
            "Current training set size: 124 samples\n",
            "Results:\n",
            "            RMSE            MAE       MAPE model_type units drop_rate  \\\n",
            "0  225759.217575  111165.351562  16.551416        cnn   N/A       0.1   \n",
            "1  219070.513580  101284.585938  14.584393        cnn   N/A       0.1   \n",
            "2  227766.298192  113325.265625  17.376490        cnn   N/A       0.2   \n",
            "3  250403.580294  116326.789062  16.266497        cnn   N/A       0.2   \n",
            "4  217407.703641   99140.649934  15.045178         rf   N/A       N/A   \n",
            "5  216149.408932   99611.649846  15.294728         rf   N/A       N/A   \n",
            "6  247461.799492  126088.945312  19.927383        xgb   N/A       N/A   \n",
            "7  247397.428475  126070.179688  19.928684        xgb   N/A       N/A   \n",
            "8  181728.704036   91458.157717  15.199082        gbm   N/A       N/A   \n",
            "9  181729.207572   91456.654771  15.199517        gbm   N/A       N/A   \n",
            "\n",
            "  dense_unit batch_size epochs  n_estimators  \n",
            "0         32          4    100           NaN  \n",
            "1         64          4    100           NaN  \n",
            "2         32          4    100           NaN  \n",
            "3         64          4    100           NaN  \n",
            "4        N/A        N/A    N/A         100.0  \n",
            "5        N/A        N/A    N/A         200.0  \n",
            "6        N/A        N/A    N/A         100.0  \n",
            "7        N/A        N/A    N/A         200.0  \n",
            "8        N/A        N/A    N/A         100.0  \n",
            "9        N/A        N/A    N/A         200.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adjusted_valuelist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "w5DPqKULNNjt",
        "outputId": "435b2555-39ee-4478-c2fc-1e6d21133522"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            RMSE            MAE       MAPE model_type units drop_rate  \\\n",
              "0  225759.217575  111165.351562  16.551416        cnn   N/A       0.1   \n",
              "1  219070.513580  101284.585938  14.584393        cnn   N/A       0.1   \n",
              "2  227766.298192  113325.265625  17.376490        cnn   N/A       0.2   \n",
              "3  250403.580294  116326.789062  16.266497        cnn   N/A       0.2   \n",
              "4  217407.703641   99140.649934  15.045178         rf   N/A       N/A   \n",
              "5  216149.408932   99611.649846  15.294728         rf   N/A       N/A   \n",
              "6  247461.799492  126088.945312  19.927383        xgb   N/A       N/A   \n",
              "7  247397.428475  126070.179688  19.928684        xgb   N/A       N/A   \n",
              "8  181728.704036   91458.157717  15.199082        gbm   N/A       N/A   \n",
              "9  181729.207572   91456.654771  15.199517        gbm   N/A       N/A   \n",
              "\n",
              "  dense_unit batch_size epochs  n_estimators  \n",
              "0         32          4    100           NaN  \n",
              "1         64          4    100           NaN  \n",
              "2         32          4    100           NaN  \n",
              "3         64          4    100           NaN  \n",
              "4        N/A        N/A    N/A         100.0  \n",
              "5        N/A        N/A    N/A         200.0  \n",
              "6        N/A        N/A    N/A         100.0  \n",
              "7        N/A        N/A    N/A         200.0  \n",
              "8        N/A        N/A    N/A         100.0  \n",
              "9        N/A        N/A    N/A         200.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3265a29f-bae2-467c-aa9b-b06a18af34fc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAE</th>\n",
              "      <th>MAPE</th>\n",
              "      <th>model_type</th>\n",
              "      <th>units</th>\n",
              "      <th>drop_rate</th>\n",
              "      <th>dense_unit</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>epochs</th>\n",
              "      <th>n_estimators</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>225759.217575</td>\n",
              "      <td>111165.351562</td>\n",
              "      <td>16.551416</td>\n",
              "      <td>cnn</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>219070.513580</td>\n",
              "      <td>101284.585938</td>\n",
              "      <td>14.584393</td>\n",
              "      <td>cnn</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>227766.298192</td>\n",
              "      <td>113325.265625</td>\n",
              "      <td>17.376490</td>\n",
              "      <td>cnn</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>250403.580294</td>\n",
              "      <td>116326.789062</td>\n",
              "      <td>16.266497</td>\n",
              "      <td>cnn</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>217407.703641</td>\n",
              "      <td>99140.649934</td>\n",
              "      <td>15.045178</td>\n",
              "      <td>rf</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>216149.408932</td>\n",
              "      <td>99611.649846</td>\n",
              "      <td>15.294728</td>\n",
              "      <td>rf</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>247461.799492</td>\n",
              "      <td>126088.945312</td>\n",
              "      <td>19.927383</td>\n",
              "      <td>xgb</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>247397.428475</td>\n",
              "      <td>126070.179688</td>\n",
              "      <td>19.928684</td>\n",
              "      <td>xgb</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>181728.704036</td>\n",
              "      <td>91458.157717</td>\n",
              "      <td>15.199082</td>\n",
              "      <td>gbm</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>181729.207572</td>\n",
              "      <td>91456.654771</td>\n",
              "      <td>15.199517</td>\n",
              "      <td>gbm</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>200.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3265a29f-bae2-467c-aa9b-b06a18af34fc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3265a29f-bae2-467c-aa9b-b06a18af34fc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3265a29f-bae2-467c-aa9b-b06a18af34fc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-bdbfd852-2d49-4f4e-9147-671692880160\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bdbfd852-2d49-4f4e-9147-671692880160')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-bdbfd852-2d49-4f4e-9147-671692880160 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_d845a361-425a-4e04-b82f-96045c741269\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('adjusted_valuelist')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d845a361-425a-4e04-b82f-96045c741269 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('adjusted_valuelist');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "adjusted_valuelist",
              "summary": "{\n  \"name\": \"adjusted_valuelist\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 24613.150631642722,\n        \"min\": 181728.70403602358,\n        \"max\": 250403.58029389277,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          181728.70403602358,\n          219070.51357953218,\n          216149.4089320423\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12914.508769799275,\n        \"min\": 91456.65477129468,\n        \"max\": 126088.9453125,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          91458.15771737079,\n          101284.5859375,\n          99611.64984610451\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.9702418882180024,\n        \"min\": 14.584392547607422,\n        \"max\": 19.92868423461914,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          15.199081612896977,\n          14.584392547607422,\n          15.29472821633098\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"rf\",\n          \"gbm\",\n          \"cnn\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"units\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"N/A\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"drop_rate\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dense_unit\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"N/A\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"epochs\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"N/A\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_estimators\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 54.772255750516614,\n        \"min\": 100.0,\n        \"max\": 200.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          200.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "P3uuXlSh2FSK",
        "OR-j7ZSzphZa",
        "L8riUYxsK5mb",
        "rmNcL9zfLBF6",
        "YEhxplP631MI",
        "PA5vNRcdd4RQ",
        "WJkpp2jydsm9"
      ],
      "mount_file_id": "1fEXxAhvhR32lyXzpHrvge7zXB86Fb8JX",
      "authorship_tag": "ABX9TyMMz2ZMNTA2WZZyuUmetO1l",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}