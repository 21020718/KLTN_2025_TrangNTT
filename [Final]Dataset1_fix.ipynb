{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1HEQyKxGdH4xMphDO9xqwvNgnJtDI2moS",
      "authorship_tag": "ABX9TyNsk9FedK5t57hOWn/TLWRX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/21020718/KLTN_2025_TrangNTT/blob/main/%5BFinal%5DDataset1_fix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bộ dữ liệu thứ nhất**"
      ],
      "metadata": {
        "id": "QkVAGA_jIyrQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "0IqtmlORmDs2",
        "outputId": "7b547365-8315-4d70-a31f-d6107f0f7ffc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "IPython.notebook.set_autosave_interval(60000)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autosaving every 60 seconds\n"
          ]
        }
      ],
      "source": [
        "%autosave 60\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "YL0QfXV6I5Uk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Define a fixed seed value\n",
        "SEED = 42\n",
        "\n",
        "# Set random seeds for all libraries\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)  # For GPU if available\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "import sklearn.preprocessing\n",
        "import sklearn.preprocessing._data as data\n",
        "import sys\n",
        "sys.modules[\"sklearn.preprocessing.data\"] = data\n",
        "\n",
        "import pandas as pd\n",
        "from numpy import concatenate\n",
        "from math import sqrt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.metrics import normalized_mutual_info_score\n",
        "\n",
        "def Scaler(data):\n",
        "\n",
        "    \"\"\"\n",
        "        Scaler all feature to range(0,1)\n",
        "        Arguments:\n",
        "          data:  Pandas DataFrame of data\n",
        "        Return:\n",
        "          scaler: scaler\n",
        "          scaledDf:Pandas DataFrame of scaled data\n",
        "    \"\"\"\n",
        "\n",
        "    values = data.values\n",
        "    values = values.astype('float32')\n",
        "    scaler = MinMaxScaler(feature_range=(0,1))\n",
        "    scaled = scaler.fit_transform(values)\n",
        "    scaledDf = pd.DataFrame(scaled,columns=data.columns)\n",
        "    return scaler,scaledDf\n",
        "\n",
        "\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    \"\"\"\n",
        "    Frame a time series as a supervised learning dataset.\n",
        "    Arguments:\n",
        "        data: Sequence of observations as a list or NumPy array.\n",
        "        n_in: Number of lag observations as input (X).\n",
        "        n_out: Number of observations as output (y).\n",
        "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
        "    Returns:\n",
        "        Pandas DataFrame of series framed for supervised learning.\n",
        "    \"\"\"\n",
        "\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = pd.DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # put it all together\n",
        "    agg = pd.concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "\n",
        "    agg.drop(agg.columns[-(df.shape[1]-1):],axis = 1,inplace=True)\n",
        "    return agg\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # Load data with date column preserved\n",
        "    data = pd.read_csv(\"/content/drive/MyDrive/VN2008-2020.csv\", encoding='utf-8-sig')\n",
        "\n",
        "    dates = data[\"date\"].copy()\n",
        "    data.drop(columns=[\"date\"], inplace=True)\n",
        "    data.fillna(0, inplace=True)\n",
        "\n",
        "    scaler, scaledDf = Scaler(data)\n",
        "\n",
        "    scaledDf_with_dates = scaledDf.copy()\n",
        "    scaledDf_with_dates['date'] = dates\n",
        "\n",
        "    reframed = series_to_supervised(scaledDf, n_in=12)\n",
        "    reframed_dates = dates.reset_index(drop=True)\n",
        "    reframed_with_dates = reframed.copy()\n",
        "    reframed_with_dates['date'] = reframed_dates"
      ],
      "metadata": {
        "id": "-oE0lyzHmGvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build model"
      ],
      "metadata": {
        "id": "LidaMlUBJDjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Giảm số lượng/GRU unit, dense unit, epochs và sử dụng batch size nhỏ hơn để huấn luyện nhanh hơn.\n",
        "# model_types = ['hybrid']\n",
        "# lstm_unit = [128, 256, 512]\n",
        "# gru_unit = [8, 16, 32]\n",
        "# drop_rate = [0.1, 0.2]\n",
        "# dense_unit = [16, 32, 64]\n",
        "# batch_size_num = [2, 4]\n",
        "# epochs = [100]\n",
        "\n",
        "model_types = ['lstm','gru','hybrid']\n",
        "lstm_unit = [256,512]\n",
        "gru_unit = [8,16]\n",
        "drop_rate = [0.1,0.2]\n",
        "dense_unit = [32,64]\n",
        "batch_size_num = [4]\n",
        "epochs = [100]\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import concatenate\n",
        "import itertools\n",
        "from math import sqrt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import time\n",
        "\n",
        "class AttentionGRU(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, gru_units, dropout_rate, dense_units):\n",
        "        super(AttentionGRU, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention1 = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # GRU layer\n",
        "        self.gru = nn.GRU(input_dim, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear(gru_units * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)\n",
        "        a = self.attention1(a)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # GRU layer - output: (batch, seq, hidden_size)\n",
        "        gru_out, _ = self.gru(attention_mul)\n",
        "\n",
        "        # Flatten and Dense layers\n",
        "        flattened = self.flatten(gru_out)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class AttentionLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, dropout_rate, dense_units):\n",
        "        super(AttentionLSTM, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention1 = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear(lstm_units * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, time_steps, input_dim)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)\n",
        "        a = self.attention1(a)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # LSTM layer - output: (batch, seq, hidden_size)\n",
        "        lstm_out, _ = self.lstm(attention_mul)\n",
        "\n",
        "        # Flatten and Dense layers\n",
        "        flattened = self.flatten(lstm_out)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class HybridLSTM_GRU(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(HybridLSTM_GRU, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention1 = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # LSTM and GRU layers\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "        self.gru = nn.GRU(input_dim, gru_units, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        # Combined size from both LSTM and GRU\n",
        "        self.dense1 = nn.Linear((lstm_units + gru_units) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, time_steps, input_dim)\n",
        "\n",
        "        # Attention mechanism\n",
        "        a = x.permute(0, 2, 1)\n",
        "        a = self.attention1(a)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attention_mul = torch.mul(x, a)\n",
        "\n",
        "        # LSTM and GRU layers\n",
        "        lstm_out, _ = self.lstm(attention_mul)\n",
        "        gru_out, _ = self.gru(attention_mul)\n",
        "\n",
        "        # Concatenate LSTM and GRU outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Flatten and Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "class MultiScaleHybrid(nn.Module):\n",
        "    def __init__(self, input_dim, time_steps, lstm_units, gru_units, dropout_rate, dense_units):\n",
        "        super(MultiScaleHybrid, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # LSTM for long-term dependencies\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)\n",
        "\n",
        "        # GRU for shorter-term dependencies (operating on windows)\n",
        "        self.gru = nn.GRU(input_dim, gru_units, batch_first=True)\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Linear(time_steps, time_steps)\n",
        "        self.attention2 = nn.Linear(time_steps, time_steps)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense1 = nn.Linear((lstm_units + gru_units) * time_steps, dense_units)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense2 = nn.Linear(dense_units, 1)\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Full sequence for LSTM (long-term)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Attention mechanism for GRU input\n",
        "        a = x.permute(0, 2, 1)\n",
        "        a = self.attention(a)\n",
        "        a = self.sigmoid(a)\n",
        "        a = self.attention2(a)\n",
        "        a = self.softmax(a)\n",
        "        a = a.permute(0, 2, 1)\n",
        "\n",
        "        # Apply attention weights for GRU (short-term focus)\n",
        "        gru_input = torch.mul(x, a)\n",
        "        gru_out, _ = self.gru(gru_input)\n",
        "\n",
        "        # Combine outputs\n",
        "        combined = torch.cat((lstm_out, gru_out), dim=2)\n",
        "\n",
        "        # Dense layers\n",
        "        flattened = self.flatten(combined)\n",
        "        dropout_out = self.dropout(flattened)\n",
        "        dense1_out = self.tanh(self.dense1(dropout_out))\n",
        "        output = self.final_activation(self.dense2(dense1_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "def build_model(train_X, train_Y, val_X, val_Y, model_type='gru', lstm_units=128, gru_units=128, drop_rate=0.3, dense_unit=64, batch_size=32, epochs=100):\n",
        "    # Print training parameters\n",
        "    train_X_tensor = torch.FloatTensor(train_X)\n",
        "    train_Y_tensor = torch.FloatTensor(train_Y.reshape(-1, 1))\n",
        "    val_X_tensor = torch.FloatTensor(val_X)\n",
        "    val_Y_tensor = torch.FloatTensor(val_Y.reshape(-1, 1))\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
        "    val_dataset = TensorDataset(val_X_tensor, val_Y_tensor)\n",
        "\n",
        "    # Create reproducible DataLoaders with fixed seeds\n",
        "    train_generator = torch.Generator()\n",
        "    train_generator.manual_seed(SEED)\n",
        "    val_generator = torch.Generator()\n",
        "    val_generator.manual_seed(SEED)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=train_generator)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, generator=val_generator)\n",
        "\n",
        "    # Initialize model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    time_steps = train_X.shape[1]\n",
        "    input_dim = train_X.shape[2]\n",
        "\n",
        "    # Initialize model with fixed initial weights\n",
        "    torch.manual_seed(SEED)\n",
        "\n",
        "    if model_type == 'gru':\n",
        "        model = AttentionGRU(input_dim, time_steps, gru_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'lstm':\n",
        "        model = AttentionLSTM(input_dim, time_steps, lstm_units, drop_rate, dense_unit).to(device)\n",
        "    elif model_type == 'hybrid':\n",
        "        model = HybridLSTM_GRU(input_dim, time_steps, lstm_units, gru_units, drop_rate, dense_unit).to(device)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "    # Initialize optimizer and loss function\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    criterion = nn.L1Loss()  # MAE loss\n",
        "\n",
        "    # Training loop\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    patience = 5\n",
        "    lr_factor = 0.01\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        # Print progress\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
        "\n",
        "        # Learning rate schedule based on validation loss\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] *= lr_factor\n",
        "                patience_counter = 0\n",
        "                print(f'Reducing learning rate by factor of {lr_factor}')\n",
        "\n",
        "    return model\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    mask = y_true != 0\n",
        "    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "    return mape\n",
        "\n",
        "def walk_forward(train_X, train_Y, val_X, val_Y, test_X, test_Y, grid_search, scaler, test_dates):\n",
        "    r, f, c = test_X.shape\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Constants to ensure reproducibility\n",
        "    SEED = 42\n",
        "    torch.manual_seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "\n",
        "    all_predictions = {}\n",
        "    all_adjusted_predictions = {}\n",
        "    all_ground_truths = {}\n",
        "\n",
        "    # Create lists to store all evaluation results\n",
        "    original_valuelists = []\n",
        "    adjusted_valuelists = []\n",
        "\n",
        "    # Dictionary to store training times\n",
        "    training_times = {}\n",
        "\n",
        "    # Sort test samples by date\n",
        "    sorted_test_indices = np.argsort(test_dates)\n",
        "\n",
        "    for x in grid_search:\n",
        "        model_type = x[0]\n",
        "\n",
        "        if model_type in ['hybrid']:\n",
        "            model_type, lstm_unit_val, gru_unit_val, drop, dense, batch, epoch = x\n",
        "            units = f\"L{lstm_unit_val}_G{gru_unit_val}\"\n",
        "            config_key = f\"{model_type}_lstmUnit{lstm_unit_val}_gruUnit{gru_unit_val}_drop{drop}_dense{dense}_batch{batch}_epochs{epoch}\"\n",
        "        else:\n",
        "            model_type, units, drop, dense, batch, epoch = x\n",
        "            config_key = f\"{model_type}_unit{units}_drop{drop}_dense{dense}_batch{batch}_epochs{epoch}\"\n",
        "\n",
        "        print(\"\\n\" + \"*\"*50)\n",
        "        print(f\"Starting walk-forward validation with parameters:\")\n",
        "        print(f\"Model Type: {model_type}, Units: {units}, Dropout: {drop}, Dense Units: {dense}\")\n",
        "        print(f\"Batch Size: {batch}, Epochs: {epoch}\")\n",
        "        print(f\"Device: {device}\")\n",
        "        print(f\"Total test samples: {len(test_X)}\")\n",
        "        print(\"*\"*50 + \"\\n\")\n",
        "\n",
        "        # Start timing for this configuration\n",
        "        start_time = time.time()\n",
        "\n",
        "        predictions = []\n",
        "        adjusted_predictions = []\n",
        "        groundtrue = []\n",
        "\n",
        "        # Initialize with original train data\n",
        "        history_x = train_X.copy()\n",
        "        history_y = train_Y.copy()\n",
        "\n",
        "        # Store the mapping between test points and their dates for ordering\n",
        "        test_dates_array = pd.to_datetime(test_dates).values\n",
        "        test_date_mapping = [(i, test_dates_array[i]) for i in range(len(test_dates_array))]\n",
        "        sorted_test_indices = [i for i, _ in sorted(test_date_mapping, key=lambda x: x[1])]\n",
        "\n",
        "        for i in sorted_test_indices:\n",
        "            test_date = pd.to_datetime(test_dates.iloc[i])\n",
        "            print(f\"\\nTest iteration for date: {test_date}\")\n",
        "\n",
        "            # For each test point, only use training data from dates before current test date\n",
        "            current_train_x = []\n",
        "            current_train_y = []\n",
        "\n",
        "            # Filter history data to only include data before current test date\n",
        "            if test_date.year == 2019:\n",
        "                # For 2019 test data, only use train data from before 2019 and validation data (2018)\n",
        "                # We use initial train and validation data directly\n",
        "                current_train_x = np.concatenate([train_X, val_X], axis=0)\n",
        "                current_train_y = np.concatenate([train_Y, val_Y], axis=0)\n",
        "            else:\n",
        "                # For 2020 test data (second test period), use all available data up to that point\n",
        "                current_train_x = history_x\n",
        "                current_train_y = history_y\n",
        "\n",
        "            print(f\"Current training set size: {len(current_train_x)} samples\")\n",
        "\n",
        "            # Skip if not enough training data\n",
        "            if len(current_train_x) < 10:\n",
        "                print(\"Not enough training data, skipping this test point\")\n",
        "                continue\n",
        "\n",
        "            # Train the model with appropriate data\n",
        "            if model_type in ['hybrid']:\n",
        "                model = build_model(current_train_x, current_train_y, val_X, val_Y, model_type=model_type,\n",
        "                                lstm_units=lstm_unit_val, gru_units=gru_unit_val, drop_rate=drop,\n",
        "                                dense_unit=dense, batch_size=batch, epochs=epoch)\n",
        "            else:\n",
        "                model = build_model(current_train_x, current_train_y, val_X, val_Y, model_type=model_type,\n",
        "                                lstm_units=units if model_type == 'lstm' else 128,\n",
        "                                gru_units=units if model_type == 'gru' else 128,\n",
        "                                drop_rate=drop, dense_unit=dense, batch_size=batch, epochs=epoch)\n",
        "\n",
        "            # Predict\n",
        "            model.eval()\n",
        "            test_tensor = torch.FloatTensor(test_X[i].reshape(1, f, c)).to(device)\n",
        "            with torch.no_grad():\n",
        "                yhat = model(test_tensor).cpu().numpy()\n",
        "\n",
        "            inv_yhat, inv_y = inverscale(yhat, test_X[i], test_Y[i], scaler)\n",
        "            prev_month_lockdown = test_X[i][11][5]\n",
        "            adjusted_inv_yhat = inv_yhat * (1 - prev_month_lockdown)\n",
        "            predictions.append(inv_yhat)\n",
        "            adjusted_predictions.append(adjusted_inv_yhat)\n",
        "            groundtrue.append(inv_y)\n",
        "\n",
        "            # Add this observation to history for the next iteration\n",
        "            history_x = np.append(history_x, [test_X[i]], axis=0)\n",
        "            history_y = np.append(history_y, test_Y[i])\n",
        "\n",
        "        # Calculate total training time\n",
        "        total_time = time.time() - start_time\n",
        "        training_times[config_key] = total_time\n",
        "        print(f\"\\nTotal training time for {config_key}: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
        "\n",
        "        # Process results as before\n",
        "        all_predictions[config_key] = np.array(predictions).flatten()\n",
        "        all_adjusted_predictions[config_key] = np.array(adjusted_predictions).flatten()\n",
        "        all_ground_truths[config_key] = np.array(groundtrue).flatten()\n",
        "\n",
        "        original_valuelist = evalue(np.array(predictions).flatten(), np.array(groundtrue).flatten())\n",
        "        original_valuelist['model_type'] = model_type\n",
        "        original_valuelist['units'] = units\n",
        "        original_valuelist['drop_rate'] = drop\n",
        "        original_valuelist['dense_unit'] = dense\n",
        "        original_valuelist['batch_size'] = batch\n",
        "        original_valuelist['epochs'] = epoch\n",
        "        original_valuelist['training_time'] = total_time  # Add training time to results\n",
        "\n",
        "        adjusted_valuelist = evalue(np.array(adjusted_predictions).flatten(), np.array(groundtrue).flatten())\n",
        "        adjusted_valuelist['model_type'] = model_type\n",
        "        adjusted_valuelist['units'] = units\n",
        "        adjusted_valuelist['drop_rate'] = drop\n",
        "        adjusted_valuelist['dense_unit'] = dense\n",
        "        adjusted_valuelist['batch_size'] = batch\n",
        "        adjusted_valuelist['epochs'] = epoch\n",
        "        adjusted_valuelist['training_time'] = total_time  # Add training time to results\n",
        "\n",
        "        original_valuelists.append(original_valuelist)\n",
        "        adjusted_valuelists.append(adjusted_valuelist)\n",
        "\n",
        "    all_original_valuelist = pd.concat(original_valuelists, ignore_index=True)\n",
        "    all_adjusted_valuelist = pd.concat(adjusted_valuelists, ignore_index=True)\n",
        "\n",
        "    # Save training times to CSV\n",
        "    pd.DataFrame(list(training_times.items()), columns=['Model', 'Training Time (s)']).to_csv('model_training_times.csv', index=False)\n",
        "\n",
        "    # Also add the time results to the return values\n",
        "    return all_original_valuelist, all_adjusted_valuelist, all_predictions, all_ground_truths, all_adjusted_predictions, training_times\n",
        "\n",
        "def evalue(yhat, inv_y):\n",
        "    valuelist = {}\n",
        "    DLM_rmse = sqrt(mean_squared_error(inv_y, yhat))\n",
        "    valuelist.update({'RMSE': {'DLM': DLM_rmse}})\n",
        "    DLM_mae = mean_absolute_error(inv_y, yhat)\n",
        "    valuelist.update({'MAE': {'DLM': DLM_mae}})\n",
        "    DLM_mape = mean_absolute_percentage_error(inv_y, yhat)\n",
        "    valuelist.update({'MAPE': {'DLM': DLM_mape}})\n",
        "    return pd.DataFrame(valuelist)\n",
        "\n",
        "def inverscale(yhat, test_X, test_Y, scaler):\n",
        "    feature = len(scaler.scale_)\n",
        "    test_Y = np.array(test_Y)\n",
        "    test_X = test_X[1, 0:feature]\n",
        "    test_X = test_X.reshape(1, test_X.shape[0])\n",
        "\n",
        "    if len(yhat.shape) == 1:\n",
        "        yhat = yhat.reshape(len(yhat), 1)\n",
        "\n",
        "    inv_yhat = concatenate((yhat, test_X[:, :-1]), axis=1)\n",
        "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "    inv_yhat = inv_yhat[:, 0]\n",
        "\n",
        "    test_Y = test_Y.reshape(1, 1)\n",
        "    inv_y = concatenate((test_Y, test_X[:, :-1]), axis=1)\n",
        "    inv_y = scaler.inverse_transform(inv_y)\n",
        "    inv_y = inv_y[:, 0]\n",
        "    return inv_yhat, inv_y\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    values = reframed.values\n",
        "    reframed_with_dates_values = reframed_with_dates.values\n",
        "\n",
        "    # Convert date strings to datetime objects\n",
        "    dates = pd.to_datetime(reframed_with_dates['date'])\n",
        "\n",
        "    # Create masks for each split according to the specified date ranges\n",
        "    train_mask = ((dates >= '2008-01-01') & (dates <= '2017-12-31')) | ((dates >= '2020-01-01') & (dates <= '2020-05-31'))\n",
        "    val_mask = (dates >= '2018-01-01') & (dates <= '2018-12-31')\n",
        "    test_mask = ((dates >= '2019-01-01') & (dates <= '2019-12-31')) | ((dates >= '2020-06-01') & (dates <= '2020-07-31'))\n",
        "\n",
        "    # Extract values for train, validation, and test sets (excluding the date column)\n",
        "    train_data = reframed.loc[train_mask].values\n",
        "    val_data = reframed.loc[val_mask].values\n",
        "    test_data = reframed.loc[test_mask].values\n",
        "\n",
        "    # Split into X and Y\n",
        "    train_X, train_Y = train_data[:, :-1], train_data[:, -1]\n",
        "    val_X, val_Y = val_data[:, :-1], val_data[:, -1]\n",
        "    test_X, test_Y = test_data[:, :-1], test_data[:, -1]\n",
        "\n",
        "    # Reshape input to be 3D [samples, timesteps, features]\n",
        "    train_X = train_X.reshape(train_X.shape[0], 12, int(train_X.shape[1]/12))\n",
        "    val_X = val_X.reshape(val_X.shape[0], 12, int(val_X.shape[1]/12))\n",
        "    test_X = test_X.reshape(test_X.shape[0], 12, int(test_X.shape[1]/12))\n",
        "\n",
        "    # Modified grid search creation for all model types\n",
        "    grid_search = []\n",
        "    for model_type in model_types:\n",
        "        if model_type == 'lstm':\n",
        "            grid_search.extend(\n",
        "                list(itertools.product([model_type], lstm_unit, drop_rate, dense_unit, batch_size_num, epochs))\n",
        "            )\n",
        "        elif model_type == 'gru':\n",
        "            grid_search.extend(\n",
        "                list(itertools.product([model_type], gru_unit, drop_rate, dense_unit, batch_size_num, epochs))\n",
        "            )\n",
        "        else:\n",
        "            # All other models (hybrid, sequential, stacked, etc.) need both LSTM and GRU units\n",
        "            grid_search.extend(\n",
        "                list(itertools.product([model_type], lstm_unit, gru_unit, drop_rate, dense_unit, batch_size_num, epochs))\n",
        "            )\n",
        "\n",
        "    # In the main section, update the function call:\n",
        "    original_valuelist, adjusted_valuelist, all_predictions, all_ground_truths, all_adjusted_predictions, training_times = walk_forward(\n",
        "        train_X, train_Y, val_X, val_Y, test_X, test_Y, grid_search, scaler,\n",
        "        dates[test_mask]  # Pass the test dates\n",
        "    )\n",
        "\n",
        "    # Print training times summary\n",
        "    print(\"\\nTraining Times Summary:\")\n",
        "    for model, time_taken in training_times.items():\n",
        "        print(f\"{model}: {time_taken:.2f} seconds ({time_taken/60:.2f} minutes)\")\n",
        "\n",
        "    # Save results with training times included\n",
        "    print(\"\\nSaving results with training times...\")\n",
        "    adjusted_valuelist.to_csv('model_results_with_times.csv', index=False)\n",
        "\n",
        "    print(\"Results:\")\n",
        "    print(adjusted_valuelist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4oqOk9RmOow",
        "outputId": "65bf76b9-14b8-478a-8216-958f173da6a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 50/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 51/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "Epoch 52/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "Epoch 54/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 55/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 56/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Epoch 57/100, Train Loss: 0.0351, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0344, Val Loss: 0.0274\n",
            "Epoch 59/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Epoch 60/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 61/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 62/100, Train Loss: 0.0335, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Epoch 64/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Epoch 65/100, Train Loss: 0.0339, Val Loss: 0.0274\n",
            "Epoch 66/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 67/100, Train Loss: 0.0343, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 69/100, Train Loss: 0.0352, Val Loss: 0.0274\n",
            "Epoch 70/100, Train Loss: 0.0345, Val Loss: 0.0274\n",
            "Epoch 71/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Epoch 72/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 74/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 75/100, Train Loss: 0.0368, Val Loss: 0.0274\n",
            "Epoch 76/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 77/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0472, Val Loss: 0.0274\n",
            "Epoch 79/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 80/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 81/100, Train Loss: 0.0363, Val Loss: 0.0274\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0395, Val Loss: 0.0274\n",
            "Epoch 84/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 85/100, Train Loss: 0.0341, Val Loss: 0.0274\n",
            "Epoch 86/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Epoch 87/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 89/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 90/100, Train Loss: 0.0343, Val Loss: 0.0274\n",
            "Epoch 91/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 92/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0359, Val Loss: 0.0274\n",
            "Epoch 94/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 95/100, Train Loss: 0.0379, Val Loss: 0.0274\n",
            "Epoch 96/100, Train Loss: 0.0373, Val Loss: 0.0274\n",
            "Epoch 97/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0362, Val Loss: 0.0274\n",
            "Epoch 99/100, Train Loss: 0.0353, Val Loss: 0.0274\n",
            "Epoch 100/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "\n",
            "Test iteration for date: 2019-02-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1381, Val Loss: 0.2823\n",
            "Epoch 2/100, Train Loss: 0.1198, Val Loss: 0.3823\n",
            "Epoch 3/100, Train Loss: 0.1231, Val Loss: 0.3381\n",
            "Epoch 4/100, Train Loss: 0.1044, Val Loss: 0.3376\n",
            "Epoch 5/100, Train Loss: 0.1142, Val Loss: 0.2691\n",
            "Epoch 6/100, Train Loss: 0.0803, Val Loss: 0.0677\n",
            "Epoch 7/100, Train Loss: 0.0691, Val Loss: 0.0604\n",
            "Epoch 8/100, Train Loss: 0.0620, Val Loss: 0.1124\n",
            "Epoch 9/100, Train Loss: 0.0601, Val Loss: 0.0728\n",
            "Epoch 10/100, Train Loss: 0.0580, Val Loss: 0.0516\n",
            "Epoch 11/100, Train Loss: 0.0545, Val Loss: 0.0442\n",
            "Epoch 12/100, Train Loss: 0.0556, Val Loss: 0.0514\n",
            "Epoch 13/100, Train Loss: 0.0593, Val Loss: 0.0620\n",
            "Epoch 14/100, Train Loss: 0.0581, Val Loss: 0.0502\n",
            "Epoch 15/100, Train Loss: 0.0533, Val Loss: 0.0485\n",
            "Epoch 16/100, Train Loss: 0.0455, Val Loss: 0.0374\n",
            "Epoch 17/100, Train Loss: 0.0469, Val Loss: 0.0266\n",
            "Epoch 18/100, Train Loss: 0.0494, Val Loss: 0.0331\n",
            "Epoch 19/100, Train Loss: 0.0485, Val Loss: 0.0768\n",
            "Epoch 20/100, Train Loss: 0.0445, Val Loss: 0.0851\n",
            "Epoch 21/100, Train Loss: 0.0428, Val Loss: 0.0285\n",
            "Epoch 22/100, Train Loss: 0.0417, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0359, Val Loss: 0.0321\n",
            "Epoch 24/100, Train Loss: 0.0359, Val Loss: 0.0306\n",
            "Epoch 25/100, Train Loss: 0.0366, Val Loss: 0.0288\n",
            "Epoch 26/100, Train Loss: 0.0348, Val Loss: 0.0284\n",
            "Epoch 27/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0433, Val Loss: 0.0274\n",
            "Epoch 29/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 30/100, Train Loss: 0.0340, Val Loss: 0.0274\n",
            "Epoch 31/100, Train Loss: 0.0359, Val Loss: 0.0274\n",
            "Epoch 32/100, Train Loss: 0.0344, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 34/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 35/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 36/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 37/100, Train Loss: 0.0361, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 39/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 40/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Epoch 41/100, Train Loss: 0.0363, Val Loss: 0.0274\n",
            "Epoch 42/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 44/100, Train Loss: 0.0365, Val Loss: 0.0274\n",
            "Epoch 45/100, Train Loss: 0.0368, Val Loss: 0.0274\n",
            "Epoch 46/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Epoch 47/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0364, Val Loss: 0.0274\n",
            "Epoch 49/100, Train Loss: 0.0351, Val Loss: 0.0274\n",
            "Epoch 50/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 51/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "Epoch 52/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "Epoch 54/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 55/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 56/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Epoch 57/100, Train Loss: 0.0351, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0344, Val Loss: 0.0274\n",
            "Epoch 59/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Epoch 60/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 61/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 62/100, Train Loss: 0.0335, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Epoch 64/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Epoch 65/100, Train Loss: 0.0339, Val Loss: 0.0274\n",
            "Epoch 66/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 67/100, Train Loss: 0.0343, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 69/100, Train Loss: 0.0352, Val Loss: 0.0274\n",
            "Epoch 70/100, Train Loss: 0.0345, Val Loss: 0.0274\n",
            "Epoch 71/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Epoch 72/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 74/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 75/100, Train Loss: 0.0368, Val Loss: 0.0274\n",
            "Epoch 76/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 77/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0472, Val Loss: 0.0274\n",
            "Epoch 79/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 80/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 81/100, Train Loss: 0.0363, Val Loss: 0.0274\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0395, Val Loss: 0.0274\n",
            "Epoch 84/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 85/100, Train Loss: 0.0341, Val Loss: 0.0274\n",
            "Epoch 86/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Epoch 87/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 89/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 90/100, Train Loss: 0.0343, Val Loss: 0.0274\n",
            "Epoch 91/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 92/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0359, Val Loss: 0.0274\n",
            "Epoch 94/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 95/100, Train Loss: 0.0379, Val Loss: 0.0274\n",
            "Epoch 96/100, Train Loss: 0.0373, Val Loss: 0.0274\n",
            "Epoch 97/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0362, Val Loss: 0.0274\n",
            "Epoch 99/100, Train Loss: 0.0353, Val Loss: 0.0274\n",
            "Epoch 100/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "\n",
            "Test iteration for date: 2019-03-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1381, Val Loss: 0.2823\n",
            "Epoch 2/100, Train Loss: 0.1198, Val Loss: 0.3823\n",
            "Epoch 3/100, Train Loss: 0.1231, Val Loss: 0.3381\n",
            "Epoch 4/100, Train Loss: 0.1044, Val Loss: 0.3376\n",
            "Epoch 5/100, Train Loss: 0.1142, Val Loss: 0.2691\n",
            "Epoch 6/100, Train Loss: 0.0803, Val Loss: 0.0677\n",
            "Epoch 7/100, Train Loss: 0.0691, Val Loss: 0.0604\n",
            "Epoch 8/100, Train Loss: 0.0620, Val Loss: 0.1124\n",
            "Epoch 9/100, Train Loss: 0.0601, Val Loss: 0.0728\n",
            "Epoch 10/100, Train Loss: 0.0580, Val Loss: 0.0516\n",
            "Epoch 11/100, Train Loss: 0.0545, Val Loss: 0.0442\n",
            "Epoch 12/100, Train Loss: 0.0556, Val Loss: 0.0514\n",
            "Epoch 13/100, Train Loss: 0.0593, Val Loss: 0.0620\n",
            "Epoch 14/100, Train Loss: 0.0581, Val Loss: 0.0502\n",
            "Epoch 15/100, Train Loss: 0.0533, Val Loss: 0.0485\n",
            "Epoch 16/100, Train Loss: 0.0455, Val Loss: 0.0374\n",
            "Epoch 17/100, Train Loss: 0.0469, Val Loss: 0.0266\n",
            "Epoch 18/100, Train Loss: 0.0494, Val Loss: 0.0331\n",
            "Epoch 19/100, Train Loss: 0.0485, Val Loss: 0.0768\n",
            "Epoch 20/100, Train Loss: 0.0445, Val Loss: 0.0851\n",
            "Epoch 21/100, Train Loss: 0.0428, Val Loss: 0.0285\n",
            "Epoch 22/100, Train Loss: 0.0417, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0359, Val Loss: 0.0321\n",
            "Epoch 24/100, Train Loss: 0.0359, Val Loss: 0.0306\n",
            "Epoch 25/100, Train Loss: 0.0366, Val Loss: 0.0288\n",
            "Epoch 26/100, Train Loss: 0.0348, Val Loss: 0.0284\n",
            "Epoch 27/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0433, Val Loss: 0.0274\n",
            "Epoch 29/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 30/100, Train Loss: 0.0340, Val Loss: 0.0274\n",
            "Epoch 31/100, Train Loss: 0.0359, Val Loss: 0.0274\n",
            "Epoch 32/100, Train Loss: 0.0344, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 34/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 35/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 36/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 37/100, Train Loss: 0.0361, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 39/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 40/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Epoch 41/100, Train Loss: 0.0363, Val Loss: 0.0274\n",
            "Epoch 42/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 44/100, Train Loss: 0.0365, Val Loss: 0.0274\n",
            "Epoch 45/100, Train Loss: 0.0368, Val Loss: 0.0274\n",
            "Epoch 46/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Epoch 47/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0364, Val Loss: 0.0274\n",
            "Epoch 49/100, Train Loss: 0.0351, Val Loss: 0.0274\n",
            "Epoch 50/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 51/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "Epoch 52/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "Epoch 54/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 55/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 56/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Epoch 57/100, Train Loss: 0.0351, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0344, Val Loss: 0.0274\n",
            "Epoch 59/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Epoch 60/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 61/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 62/100, Train Loss: 0.0335, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Epoch 64/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Epoch 65/100, Train Loss: 0.0339, Val Loss: 0.0274\n",
            "Epoch 66/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 67/100, Train Loss: 0.0343, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 69/100, Train Loss: 0.0352, Val Loss: 0.0274\n",
            "Epoch 70/100, Train Loss: 0.0345, Val Loss: 0.0274\n",
            "Epoch 71/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Epoch 72/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 74/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 75/100, Train Loss: 0.0368, Val Loss: 0.0274\n",
            "Epoch 76/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 77/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0472, Val Loss: 0.0274\n",
            "Epoch 79/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 80/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 81/100, Train Loss: 0.0363, Val Loss: 0.0274\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0395, Val Loss: 0.0274\n",
            "Epoch 84/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 85/100, Train Loss: 0.0341, Val Loss: 0.0274\n",
            "Epoch 86/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Epoch 87/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 89/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 90/100, Train Loss: 0.0343, Val Loss: 0.0274\n",
            "Epoch 91/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 92/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0359, Val Loss: 0.0274\n",
            "Epoch 94/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 95/100, Train Loss: 0.0379, Val Loss: 0.0274\n",
            "Epoch 96/100, Train Loss: 0.0373, Val Loss: 0.0274\n",
            "Epoch 97/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0362, Val Loss: 0.0274\n",
            "Epoch 99/100, Train Loss: 0.0353, Val Loss: 0.0274\n",
            "Epoch 100/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "\n",
            "Test iteration for date: 2019-04-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1381, Val Loss: 0.2823\n",
            "Epoch 2/100, Train Loss: 0.1198, Val Loss: 0.3823\n",
            "Epoch 3/100, Train Loss: 0.1231, Val Loss: 0.3381\n",
            "Epoch 4/100, Train Loss: 0.1044, Val Loss: 0.3376\n",
            "Epoch 5/100, Train Loss: 0.1142, Val Loss: 0.2691\n",
            "Epoch 6/100, Train Loss: 0.0803, Val Loss: 0.0677\n",
            "Epoch 7/100, Train Loss: 0.0691, Val Loss: 0.0604\n",
            "Epoch 8/100, Train Loss: 0.0620, Val Loss: 0.1124\n",
            "Epoch 9/100, Train Loss: 0.0601, Val Loss: 0.0728\n",
            "Epoch 10/100, Train Loss: 0.0580, Val Loss: 0.0516\n",
            "Epoch 11/100, Train Loss: 0.0545, Val Loss: 0.0442\n",
            "Epoch 12/100, Train Loss: 0.0556, Val Loss: 0.0514\n",
            "Epoch 13/100, Train Loss: 0.0593, Val Loss: 0.0620\n",
            "Epoch 14/100, Train Loss: 0.0581, Val Loss: 0.0502\n",
            "Epoch 15/100, Train Loss: 0.0533, Val Loss: 0.0485\n",
            "Epoch 16/100, Train Loss: 0.0455, Val Loss: 0.0374\n",
            "Epoch 17/100, Train Loss: 0.0469, Val Loss: 0.0266\n",
            "Epoch 18/100, Train Loss: 0.0494, Val Loss: 0.0331\n",
            "Epoch 19/100, Train Loss: 0.0485, Val Loss: 0.0768\n",
            "Epoch 20/100, Train Loss: 0.0445, Val Loss: 0.0851\n",
            "Epoch 21/100, Train Loss: 0.0428, Val Loss: 0.0285\n",
            "Epoch 22/100, Train Loss: 0.0417, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0359, Val Loss: 0.0321\n",
            "Epoch 24/100, Train Loss: 0.0359, Val Loss: 0.0306\n",
            "Epoch 25/100, Train Loss: 0.0366, Val Loss: 0.0288\n",
            "Epoch 26/100, Train Loss: 0.0348, Val Loss: 0.0284\n",
            "Epoch 27/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0433, Val Loss: 0.0274\n",
            "Epoch 29/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 30/100, Train Loss: 0.0340, Val Loss: 0.0274\n",
            "Epoch 31/100, Train Loss: 0.0359, Val Loss: 0.0274\n",
            "Epoch 32/100, Train Loss: 0.0344, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 34/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 35/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 36/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 37/100, Train Loss: 0.0361, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 39/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 40/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Epoch 41/100, Train Loss: 0.0363, Val Loss: 0.0274\n",
            "Epoch 42/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 44/100, Train Loss: 0.0365, Val Loss: 0.0274\n",
            "Epoch 45/100, Train Loss: 0.0368, Val Loss: 0.0274\n",
            "Epoch 46/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Epoch 47/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0364, Val Loss: 0.0274\n",
            "Epoch 49/100, Train Loss: 0.0351, Val Loss: 0.0274\n",
            "Epoch 50/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 51/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "Epoch 52/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "Epoch 54/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 55/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 56/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Epoch 57/100, Train Loss: 0.0351, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0344, Val Loss: 0.0274\n",
            "Epoch 59/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Epoch 60/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 61/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 62/100, Train Loss: 0.0335, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Epoch 64/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Epoch 65/100, Train Loss: 0.0339, Val Loss: 0.0274\n",
            "Epoch 66/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 67/100, Train Loss: 0.0343, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 69/100, Train Loss: 0.0352, Val Loss: 0.0274\n",
            "Epoch 70/100, Train Loss: 0.0345, Val Loss: 0.0274\n",
            "Epoch 71/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Epoch 72/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 74/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 75/100, Train Loss: 0.0368, Val Loss: 0.0274\n",
            "Epoch 76/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 77/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0472, Val Loss: 0.0274\n",
            "Epoch 79/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 80/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 81/100, Train Loss: 0.0363, Val Loss: 0.0274\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0395, Val Loss: 0.0274\n",
            "Epoch 84/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 85/100, Train Loss: 0.0341, Val Loss: 0.0274\n",
            "Epoch 86/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Epoch 87/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 89/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 90/100, Train Loss: 0.0343, Val Loss: 0.0274\n",
            "Epoch 91/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 92/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0359, Val Loss: 0.0274\n",
            "Epoch 94/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 95/100, Train Loss: 0.0379, Val Loss: 0.0274\n",
            "Epoch 96/100, Train Loss: 0.0373, Val Loss: 0.0274\n",
            "Epoch 97/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0362, Val Loss: 0.0274\n",
            "Epoch 99/100, Train Loss: 0.0353, Val Loss: 0.0274\n",
            "Epoch 100/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "\n",
            "Test iteration for date: 2019-05-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1381, Val Loss: 0.2823\n",
            "Epoch 2/100, Train Loss: 0.1198, Val Loss: 0.3823\n",
            "Epoch 3/100, Train Loss: 0.1231, Val Loss: 0.3381\n",
            "Epoch 4/100, Train Loss: 0.1044, Val Loss: 0.3376\n",
            "Epoch 5/100, Train Loss: 0.1142, Val Loss: 0.2691\n",
            "Epoch 6/100, Train Loss: 0.0803, Val Loss: 0.0677\n",
            "Epoch 7/100, Train Loss: 0.0691, Val Loss: 0.0604\n",
            "Epoch 8/100, Train Loss: 0.0620, Val Loss: 0.1124\n",
            "Epoch 9/100, Train Loss: 0.0601, Val Loss: 0.0728\n",
            "Epoch 10/100, Train Loss: 0.0580, Val Loss: 0.0516\n",
            "Epoch 11/100, Train Loss: 0.0545, Val Loss: 0.0442\n",
            "Epoch 12/100, Train Loss: 0.0556, Val Loss: 0.0514\n",
            "Epoch 13/100, Train Loss: 0.0593, Val Loss: 0.0620\n",
            "Epoch 14/100, Train Loss: 0.0581, Val Loss: 0.0502\n",
            "Epoch 15/100, Train Loss: 0.0533, Val Loss: 0.0485\n",
            "Epoch 16/100, Train Loss: 0.0455, Val Loss: 0.0374\n",
            "Epoch 17/100, Train Loss: 0.0469, Val Loss: 0.0266\n",
            "Epoch 18/100, Train Loss: 0.0494, Val Loss: 0.0331\n",
            "Epoch 19/100, Train Loss: 0.0485, Val Loss: 0.0768\n",
            "Epoch 20/100, Train Loss: 0.0445, Val Loss: 0.0851\n",
            "Epoch 21/100, Train Loss: 0.0428, Val Loss: 0.0285\n",
            "Epoch 22/100, Train Loss: 0.0417, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0359, Val Loss: 0.0321\n",
            "Epoch 24/100, Train Loss: 0.0359, Val Loss: 0.0306\n",
            "Epoch 25/100, Train Loss: 0.0366, Val Loss: 0.0288\n",
            "Epoch 26/100, Train Loss: 0.0348, Val Loss: 0.0284\n",
            "Epoch 27/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0433, Val Loss: 0.0274\n",
            "Epoch 29/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 30/100, Train Loss: 0.0340, Val Loss: 0.0274\n",
            "Epoch 31/100, Train Loss: 0.0359, Val Loss: 0.0274\n",
            "Epoch 32/100, Train Loss: 0.0344, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 34/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 35/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 36/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 37/100, Train Loss: 0.0361, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 39/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 40/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Epoch 41/100, Train Loss: 0.0363, Val Loss: 0.0274\n",
            "Epoch 42/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 44/100, Train Loss: 0.0365, Val Loss: 0.0274\n",
            "Epoch 45/100, Train Loss: 0.0368, Val Loss: 0.0274\n",
            "Epoch 46/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Epoch 47/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0364, Val Loss: 0.0274\n",
            "Epoch 49/100, Train Loss: 0.0351, Val Loss: 0.0274\n",
            "Epoch 50/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 51/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "Epoch 52/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "Epoch 54/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 55/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 56/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Epoch 57/100, Train Loss: 0.0351, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0344, Val Loss: 0.0274\n",
            "Epoch 59/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Epoch 60/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 61/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 62/100, Train Loss: 0.0335, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Epoch 64/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Epoch 65/100, Train Loss: 0.0339, Val Loss: 0.0274\n",
            "Epoch 66/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 67/100, Train Loss: 0.0343, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 69/100, Train Loss: 0.0352, Val Loss: 0.0274\n",
            "Epoch 70/100, Train Loss: 0.0345, Val Loss: 0.0274\n",
            "Epoch 71/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Epoch 72/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 74/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 75/100, Train Loss: 0.0368, Val Loss: 0.0274\n",
            "Epoch 76/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 77/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0472, Val Loss: 0.0274\n",
            "Epoch 79/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 80/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 81/100, Train Loss: 0.0363, Val Loss: 0.0274\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0395, Val Loss: 0.0274\n",
            "Epoch 84/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 85/100, Train Loss: 0.0341, Val Loss: 0.0274\n",
            "Epoch 86/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Epoch 87/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 89/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 90/100, Train Loss: 0.0343, Val Loss: 0.0274\n",
            "Epoch 91/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 92/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0359, Val Loss: 0.0274\n",
            "Epoch 94/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 95/100, Train Loss: 0.0379, Val Loss: 0.0274\n",
            "Epoch 96/100, Train Loss: 0.0373, Val Loss: 0.0274\n",
            "Epoch 97/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0362, Val Loss: 0.0274\n",
            "Epoch 99/100, Train Loss: 0.0353, Val Loss: 0.0274\n",
            "Epoch 100/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "\n",
            "Test iteration for date: 2019-06-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1381, Val Loss: 0.2823\n",
            "Epoch 2/100, Train Loss: 0.1198, Val Loss: 0.3823\n",
            "Epoch 3/100, Train Loss: 0.1231, Val Loss: 0.3381\n",
            "Epoch 4/100, Train Loss: 0.1044, Val Loss: 0.3376\n",
            "Epoch 5/100, Train Loss: 0.1142, Val Loss: 0.2691\n",
            "Epoch 6/100, Train Loss: 0.0803, Val Loss: 0.0677\n",
            "Epoch 7/100, Train Loss: 0.0691, Val Loss: 0.0604\n",
            "Epoch 8/100, Train Loss: 0.0620, Val Loss: 0.1124\n",
            "Epoch 9/100, Train Loss: 0.0601, Val Loss: 0.0728\n",
            "Epoch 10/100, Train Loss: 0.0580, Val Loss: 0.0516\n",
            "Epoch 11/100, Train Loss: 0.0545, Val Loss: 0.0442\n",
            "Epoch 12/100, Train Loss: 0.0556, Val Loss: 0.0514\n",
            "Epoch 13/100, Train Loss: 0.0593, Val Loss: 0.0620\n",
            "Epoch 14/100, Train Loss: 0.0581, Val Loss: 0.0502\n",
            "Epoch 15/100, Train Loss: 0.0533, Val Loss: 0.0485\n",
            "Epoch 16/100, Train Loss: 0.0455, Val Loss: 0.0374\n",
            "Epoch 17/100, Train Loss: 0.0469, Val Loss: 0.0266\n",
            "Epoch 18/100, Train Loss: 0.0494, Val Loss: 0.0331\n",
            "Epoch 19/100, Train Loss: 0.0485, Val Loss: 0.0768\n",
            "Epoch 20/100, Train Loss: 0.0445, Val Loss: 0.0851\n",
            "Epoch 21/100, Train Loss: 0.0428, Val Loss: 0.0285\n",
            "Epoch 22/100, Train Loss: 0.0417, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0359, Val Loss: 0.0321\n",
            "Epoch 24/100, Train Loss: 0.0359, Val Loss: 0.0306\n",
            "Epoch 25/100, Train Loss: 0.0366, Val Loss: 0.0288\n",
            "Epoch 26/100, Train Loss: 0.0348, Val Loss: 0.0284\n",
            "Epoch 27/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0433, Val Loss: 0.0274\n",
            "Epoch 29/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 30/100, Train Loss: 0.0340, Val Loss: 0.0274\n",
            "Epoch 31/100, Train Loss: 0.0359, Val Loss: 0.0274\n",
            "Epoch 32/100, Train Loss: 0.0344, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 34/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 35/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 36/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 37/100, Train Loss: 0.0361, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 39/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 40/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Epoch 41/100, Train Loss: 0.0363, Val Loss: 0.0274\n",
            "Epoch 42/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 44/100, Train Loss: 0.0365, Val Loss: 0.0274\n",
            "Epoch 45/100, Train Loss: 0.0368, Val Loss: 0.0274\n",
            "Epoch 46/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Epoch 47/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0364, Val Loss: 0.0274\n",
            "Epoch 49/100, Train Loss: 0.0351, Val Loss: 0.0274\n",
            "Epoch 50/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 51/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "Epoch 52/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "Epoch 54/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 55/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 56/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Epoch 57/100, Train Loss: 0.0351, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0344, Val Loss: 0.0274\n",
            "Epoch 59/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Epoch 60/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 61/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 62/100, Train Loss: 0.0335, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Epoch 64/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Epoch 65/100, Train Loss: 0.0339, Val Loss: 0.0274\n",
            "Epoch 66/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 67/100, Train Loss: 0.0343, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 69/100, Train Loss: 0.0352, Val Loss: 0.0274\n",
            "Epoch 70/100, Train Loss: 0.0345, Val Loss: 0.0274\n",
            "Epoch 71/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Epoch 72/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 74/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 75/100, Train Loss: 0.0368, Val Loss: 0.0274\n",
            "Epoch 76/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 77/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0472, Val Loss: 0.0274\n",
            "Epoch 79/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 80/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 81/100, Train Loss: 0.0363, Val Loss: 0.0274\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0395, Val Loss: 0.0274\n",
            "Epoch 84/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 85/100, Train Loss: 0.0341, Val Loss: 0.0274\n",
            "Epoch 86/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Epoch 87/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 89/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 90/100, Train Loss: 0.0343, Val Loss: 0.0274\n",
            "Epoch 91/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 92/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0359, Val Loss: 0.0274\n",
            "Epoch 94/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 95/100, Train Loss: 0.0379, Val Loss: 0.0274\n",
            "Epoch 96/100, Train Loss: 0.0373, Val Loss: 0.0274\n",
            "Epoch 97/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0362, Val Loss: 0.0274\n",
            "Epoch 99/100, Train Loss: 0.0353, Val Loss: 0.0274\n",
            "Epoch 100/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "\n",
            "Test iteration for date: 2019-07-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1381, Val Loss: 0.2823\n",
            "Epoch 2/100, Train Loss: 0.1198, Val Loss: 0.3823\n",
            "Epoch 3/100, Train Loss: 0.1231, Val Loss: 0.3381\n",
            "Epoch 4/100, Train Loss: 0.1044, Val Loss: 0.3376\n",
            "Epoch 5/100, Train Loss: 0.1142, Val Loss: 0.2691\n",
            "Epoch 6/100, Train Loss: 0.0803, Val Loss: 0.0677\n",
            "Epoch 7/100, Train Loss: 0.0691, Val Loss: 0.0604\n",
            "Epoch 8/100, Train Loss: 0.0620, Val Loss: 0.1124\n",
            "Epoch 9/100, Train Loss: 0.0601, Val Loss: 0.0728\n",
            "Epoch 10/100, Train Loss: 0.0580, Val Loss: 0.0516\n",
            "Epoch 11/100, Train Loss: 0.0545, Val Loss: 0.0442\n",
            "Epoch 12/100, Train Loss: 0.0556, Val Loss: 0.0514\n",
            "Epoch 13/100, Train Loss: 0.0593, Val Loss: 0.0620\n",
            "Epoch 14/100, Train Loss: 0.0581, Val Loss: 0.0502\n",
            "Epoch 15/100, Train Loss: 0.0533, Val Loss: 0.0485\n",
            "Epoch 16/100, Train Loss: 0.0455, Val Loss: 0.0374\n",
            "Epoch 17/100, Train Loss: 0.0469, Val Loss: 0.0266\n",
            "Epoch 18/100, Train Loss: 0.0494, Val Loss: 0.0331\n",
            "Epoch 19/100, Train Loss: 0.0485, Val Loss: 0.0768\n",
            "Epoch 20/100, Train Loss: 0.0445, Val Loss: 0.0851\n",
            "Epoch 21/100, Train Loss: 0.0428, Val Loss: 0.0285\n",
            "Epoch 22/100, Train Loss: 0.0417, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0359, Val Loss: 0.0321\n",
            "Epoch 24/100, Train Loss: 0.0359, Val Loss: 0.0306\n",
            "Epoch 25/100, Train Loss: 0.0366, Val Loss: 0.0288\n",
            "Epoch 26/100, Train Loss: 0.0348, Val Loss: 0.0284\n",
            "Epoch 27/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0433, Val Loss: 0.0274\n",
            "Epoch 29/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 30/100, Train Loss: 0.0340, Val Loss: 0.0274\n",
            "Epoch 31/100, Train Loss: 0.0359, Val Loss: 0.0274\n",
            "Epoch 32/100, Train Loss: 0.0344, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 34/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 35/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 36/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 37/100, Train Loss: 0.0361, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 39/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 40/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Epoch 41/100, Train Loss: 0.0363, Val Loss: 0.0274\n",
            "Epoch 42/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 44/100, Train Loss: 0.0365, Val Loss: 0.0274\n",
            "Epoch 45/100, Train Loss: 0.0368, Val Loss: 0.0274\n",
            "Epoch 46/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Epoch 47/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0364, Val Loss: 0.0274\n",
            "Epoch 49/100, Train Loss: 0.0351, Val Loss: 0.0274\n",
            "Epoch 50/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 51/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "Epoch 52/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "Epoch 54/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 55/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 56/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Epoch 57/100, Train Loss: 0.0351, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0344, Val Loss: 0.0274\n",
            "Epoch 59/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Epoch 60/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 61/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 62/100, Train Loss: 0.0335, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Epoch 64/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Epoch 65/100, Train Loss: 0.0339, Val Loss: 0.0274\n",
            "Epoch 66/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 67/100, Train Loss: 0.0343, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 69/100, Train Loss: 0.0352, Val Loss: 0.0274\n",
            "Epoch 70/100, Train Loss: 0.0345, Val Loss: 0.0274\n",
            "Epoch 71/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Epoch 72/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 74/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 75/100, Train Loss: 0.0368, Val Loss: 0.0274\n",
            "Epoch 76/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 77/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0472, Val Loss: 0.0274\n",
            "Epoch 79/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 80/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 81/100, Train Loss: 0.0363, Val Loss: 0.0274\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0395, Val Loss: 0.0274\n",
            "Epoch 84/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 85/100, Train Loss: 0.0341, Val Loss: 0.0274\n",
            "Epoch 86/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Epoch 87/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 89/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 90/100, Train Loss: 0.0343, Val Loss: 0.0274\n",
            "Epoch 91/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 92/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0359, Val Loss: 0.0274\n",
            "Epoch 94/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 95/100, Train Loss: 0.0379, Val Loss: 0.0274\n",
            "Epoch 96/100, Train Loss: 0.0373, Val Loss: 0.0274\n",
            "Epoch 97/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0362, Val Loss: 0.0274\n",
            "Epoch 99/100, Train Loss: 0.0353, Val Loss: 0.0274\n",
            "Epoch 100/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "\n",
            "Test iteration for date: 2019-08-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1381, Val Loss: 0.2823\n",
            "Epoch 2/100, Train Loss: 0.1198, Val Loss: 0.3823\n",
            "Epoch 3/100, Train Loss: 0.1231, Val Loss: 0.3381\n",
            "Epoch 4/100, Train Loss: 0.1044, Val Loss: 0.3376\n",
            "Epoch 5/100, Train Loss: 0.1142, Val Loss: 0.2691\n",
            "Epoch 6/100, Train Loss: 0.0803, Val Loss: 0.0677\n",
            "Epoch 7/100, Train Loss: 0.0691, Val Loss: 0.0604\n",
            "Epoch 8/100, Train Loss: 0.0620, Val Loss: 0.1124\n",
            "Epoch 9/100, Train Loss: 0.0601, Val Loss: 0.0728\n",
            "Epoch 10/100, Train Loss: 0.0580, Val Loss: 0.0516\n",
            "Epoch 11/100, Train Loss: 0.0545, Val Loss: 0.0442\n",
            "Epoch 12/100, Train Loss: 0.0556, Val Loss: 0.0514\n",
            "Epoch 13/100, Train Loss: 0.0593, Val Loss: 0.0620\n",
            "Epoch 14/100, Train Loss: 0.0581, Val Loss: 0.0502\n",
            "Epoch 15/100, Train Loss: 0.0533, Val Loss: 0.0485\n",
            "Epoch 16/100, Train Loss: 0.0455, Val Loss: 0.0374\n",
            "Epoch 17/100, Train Loss: 0.0469, Val Loss: 0.0266\n",
            "Epoch 18/100, Train Loss: 0.0494, Val Loss: 0.0331\n",
            "Epoch 19/100, Train Loss: 0.0485, Val Loss: 0.0768\n",
            "Epoch 20/100, Train Loss: 0.0445, Val Loss: 0.0851\n",
            "Epoch 21/100, Train Loss: 0.0428, Val Loss: 0.0285\n",
            "Epoch 22/100, Train Loss: 0.0417, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0359, Val Loss: 0.0321\n",
            "Epoch 24/100, Train Loss: 0.0359, Val Loss: 0.0306\n",
            "Epoch 25/100, Train Loss: 0.0366, Val Loss: 0.0288\n",
            "Epoch 26/100, Train Loss: 0.0348, Val Loss: 0.0284\n",
            "Epoch 27/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0433, Val Loss: 0.0274\n",
            "Epoch 29/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 30/100, Train Loss: 0.0340, Val Loss: 0.0274\n",
            "Epoch 31/100, Train Loss: 0.0359, Val Loss: 0.0274\n",
            "Epoch 32/100, Train Loss: 0.0344, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 34/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 35/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 36/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 37/100, Train Loss: 0.0361, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 39/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 40/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Epoch 41/100, Train Loss: 0.0363, Val Loss: 0.0274\n",
            "Epoch 42/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 44/100, Train Loss: 0.0365, Val Loss: 0.0274\n",
            "Epoch 45/100, Train Loss: 0.0368, Val Loss: 0.0274\n",
            "Epoch 46/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Epoch 47/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0364, Val Loss: 0.0274\n",
            "Epoch 49/100, Train Loss: 0.0351, Val Loss: 0.0274\n",
            "Epoch 50/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 51/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "Epoch 52/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "Epoch 54/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 55/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 56/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Epoch 57/100, Train Loss: 0.0351, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0344, Val Loss: 0.0274\n",
            "Epoch 59/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Epoch 60/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 61/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 62/100, Train Loss: 0.0335, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Epoch 64/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Epoch 65/100, Train Loss: 0.0339, Val Loss: 0.0274\n",
            "Epoch 66/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 67/100, Train Loss: 0.0343, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 69/100, Train Loss: 0.0352, Val Loss: 0.0274\n",
            "Epoch 70/100, Train Loss: 0.0345, Val Loss: 0.0274\n",
            "Epoch 71/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Epoch 72/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 74/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 75/100, Train Loss: 0.0368, Val Loss: 0.0274\n",
            "Epoch 76/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 77/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0472, Val Loss: 0.0274\n",
            "Epoch 79/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 80/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 81/100, Train Loss: 0.0363, Val Loss: 0.0274\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0395, Val Loss: 0.0274\n",
            "Epoch 84/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 85/100, Train Loss: 0.0341, Val Loss: 0.0274\n",
            "Epoch 86/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Epoch 87/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 89/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 90/100, Train Loss: 0.0343, Val Loss: 0.0274\n",
            "Epoch 91/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 92/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0359, Val Loss: 0.0274\n",
            "Epoch 94/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 95/100, Train Loss: 0.0379, Val Loss: 0.0274\n",
            "Epoch 96/100, Train Loss: 0.0373, Val Loss: 0.0274\n",
            "Epoch 97/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0362, Val Loss: 0.0274\n",
            "Epoch 99/100, Train Loss: 0.0353, Val Loss: 0.0274\n",
            "Epoch 100/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "\n",
            "Test iteration for date: 2019-09-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1381, Val Loss: 0.2823\n",
            "Epoch 2/100, Train Loss: 0.1198, Val Loss: 0.3823\n",
            "Epoch 3/100, Train Loss: 0.1231, Val Loss: 0.3381\n",
            "Epoch 4/100, Train Loss: 0.1044, Val Loss: 0.3376\n",
            "Epoch 5/100, Train Loss: 0.1142, Val Loss: 0.2691\n",
            "Epoch 6/100, Train Loss: 0.0803, Val Loss: 0.0677\n",
            "Epoch 7/100, Train Loss: 0.0691, Val Loss: 0.0604\n",
            "Epoch 8/100, Train Loss: 0.0620, Val Loss: 0.1124\n",
            "Epoch 9/100, Train Loss: 0.0601, Val Loss: 0.0728\n",
            "Epoch 10/100, Train Loss: 0.0580, Val Loss: 0.0516\n",
            "Epoch 11/100, Train Loss: 0.0545, Val Loss: 0.0442\n",
            "Epoch 12/100, Train Loss: 0.0556, Val Loss: 0.0514\n",
            "Epoch 13/100, Train Loss: 0.0593, Val Loss: 0.0620\n",
            "Epoch 14/100, Train Loss: 0.0581, Val Loss: 0.0502\n",
            "Epoch 15/100, Train Loss: 0.0533, Val Loss: 0.0485\n",
            "Epoch 16/100, Train Loss: 0.0455, Val Loss: 0.0374\n",
            "Epoch 17/100, Train Loss: 0.0469, Val Loss: 0.0266\n",
            "Epoch 18/100, Train Loss: 0.0494, Val Loss: 0.0331\n",
            "Epoch 19/100, Train Loss: 0.0485, Val Loss: 0.0768\n",
            "Epoch 20/100, Train Loss: 0.0445, Val Loss: 0.0851\n",
            "Epoch 21/100, Train Loss: 0.0428, Val Loss: 0.0285\n",
            "Epoch 22/100, Train Loss: 0.0417, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0359, Val Loss: 0.0321\n",
            "Epoch 24/100, Train Loss: 0.0359, Val Loss: 0.0306\n",
            "Epoch 25/100, Train Loss: 0.0366, Val Loss: 0.0288\n",
            "Epoch 26/100, Train Loss: 0.0348, Val Loss: 0.0284\n",
            "Epoch 27/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0433, Val Loss: 0.0274\n",
            "Epoch 29/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 30/100, Train Loss: 0.0340, Val Loss: 0.0274\n",
            "Epoch 31/100, Train Loss: 0.0359, Val Loss: 0.0274\n",
            "Epoch 32/100, Train Loss: 0.0344, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 34/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 35/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 36/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 37/100, Train Loss: 0.0361, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 39/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 40/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Epoch 41/100, Train Loss: 0.0363, Val Loss: 0.0274\n",
            "Epoch 42/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 44/100, Train Loss: 0.0365, Val Loss: 0.0274\n",
            "Epoch 45/100, Train Loss: 0.0368, Val Loss: 0.0274\n",
            "Epoch 46/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Epoch 47/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0364, Val Loss: 0.0274\n",
            "Epoch 49/100, Train Loss: 0.0351, Val Loss: 0.0274\n",
            "Epoch 50/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 51/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "Epoch 52/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "Epoch 54/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 55/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 56/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Epoch 57/100, Train Loss: 0.0351, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0344, Val Loss: 0.0274\n",
            "Epoch 59/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Epoch 60/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 61/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 62/100, Train Loss: 0.0335, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Epoch 64/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Epoch 65/100, Train Loss: 0.0339, Val Loss: 0.0274\n",
            "Epoch 66/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 67/100, Train Loss: 0.0343, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 69/100, Train Loss: 0.0352, Val Loss: 0.0274\n",
            "Epoch 70/100, Train Loss: 0.0345, Val Loss: 0.0274\n",
            "Epoch 71/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Epoch 72/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 74/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 75/100, Train Loss: 0.0368, Val Loss: 0.0274\n",
            "Epoch 76/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 77/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0472, Val Loss: 0.0274\n",
            "Epoch 79/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 80/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 81/100, Train Loss: 0.0363, Val Loss: 0.0274\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0395, Val Loss: 0.0274\n",
            "Epoch 84/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 85/100, Train Loss: 0.0341, Val Loss: 0.0274\n",
            "Epoch 86/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Epoch 87/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 89/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 90/100, Train Loss: 0.0343, Val Loss: 0.0274\n",
            "Epoch 91/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 92/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0359, Val Loss: 0.0274\n",
            "Epoch 94/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 95/100, Train Loss: 0.0379, Val Loss: 0.0274\n",
            "Epoch 96/100, Train Loss: 0.0373, Val Loss: 0.0274\n",
            "Epoch 97/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0362, Val Loss: 0.0274\n",
            "Epoch 99/100, Train Loss: 0.0353, Val Loss: 0.0274\n",
            "Epoch 100/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "\n",
            "Test iteration for date: 2019-10-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1381, Val Loss: 0.2823\n",
            "Epoch 2/100, Train Loss: 0.1198, Val Loss: 0.3823\n",
            "Epoch 3/100, Train Loss: 0.1231, Val Loss: 0.3381\n",
            "Epoch 4/100, Train Loss: 0.1044, Val Loss: 0.3376\n",
            "Epoch 5/100, Train Loss: 0.1142, Val Loss: 0.2691\n",
            "Epoch 6/100, Train Loss: 0.0803, Val Loss: 0.0677\n",
            "Epoch 7/100, Train Loss: 0.0691, Val Loss: 0.0604\n",
            "Epoch 8/100, Train Loss: 0.0620, Val Loss: 0.1124\n",
            "Epoch 9/100, Train Loss: 0.0601, Val Loss: 0.0728\n",
            "Epoch 10/100, Train Loss: 0.0580, Val Loss: 0.0516\n",
            "Epoch 11/100, Train Loss: 0.0545, Val Loss: 0.0442\n",
            "Epoch 12/100, Train Loss: 0.0556, Val Loss: 0.0514\n",
            "Epoch 13/100, Train Loss: 0.0593, Val Loss: 0.0620\n",
            "Epoch 14/100, Train Loss: 0.0581, Val Loss: 0.0502\n",
            "Epoch 15/100, Train Loss: 0.0533, Val Loss: 0.0485\n",
            "Epoch 16/100, Train Loss: 0.0455, Val Loss: 0.0374\n",
            "Epoch 17/100, Train Loss: 0.0469, Val Loss: 0.0266\n",
            "Epoch 18/100, Train Loss: 0.0494, Val Loss: 0.0331\n",
            "Epoch 19/100, Train Loss: 0.0485, Val Loss: 0.0768\n",
            "Epoch 20/100, Train Loss: 0.0445, Val Loss: 0.0851\n",
            "Epoch 21/100, Train Loss: 0.0428, Val Loss: 0.0285\n",
            "Epoch 22/100, Train Loss: 0.0417, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0359, Val Loss: 0.0321\n",
            "Epoch 24/100, Train Loss: 0.0359, Val Loss: 0.0306\n",
            "Epoch 25/100, Train Loss: 0.0366, Val Loss: 0.0288\n",
            "Epoch 26/100, Train Loss: 0.0348, Val Loss: 0.0284\n",
            "Epoch 27/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0433, Val Loss: 0.0274\n",
            "Epoch 29/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 30/100, Train Loss: 0.0340, Val Loss: 0.0274\n",
            "Epoch 31/100, Train Loss: 0.0359, Val Loss: 0.0274\n",
            "Epoch 32/100, Train Loss: 0.0344, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 34/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 35/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 36/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 37/100, Train Loss: 0.0361, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 39/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 40/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Epoch 41/100, Train Loss: 0.0363, Val Loss: 0.0274\n",
            "Epoch 42/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 44/100, Train Loss: 0.0365, Val Loss: 0.0274\n",
            "Epoch 45/100, Train Loss: 0.0368, Val Loss: 0.0274\n",
            "Epoch 46/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Epoch 47/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0364, Val Loss: 0.0274\n",
            "Epoch 49/100, Train Loss: 0.0351, Val Loss: 0.0274\n",
            "Epoch 50/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 51/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "Epoch 52/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "Epoch 54/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 55/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 56/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Epoch 57/100, Train Loss: 0.0351, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0344, Val Loss: 0.0274\n",
            "Epoch 59/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Epoch 60/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 61/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 62/100, Train Loss: 0.0335, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Epoch 64/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Epoch 65/100, Train Loss: 0.0339, Val Loss: 0.0274\n",
            "Epoch 66/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 67/100, Train Loss: 0.0343, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 69/100, Train Loss: 0.0352, Val Loss: 0.0274\n",
            "Epoch 70/100, Train Loss: 0.0345, Val Loss: 0.0274\n",
            "Epoch 71/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Epoch 72/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 74/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 75/100, Train Loss: 0.0368, Val Loss: 0.0274\n",
            "Epoch 76/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 77/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0472, Val Loss: 0.0274\n",
            "Epoch 79/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 80/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 81/100, Train Loss: 0.0363, Val Loss: 0.0274\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0395, Val Loss: 0.0274\n",
            "Epoch 84/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 85/100, Train Loss: 0.0341, Val Loss: 0.0274\n",
            "Epoch 86/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Epoch 87/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 89/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 90/100, Train Loss: 0.0343, Val Loss: 0.0274\n",
            "Epoch 91/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 92/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0359, Val Loss: 0.0274\n",
            "Epoch 94/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 95/100, Train Loss: 0.0379, Val Loss: 0.0274\n",
            "Epoch 96/100, Train Loss: 0.0373, Val Loss: 0.0274\n",
            "Epoch 97/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0362, Val Loss: 0.0274\n",
            "Epoch 99/100, Train Loss: 0.0353, Val Loss: 0.0274\n",
            "Epoch 100/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "\n",
            "Test iteration for date: 2019-11-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1381, Val Loss: 0.2823\n",
            "Epoch 2/100, Train Loss: 0.1198, Val Loss: 0.3823\n",
            "Epoch 3/100, Train Loss: 0.1231, Val Loss: 0.3381\n",
            "Epoch 4/100, Train Loss: 0.1044, Val Loss: 0.3376\n",
            "Epoch 5/100, Train Loss: 0.1142, Val Loss: 0.2691\n",
            "Epoch 6/100, Train Loss: 0.0803, Val Loss: 0.0677\n",
            "Epoch 7/100, Train Loss: 0.0691, Val Loss: 0.0604\n",
            "Epoch 8/100, Train Loss: 0.0620, Val Loss: 0.1124\n",
            "Epoch 9/100, Train Loss: 0.0601, Val Loss: 0.0728\n",
            "Epoch 10/100, Train Loss: 0.0580, Val Loss: 0.0516\n",
            "Epoch 11/100, Train Loss: 0.0545, Val Loss: 0.0442\n",
            "Epoch 12/100, Train Loss: 0.0556, Val Loss: 0.0514\n",
            "Epoch 13/100, Train Loss: 0.0593, Val Loss: 0.0620\n",
            "Epoch 14/100, Train Loss: 0.0581, Val Loss: 0.0502\n",
            "Epoch 15/100, Train Loss: 0.0533, Val Loss: 0.0485\n",
            "Epoch 16/100, Train Loss: 0.0455, Val Loss: 0.0374\n",
            "Epoch 17/100, Train Loss: 0.0469, Val Loss: 0.0266\n",
            "Epoch 18/100, Train Loss: 0.0494, Val Loss: 0.0331\n",
            "Epoch 19/100, Train Loss: 0.0485, Val Loss: 0.0768\n",
            "Epoch 20/100, Train Loss: 0.0445, Val Loss: 0.0851\n",
            "Epoch 21/100, Train Loss: 0.0428, Val Loss: 0.0285\n",
            "Epoch 22/100, Train Loss: 0.0417, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0359, Val Loss: 0.0321\n",
            "Epoch 24/100, Train Loss: 0.0359, Val Loss: 0.0306\n",
            "Epoch 25/100, Train Loss: 0.0366, Val Loss: 0.0288\n",
            "Epoch 26/100, Train Loss: 0.0348, Val Loss: 0.0284\n",
            "Epoch 27/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0433, Val Loss: 0.0274\n",
            "Epoch 29/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 30/100, Train Loss: 0.0340, Val Loss: 0.0274\n",
            "Epoch 31/100, Train Loss: 0.0359, Val Loss: 0.0274\n",
            "Epoch 32/100, Train Loss: 0.0344, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 34/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 35/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 36/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 37/100, Train Loss: 0.0361, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 39/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 40/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Epoch 41/100, Train Loss: 0.0363, Val Loss: 0.0274\n",
            "Epoch 42/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 44/100, Train Loss: 0.0365, Val Loss: 0.0274\n",
            "Epoch 45/100, Train Loss: 0.0368, Val Loss: 0.0274\n",
            "Epoch 46/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Epoch 47/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0364, Val Loss: 0.0274\n",
            "Epoch 49/100, Train Loss: 0.0351, Val Loss: 0.0274\n",
            "Epoch 50/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 51/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "Epoch 52/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "Epoch 54/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 55/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 56/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Epoch 57/100, Train Loss: 0.0351, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0344, Val Loss: 0.0274\n",
            "Epoch 59/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Epoch 60/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 61/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 62/100, Train Loss: 0.0335, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Epoch 64/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Epoch 65/100, Train Loss: 0.0339, Val Loss: 0.0274\n",
            "Epoch 66/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 67/100, Train Loss: 0.0343, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 69/100, Train Loss: 0.0352, Val Loss: 0.0274\n",
            "Epoch 70/100, Train Loss: 0.0345, Val Loss: 0.0274\n",
            "Epoch 71/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Epoch 72/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 74/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 75/100, Train Loss: 0.0368, Val Loss: 0.0274\n",
            "Epoch 76/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 77/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0472, Val Loss: 0.0274\n",
            "Epoch 79/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 80/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 81/100, Train Loss: 0.0363, Val Loss: 0.0274\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0395, Val Loss: 0.0274\n",
            "Epoch 84/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 85/100, Train Loss: 0.0341, Val Loss: 0.0274\n",
            "Epoch 86/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Epoch 87/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 89/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 90/100, Train Loss: 0.0343, Val Loss: 0.0274\n",
            "Epoch 91/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 92/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0359, Val Loss: 0.0274\n",
            "Epoch 94/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 95/100, Train Loss: 0.0379, Val Loss: 0.0274\n",
            "Epoch 96/100, Train Loss: 0.0373, Val Loss: 0.0274\n",
            "Epoch 97/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0362, Val Loss: 0.0274\n",
            "Epoch 99/100, Train Loss: 0.0353, Val Loss: 0.0274\n",
            "Epoch 100/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "\n",
            "Test iteration for date: 2019-12-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1381, Val Loss: 0.2823\n",
            "Epoch 2/100, Train Loss: 0.1198, Val Loss: 0.3823\n",
            "Epoch 3/100, Train Loss: 0.1231, Val Loss: 0.3381\n",
            "Epoch 4/100, Train Loss: 0.1044, Val Loss: 0.3376\n",
            "Epoch 5/100, Train Loss: 0.1142, Val Loss: 0.2691\n",
            "Epoch 6/100, Train Loss: 0.0803, Val Loss: 0.0677\n",
            "Epoch 7/100, Train Loss: 0.0691, Val Loss: 0.0604\n",
            "Epoch 8/100, Train Loss: 0.0620, Val Loss: 0.1124\n",
            "Epoch 9/100, Train Loss: 0.0601, Val Loss: 0.0728\n",
            "Epoch 10/100, Train Loss: 0.0580, Val Loss: 0.0516\n",
            "Epoch 11/100, Train Loss: 0.0545, Val Loss: 0.0442\n",
            "Epoch 12/100, Train Loss: 0.0556, Val Loss: 0.0514\n",
            "Epoch 13/100, Train Loss: 0.0593, Val Loss: 0.0620\n",
            "Epoch 14/100, Train Loss: 0.0581, Val Loss: 0.0502\n",
            "Epoch 15/100, Train Loss: 0.0533, Val Loss: 0.0485\n",
            "Epoch 16/100, Train Loss: 0.0455, Val Loss: 0.0374\n",
            "Epoch 17/100, Train Loss: 0.0469, Val Loss: 0.0266\n",
            "Epoch 18/100, Train Loss: 0.0494, Val Loss: 0.0331\n",
            "Epoch 19/100, Train Loss: 0.0485, Val Loss: 0.0768\n",
            "Epoch 20/100, Train Loss: 0.0445, Val Loss: 0.0851\n",
            "Epoch 21/100, Train Loss: 0.0428, Val Loss: 0.0285\n",
            "Epoch 22/100, Train Loss: 0.0417, Val Loss: 0.0330\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0359, Val Loss: 0.0321\n",
            "Epoch 24/100, Train Loss: 0.0359, Val Loss: 0.0306\n",
            "Epoch 25/100, Train Loss: 0.0366, Val Loss: 0.0288\n",
            "Epoch 26/100, Train Loss: 0.0348, Val Loss: 0.0284\n",
            "Epoch 27/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0433, Val Loss: 0.0274\n",
            "Epoch 29/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 30/100, Train Loss: 0.0340, Val Loss: 0.0274\n",
            "Epoch 31/100, Train Loss: 0.0359, Val Loss: 0.0274\n",
            "Epoch 32/100, Train Loss: 0.0344, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 34/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 35/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 36/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 37/100, Train Loss: 0.0361, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 39/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 40/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Epoch 41/100, Train Loss: 0.0363, Val Loss: 0.0274\n",
            "Epoch 42/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 44/100, Train Loss: 0.0365, Val Loss: 0.0274\n",
            "Epoch 45/100, Train Loss: 0.0368, Val Loss: 0.0274\n",
            "Epoch 46/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Epoch 47/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0364, Val Loss: 0.0274\n",
            "Epoch 49/100, Train Loss: 0.0351, Val Loss: 0.0274\n",
            "Epoch 50/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 51/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "Epoch 52/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "Epoch 54/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 55/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 56/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Epoch 57/100, Train Loss: 0.0351, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0344, Val Loss: 0.0274\n",
            "Epoch 59/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Epoch 60/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 61/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 62/100, Train Loss: 0.0335, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Epoch 64/100, Train Loss: 0.0342, Val Loss: 0.0274\n",
            "Epoch 65/100, Train Loss: 0.0339, Val Loss: 0.0274\n",
            "Epoch 66/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 67/100, Train Loss: 0.0343, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 69/100, Train Loss: 0.0352, Val Loss: 0.0274\n",
            "Epoch 70/100, Train Loss: 0.0345, Val Loss: 0.0274\n",
            "Epoch 71/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Epoch 72/100, Train Loss: 0.0356, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 74/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 75/100, Train Loss: 0.0368, Val Loss: 0.0274\n",
            "Epoch 76/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 77/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0472, Val Loss: 0.0274\n",
            "Epoch 79/100, Train Loss: 0.0358, Val Loss: 0.0274\n",
            "Epoch 80/100, Train Loss: 0.0354, Val Loss: 0.0274\n",
            "Epoch 81/100, Train Loss: 0.0363, Val Loss: 0.0274\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0395, Val Loss: 0.0274\n",
            "Epoch 84/100, Train Loss: 0.0355, Val Loss: 0.0274\n",
            "Epoch 85/100, Train Loss: 0.0341, Val Loss: 0.0274\n",
            "Epoch 86/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Epoch 87/100, Train Loss: 0.0348, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Epoch 89/100, Train Loss: 0.0357, Val Loss: 0.0274\n",
            "Epoch 90/100, Train Loss: 0.0343, Val Loss: 0.0274\n",
            "Epoch 91/100, Train Loss: 0.0349, Val Loss: 0.0274\n",
            "Epoch 92/100, Train Loss: 0.0350, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0359, Val Loss: 0.0274\n",
            "Epoch 94/100, Train Loss: 0.0347, Val Loss: 0.0274\n",
            "Epoch 95/100, Train Loss: 0.0379, Val Loss: 0.0274\n",
            "Epoch 96/100, Train Loss: 0.0373, Val Loss: 0.0274\n",
            "Epoch 97/100, Train Loss: 0.0360, Val Loss: 0.0274\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0362, Val Loss: 0.0274\n",
            "Epoch 99/100, Train Loss: 0.0353, Val Loss: 0.0274\n",
            "Epoch 100/100, Train Loss: 0.0346, Val Loss: 0.0274\n",
            "\n",
            "Test iteration for date: 2020-06-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1478, Val Loss: 0.2809\n",
            "Epoch 2/100, Train Loss: 0.1290, Val Loss: 0.4113\n",
            "Epoch 3/100, Train Loss: 0.1321, Val Loss: 0.3780\n",
            "Epoch 4/100, Train Loss: 0.1187, Val Loss: 0.3679\n",
            "Epoch 5/100, Train Loss: 0.1295, Val Loss: 0.2672\n",
            "Epoch 6/100, Train Loss: 0.0885, Val Loss: 0.0607\n",
            "Epoch 7/100, Train Loss: 0.0704, Val Loss: 0.0608\n",
            "Epoch 8/100, Train Loss: 0.0627, Val Loss: 0.0756\n",
            "Epoch 9/100, Train Loss: 0.0605, Val Loss: 0.0544\n",
            "Epoch 10/100, Train Loss: 0.0576, Val Loss: 0.0670\n",
            "Epoch 11/100, Train Loss: 0.0566, Val Loss: 0.0638\n",
            "Epoch 12/100, Train Loss: 0.0550, Val Loss: 0.0508\n",
            "Epoch 13/100, Train Loss: 0.0583, Val Loss: 0.0431\n",
            "Epoch 14/100, Train Loss: 0.0548, Val Loss: 0.0649\n",
            "Epoch 15/100, Train Loss: 0.0541, Val Loss: 0.0458\n",
            "Epoch 16/100, Train Loss: 0.0469, Val Loss: 0.0437\n",
            "Epoch 17/100, Train Loss: 0.0532, Val Loss: 0.0635\n",
            "Epoch 18/100, Train Loss: 0.0523, Val Loss: 0.0542\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 19/100, Train Loss: 0.0400, Val Loss: 0.0524\n",
            "Epoch 20/100, Train Loss: 0.0402, Val Loss: 0.0511\n",
            "Epoch 21/100, Train Loss: 0.0396, Val Loss: 0.0502\n",
            "Epoch 22/100, Train Loss: 0.0397, Val Loss: 0.0499\n",
            "Epoch 23/100, Train Loss: 0.0388, Val Loss: 0.0489\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 24/100, Train Loss: 0.0394, Val Loss: 0.0489\n",
            "Epoch 25/100, Train Loss: 0.0395, Val Loss: 0.0489\n",
            "Epoch 26/100, Train Loss: 0.0390, Val Loss: 0.0489\n",
            "Epoch 27/100, Train Loss: 0.0389, Val Loss: 0.0489\n",
            "Epoch 28/100, Train Loss: 0.0500, Val Loss: 0.0489\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 29/100, Train Loss: 0.0387, Val Loss: 0.0489\n",
            "Epoch 30/100, Train Loss: 0.0385, Val Loss: 0.0489\n",
            "Epoch 31/100, Train Loss: 0.0398, Val Loss: 0.0489\n",
            "Epoch 32/100, Train Loss: 0.0386, Val Loss: 0.0489\n",
            "Epoch 33/100, Train Loss: 0.0390, Val Loss: 0.0489\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 34/100, Train Loss: 0.0393, Val Loss: 0.0489\n",
            "Epoch 35/100, Train Loss: 0.0390, Val Loss: 0.0489\n",
            "Epoch 36/100, Train Loss: 0.0389, Val Loss: 0.0489\n",
            "Epoch 37/100, Train Loss: 0.0401, Val Loss: 0.0489\n",
            "Epoch 38/100, Train Loss: 0.0388, Val Loss: 0.0489\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 39/100, Train Loss: 0.0389, Val Loss: 0.0489\n",
            "Epoch 40/100, Train Loss: 0.0392, Val Loss: 0.0489\n",
            "Epoch 41/100, Train Loss: 0.0395, Val Loss: 0.0489\n",
            "Epoch 42/100, Train Loss: 0.0392, Val Loss: 0.0489\n",
            "Epoch 43/100, Train Loss: 0.0388, Val Loss: 0.0489\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 44/100, Train Loss: 0.0405, Val Loss: 0.0489\n",
            "Epoch 45/100, Train Loss: 0.0407, Val Loss: 0.0489\n",
            "Epoch 46/100, Train Loss: 0.0390, Val Loss: 0.0489\n",
            "Epoch 47/100, Train Loss: 0.0384, Val Loss: 0.0489\n",
            "Epoch 48/100, Train Loss: 0.0401, Val Loss: 0.0489\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 49/100, Train Loss: 0.0394, Val Loss: 0.0489\n",
            "Epoch 50/100, Train Loss: 0.0397, Val Loss: 0.0489\n",
            "Epoch 51/100, Train Loss: 0.0386, Val Loss: 0.0489\n",
            "Epoch 52/100, Train Loss: 0.0388, Val Loss: 0.0489\n",
            "Epoch 53/100, Train Loss: 0.0393, Val Loss: 0.0489\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 54/100, Train Loss: 0.0386, Val Loss: 0.0489\n",
            "Epoch 55/100, Train Loss: 0.0387, Val Loss: 0.0489\n",
            "Epoch 56/100, Train Loss: 0.0402, Val Loss: 0.0489\n",
            "Epoch 57/100, Train Loss: 0.0392, Val Loss: 0.0489\n",
            "Epoch 58/100, Train Loss: 0.0385, Val Loss: 0.0489\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 59/100, Train Loss: 0.0386, Val Loss: 0.0489\n",
            "Epoch 60/100, Train Loss: 0.0401, Val Loss: 0.0489\n",
            "Epoch 61/100, Train Loss: 0.0395, Val Loss: 0.0489\n",
            "Epoch 62/100, Train Loss: 0.0381, Val Loss: 0.0489\n",
            "Epoch 63/100, Train Loss: 0.0381, Val Loss: 0.0489\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 64/100, Train Loss: 0.0387, Val Loss: 0.0489\n",
            "Epoch 65/100, Train Loss: 0.0376, Val Loss: 0.0489\n",
            "Epoch 66/100, Train Loss: 0.0402, Val Loss: 0.0489\n",
            "Epoch 67/100, Train Loss: 0.0391, Val Loss: 0.0489\n",
            "Epoch 68/100, Train Loss: 0.0400, Val Loss: 0.0489\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 69/100, Train Loss: 0.0395, Val Loss: 0.0489\n",
            "Epoch 70/100, Train Loss: 0.0382, Val Loss: 0.0489\n",
            "Epoch 71/100, Train Loss: 0.0393, Val Loss: 0.0489\n",
            "Epoch 72/100, Train Loss: 0.0412, Val Loss: 0.0489\n",
            "Epoch 73/100, Train Loss: 0.0396, Val Loss: 0.0489\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 74/100, Train Loss: 0.0386, Val Loss: 0.0489\n",
            "Epoch 75/100, Train Loss: 0.0403, Val Loss: 0.0489\n",
            "Epoch 76/100, Train Loss: 0.0405, Val Loss: 0.0489\n",
            "Epoch 77/100, Train Loss: 0.0389, Val Loss: 0.0489\n",
            "Epoch 78/100, Train Loss: 0.0519, Val Loss: 0.0489\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 79/100, Train Loss: 0.0398, Val Loss: 0.0489\n",
            "Epoch 80/100, Train Loss: 0.0384, Val Loss: 0.0489\n",
            "Epoch 81/100, Train Loss: 0.0404, Val Loss: 0.0489\n",
            "Epoch 82/100, Train Loss: 0.0400, Val Loss: 0.0489\n",
            "Epoch 83/100, Train Loss: 0.0439, Val Loss: 0.0489\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 84/100, Train Loss: 0.0398, Val Loss: 0.0489\n",
            "Epoch 85/100, Train Loss: 0.0391, Val Loss: 0.0489\n",
            "Epoch 86/100, Train Loss: 0.0393, Val Loss: 0.0489\n",
            "Epoch 87/100, Train Loss: 0.0389, Val Loss: 0.0489\n",
            "Epoch 88/100, Train Loss: 0.0394, Val Loss: 0.0489\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 89/100, Train Loss: 0.0395, Val Loss: 0.0489\n",
            "Epoch 90/100, Train Loss: 0.0387, Val Loss: 0.0489\n",
            "Epoch 91/100, Train Loss: 0.0390, Val Loss: 0.0489\n",
            "Epoch 92/100, Train Loss: 0.0385, Val Loss: 0.0489\n",
            "Epoch 93/100, Train Loss: 0.0406, Val Loss: 0.0489\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 94/100, Train Loss: 0.0383, Val Loss: 0.0489\n",
            "Epoch 95/100, Train Loss: 0.0420, Val Loss: 0.0489\n",
            "Epoch 96/100, Train Loss: 0.0410, Val Loss: 0.0489\n",
            "Epoch 97/100, Train Loss: 0.0401, Val Loss: 0.0489\n",
            "Epoch 98/100, Train Loss: 0.0398, Val Loss: 0.0489\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 99/100, Train Loss: 0.0388, Val Loss: 0.0489\n",
            "Epoch 100/100, Train Loss: 0.0384, Val Loss: 0.0489\n",
            "\n",
            "Test iteration for date: 2020-07-01 00:00:00\n",
            "Current training set size: 126 samples\n",
            "Epoch 1/100, Train Loss: 0.1582, Val Loss: 0.3890\n",
            "Epoch 2/100, Train Loss: 0.1333, Val Loss: 0.2827\n",
            "Epoch 3/100, Train Loss: 0.1249, Val Loss: 0.3277\n",
            "Epoch 4/100, Train Loss: 0.1182, Val Loss: 0.2474\n",
            "Epoch 5/100, Train Loss: 0.0959, Val Loss: 0.2077\n",
            "Epoch 6/100, Train Loss: 0.0823, Val Loss: 0.0653\n",
            "Epoch 7/100, Train Loss: 0.0690, Val Loss: 0.0789\n",
            "Epoch 8/100, Train Loss: 0.0667, Val Loss: 0.1549\n",
            "Epoch 9/100, Train Loss: 0.0647, Val Loss: 0.0643\n",
            "Epoch 10/100, Train Loss: 0.0577, Val Loss: 0.1001\n",
            "Epoch 11/100, Train Loss: 0.0578, Val Loss: 0.0519\n",
            "Epoch 12/100, Train Loss: 0.0522, Val Loss: 0.0408\n",
            "Epoch 13/100, Train Loss: 0.0580, Val Loss: 0.0482\n",
            "Epoch 14/100, Train Loss: 0.0581, Val Loss: 0.0462\n",
            "Epoch 15/100, Train Loss: 0.0492, Val Loss: 0.0450\n",
            "Epoch 16/100, Train Loss: 0.0436, Val Loss: 0.0395\n",
            "Epoch 17/100, Train Loss: 0.0417, Val Loss: 0.0372\n",
            "Epoch 18/100, Train Loss: 0.0447, Val Loss: 0.0357\n",
            "Epoch 19/100, Train Loss: 0.0420, Val Loss: 0.0655\n",
            "Epoch 20/100, Train Loss: 0.0436, Val Loss: 0.0586\n",
            "Epoch 21/100, Train Loss: 0.0411, Val Loss: 0.0472\n",
            "Epoch 22/100, Train Loss: 0.0409, Val Loss: 0.0423\n",
            "Epoch 23/100, Train Loss: 0.0444, Val Loss: 0.0351\n",
            "Epoch 24/100, Train Loss: 0.0413, Val Loss: 0.0360\n",
            "Epoch 25/100, Train Loss: 0.0366, Val Loss: 0.0445\n",
            "Epoch 26/100, Train Loss: 0.0409, Val Loss: 0.0318\n",
            "Epoch 27/100, Train Loss: 0.0396, Val Loss: 0.0362\n",
            "Epoch 28/100, Train Loss: 0.0402, Val Loss: 0.0572\n",
            "Epoch 29/100, Train Loss: 0.0436, Val Loss: 0.0493\n",
            "Epoch 30/100, Train Loss: 0.0414, Val Loss: 0.0370\n",
            "Epoch 31/100, Train Loss: 0.0375, Val Loss: 0.0429\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0364, Val Loss: 0.0400\n",
            "Epoch 33/100, Train Loss: 0.0347, Val Loss: 0.0351\n",
            "Epoch 34/100, Train Loss: 0.0351, Val Loss: 0.0310\n",
            "Epoch 35/100, Train Loss: 0.0334, Val Loss: 0.0274\n",
            "Epoch 36/100, Train Loss: 0.0336, Val Loss: 0.0270\n",
            "Epoch 37/100, Train Loss: 0.0335, Val Loss: 0.0269\n",
            "Epoch 38/100, Train Loss: 0.0341, Val Loss: 0.0270\n",
            "Epoch 39/100, Train Loss: 0.0344, Val Loss: 0.0272\n",
            "Epoch 40/100, Train Loss: 0.0332, Val Loss: 0.0274\n",
            "Epoch 41/100, Train Loss: 0.0318, Val Loss: 0.0275\n",
            "Epoch 42/100, Train Loss: 0.0329, Val Loss: 0.0275\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0321, Val Loss: 0.0275\n",
            "Epoch 44/100, Train Loss: 0.0325, Val Loss: 0.0275\n",
            "Epoch 45/100, Train Loss: 0.0327, Val Loss: 0.0275\n",
            "Epoch 46/100, Train Loss: 0.0328, Val Loss: 0.0275\n",
            "Epoch 47/100, Train Loss: 0.0330, Val Loss: 0.0275\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0344, Val Loss: 0.0275\n",
            "Epoch 49/100, Train Loss: 0.0325, Val Loss: 0.0275\n",
            "Epoch 50/100, Train Loss: 0.0327, Val Loss: 0.0275\n",
            "Epoch 51/100, Train Loss: 0.0322, Val Loss: 0.0275\n",
            "Epoch 52/100, Train Loss: 0.0318, Val Loss: 0.0275\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0315, Val Loss: 0.0275\n",
            "Epoch 54/100, Train Loss: 0.0328, Val Loss: 0.0275\n",
            "Epoch 55/100, Train Loss: 0.0326, Val Loss: 0.0275\n",
            "Epoch 56/100, Train Loss: 0.0316, Val Loss: 0.0275\n",
            "Epoch 57/100, Train Loss: 0.0327, Val Loss: 0.0275\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0334, Val Loss: 0.0275\n",
            "Epoch 59/100, Train Loss: 0.0324, Val Loss: 0.0275\n",
            "Epoch 60/100, Train Loss: 0.0327, Val Loss: 0.0275\n",
            "Epoch 61/100, Train Loss: 0.0357, Val Loss: 0.0275\n",
            "Epoch 62/100, Train Loss: 0.0329, Val Loss: 0.0275\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0319, Val Loss: 0.0275\n",
            "Epoch 64/100, Train Loss: 0.0322, Val Loss: 0.0275\n",
            "Epoch 65/100, Train Loss: 0.0337, Val Loss: 0.0275\n",
            "Epoch 66/100, Train Loss: 0.0320, Val Loss: 0.0275\n",
            "Epoch 67/100, Train Loss: 0.0330, Val Loss: 0.0275\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0328, Val Loss: 0.0275\n",
            "Epoch 69/100, Train Loss: 0.0327, Val Loss: 0.0275\n",
            "Epoch 70/100, Train Loss: 0.0319, Val Loss: 0.0275\n",
            "Epoch 71/100, Train Loss: 0.0316, Val Loss: 0.0275\n",
            "Epoch 72/100, Train Loss: 0.0328, Val Loss: 0.0275\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0329, Val Loss: 0.0275\n",
            "Epoch 74/100, Train Loss: 0.0324, Val Loss: 0.0275\n",
            "Epoch 75/100, Train Loss: 0.0312, Val Loss: 0.0275\n",
            "Epoch 76/100, Train Loss: 0.0322, Val Loss: 0.0275\n",
            "Epoch 77/100, Train Loss: 0.0335, Val Loss: 0.0275\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0314, Val Loss: 0.0275\n",
            "Epoch 79/100, Train Loss: 0.0319, Val Loss: 0.0275\n",
            "Epoch 80/100, Train Loss: 0.0323, Val Loss: 0.0275\n",
            "Epoch 81/100, Train Loss: 0.0331, Val Loss: 0.0275\n",
            "Epoch 82/100, Train Loss: 0.0324, Val Loss: 0.0275\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0321, Val Loss: 0.0275\n",
            "Epoch 84/100, Train Loss: 0.0338, Val Loss: 0.0275\n",
            "Epoch 85/100, Train Loss: 0.0319, Val Loss: 0.0275\n",
            "Epoch 86/100, Train Loss: 0.0315, Val Loss: 0.0275\n",
            "Epoch 87/100, Train Loss: 0.0326, Val Loss: 0.0275\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0326, Val Loss: 0.0275\n",
            "Epoch 89/100, Train Loss: 0.0328, Val Loss: 0.0275\n",
            "Epoch 90/100, Train Loss: 0.0322, Val Loss: 0.0275\n",
            "Epoch 91/100, Train Loss: 0.0318, Val Loss: 0.0275\n",
            "Epoch 92/100, Train Loss: 0.0325, Val Loss: 0.0275\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0318, Val Loss: 0.0275\n",
            "Epoch 94/100, Train Loss: 0.0316, Val Loss: 0.0275\n",
            "Epoch 95/100, Train Loss: 0.0328, Val Loss: 0.0275\n",
            "Epoch 96/100, Train Loss: 0.0315, Val Loss: 0.0275\n",
            "Epoch 97/100, Train Loss: 0.0324, Val Loss: 0.0275\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0329, Val Loss: 0.0275\n",
            "Epoch 99/100, Train Loss: 0.0323, Val Loss: 0.0275\n",
            "Epoch 100/100, Train Loss: 0.0326, Val Loss: 0.0275\n",
            "\n",
            "Total training time for hybrid_lstmUnit512_gruUnit16_drop0.1_dense64_batch4_epochs100: 195.41 seconds (3.26 minutes)\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: hybrid, Units: L512_G16, Dropout: 0.2, Dense Units: 32\n",
            "Batch Size: 4, Epochs: 100\n",
            "Device: cuda\n",
            "Total test samples: 14\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration for date: 2019-01-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1397, Val Loss: 0.2762\n",
            "Epoch 2/100, Train Loss: 0.1241, Val Loss: 0.3437\n",
            "Epoch 3/100, Train Loss: 0.1174, Val Loss: 0.3578\n",
            "Epoch 4/100, Train Loss: 0.1072, Val Loss: 0.3491\n",
            "Epoch 5/100, Train Loss: 0.1128, Val Loss: 0.2711\n",
            "Epoch 6/100, Train Loss: 0.0923, Val Loss: 0.1188\n",
            "Epoch 7/100, Train Loss: 0.0855, Val Loss: 0.1048\n",
            "Epoch 8/100, Train Loss: 0.0862, Val Loss: 0.1720\n",
            "Epoch 9/100, Train Loss: 0.0721, Val Loss: 0.0721\n",
            "Epoch 10/100, Train Loss: 0.0579, Val Loss: 0.0589\n",
            "Epoch 11/100, Train Loss: 0.0573, Val Loss: 0.0657\n",
            "Epoch 12/100, Train Loss: 0.0608, Val Loss: 0.0505\n",
            "Epoch 13/100, Train Loss: 0.0710, Val Loss: 0.0713\n",
            "Epoch 14/100, Train Loss: 0.0555, Val Loss: 0.0626\n",
            "Epoch 15/100, Train Loss: 0.0557, Val Loss: 0.0534\n",
            "Epoch 16/100, Train Loss: 0.0524, Val Loss: 0.0435\n",
            "Epoch 17/100, Train Loss: 0.0489, Val Loss: 0.0419\n",
            "Epoch 18/100, Train Loss: 0.0549, Val Loss: 0.0497\n",
            "Epoch 19/100, Train Loss: 0.0495, Val Loss: 0.0409\n",
            "Epoch 20/100, Train Loss: 0.0487, Val Loss: 0.0398\n",
            "Epoch 21/100, Train Loss: 0.0463, Val Loss: 0.0341\n",
            "Epoch 22/100, Train Loss: 0.0526, Val Loss: 0.0592\n",
            "Epoch 23/100, Train Loss: 0.0478, Val Loss: 0.0366\n",
            "Epoch 24/100, Train Loss: 0.0435, Val Loss: 0.0323\n",
            "Epoch 25/100, Train Loss: 0.0465, Val Loss: 0.0573\n",
            "Epoch 26/100, Train Loss: 0.0418, Val Loss: 0.0302\n",
            "Epoch 27/100, Train Loss: 0.0410, Val Loss: 0.0413\n",
            "Epoch 28/100, Train Loss: 0.0488, Val Loss: 0.0799\n",
            "Epoch 29/100, Train Loss: 0.0427, Val Loss: 0.0315\n",
            "Epoch 30/100, Train Loss: 0.0484, Val Loss: 0.0387\n",
            "Epoch 31/100, Train Loss: 0.0374, Val Loss: 0.0309\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0546, Val Loss: 0.0353\n",
            "Epoch 33/100, Train Loss: 0.0473, Val Loss: 0.0400\n",
            "Epoch 34/100, Train Loss: 0.0418, Val Loss: 0.0421\n",
            "Epoch 35/100, Train Loss: 0.0396, Val Loss: 0.0434\n",
            "Epoch 36/100, Train Loss: 0.0375, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0386, Val Loss: 0.0423\n",
            "Epoch 38/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "Epoch 39/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "Epoch 40/100, Train Loss: 0.0382, Val Loss: 0.0423\n",
            "Epoch 41/100, Train Loss: 0.0379, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Epoch 43/100, Train Loss: 0.0378, Val Loss: 0.0423\n",
            "Epoch 44/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 45/100, Train Loss: 0.0375, Val Loss: 0.0423\n",
            "Epoch 46/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 48/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 49/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 50/100, Train Loss: 0.0385, Val Loss: 0.0423\n",
            "Epoch 51/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 53/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 54/100, Train Loss: 0.0374, Val Loss: 0.0423\n",
            "Epoch 55/100, Train Loss: 0.0376, Val Loss: 0.0423\n",
            "Epoch 56/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0367, Val Loss: 0.0423\n",
            "Epoch 58/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Epoch 59/100, Train Loss: 0.0359, Val Loss: 0.0423\n",
            "Epoch 60/100, Train Loss: 0.0367, Val Loss: 0.0423\n",
            "Epoch 61/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 63/100, Train Loss: 0.0363, Val Loss: 0.0423\n",
            "Epoch 64/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Epoch 65/100, Train Loss: 0.0359, Val Loss: 0.0423\n",
            "Epoch 66/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 68/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 69/100, Train Loss: 0.0369, Val Loss: 0.0423\n",
            "Epoch 70/100, Train Loss: 0.0357, Val Loss: 0.0423\n",
            "Epoch 71/100, Train Loss: 0.0363, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 73/100, Train Loss: 0.0387, Val Loss: 0.0423\n",
            "Epoch 74/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 75/100, Train Loss: 0.0382, Val Loss: 0.0423\n",
            "Epoch 76/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 78/100, Train Loss: 0.0475, Val Loss: 0.0423\n",
            "Epoch 79/100, Train Loss: 0.0357, Val Loss: 0.0423\n",
            "Epoch 80/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Epoch 81/100, Train Loss: 0.0381, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 83/100, Train Loss: 0.0408, Val Loss: 0.0423\n",
            "Epoch 84/100, Train Loss: 0.0385, Val Loss: 0.0423\n",
            "Epoch 85/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 86/100, Train Loss: 0.0377, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 88/100, Train Loss: 0.0374, Val Loss: 0.0423\n",
            "Epoch 89/100, Train Loss: 0.0384, Val Loss: 0.0423\n",
            "Epoch 90/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 91/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0360, Val Loss: 0.0423\n",
            "Epoch 93/100, Train Loss: 0.0377, Val Loss: 0.0423\n",
            "Epoch 94/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Epoch 95/100, Train Loss: 0.0402, Val Loss: 0.0423\n",
            "Epoch 96/100, Train Loss: 0.0386, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 98/100, Train Loss: 0.0390, Val Loss: 0.0423\n",
            "Epoch 99/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Epoch 100/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "\n",
            "Test iteration for date: 2019-02-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1397, Val Loss: 0.2762\n",
            "Epoch 2/100, Train Loss: 0.1241, Val Loss: 0.3437\n",
            "Epoch 3/100, Train Loss: 0.1174, Val Loss: 0.3578\n",
            "Epoch 4/100, Train Loss: 0.1072, Val Loss: 0.3491\n",
            "Epoch 5/100, Train Loss: 0.1128, Val Loss: 0.2711\n",
            "Epoch 6/100, Train Loss: 0.0923, Val Loss: 0.1188\n",
            "Epoch 7/100, Train Loss: 0.0855, Val Loss: 0.1048\n",
            "Epoch 8/100, Train Loss: 0.0862, Val Loss: 0.1720\n",
            "Epoch 9/100, Train Loss: 0.0721, Val Loss: 0.0721\n",
            "Epoch 10/100, Train Loss: 0.0579, Val Loss: 0.0589\n",
            "Epoch 11/100, Train Loss: 0.0573, Val Loss: 0.0657\n",
            "Epoch 12/100, Train Loss: 0.0608, Val Loss: 0.0505\n",
            "Epoch 13/100, Train Loss: 0.0710, Val Loss: 0.0713\n",
            "Epoch 14/100, Train Loss: 0.0555, Val Loss: 0.0626\n",
            "Epoch 15/100, Train Loss: 0.0557, Val Loss: 0.0534\n",
            "Epoch 16/100, Train Loss: 0.0524, Val Loss: 0.0435\n",
            "Epoch 17/100, Train Loss: 0.0489, Val Loss: 0.0419\n",
            "Epoch 18/100, Train Loss: 0.0549, Val Loss: 0.0497\n",
            "Epoch 19/100, Train Loss: 0.0495, Val Loss: 0.0409\n",
            "Epoch 20/100, Train Loss: 0.0487, Val Loss: 0.0398\n",
            "Epoch 21/100, Train Loss: 0.0463, Val Loss: 0.0341\n",
            "Epoch 22/100, Train Loss: 0.0526, Val Loss: 0.0592\n",
            "Epoch 23/100, Train Loss: 0.0478, Val Loss: 0.0366\n",
            "Epoch 24/100, Train Loss: 0.0435, Val Loss: 0.0323\n",
            "Epoch 25/100, Train Loss: 0.0465, Val Loss: 0.0573\n",
            "Epoch 26/100, Train Loss: 0.0418, Val Loss: 0.0302\n",
            "Epoch 27/100, Train Loss: 0.0410, Val Loss: 0.0413\n",
            "Epoch 28/100, Train Loss: 0.0488, Val Loss: 0.0799\n",
            "Epoch 29/100, Train Loss: 0.0427, Val Loss: 0.0315\n",
            "Epoch 30/100, Train Loss: 0.0484, Val Loss: 0.0387\n",
            "Epoch 31/100, Train Loss: 0.0374, Val Loss: 0.0309\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0546, Val Loss: 0.0353\n",
            "Epoch 33/100, Train Loss: 0.0473, Val Loss: 0.0400\n",
            "Epoch 34/100, Train Loss: 0.0418, Val Loss: 0.0421\n",
            "Epoch 35/100, Train Loss: 0.0396, Val Loss: 0.0434\n",
            "Epoch 36/100, Train Loss: 0.0375, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0386, Val Loss: 0.0423\n",
            "Epoch 38/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "Epoch 39/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "Epoch 40/100, Train Loss: 0.0382, Val Loss: 0.0423\n",
            "Epoch 41/100, Train Loss: 0.0379, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Epoch 43/100, Train Loss: 0.0378, Val Loss: 0.0423\n",
            "Epoch 44/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 45/100, Train Loss: 0.0375, Val Loss: 0.0423\n",
            "Epoch 46/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 48/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 49/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 50/100, Train Loss: 0.0385, Val Loss: 0.0423\n",
            "Epoch 51/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 53/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 54/100, Train Loss: 0.0374, Val Loss: 0.0423\n",
            "Epoch 55/100, Train Loss: 0.0376, Val Loss: 0.0423\n",
            "Epoch 56/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0367, Val Loss: 0.0423\n",
            "Epoch 58/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Epoch 59/100, Train Loss: 0.0359, Val Loss: 0.0423\n",
            "Epoch 60/100, Train Loss: 0.0367, Val Loss: 0.0423\n",
            "Epoch 61/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 63/100, Train Loss: 0.0363, Val Loss: 0.0423\n",
            "Epoch 64/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Epoch 65/100, Train Loss: 0.0359, Val Loss: 0.0423\n",
            "Epoch 66/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 68/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 69/100, Train Loss: 0.0369, Val Loss: 0.0423\n",
            "Epoch 70/100, Train Loss: 0.0357, Val Loss: 0.0423\n",
            "Epoch 71/100, Train Loss: 0.0363, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 73/100, Train Loss: 0.0387, Val Loss: 0.0423\n",
            "Epoch 74/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 75/100, Train Loss: 0.0382, Val Loss: 0.0423\n",
            "Epoch 76/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 78/100, Train Loss: 0.0475, Val Loss: 0.0423\n",
            "Epoch 79/100, Train Loss: 0.0357, Val Loss: 0.0423\n",
            "Epoch 80/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Epoch 81/100, Train Loss: 0.0381, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 83/100, Train Loss: 0.0408, Val Loss: 0.0423\n",
            "Epoch 84/100, Train Loss: 0.0385, Val Loss: 0.0423\n",
            "Epoch 85/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 86/100, Train Loss: 0.0377, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 88/100, Train Loss: 0.0374, Val Loss: 0.0423\n",
            "Epoch 89/100, Train Loss: 0.0384, Val Loss: 0.0423\n",
            "Epoch 90/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 91/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0360, Val Loss: 0.0423\n",
            "Epoch 93/100, Train Loss: 0.0377, Val Loss: 0.0423\n",
            "Epoch 94/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Epoch 95/100, Train Loss: 0.0402, Val Loss: 0.0423\n",
            "Epoch 96/100, Train Loss: 0.0386, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 98/100, Train Loss: 0.0390, Val Loss: 0.0423\n",
            "Epoch 99/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Epoch 100/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "\n",
            "Test iteration for date: 2019-03-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1397, Val Loss: 0.2762\n",
            "Epoch 2/100, Train Loss: 0.1241, Val Loss: 0.3437\n",
            "Epoch 3/100, Train Loss: 0.1174, Val Loss: 0.3578\n",
            "Epoch 4/100, Train Loss: 0.1072, Val Loss: 0.3491\n",
            "Epoch 5/100, Train Loss: 0.1128, Val Loss: 0.2711\n",
            "Epoch 6/100, Train Loss: 0.0923, Val Loss: 0.1188\n",
            "Epoch 7/100, Train Loss: 0.0855, Val Loss: 0.1048\n",
            "Epoch 8/100, Train Loss: 0.0862, Val Loss: 0.1720\n",
            "Epoch 9/100, Train Loss: 0.0721, Val Loss: 0.0721\n",
            "Epoch 10/100, Train Loss: 0.0579, Val Loss: 0.0589\n",
            "Epoch 11/100, Train Loss: 0.0573, Val Loss: 0.0657\n",
            "Epoch 12/100, Train Loss: 0.0608, Val Loss: 0.0505\n",
            "Epoch 13/100, Train Loss: 0.0710, Val Loss: 0.0713\n",
            "Epoch 14/100, Train Loss: 0.0555, Val Loss: 0.0626\n",
            "Epoch 15/100, Train Loss: 0.0557, Val Loss: 0.0534\n",
            "Epoch 16/100, Train Loss: 0.0524, Val Loss: 0.0435\n",
            "Epoch 17/100, Train Loss: 0.0489, Val Loss: 0.0419\n",
            "Epoch 18/100, Train Loss: 0.0549, Val Loss: 0.0497\n",
            "Epoch 19/100, Train Loss: 0.0495, Val Loss: 0.0409\n",
            "Epoch 20/100, Train Loss: 0.0487, Val Loss: 0.0398\n",
            "Epoch 21/100, Train Loss: 0.0463, Val Loss: 0.0341\n",
            "Epoch 22/100, Train Loss: 0.0526, Val Loss: 0.0592\n",
            "Epoch 23/100, Train Loss: 0.0478, Val Loss: 0.0366\n",
            "Epoch 24/100, Train Loss: 0.0435, Val Loss: 0.0323\n",
            "Epoch 25/100, Train Loss: 0.0465, Val Loss: 0.0573\n",
            "Epoch 26/100, Train Loss: 0.0418, Val Loss: 0.0302\n",
            "Epoch 27/100, Train Loss: 0.0410, Val Loss: 0.0413\n",
            "Epoch 28/100, Train Loss: 0.0488, Val Loss: 0.0799\n",
            "Epoch 29/100, Train Loss: 0.0427, Val Loss: 0.0315\n",
            "Epoch 30/100, Train Loss: 0.0484, Val Loss: 0.0387\n",
            "Epoch 31/100, Train Loss: 0.0374, Val Loss: 0.0309\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0546, Val Loss: 0.0353\n",
            "Epoch 33/100, Train Loss: 0.0473, Val Loss: 0.0400\n",
            "Epoch 34/100, Train Loss: 0.0418, Val Loss: 0.0421\n",
            "Epoch 35/100, Train Loss: 0.0396, Val Loss: 0.0434\n",
            "Epoch 36/100, Train Loss: 0.0375, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0386, Val Loss: 0.0423\n",
            "Epoch 38/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "Epoch 39/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "Epoch 40/100, Train Loss: 0.0382, Val Loss: 0.0423\n",
            "Epoch 41/100, Train Loss: 0.0379, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Epoch 43/100, Train Loss: 0.0378, Val Loss: 0.0423\n",
            "Epoch 44/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 45/100, Train Loss: 0.0375, Val Loss: 0.0423\n",
            "Epoch 46/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 48/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 49/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 50/100, Train Loss: 0.0385, Val Loss: 0.0423\n",
            "Epoch 51/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 53/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 54/100, Train Loss: 0.0374, Val Loss: 0.0423\n",
            "Epoch 55/100, Train Loss: 0.0376, Val Loss: 0.0423\n",
            "Epoch 56/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0367, Val Loss: 0.0423\n",
            "Epoch 58/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Epoch 59/100, Train Loss: 0.0359, Val Loss: 0.0423\n",
            "Epoch 60/100, Train Loss: 0.0367, Val Loss: 0.0423\n",
            "Epoch 61/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 63/100, Train Loss: 0.0363, Val Loss: 0.0423\n",
            "Epoch 64/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Epoch 65/100, Train Loss: 0.0359, Val Loss: 0.0423\n",
            "Epoch 66/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 68/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 69/100, Train Loss: 0.0369, Val Loss: 0.0423\n",
            "Epoch 70/100, Train Loss: 0.0357, Val Loss: 0.0423\n",
            "Epoch 71/100, Train Loss: 0.0363, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 73/100, Train Loss: 0.0387, Val Loss: 0.0423\n",
            "Epoch 74/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 75/100, Train Loss: 0.0382, Val Loss: 0.0423\n",
            "Epoch 76/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 78/100, Train Loss: 0.0475, Val Loss: 0.0423\n",
            "Epoch 79/100, Train Loss: 0.0357, Val Loss: 0.0423\n",
            "Epoch 80/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Epoch 81/100, Train Loss: 0.0381, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 83/100, Train Loss: 0.0408, Val Loss: 0.0423\n",
            "Epoch 84/100, Train Loss: 0.0385, Val Loss: 0.0423\n",
            "Epoch 85/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 86/100, Train Loss: 0.0377, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 88/100, Train Loss: 0.0374, Val Loss: 0.0423\n",
            "Epoch 89/100, Train Loss: 0.0384, Val Loss: 0.0423\n",
            "Epoch 90/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 91/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0360, Val Loss: 0.0423\n",
            "Epoch 93/100, Train Loss: 0.0377, Val Loss: 0.0423\n",
            "Epoch 94/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Epoch 95/100, Train Loss: 0.0402, Val Loss: 0.0423\n",
            "Epoch 96/100, Train Loss: 0.0386, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 98/100, Train Loss: 0.0390, Val Loss: 0.0423\n",
            "Epoch 99/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Epoch 100/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "\n",
            "Test iteration for date: 2019-04-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1397, Val Loss: 0.2762\n",
            "Epoch 2/100, Train Loss: 0.1241, Val Loss: 0.3437\n",
            "Epoch 3/100, Train Loss: 0.1174, Val Loss: 0.3578\n",
            "Epoch 4/100, Train Loss: 0.1072, Val Loss: 0.3491\n",
            "Epoch 5/100, Train Loss: 0.1128, Val Loss: 0.2711\n",
            "Epoch 6/100, Train Loss: 0.0923, Val Loss: 0.1188\n",
            "Epoch 7/100, Train Loss: 0.0855, Val Loss: 0.1048\n",
            "Epoch 8/100, Train Loss: 0.0862, Val Loss: 0.1720\n",
            "Epoch 9/100, Train Loss: 0.0721, Val Loss: 0.0721\n",
            "Epoch 10/100, Train Loss: 0.0579, Val Loss: 0.0589\n",
            "Epoch 11/100, Train Loss: 0.0573, Val Loss: 0.0657\n",
            "Epoch 12/100, Train Loss: 0.0608, Val Loss: 0.0505\n",
            "Epoch 13/100, Train Loss: 0.0710, Val Loss: 0.0713\n",
            "Epoch 14/100, Train Loss: 0.0555, Val Loss: 0.0626\n",
            "Epoch 15/100, Train Loss: 0.0557, Val Loss: 0.0534\n",
            "Epoch 16/100, Train Loss: 0.0524, Val Loss: 0.0435\n",
            "Epoch 17/100, Train Loss: 0.0489, Val Loss: 0.0419\n",
            "Epoch 18/100, Train Loss: 0.0549, Val Loss: 0.0497\n",
            "Epoch 19/100, Train Loss: 0.0495, Val Loss: 0.0409\n",
            "Epoch 20/100, Train Loss: 0.0487, Val Loss: 0.0398\n",
            "Epoch 21/100, Train Loss: 0.0463, Val Loss: 0.0341\n",
            "Epoch 22/100, Train Loss: 0.0526, Val Loss: 0.0592\n",
            "Epoch 23/100, Train Loss: 0.0478, Val Loss: 0.0366\n",
            "Epoch 24/100, Train Loss: 0.0435, Val Loss: 0.0323\n",
            "Epoch 25/100, Train Loss: 0.0465, Val Loss: 0.0573\n",
            "Epoch 26/100, Train Loss: 0.0418, Val Loss: 0.0302\n",
            "Epoch 27/100, Train Loss: 0.0410, Val Loss: 0.0413\n",
            "Epoch 28/100, Train Loss: 0.0488, Val Loss: 0.0799\n",
            "Epoch 29/100, Train Loss: 0.0427, Val Loss: 0.0315\n",
            "Epoch 30/100, Train Loss: 0.0484, Val Loss: 0.0387\n",
            "Epoch 31/100, Train Loss: 0.0374, Val Loss: 0.0309\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0546, Val Loss: 0.0353\n",
            "Epoch 33/100, Train Loss: 0.0473, Val Loss: 0.0400\n",
            "Epoch 34/100, Train Loss: 0.0418, Val Loss: 0.0421\n",
            "Epoch 35/100, Train Loss: 0.0396, Val Loss: 0.0434\n",
            "Epoch 36/100, Train Loss: 0.0375, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0386, Val Loss: 0.0423\n",
            "Epoch 38/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "Epoch 39/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "Epoch 40/100, Train Loss: 0.0382, Val Loss: 0.0423\n",
            "Epoch 41/100, Train Loss: 0.0379, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Epoch 43/100, Train Loss: 0.0378, Val Loss: 0.0423\n",
            "Epoch 44/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 45/100, Train Loss: 0.0375, Val Loss: 0.0423\n",
            "Epoch 46/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 48/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 49/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 50/100, Train Loss: 0.0385, Val Loss: 0.0423\n",
            "Epoch 51/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 53/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 54/100, Train Loss: 0.0374, Val Loss: 0.0423\n",
            "Epoch 55/100, Train Loss: 0.0376, Val Loss: 0.0423\n",
            "Epoch 56/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0367, Val Loss: 0.0423\n",
            "Epoch 58/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Epoch 59/100, Train Loss: 0.0359, Val Loss: 0.0423\n",
            "Epoch 60/100, Train Loss: 0.0367, Val Loss: 0.0423\n",
            "Epoch 61/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 63/100, Train Loss: 0.0363, Val Loss: 0.0423\n",
            "Epoch 64/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Epoch 65/100, Train Loss: 0.0359, Val Loss: 0.0423\n",
            "Epoch 66/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 68/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 69/100, Train Loss: 0.0369, Val Loss: 0.0423\n",
            "Epoch 70/100, Train Loss: 0.0357, Val Loss: 0.0423\n",
            "Epoch 71/100, Train Loss: 0.0363, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 73/100, Train Loss: 0.0387, Val Loss: 0.0423\n",
            "Epoch 74/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 75/100, Train Loss: 0.0382, Val Loss: 0.0423\n",
            "Epoch 76/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 78/100, Train Loss: 0.0475, Val Loss: 0.0423\n",
            "Epoch 79/100, Train Loss: 0.0357, Val Loss: 0.0423\n",
            "Epoch 80/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Epoch 81/100, Train Loss: 0.0381, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 83/100, Train Loss: 0.0408, Val Loss: 0.0423\n",
            "Epoch 84/100, Train Loss: 0.0385, Val Loss: 0.0423\n",
            "Epoch 85/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 86/100, Train Loss: 0.0377, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 88/100, Train Loss: 0.0374, Val Loss: 0.0423\n",
            "Epoch 89/100, Train Loss: 0.0384, Val Loss: 0.0423\n",
            "Epoch 90/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 91/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0360, Val Loss: 0.0423\n",
            "Epoch 93/100, Train Loss: 0.0377, Val Loss: 0.0423\n",
            "Epoch 94/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Epoch 95/100, Train Loss: 0.0402, Val Loss: 0.0423\n",
            "Epoch 96/100, Train Loss: 0.0386, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 98/100, Train Loss: 0.0390, Val Loss: 0.0423\n",
            "Epoch 99/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Epoch 100/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "\n",
            "Test iteration for date: 2019-05-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1397, Val Loss: 0.2762\n",
            "Epoch 2/100, Train Loss: 0.1241, Val Loss: 0.3437\n",
            "Epoch 3/100, Train Loss: 0.1174, Val Loss: 0.3578\n",
            "Epoch 4/100, Train Loss: 0.1072, Val Loss: 0.3491\n",
            "Epoch 5/100, Train Loss: 0.1128, Val Loss: 0.2711\n",
            "Epoch 6/100, Train Loss: 0.0923, Val Loss: 0.1188\n",
            "Epoch 7/100, Train Loss: 0.0855, Val Loss: 0.1048\n",
            "Epoch 8/100, Train Loss: 0.0862, Val Loss: 0.1720\n",
            "Epoch 9/100, Train Loss: 0.0721, Val Loss: 0.0721\n",
            "Epoch 10/100, Train Loss: 0.0579, Val Loss: 0.0589\n",
            "Epoch 11/100, Train Loss: 0.0573, Val Loss: 0.0657\n",
            "Epoch 12/100, Train Loss: 0.0608, Val Loss: 0.0505\n",
            "Epoch 13/100, Train Loss: 0.0710, Val Loss: 0.0713\n",
            "Epoch 14/100, Train Loss: 0.0555, Val Loss: 0.0626\n",
            "Epoch 15/100, Train Loss: 0.0557, Val Loss: 0.0534\n",
            "Epoch 16/100, Train Loss: 0.0524, Val Loss: 0.0435\n",
            "Epoch 17/100, Train Loss: 0.0489, Val Loss: 0.0419\n",
            "Epoch 18/100, Train Loss: 0.0549, Val Loss: 0.0497\n",
            "Epoch 19/100, Train Loss: 0.0495, Val Loss: 0.0409\n",
            "Epoch 20/100, Train Loss: 0.0487, Val Loss: 0.0398\n",
            "Epoch 21/100, Train Loss: 0.0463, Val Loss: 0.0341\n",
            "Epoch 22/100, Train Loss: 0.0526, Val Loss: 0.0592\n",
            "Epoch 23/100, Train Loss: 0.0478, Val Loss: 0.0366\n",
            "Epoch 24/100, Train Loss: 0.0435, Val Loss: 0.0323\n",
            "Epoch 25/100, Train Loss: 0.0465, Val Loss: 0.0573\n",
            "Epoch 26/100, Train Loss: 0.0418, Val Loss: 0.0302\n",
            "Epoch 27/100, Train Loss: 0.0410, Val Loss: 0.0413\n",
            "Epoch 28/100, Train Loss: 0.0488, Val Loss: 0.0799\n",
            "Epoch 29/100, Train Loss: 0.0427, Val Loss: 0.0315\n",
            "Epoch 30/100, Train Loss: 0.0484, Val Loss: 0.0387\n",
            "Epoch 31/100, Train Loss: 0.0374, Val Loss: 0.0309\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0546, Val Loss: 0.0353\n",
            "Epoch 33/100, Train Loss: 0.0473, Val Loss: 0.0400\n",
            "Epoch 34/100, Train Loss: 0.0418, Val Loss: 0.0421\n",
            "Epoch 35/100, Train Loss: 0.0396, Val Loss: 0.0434\n",
            "Epoch 36/100, Train Loss: 0.0375, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0386, Val Loss: 0.0423\n",
            "Epoch 38/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "Epoch 39/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "Epoch 40/100, Train Loss: 0.0382, Val Loss: 0.0423\n",
            "Epoch 41/100, Train Loss: 0.0379, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Epoch 43/100, Train Loss: 0.0378, Val Loss: 0.0423\n",
            "Epoch 44/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 45/100, Train Loss: 0.0375, Val Loss: 0.0423\n",
            "Epoch 46/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 48/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 49/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 50/100, Train Loss: 0.0385, Val Loss: 0.0423\n",
            "Epoch 51/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 53/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 54/100, Train Loss: 0.0374, Val Loss: 0.0423\n",
            "Epoch 55/100, Train Loss: 0.0376, Val Loss: 0.0423\n",
            "Epoch 56/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0367, Val Loss: 0.0423\n",
            "Epoch 58/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Epoch 59/100, Train Loss: 0.0359, Val Loss: 0.0423\n",
            "Epoch 60/100, Train Loss: 0.0367, Val Loss: 0.0423\n",
            "Epoch 61/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 63/100, Train Loss: 0.0363, Val Loss: 0.0423\n",
            "Epoch 64/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Epoch 65/100, Train Loss: 0.0359, Val Loss: 0.0423\n",
            "Epoch 66/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 68/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 69/100, Train Loss: 0.0369, Val Loss: 0.0423\n",
            "Epoch 70/100, Train Loss: 0.0357, Val Loss: 0.0423\n",
            "Epoch 71/100, Train Loss: 0.0363, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 73/100, Train Loss: 0.0387, Val Loss: 0.0423\n",
            "Epoch 74/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 75/100, Train Loss: 0.0382, Val Loss: 0.0423\n",
            "Epoch 76/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 78/100, Train Loss: 0.0475, Val Loss: 0.0423\n",
            "Epoch 79/100, Train Loss: 0.0357, Val Loss: 0.0423\n",
            "Epoch 80/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Epoch 81/100, Train Loss: 0.0381, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 83/100, Train Loss: 0.0408, Val Loss: 0.0423\n",
            "Epoch 84/100, Train Loss: 0.0385, Val Loss: 0.0423\n",
            "Epoch 85/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 86/100, Train Loss: 0.0377, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 88/100, Train Loss: 0.0374, Val Loss: 0.0423\n",
            "Epoch 89/100, Train Loss: 0.0384, Val Loss: 0.0423\n",
            "Epoch 90/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 91/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0360, Val Loss: 0.0423\n",
            "Epoch 93/100, Train Loss: 0.0377, Val Loss: 0.0423\n",
            "Epoch 94/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Epoch 95/100, Train Loss: 0.0402, Val Loss: 0.0423\n",
            "Epoch 96/100, Train Loss: 0.0386, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 98/100, Train Loss: 0.0390, Val Loss: 0.0423\n",
            "Epoch 99/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Epoch 100/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "\n",
            "Test iteration for date: 2019-06-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1397, Val Loss: 0.2762\n",
            "Epoch 2/100, Train Loss: 0.1241, Val Loss: 0.3437\n",
            "Epoch 3/100, Train Loss: 0.1174, Val Loss: 0.3578\n",
            "Epoch 4/100, Train Loss: 0.1072, Val Loss: 0.3491\n",
            "Epoch 5/100, Train Loss: 0.1128, Val Loss: 0.2711\n",
            "Epoch 6/100, Train Loss: 0.0923, Val Loss: 0.1188\n",
            "Epoch 7/100, Train Loss: 0.0855, Val Loss: 0.1048\n",
            "Epoch 8/100, Train Loss: 0.0862, Val Loss: 0.1720\n",
            "Epoch 9/100, Train Loss: 0.0721, Val Loss: 0.0721\n",
            "Epoch 10/100, Train Loss: 0.0579, Val Loss: 0.0589\n",
            "Epoch 11/100, Train Loss: 0.0573, Val Loss: 0.0657\n",
            "Epoch 12/100, Train Loss: 0.0608, Val Loss: 0.0505\n",
            "Epoch 13/100, Train Loss: 0.0710, Val Loss: 0.0713\n",
            "Epoch 14/100, Train Loss: 0.0555, Val Loss: 0.0626\n",
            "Epoch 15/100, Train Loss: 0.0557, Val Loss: 0.0534\n",
            "Epoch 16/100, Train Loss: 0.0524, Val Loss: 0.0435\n",
            "Epoch 17/100, Train Loss: 0.0489, Val Loss: 0.0419\n",
            "Epoch 18/100, Train Loss: 0.0549, Val Loss: 0.0497\n",
            "Epoch 19/100, Train Loss: 0.0495, Val Loss: 0.0409\n",
            "Epoch 20/100, Train Loss: 0.0487, Val Loss: 0.0398\n",
            "Epoch 21/100, Train Loss: 0.0463, Val Loss: 0.0341\n",
            "Epoch 22/100, Train Loss: 0.0526, Val Loss: 0.0592\n",
            "Epoch 23/100, Train Loss: 0.0478, Val Loss: 0.0366\n",
            "Epoch 24/100, Train Loss: 0.0435, Val Loss: 0.0323\n",
            "Epoch 25/100, Train Loss: 0.0465, Val Loss: 0.0573\n",
            "Epoch 26/100, Train Loss: 0.0418, Val Loss: 0.0302\n",
            "Epoch 27/100, Train Loss: 0.0410, Val Loss: 0.0413\n",
            "Epoch 28/100, Train Loss: 0.0488, Val Loss: 0.0799\n",
            "Epoch 29/100, Train Loss: 0.0427, Val Loss: 0.0315\n",
            "Epoch 30/100, Train Loss: 0.0484, Val Loss: 0.0387\n",
            "Epoch 31/100, Train Loss: 0.0374, Val Loss: 0.0309\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0546, Val Loss: 0.0353\n",
            "Epoch 33/100, Train Loss: 0.0473, Val Loss: 0.0400\n",
            "Epoch 34/100, Train Loss: 0.0418, Val Loss: 0.0421\n",
            "Epoch 35/100, Train Loss: 0.0396, Val Loss: 0.0434\n",
            "Epoch 36/100, Train Loss: 0.0375, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0386, Val Loss: 0.0423\n",
            "Epoch 38/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "Epoch 39/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "Epoch 40/100, Train Loss: 0.0382, Val Loss: 0.0423\n",
            "Epoch 41/100, Train Loss: 0.0379, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Epoch 43/100, Train Loss: 0.0378, Val Loss: 0.0423\n",
            "Epoch 44/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 45/100, Train Loss: 0.0375, Val Loss: 0.0423\n",
            "Epoch 46/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 48/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 49/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 50/100, Train Loss: 0.0385, Val Loss: 0.0423\n",
            "Epoch 51/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 53/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 54/100, Train Loss: 0.0374, Val Loss: 0.0423\n",
            "Epoch 55/100, Train Loss: 0.0376, Val Loss: 0.0423\n",
            "Epoch 56/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0367, Val Loss: 0.0423\n",
            "Epoch 58/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Epoch 59/100, Train Loss: 0.0359, Val Loss: 0.0423\n",
            "Epoch 60/100, Train Loss: 0.0367, Val Loss: 0.0423\n",
            "Epoch 61/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 63/100, Train Loss: 0.0363, Val Loss: 0.0423\n",
            "Epoch 64/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Epoch 65/100, Train Loss: 0.0359, Val Loss: 0.0423\n",
            "Epoch 66/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 68/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 69/100, Train Loss: 0.0369, Val Loss: 0.0423\n",
            "Epoch 70/100, Train Loss: 0.0357, Val Loss: 0.0423\n",
            "Epoch 71/100, Train Loss: 0.0363, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 73/100, Train Loss: 0.0387, Val Loss: 0.0423\n",
            "Epoch 74/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 75/100, Train Loss: 0.0382, Val Loss: 0.0423\n",
            "Epoch 76/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 78/100, Train Loss: 0.0475, Val Loss: 0.0423\n",
            "Epoch 79/100, Train Loss: 0.0357, Val Loss: 0.0423\n",
            "Epoch 80/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Epoch 81/100, Train Loss: 0.0381, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 83/100, Train Loss: 0.0408, Val Loss: 0.0423\n",
            "Epoch 84/100, Train Loss: 0.0385, Val Loss: 0.0423\n",
            "Epoch 85/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 86/100, Train Loss: 0.0377, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 88/100, Train Loss: 0.0374, Val Loss: 0.0423\n",
            "Epoch 89/100, Train Loss: 0.0384, Val Loss: 0.0423\n",
            "Epoch 90/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 91/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0360, Val Loss: 0.0423\n",
            "Epoch 93/100, Train Loss: 0.0377, Val Loss: 0.0423\n",
            "Epoch 94/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Epoch 95/100, Train Loss: 0.0402, Val Loss: 0.0423\n",
            "Epoch 96/100, Train Loss: 0.0386, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 98/100, Train Loss: 0.0390, Val Loss: 0.0423\n",
            "Epoch 99/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Epoch 100/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "\n",
            "Test iteration for date: 2019-07-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1397, Val Loss: 0.2762\n",
            "Epoch 2/100, Train Loss: 0.1241, Val Loss: 0.3437\n",
            "Epoch 3/100, Train Loss: 0.1174, Val Loss: 0.3578\n",
            "Epoch 4/100, Train Loss: 0.1072, Val Loss: 0.3491\n",
            "Epoch 5/100, Train Loss: 0.1128, Val Loss: 0.2711\n",
            "Epoch 6/100, Train Loss: 0.0923, Val Loss: 0.1188\n",
            "Epoch 7/100, Train Loss: 0.0855, Val Loss: 0.1048\n",
            "Epoch 8/100, Train Loss: 0.0862, Val Loss: 0.1720\n",
            "Epoch 9/100, Train Loss: 0.0721, Val Loss: 0.0721\n",
            "Epoch 10/100, Train Loss: 0.0579, Val Loss: 0.0589\n",
            "Epoch 11/100, Train Loss: 0.0573, Val Loss: 0.0657\n",
            "Epoch 12/100, Train Loss: 0.0608, Val Loss: 0.0505\n",
            "Epoch 13/100, Train Loss: 0.0710, Val Loss: 0.0713\n",
            "Epoch 14/100, Train Loss: 0.0555, Val Loss: 0.0626\n",
            "Epoch 15/100, Train Loss: 0.0557, Val Loss: 0.0534\n",
            "Epoch 16/100, Train Loss: 0.0524, Val Loss: 0.0435\n",
            "Epoch 17/100, Train Loss: 0.0489, Val Loss: 0.0419\n",
            "Epoch 18/100, Train Loss: 0.0549, Val Loss: 0.0497\n",
            "Epoch 19/100, Train Loss: 0.0495, Val Loss: 0.0409\n",
            "Epoch 20/100, Train Loss: 0.0487, Val Loss: 0.0398\n",
            "Epoch 21/100, Train Loss: 0.0463, Val Loss: 0.0341\n",
            "Epoch 22/100, Train Loss: 0.0526, Val Loss: 0.0592\n",
            "Epoch 23/100, Train Loss: 0.0478, Val Loss: 0.0366\n",
            "Epoch 24/100, Train Loss: 0.0435, Val Loss: 0.0323\n",
            "Epoch 25/100, Train Loss: 0.0465, Val Loss: 0.0573\n",
            "Epoch 26/100, Train Loss: 0.0418, Val Loss: 0.0302\n",
            "Epoch 27/100, Train Loss: 0.0410, Val Loss: 0.0413\n",
            "Epoch 28/100, Train Loss: 0.0488, Val Loss: 0.0799\n",
            "Epoch 29/100, Train Loss: 0.0427, Val Loss: 0.0315\n",
            "Epoch 30/100, Train Loss: 0.0484, Val Loss: 0.0387\n",
            "Epoch 31/100, Train Loss: 0.0374, Val Loss: 0.0309\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0546, Val Loss: 0.0353\n",
            "Epoch 33/100, Train Loss: 0.0473, Val Loss: 0.0400\n",
            "Epoch 34/100, Train Loss: 0.0418, Val Loss: 0.0421\n",
            "Epoch 35/100, Train Loss: 0.0396, Val Loss: 0.0434\n",
            "Epoch 36/100, Train Loss: 0.0375, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0386, Val Loss: 0.0423\n",
            "Epoch 38/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "Epoch 39/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "Epoch 40/100, Train Loss: 0.0382, Val Loss: 0.0423\n",
            "Epoch 41/100, Train Loss: 0.0379, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Epoch 43/100, Train Loss: 0.0378, Val Loss: 0.0423\n",
            "Epoch 44/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 45/100, Train Loss: 0.0375, Val Loss: 0.0423\n",
            "Epoch 46/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 48/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 49/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 50/100, Train Loss: 0.0385, Val Loss: 0.0423\n",
            "Epoch 51/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 53/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 54/100, Train Loss: 0.0374, Val Loss: 0.0423\n",
            "Epoch 55/100, Train Loss: 0.0376, Val Loss: 0.0423\n",
            "Epoch 56/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0367, Val Loss: 0.0423\n",
            "Epoch 58/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Epoch 59/100, Train Loss: 0.0359, Val Loss: 0.0423\n",
            "Epoch 60/100, Train Loss: 0.0367, Val Loss: 0.0423\n",
            "Epoch 61/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 63/100, Train Loss: 0.0363, Val Loss: 0.0423\n",
            "Epoch 64/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Epoch 65/100, Train Loss: 0.0359, Val Loss: 0.0423\n",
            "Epoch 66/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 68/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 69/100, Train Loss: 0.0369, Val Loss: 0.0423\n",
            "Epoch 70/100, Train Loss: 0.0357, Val Loss: 0.0423\n",
            "Epoch 71/100, Train Loss: 0.0363, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 73/100, Train Loss: 0.0387, Val Loss: 0.0423\n",
            "Epoch 74/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 75/100, Train Loss: 0.0382, Val Loss: 0.0423\n",
            "Epoch 76/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 78/100, Train Loss: 0.0475, Val Loss: 0.0423\n",
            "Epoch 79/100, Train Loss: 0.0357, Val Loss: 0.0423\n",
            "Epoch 80/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Epoch 81/100, Train Loss: 0.0381, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 83/100, Train Loss: 0.0408, Val Loss: 0.0423\n",
            "Epoch 84/100, Train Loss: 0.0385, Val Loss: 0.0423\n",
            "Epoch 85/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 86/100, Train Loss: 0.0377, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 88/100, Train Loss: 0.0374, Val Loss: 0.0423\n",
            "Epoch 89/100, Train Loss: 0.0384, Val Loss: 0.0423\n",
            "Epoch 90/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 91/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0360, Val Loss: 0.0423\n",
            "Epoch 93/100, Train Loss: 0.0377, Val Loss: 0.0423\n",
            "Epoch 94/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Epoch 95/100, Train Loss: 0.0402, Val Loss: 0.0423\n",
            "Epoch 96/100, Train Loss: 0.0386, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 98/100, Train Loss: 0.0390, Val Loss: 0.0423\n",
            "Epoch 99/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Epoch 100/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "\n",
            "Test iteration for date: 2019-08-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1397, Val Loss: 0.2762\n",
            "Epoch 2/100, Train Loss: 0.1241, Val Loss: 0.3437\n",
            "Epoch 3/100, Train Loss: 0.1174, Val Loss: 0.3578\n",
            "Epoch 4/100, Train Loss: 0.1072, Val Loss: 0.3491\n",
            "Epoch 5/100, Train Loss: 0.1128, Val Loss: 0.2711\n",
            "Epoch 6/100, Train Loss: 0.0923, Val Loss: 0.1188\n",
            "Epoch 7/100, Train Loss: 0.0855, Val Loss: 0.1048\n",
            "Epoch 8/100, Train Loss: 0.0862, Val Loss: 0.1720\n",
            "Epoch 9/100, Train Loss: 0.0721, Val Loss: 0.0721\n",
            "Epoch 10/100, Train Loss: 0.0579, Val Loss: 0.0589\n",
            "Epoch 11/100, Train Loss: 0.0573, Val Loss: 0.0657\n",
            "Epoch 12/100, Train Loss: 0.0608, Val Loss: 0.0505\n",
            "Epoch 13/100, Train Loss: 0.0710, Val Loss: 0.0713\n",
            "Epoch 14/100, Train Loss: 0.0555, Val Loss: 0.0626\n",
            "Epoch 15/100, Train Loss: 0.0557, Val Loss: 0.0534\n",
            "Epoch 16/100, Train Loss: 0.0524, Val Loss: 0.0435\n",
            "Epoch 17/100, Train Loss: 0.0489, Val Loss: 0.0419\n",
            "Epoch 18/100, Train Loss: 0.0549, Val Loss: 0.0497\n",
            "Epoch 19/100, Train Loss: 0.0495, Val Loss: 0.0409\n",
            "Epoch 20/100, Train Loss: 0.0487, Val Loss: 0.0398\n",
            "Epoch 21/100, Train Loss: 0.0463, Val Loss: 0.0341\n",
            "Epoch 22/100, Train Loss: 0.0526, Val Loss: 0.0592\n",
            "Epoch 23/100, Train Loss: 0.0478, Val Loss: 0.0366\n",
            "Epoch 24/100, Train Loss: 0.0435, Val Loss: 0.0323\n",
            "Epoch 25/100, Train Loss: 0.0465, Val Loss: 0.0573\n",
            "Epoch 26/100, Train Loss: 0.0418, Val Loss: 0.0302\n",
            "Epoch 27/100, Train Loss: 0.0410, Val Loss: 0.0413\n",
            "Epoch 28/100, Train Loss: 0.0488, Val Loss: 0.0799\n",
            "Epoch 29/100, Train Loss: 0.0427, Val Loss: 0.0315\n",
            "Epoch 30/100, Train Loss: 0.0484, Val Loss: 0.0387\n",
            "Epoch 31/100, Train Loss: 0.0374, Val Loss: 0.0309\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0546, Val Loss: 0.0353\n",
            "Epoch 33/100, Train Loss: 0.0473, Val Loss: 0.0400\n",
            "Epoch 34/100, Train Loss: 0.0418, Val Loss: 0.0421\n",
            "Epoch 35/100, Train Loss: 0.0396, Val Loss: 0.0434\n",
            "Epoch 36/100, Train Loss: 0.0375, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0386, Val Loss: 0.0423\n",
            "Epoch 38/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "Epoch 39/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "Epoch 40/100, Train Loss: 0.0382, Val Loss: 0.0423\n",
            "Epoch 41/100, Train Loss: 0.0379, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Epoch 43/100, Train Loss: 0.0378, Val Loss: 0.0423\n",
            "Epoch 44/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 45/100, Train Loss: 0.0375, Val Loss: 0.0423\n",
            "Epoch 46/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 48/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 49/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 50/100, Train Loss: 0.0385, Val Loss: 0.0423\n",
            "Epoch 51/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 53/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 54/100, Train Loss: 0.0374, Val Loss: 0.0423\n",
            "Epoch 55/100, Train Loss: 0.0376, Val Loss: 0.0423\n",
            "Epoch 56/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0367, Val Loss: 0.0423\n",
            "Epoch 58/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Epoch 59/100, Train Loss: 0.0359, Val Loss: 0.0423\n",
            "Epoch 60/100, Train Loss: 0.0367, Val Loss: 0.0423\n",
            "Epoch 61/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 63/100, Train Loss: 0.0363, Val Loss: 0.0423\n",
            "Epoch 64/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Epoch 65/100, Train Loss: 0.0359, Val Loss: 0.0423\n",
            "Epoch 66/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 68/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 69/100, Train Loss: 0.0369, Val Loss: 0.0423\n",
            "Epoch 70/100, Train Loss: 0.0357, Val Loss: 0.0423\n",
            "Epoch 71/100, Train Loss: 0.0363, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 73/100, Train Loss: 0.0387, Val Loss: 0.0423\n",
            "Epoch 74/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 75/100, Train Loss: 0.0382, Val Loss: 0.0423\n",
            "Epoch 76/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 78/100, Train Loss: 0.0475, Val Loss: 0.0423\n",
            "Epoch 79/100, Train Loss: 0.0357, Val Loss: 0.0423\n",
            "Epoch 80/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Epoch 81/100, Train Loss: 0.0381, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 83/100, Train Loss: 0.0408, Val Loss: 0.0423\n",
            "Epoch 84/100, Train Loss: 0.0385, Val Loss: 0.0423\n",
            "Epoch 85/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 86/100, Train Loss: 0.0377, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 88/100, Train Loss: 0.0374, Val Loss: 0.0423\n",
            "Epoch 89/100, Train Loss: 0.0384, Val Loss: 0.0423\n",
            "Epoch 90/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 91/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0360, Val Loss: 0.0423\n",
            "Epoch 93/100, Train Loss: 0.0377, Val Loss: 0.0423\n",
            "Epoch 94/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Epoch 95/100, Train Loss: 0.0402, Val Loss: 0.0423\n",
            "Epoch 96/100, Train Loss: 0.0386, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 98/100, Train Loss: 0.0390, Val Loss: 0.0423\n",
            "Epoch 99/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Epoch 100/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "\n",
            "Test iteration for date: 2019-09-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1397, Val Loss: 0.2762\n",
            "Epoch 2/100, Train Loss: 0.1241, Val Loss: 0.3437\n",
            "Epoch 3/100, Train Loss: 0.1174, Val Loss: 0.3578\n",
            "Epoch 4/100, Train Loss: 0.1072, Val Loss: 0.3491\n",
            "Epoch 5/100, Train Loss: 0.1128, Val Loss: 0.2711\n",
            "Epoch 6/100, Train Loss: 0.0923, Val Loss: 0.1188\n",
            "Epoch 7/100, Train Loss: 0.0855, Val Loss: 0.1048\n",
            "Epoch 8/100, Train Loss: 0.0862, Val Loss: 0.1720\n",
            "Epoch 9/100, Train Loss: 0.0721, Val Loss: 0.0721\n",
            "Epoch 10/100, Train Loss: 0.0579, Val Loss: 0.0589\n",
            "Epoch 11/100, Train Loss: 0.0573, Val Loss: 0.0657\n",
            "Epoch 12/100, Train Loss: 0.0608, Val Loss: 0.0505\n",
            "Epoch 13/100, Train Loss: 0.0710, Val Loss: 0.0713\n",
            "Epoch 14/100, Train Loss: 0.0555, Val Loss: 0.0626\n",
            "Epoch 15/100, Train Loss: 0.0557, Val Loss: 0.0534\n",
            "Epoch 16/100, Train Loss: 0.0524, Val Loss: 0.0435\n",
            "Epoch 17/100, Train Loss: 0.0489, Val Loss: 0.0419\n",
            "Epoch 18/100, Train Loss: 0.0549, Val Loss: 0.0497\n",
            "Epoch 19/100, Train Loss: 0.0495, Val Loss: 0.0409\n",
            "Epoch 20/100, Train Loss: 0.0487, Val Loss: 0.0398\n",
            "Epoch 21/100, Train Loss: 0.0463, Val Loss: 0.0341\n",
            "Epoch 22/100, Train Loss: 0.0526, Val Loss: 0.0592\n",
            "Epoch 23/100, Train Loss: 0.0478, Val Loss: 0.0366\n",
            "Epoch 24/100, Train Loss: 0.0435, Val Loss: 0.0323\n",
            "Epoch 25/100, Train Loss: 0.0465, Val Loss: 0.0573\n",
            "Epoch 26/100, Train Loss: 0.0418, Val Loss: 0.0302\n",
            "Epoch 27/100, Train Loss: 0.0410, Val Loss: 0.0413\n",
            "Epoch 28/100, Train Loss: 0.0488, Val Loss: 0.0799\n",
            "Epoch 29/100, Train Loss: 0.0427, Val Loss: 0.0315\n",
            "Epoch 30/100, Train Loss: 0.0484, Val Loss: 0.0387\n",
            "Epoch 31/100, Train Loss: 0.0374, Val Loss: 0.0309\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0546, Val Loss: 0.0353\n",
            "Epoch 33/100, Train Loss: 0.0473, Val Loss: 0.0400\n",
            "Epoch 34/100, Train Loss: 0.0418, Val Loss: 0.0421\n",
            "Epoch 35/100, Train Loss: 0.0396, Val Loss: 0.0434\n",
            "Epoch 36/100, Train Loss: 0.0375, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0386, Val Loss: 0.0423\n",
            "Epoch 38/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "Epoch 39/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "Epoch 40/100, Train Loss: 0.0382, Val Loss: 0.0423\n",
            "Epoch 41/100, Train Loss: 0.0379, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Epoch 43/100, Train Loss: 0.0378, Val Loss: 0.0423\n",
            "Epoch 44/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 45/100, Train Loss: 0.0375, Val Loss: 0.0423\n",
            "Epoch 46/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 48/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 49/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 50/100, Train Loss: 0.0385, Val Loss: 0.0423\n",
            "Epoch 51/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 53/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 54/100, Train Loss: 0.0374, Val Loss: 0.0423\n",
            "Epoch 55/100, Train Loss: 0.0376, Val Loss: 0.0423\n",
            "Epoch 56/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0367, Val Loss: 0.0423\n",
            "Epoch 58/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Epoch 59/100, Train Loss: 0.0359, Val Loss: 0.0423\n",
            "Epoch 60/100, Train Loss: 0.0367, Val Loss: 0.0423\n",
            "Epoch 61/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 63/100, Train Loss: 0.0363, Val Loss: 0.0423\n",
            "Epoch 64/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Epoch 65/100, Train Loss: 0.0359, Val Loss: 0.0423\n",
            "Epoch 66/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 68/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 69/100, Train Loss: 0.0369, Val Loss: 0.0423\n",
            "Epoch 70/100, Train Loss: 0.0357, Val Loss: 0.0423\n",
            "Epoch 71/100, Train Loss: 0.0363, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 73/100, Train Loss: 0.0387, Val Loss: 0.0423\n",
            "Epoch 74/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 75/100, Train Loss: 0.0382, Val Loss: 0.0423\n",
            "Epoch 76/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 78/100, Train Loss: 0.0475, Val Loss: 0.0423\n",
            "Epoch 79/100, Train Loss: 0.0357, Val Loss: 0.0423\n",
            "Epoch 80/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Epoch 81/100, Train Loss: 0.0381, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 83/100, Train Loss: 0.0408, Val Loss: 0.0423\n",
            "Epoch 84/100, Train Loss: 0.0385, Val Loss: 0.0423\n",
            "Epoch 85/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 86/100, Train Loss: 0.0377, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 88/100, Train Loss: 0.0374, Val Loss: 0.0423\n",
            "Epoch 89/100, Train Loss: 0.0384, Val Loss: 0.0423\n",
            "Epoch 90/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 91/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0360, Val Loss: 0.0423\n",
            "Epoch 93/100, Train Loss: 0.0377, Val Loss: 0.0423\n",
            "Epoch 94/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Epoch 95/100, Train Loss: 0.0402, Val Loss: 0.0423\n",
            "Epoch 96/100, Train Loss: 0.0386, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 98/100, Train Loss: 0.0390, Val Loss: 0.0423\n",
            "Epoch 99/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Epoch 100/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "\n",
            "Test iteration for date: 2019-10-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1397, Val Loss: 0.2762\n",
            "Epoch 2/100, Train Loss: 0.1241, Val Loss: 0.3437\n",
            "Epoch 3/100, Train Loss: 0.1174, Val Loss: 0.3578\n",
            "Epoch 4/100, Train Loss: 0.1072, Val Loss: 0.3491\n",
            "Epoch 5/100, Train Loss: 0.1128, Val Loss: 0.2711\n",
            "Epoch 6/100, Train Loss: 0.0923, Val Loss: 0.1188\n",
            "Epoch 7/100, Train Loss: 0.0855, Val Loss: 0.1048\n",
            "Epoch 8/100, Train Loss: 0.0862, Val Loss: 0.1720\n",
            "Epoch 9/100, Train Loss: 0.0721, Val Loss: 0.0721\n",
            "Epoch 10/100, Train Loss: 0.0579, Val Loss: 0.0589\n",
            "Epoch 11/100, Train Loss: 0.0573, Val Loss: 0.0657\n",
            "Epoch 12/100, Train Loss: 0.0608, Val Loss: 0.0505\n",
            "Epoch 13/100, Train Loss: 0.0710, Val Loss: 0.0713\n",
            "Epoch 14/100, Train Loss: 0.0555, Val Loss: 0.0626\n",
            "Epoch 15/100, Train Loss: 0.0557, Val Loss: 0.0534\n",
            "Epoch 16/100, Train Loss: 0.0524, Val Loss: 0.0435\n",
            "Epoch 17/100, Train Loss: 0.0489, Val Loss: 0.0419\n",
            "Epoch 18/100, Train Loss: 0.0549, Val Loss: 0.0497\n",
            "Epoch 19/100, Train Loss: 0.0495, Val Loss: 0.0409\n",
            "Epoch 20/100, Train Loss: 0.0487, Val Loss: 0.0398\n",
            "Epoch 21/100, Train Loss: 0.0463, Val Loss: 0.0341\n",
            "Epoch 22/100, Train Loss: 0.0526, Val Loss: 0.0592\n",
            "Epoch 23/100, Train Loss: 0.0478, Val Loss: 0.0366\n",
            "Epoch 24/100, Train Loss: 0.0435, Val Loss: 0.0323\n",
            "Epoch 25/100, Train Loss: 0.0465, Val Loss: 0.0573\n",
            "Epoch 26/100, Train Loss: 0.0418, Val Loss: 0.0302\n",
            "Epoch 27/100, Train Loss: 0.0410, Val Loss: 0.0413\n",
            "Epoch 28/100, Train Loss: 0.0488, Val Loss: 0.0799\n",
            "Epoch 29/100, Train Loss: 0.0427, Val Loss: 0.0315\n",
            "Epoch 30/100, Train Loss: 0.0484, Val Loss: 0.0387\n",
            "Epoch 31/100, Train Loss: 0.0374, Val Loss: 0.0309\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0546, Val Loss: 0.0353\n",
            "Epoch 33/100, Train Loss: 0.0473, Val Loss: 0.0400\n",
            "Epoch 34/100, Train Loss: 0.0418, Val Loss: 0.0421\n",
            "Epoch 35/100, Train Loss: 0.0396, Val Loss: 0.0434\n",
            "Epoch 36/100, Train Loss: 0.0375, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0386, Val Loss: 0.0423\n",
            "Epoch 38/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "Epoch 39/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "Epoch 40/100, Train Loss: 0.0382, Val Loss: 0.0423\n",
            "Epoch 41/100, Train Loss: 0.0379, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Epoch 43/100, Train Loss: 0.0378, Val Loss: 0.0423\n",
            "Epoch 44/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 45/100, Train Loss: 0.0375, Val Loss: 0.0423\n",
            "Epoch 46/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 48/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 49/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 50/100, Train Loss: 0.0385, Val Loss: 0.0423\n",
            "Epoch 51/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 53/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 54/100, Train Loss: 0.0374, Val Loss: 0.0423\n",
            "Epoch 55/100, Train Loss: 0.0376, Val Loss: 0.0423\n",
            "Epoch 56/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0367, Val Loss: 0.0423\n",
            "Epoch 58/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Epoch 59/100, Train Loss: 0.0359, Val Loss: 0.0423\n",
            "Epoch 60/100, Train Loss: 0.0367, Val Loss: 0.0423\n",
            "Epoch 61/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 63/100, Train Loss: 0.0363, Val Loss: 0.0423\n",
            "Epoch 64/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Epoch 65/100, Train Loss: 0.0359, Val Loss: 0.0423\n",
            "Epoch 66/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 68/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 69/100, Train Loss: 0.0369, Val Loss: 0.0423\n",
            "Epoch 70/100, Train Loss: 0.0357, Val Loss: 0.0423\n",
            "Epoch 71/100, Train Loss: 0.0363, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 73/100, Train Loss: 0.0387, Val Loss: 0.0423\n",
            "Epoch 74/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 75/100, Train Loss: 0.0382, Val Loss: 0.0423\n",
            "Epoch 76/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 78/100, Train Loss: 0.0475, Val Loss: 0.0423\n",
            "Epoch 79/100, Train Loss: 0.0357, Val Loss: 0.0423\n",
            "Epoch 80/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Epoch 81/100, Train Loss: 0.0381, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 83/100, Train Loss: 0.0408, Val Loss: 0.0423\n",
            "Epoch 84/100, Train Loss: 0.0385, Val Loss: 0.0423\n",
            "Epoch 85/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 86/100, Train Loss: 0.0377, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 88/100, Train Loss: 0.0374, Val Loss: 0.0423\n",
            "Epoch 89/100, Train Loss: 0.0384, Val Loss: 0.0423\n",
            "Epoch 90/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 91/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0360, Val Loss: 0.0423\n",
            "Epoch 93/100, Train Loss: 0.0377, Val Loss: 0.0423\n",
            "Epoch 94/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Epoch 95/100, Train Loss: 0.0402, Val Loss: 0.0423\n",
            "Epoch 96/100, Train Loss: 0.0386, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 98/100, Train Loss: 0.0390, Val Loss: 0.0423\n",
            "Epoch 99/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Epoch 100/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "\n",
            "Test iteration for date: 2019-11-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1397, Val Loss: 0.2762\n",
            "Epoch 2/100, Train Loss: 0.1241, Val Loss: 0.3437\n",
            "Epoch 3/100, Train Loss: 0.1174, Val Loss: 0.3578\n",
            "Epoch 4/100, Train Loss: 0.1072, Val Loss: 0.3491\n",
            "Epoch 5/100, Train Loss: 0.1128, Val Loss: 0.2711\n",
            "Epoch 6/100, Train Loss: 0.0923, Val Loss: 0.1188\n",
            "Epoch 7/100, Train Loss: 0.0855, Val Loss: 0.1048\n",
            "Epoch 8/100, Train Loss: 0.0862, Val Loss: 0.1720\n",
            "Epoch 9/100, Train Loss: 0.0721, Val Loss: 0.0721\n",
            "Epoch 10/100, Train Loss: 0.0579, Val Loss: 0.0589\n",
            "Epoch 11/100, Train Loss: 0.0573, Val Loss: 0.0657\n",
            "Epoch 12/100, Train Loss: 0.0608, Val Loss: 0.0505\n",
            "Epoch 13/100, Train Loss: 0.0710, Val Loss: 0.0713\n",
            "Epoch 14/100, Train Loss: 0.0555, Val Loss: 0.0626\n",
            "Epoch 15/100, Train Loss: 0.0557, Val Loss: 0.0534\n",
            "Epoch 16/100, Train Loss: 0.0524, Val Loss: 0.0435\n",
            "Epoch 17/100, Train Loss: 0.0489, Val Loss: 0.0419\n",
            "Epoch 18/100, Train Loss: 0.0549, Val Loss: 0.0497\n",
            "Epoch 19/100, Train Loss: 0.0495, Val Loss: 0.0409\n",
            "Epoch 20/100, Train Loss: 0.0487, Val Loss: 0.0398\n",
            "Epoch 21/100, Train Loss: 0.0463, Val Loss: 0.0341\n",
            "Epoch 22/100, Train Loss: 0.0526, Val Loss: 0.0592\n",
            "Epoch 23/100, Train Loss: 0.0478, Val Loss: 0.0366\n",
            "Epoch 24/100, Train Loss: 0.0435, Val Loss: 0.0323\n",
            "Epoch 25/100, Train Loss: 0.0465, Val Loss: 0.0573\n",
            "Epoch 26/100, Train Loss: 0.0418, Val Loss: 0.0302\n",
            "Epoch 27/100, Train Loss: 0.0410, Val Loss: 0.0413\n",
            "Epoch 28/100, Train Loss: 0.0488, Val Loss: 0.0799\n",
            "Epoch 29/100, Train Loss: 0.0427, Val Loss: 0.0315\n",
            "Epoch 30/100, Train Loss: 0.0484, Val Loss: 0.0387\n",
            "Epoch 31/100, Train Loss: 0.0374, Val Loss: 0.0309\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0546, Val Loss: 0.0353\n",
            "Epoch 33/100, Train Loss: 0.0473, Val Loss: 0.0400\n",
            "Epoch 34/100, Train Loss: 0.0418, Val Loss: 0.0421\n",
            "Epoch 35/100, Train Loss: 0.0396, Val Loss: 0.0434\n",
            "Epoch 36/100, Train Loss: 0.0375, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0386, Val Loss: 0.0423\n",
            "Epoch 38/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "Epoch 39/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "Epoch 40/100, Train Loss: 0.0382, Val Loss: 0.0423\n",
            "Epoch 41/100, Train Loss: 0.0379, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Epoch 43/100, Train Loss: 0.0378, Val Loss: 0.0423\n",
            "Epoch 44/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 45/100, Train Loss: 0.0375, Val Loss: 0.0423\n",
            "Epoch 46/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 48/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 49/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 50/100, Train Loss: 0.0385, Val Loss: 0.0423\n",
            "Epoch 51/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 53/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 54/100, Train Loss: 0.0374, Val Loss: 0.0423\n",
            "Epoch 55/100, Train Loss: 0.0376, Val Loss: 0.0423\n",
            "Epoch 56/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0367, Val Loss: 0.0423\n",
            "Epoch 58/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Epoch 59/100, Train Loss: 0.0359, Val Loss: 0.0423\n",
            "Epoch 60/100, Train Loss: 0.0367, Val Loss: 0.0423\n",
            "Epoch 61/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 63/100, Train Loss: 0.0363, Val Loss: 0.0423\n",
            "Epoch 64/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Epoch 65/100, Train Loss: 0.0359, Val Loss: 0.0423\n",
            "Epoch 66/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 68/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 69/100, Train Loss: 0.0369, Val Loss: 0.0423\n",
            "Epoch 70/100, Train Loss: 0.0357, Val Loss: 0.0423\n",
            "Epoch 71/100, Train Loss: 0.0363, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 73/100, Train Loss: 0.0387, Val Loss: 0.0423\n",
            "Epoch 74/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 75/100, Train Loss: 0.0382, Val Loss: 0.0423\n",
            "Epoch 76/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 78/100, Train Loss: 0.0475, Val Loss: 0.0423\n",
            "Epoch 79/100, Train Loss: 0.0357, Val Loss: 0.0423\n",
            "Epoch 80/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Epoch 81/100, Train Loss: 0.0381, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 83/100, Train Loss: 0.0408, Val Loss: 0.0423\n",
            "Epoch 84/100, Train Loss: 0.0385, Val Loss: 0.0423\n",
            "Epoch 85/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 86/100, Train Loss: 0.0377, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 88/100, Train Loss: 0.0374, Val Loss: 0.0423\n",
            "Epoch 89/100, Train Loss: 0.0384, Val Loss: 0.0423\n",
            "Epoch 90/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 91/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0360, Val Loss: 0.0423\n",
            "Epoch 93/100, Train Loss: 0.0377, Val Loss: 0.0423\n",
            "Epoch 94/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Epoch 95/100, Train Loss: 0.0402, Val Loss: 0.0423\n",
            "Epoch 96/100, Train Loss: 0.0386, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 98/100, Train Loss: 0.0390, Val Loss: 0.0423\n",
            "Epoch 99/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Epoch 100/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "\n",
            "Test iteration for date: 2019-12-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1397, Val Loss: 0.2762\n",
            "Epoch 2/100, Train Loss: 0.1241, Val Loss: 0.3437\n",
            "Epoch 3/100, Train Loss: 0.1174, Val Loss: 0.3578\n",
            "Epoch 4/100, Train Loss: 0.1072, Val Loss: 0.3491\n",
            "Epoch 5/100, Train Loss: 0.1128, Val Loss: 0.2711\n",
            "Epoch 6/100, Train Loss: 0.0923, Val Loss: 0.1188\n",
            "Epoch 7/100, Train Loss: 0.0855, Val Loss: 0.1048\n",
            "Epoch 8/100, Train Loss: 0.0862, Val Loss: 0.1720\n",
            "Epoch 9/100, Train Loss: 0.0721, Val Loss: 0.0721\n",
            "Epoch 10/100, Train Loss: 0.0579, Val Loss: 0.0589\n",
            "Epoch 11/100, Train Loss: 0.0573, Val Loss: 0.0657\n",
            "Epoch 12/100, Train Loss: 0.0608, Val Loss: 0.0505\n",
            "Epoch 13/100, Train Loss: 0.0710, Val Loss: 0.0713\n",
            "Epoch 14/100, Train Loss: 0.0555, Val Loss: 0.0626\n",
            "Epoch 15/100, Train Loss: 0.0557, Val Loss: 0.0534\n",
            "Epoch 16/100, Train Loss: 0.0524, Val Loss: 0.0435\n",
            "Epoch 17/100, Train Loss: 0.0489, Val Loss: 0.0419\n",
            "Epoch 18/100, Train Loss: 0.0549, Val Loss: 0.0497\n",
            "Epoch 19/100, Train Loss: 0.0495, Val Loss: 0.0409\n",
            "Epoch 20/100, Train Loss: 0.0487, Val Loss: 0.0398\n",
            "Epoch 21/100, Train Loss: 0.0463, Val Loss: 0.0341\n",
            "Epoch 22/100, Train Loss: 0.0526, Val Loss: 0.0592\n",
            "Epoch 23/100, Train Loss: 0.0478, Val Loss: 0.0366\n",
            "Epoch 24/100, Train Loss: 0.0435, Val Loss: 0.0323\n",
            "Epoch 25/100, Train Loss: 0.0465, Val Loss: 0.0573\n",
            "Epoch 26/100, Train Loss: 0.0418, Val Loss: 0.0302\n",
            "Epoch 27/100, Train Loss: 0.0410, Val Loss: 0.0413\n",
            "Epoch 28/100, Train Loss: 0.0488, Val Loss: 0.0799\n",
            "Epoch 29/100, Train Loss: 0.0427, Val Loss: 0.0315\n",
            "Epoch 30/100, Train Loss: 0.0484, Val Loss: 0.0387\n",
            "Epoch 31/100, Train Loss: 0.0374, Val Loss: 0.0309\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0546, Val Loss: 0.0353\n",
            "Epoch 33/100, Train Loss: 0.0473, Val Loss: 0.0400\n",
            "Epoch 34/100, Train Loss: 0.0418, Val Loss: 0.0421\n",
            "Epoch 35/100, Train Loss: 0.0396, Val Loss: 0.0434\n",
            "Epoch 36/100, Train Loss: 0.0375, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0386, Val Loss: 0.0423\n",
            "Epoch 38/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "Epoch 39/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "Epoch 40/100, Train Loss: 0.0382, Val Loss: 0.0423\n",
            "Epoch 41/100, Train Loss: 0.0379, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Epoch 43/100, Train Loss: 0.0378, Val Loss: 0.0423\n",
            "Epoch 44/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 45/100, Train Loss: 0.0375, Val Loss: 0.0423\n",
            "Epoch 46/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 48/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 49/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 50/100, Train Loss: 0.0385, Val Loss: 0.0423\n",
            "Epoch 51/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 53/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 54/100, Train Loss: 0.0374, Val Loss: 0.0423\n",
            "Epoch 55/100, Train Loss: 0.0376, Val Loss: 0.0423\n",
            "Epoch 56/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0367, Val Loss: 0.0423\n",
            "Epoch 58/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Epoch 59/100, Train Loss: 0.0359, Val Loss: 0.0423\n",
            "Epoch 60/100, Train Loss: 0.0367, Val Loss: 0.0423\n",
            "Epoch 61/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Epoch 63/100, Train Loss: 0.0363, Val Loss: 0.0423\n",
            "Epoch 64/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Epoch 65/100, Train Loss: 0.0359, Val Loss: 0.0423\n",
            "Epoch 66/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 68/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 69/100, Train Loss: 0.0369, Val Loss: 0.0423\n",
            "Epoch 70/100, Train Loss: 0.0357, Val Loss: 0.0423\n",
            "Epoch 71/100, Train Loss: 0.0363, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 73/100, Train Loss: 0.0387, Val Loss: 0.0423\n",
            "Epoch 74/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 75/100, Train Loss: 0.0382, Val Loss: 0.0423\n",
            "Epoch 76/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 78/100, Train Loss: 0.0475, Val Loss: 0.0423\n",
            "Epoch 79/100, Train Loss: 0.0357, Val Loss: 0.0423\n",
            "Epoch 80/100, Train Loss: 0.0356, Val Loss: 0.0423\n",
            "Epoch 81/100, Train Loss: 0.0381, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0364, Val Loss: 0.0423\n",
            "Epoch 83/100, Train Loss: 0.0408, Val Loss: 0.0423\n",
            "Epoch 84/100, Train Loss: 0.0385, Val Loss: 0.0423\n",
            "Epoch 85/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 86/100, Train Loss: 0.0377, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0373, Val Loss: 0.0423\n",
            "Epoch 88/100, Train Loss: 0.0374, Val Loss: 0.0423\n",
            "Epoch 89/100, Train Loss: 0.0384, Val Loss: 0.0423\n",
            "Epoch 90/100, Train Loss: 0.0361, Val Loss: 0.0423\n",
            "Epoch 91/100, Train Loss: 0.0372, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0360, Val Loss: 0.0423\n",
            "Epoch 93/100, Train Loss: 0.0377, Val Loss: 0.0423\n",
            "Epoch 94/100, Train Loss: 0.0371, Val Loss: 0.0423\n",
            "Epoch 95/100, Train Loss: 0.0402, Val Loss: 0.0423\n",
            "Epoch 96/100, Train Loss: 0.0386, Val Loss: 0.0423\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0366, Val Loss: 0.0423\n",
            "Epoch 98/100, Train Loss: 0.0390, Val Loss: 0.0423\n",
            "Epoch 99/100, Train Loss: 0.0368, Val Loss: 0.0423\n",
            "Epoch 100/100, Train Loss: 0.0370, Val Loss: 0.0423\n",
            "\n",
            "Test iteration for date: 2020-06-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1494, Val Loss: 0.2747\n",
            "Epoch 2/100, Train Loss: 0.1336, Val Loss: 0.3429\n",
            "Epoch 3/100, Train Loss: 0.1259, Val Loss: 0.3556\n",
            "Epoch 4/100, Train Loss: 0.1124, Val Loss: 0.2868\n",
            "Epoch 5/100, Train Loss: 0.1157, Val Loss: 0.2520\n",
            "Epoch 6/100, Train Loss: 0.0865, Val Loss: 0.0701\n",
            "Epoch 7/100, Train Loss: 0.0747, Val Loss: 0.0925\n",
            "Epoch 8/100, Train Loss: 0.0655, Val Loss: 0.0615\n",
            "Epoch 9/100, Train Loss: 0.0613, Val Loss: 0.0625\n",
            "Epoch 10/100, Train Loss: 0.0571, Val Loss: 0.0650\n",
            "Epoch 11/100, Train Loss: 0.0619, Val Loss: 0.0732\n",
            "Epoch 12/100, Train Loss: 0.0597, Val Loss: 0.0517\n",
            "Epoch 13/100, Train Loss: 0.0588, Val Loss: 0.0586\n",
            "Epoch 14/100, Train Loss: 0.0544, Val Loss: 0.0523\n",
            "Epoch 15/100, Train Loss: 0.0501, Val Loss: 0.0569\n",
            "Epoch 16/100, Train Loss: 0.0516, Val Loss: 0.0669\n",
            "Epoch 17/100, Train Loss: 0.0497, Val Loss: 0.0657\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0460, Val Loss: 0.0610\n",
            "Epoch 19/100, Train Loss: 0.0439, Val Loss: 0.0577\n",
            "Epoch 20/100, Train Loss: 0.0431, Val Loss: 0.0550\n",
            "Epoch 21/100, Train Loss: 0.0417, Val Loss: 0.0529\n",
            "Epoch 22/100, Train Loss: 0.0420, Val Loss: 0.0514\n",
            "Epoch 23/100, Train Loss: 0.0418, Val Loss: 0.0486\n",
            "Epoch 24/100, Train Loss: 0.0412, Val Loss: 0.0482\n",
            "Epoch 25/100, Train Loss: 0.0414, Val Loss: 0.0473\n",
            "Epoch 26/100, Train Loss: 0.0399, Val Loss: 0.0460\n",
            "Epoch 27/100, Train Loss: 0.0410, Val Loss: 0.0457\n",
            "Epoch 28/100, Train Loss: 0.0532, Val Loss: 0.0455\n",
            "Epoch 29/100, Train Loss: 0.0416, Val Loss: 0.0448\n",
            "Epoch 30/100, Train Loss: 0.0398, Val Loss: 0.0448\n",
            "Epoch 31/100, Train Loss: 0.0408, Val Loss: 0.0444\n",
            "Epoch 32/100, Train Loss: 0.0412, Val Loss: 0.0436\n",
            "Epoch 33/100, Train Loss: 0.0412, Val Loss: 0.0439\n",
            "Epoch 34/100, Train Loss: 0.0421, Val Loss: 0.0438\n",
            "Epoch 35/100, Train Loss: 0.0410, Val Loss: 0.0438\n",
            "Epoch 36/100, Train Loss: 0.0403, Val Loss: 0.0439\n",
            "Epoch 37/100, Train Loss: 0.0414, Val Loss: 0.0439\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0400, Val Loss: 0.0439\n",
            "Epoch 39/100, Train Loss: 0.0402, Val Loss: 0.0439\n",
            "Epoch 40/100, Train Loss: 0.0409, Val Loss: 0.0439\n",
            "Epoch 41/100, Train Loss: 0.0399, Val Loss: 0.0439\n",
            "Epoch 42/100, Train Loss: 0.0407, Val Loss: 0.0439\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0410, Val Loss: 0.0439\n",
            "Epoch 44/100, Train Loss: 0.0421, Val Loss: 0.0439\n",
            "Epoch 45/100, Train Loss: 0.0420, Val Loss: 0.0439\n",
            "Epoch 46/100, Train Loss: 0.0407, Val Loss: 0.0439\n",
            "Epoch 47/100, Train Loss: 0.0408, Val Loss: 0.0439\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0403, Val Loss: 0.0439\n",
            "Epoch 49/100, Train Loss: 0.0407, Val Loss: 0.0439\n",
            "Epoch 50/100, Train Loss: 0.0413, Val Loss: 0.0439\n",
            "Epoch 51/100, Train Loss: 0.0405, Val Loss: 0.0439\n",
            "Epoch 52/100, Train Loss: 0.0396, Val Loss: 0.0439\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0413, Val Loss: 0.0439\n",
            "Epoch 54/100, Train Loss: 0.0396, Val Loss: 0.0439\n",
            "Epoch 55/100, Train Loss: 0.0404, Val Loss: 0.0439\n",
            "Epoch 56/100, Train Loss: 0.0417, Val Loss: 0.0439\n",
            "Epoch 57/100, Train Loss: 0.0411, Val Loss: 0.0439\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0399, Val Loss: 0.0439\n",
            "Epoch 59/100, Train Loss: 0.0405, Val Loss: 0.0439\n",
            "Epoch 60/100, Train Loss: 0.0419, Val Loss: 0.0439\n",
            "Epoch 61/100, Train Loss: 0.0415, Val Loss: 0.0439\n",
            "Epoch 62/100, Train Loss: 0.0395, Val Loss: 0.0439\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0404, Val Loss: 0.0439\n",
            "Epoch 64/100, Train Loss: 0.0406, Val Loss: 0.0439\n",
            "Epoch 65/100, Train Loss: 0.0391, Val Loss: 0.0439\n",
            "Epoch 66/100, Train Loss: 0.0406, Val Loss: 0.0439\n",
            "Epoch 67/100, Train Loss: 0.0396, Val Loss: 0.0439\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0407, Val Loss: 0.0439\n",
            "Epoch 69/100, Train Loss: 0.0409, Val Loss: 0.0439\n",
            "Epoch 70/100, Train Loss: 0.0398, Val Loss: 0.0439\n",
            "Epoch 71/100, Train Loss: 0.0410, Val Loss: 0.0439\n",
            "Epoch 72/100, Train Loss: 0.0417, Val Loss: 0.0439\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0403, Val Loss: 0.0439\n",
            "Epoch 74/100, Train Loss: 0.0394, Val Loss: 0.0439\n",
            "Epoch 75/100, Train Loss: 0.0410, Val Loss: 0.0439\n",
            "Epoch 76/100, Train Loss: 0.0422, Val Loss: 0.0439\n",
            "Epoch 77/100, Train Loss: 0.0398, Val Loss: 0.0439\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0537, Val Loss: 0.0439\n",
            "Epoch 79/100, Train Loss: 0.0407, Val Loss: 0.0439\n",
            "Epoch 80/100, Train Loss: 0.0411, Val Loss: 0.0439\n",
            "Epoch 81/100, Train Loss: 0.0419, Val Loss: 0.0439\n",
            "Epoch 82/100, Train Loss: 0.0405, Val Loss: 0.0439\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0466, Val Loss: 0.0439\n",
            "Epoch 84/100, Train Loss: 0.0414, Val Loss: 0.0439\n",
            "Epoch 85/100, Train Loss: 0.0401, Val Loss: 0.0439\n",
            "Epoch 86/100, Train Loss: 0.0405, Val Loss: 0.0439\n",
            "Epoch 87/100, Train Loss: 0.0391, Val Loss: 0.0439\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0410, Val Loss: 0.0439\n",
            "Epoch 89/100, Train Loss: 0.0417, Val Loss: 0.0439\n",
            "Epoch 90/100, Train Loss: 0.0398, Val Loss: 0.0439\n",
            "Epoch 91/100, Train Loss: 0.0413, Val Loss: 0.0439\n",
            "Epoch 92/100, Train Loss: 0.0400, Val Loss: 0.0439\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0419, Val Loss: 0.0439\n",
            "Epoch 94/100, Train Loss: 0.0405, Val Loss: 0.0439\n",
            "Epoch 95/100, Train Loss: 0.0438, Val Loss: 0.0439\n",
            "Epoch 96/100, Train Loss: 0.0427, Val Loss: 0.0439\n",
            "Epoch 97/100, Train Loss: 0.0396, Val Loss: 0.0439\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0412, Val Loss: 0.0439\n",
            "Epoch 99/100, Train Loss: 0.0399, Val Loss: 0.0439\n",
            "Epoch 100/100, Train Loss: 0.0401, Val Loss: 0.0439\n",
            "\n",
            "Test iteration for date: 2020-07-01 00:00:00\n",
            "Current training set size: 126 samples\n",
            "Epoch 1/100, Train Loss: 0.1600, Val Loss: 0.3794\n",
            "Epoch 2/100, Train Loss: 0.1331, Val Loss: 0.3027\n",
            "Epoch 3/100, Train Loss: 0.1238, Val Loss: 0.2957\n",
            "Epoch 4/100, Train Loss: 0.1085, Val Loss: 0.0740\n",
            "Epoch 5/100, Train Loss: 0.0841, Val Loss: 0.0926\n",
            "Epoch 6/100, Train Loss: 0.0724, Val Loss: 0.0635\n",
            "Epoch 7/100, Train Loss: 0.0749, Val Loss: 0.1002\n",
            "Epoch 8/100, Train Loss: 0.0865, Val Loss: 0.0999\n",
            "Epoch 9/100, Train Loss: 0.0685, Val Loss: 0.0654\n",
            "Epoch 10/100, Train Loss: 0.0639, Val Loss: 0.0758\n",
            "Epoch 11/100, Train Loss: 0.0604, Val Loss: 0.0632\n",
            "Epoch 12/100, Train Loss: 0.0585, Val Loss: 0.0492\n",
            "Epoch 13/100, Train Loss: 0.0661, Val Loss: 0.0473\n",
            "Epoch 14/100, Train Loss: 0.0523, Val Loss: 0.0802\n",
            "Epoch 15/100, Train Loss: 0.0540, Val Loss: 0.0454\n",
            "Epoch 16/100, Train Loss: 0.0576, Val Loss: 0.0523\n",
            "Epoch 17/100, Train Loss: 0.0470, Val Loss: 0.0407\n",
            "Epoch 18/100, Train Loss: 0.0438, Val Loss: 0.0427\n",
            "Epoch 19/100, Train Loss: 0.0436, Val Loss: 0.0765\n",
            "Epoch 20/100, Train Loss: 0.0457, Val Loss: 0.0652\n",
            "Epoch 21/100, Train Loss: 0.0431, Val Loss: 0.0457\n",
            "Epoch 22/100, Train Loss: 0.0438, Val Loss: 0.0473\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 23/100, Train Loss: 0.0382, Val Loss: 0.0471\n",
            "Epoch 24/100, Train Loss: 0.0378, Val Loss: 0.0481\n",
            "Epoch 25/100, Train Loss: 0.0372, Val Loss: 0.0470\n",
            "Epoch 26/100, Train Loss: 0.0396, Val Loss: 0.0472\n",
            "Epoch 27/100, Train Loss: 0.0379, Val Loss: 0.0467\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 28/100, Train Loss: 0.0387, Val Loss: 0.0467\n",
            "Epoch 29/100, Train Loss: 0.0381, Val Loss: 0.0467\n",
            "Epoch 30/100, Train Loss: 0.0376, Val Loss: 0.0467\n",
            "Epoch 31/100, Train Loss: 0.0404, Val Loss: 0.0467\n",
            "Epoch 32/100, Train Loss: 0.0374, Val Loss: 0.0467\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 33/100, Train Loss: 0.0374, Val Loss: 0.0467\n",
            "Epoch 34/100, Train Loss: 0.0382, Val Loss: 0.0467\n",
            "Epoch 35/100, Train Loss: 0.0366, Val Loss: 0.0467\n",
            "Epoch 36/100, Train Loss: 0.0375, Val Loss: 0.0467\n",
            "Epoch 37/100, Train Loss: 0.0374, Val Loss: 0.0467\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0385, Val Loss: 0.0467\n",
            "Epoch 39/100, Train Loss: 0.0381, Val Loss: 0.0467\n",
            "Epoch 40/100, Train Loss: 0.0378, Val Loss: 0.0467\n",
            "Epoch 41/100, Train Loss: 0.0370, Val Loss: 0.0467\n",
            "Epoch 42/100, Train Loss: 0.0378, Val Loss: 0.0467\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0372, Val Loss: 0.0467\n",
            "Epoch 44/100, Train Loss: 0.0373, Val Loss: 0.0467\n",
            "Epoch 45/100, Train Loss: 0.0382, Val Loss: 0.0467\n",
            "Epoch 46/100, Train Loss: 0.0376, Val Loss: 0.0467\n",
            "Epoch 47/100, Train Loss: 0.0374, Val Loss: 0.0467\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0388, Val Loss: 0.0467\n",
            "Epoch 49/100, Train Loss: 0.0375, Val Loss: 0.0467\n",
            "Epoch 50/100, Train Loss: 0.0377, Val Loss: 0.0467\n",
            "Epoch 51/100, Train Loss: 0.0376, Val Loss: 0.0467\n",
            "Epoch 52/100, Train Loss: 0.0374, Val Loss: 0.0467\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0382, Val Loss: 0.0467\n",
            "Epoch 54/100, Train Loss: 0.0366, Val Loss: 0.0467\n",
            "Epoch 55/100, Train Loss: 0.0371, Val Loss: 0.0467\n",
            "Epoch 56/100, Train Loss: 0.0365, Val Loss: 0.0467\n",
            "Epoch 57/100, Train Loss: 0.0381, Val Loss: 0.0467\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0372, Val Loss: 0.0467\n",
            "Epoch 59/100, Train Loss: 0.0376, Val Loss: 0.0467\n",
            "Epoch 60/100, Train Loss: 0.0381, Val Loss: 0.0467\n",
            "Epoch 61/100, Train Loss: 0.0393, Val Loss: 0.0467\n",
            "Epoch 62/100, Train Loss: 0.0383, Val Loss: 0.0467\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0376, Val Loss: 0.0467\n",
            "Epoch 64/100, Train Loss: 0.0377, Val Loss: 0.0467\n",
            "Epoch 65/100, Train Loss: 0.0379, Val Loss: 0.0467\n",
            "Epoch 66/100, Train Loss: 0.0361, Val Loss: 0.0467\n",
            "Epoch 67/100, Train Loss: 0.0380, Val Loss: 0.0467\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0371, Val Loss: 0.0467\n",
            "Epoch 69/100, Train Loss: 0.0376, Val Loss: 0.0467\n",
            "Epoch 70/100, Train Loss: 0.0379, Val Loss: 0.0467\n",
            "Epoch 71/100, Train Loss: 0.0383, Val Loss: 0.0467\n",
            "Epoch 72/100, Train Loss: 0.0378, Val Loss: 0.0467\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0372, Val Loss: 0.0467\n",
            "Epoch 74/100, Train Loss: 0.0358, Val Loss: 0.0467\n",
            "Epoch 75/100, Train Loss: 0.0383, Val Loss: 0.0467\n",
            "Epoch 76/100, Train Loss: 0.0382, Val Loss: 0.0467\n",
            "Epoch 77/100, Train Loss: 0.0379, Val Loss: 0.0467\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0360, Val Loss: 0.0467\n",
            "Epoch 79/100, Train Loss: 0.0372, Val Loss: 0.0467\n",
            "Epoch 80/100, Train Loss: 0.0379, Val Loss: 0.0467\n",
            "Epoch 81/100, Train Loss: 0.0368, Val Loss: 0.0467\n",
            "Epoch 82/100, Train Loss: 0.0371, Val Loss: 0.0467\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0371, Val Loss: 0.0467\n",
            "Epoch 84/100, Train Loss: 0.0368, Val Loss: 0.0467\n",
            "Epoch 85/100, Train Loss: 0.0369, Val Loss: 0.0467\n",
            "Epoch 86/100, Train Loss: 0.0377, Val Loss: 0.0467\n",
            "Epoch 87/100, Train Loss: 0.0371, Val Loss: 0.0467\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0380, Val Loss: 0.0467\n",
            "Epoch 89/100, Train Loss: 0.0361, Val Loss: 0.0467\n",
            "Epoch 90/100, Train Loss: 0.0369, Val Loss: 0.0467\n",
            "Epoch 91/100, Train Loss: 0.0362, Val Loss: 0.0467\n",
            "Epoch 92/100, Train Loss: 0.0362, Val Loss: 0.0467\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0377, Val Loss: 0.0467\n",
            "Epoch 94/100, Train Loss: 0.0361, Val Loss: 0.0467\n",
            "Epoch 95/100, Train Loss: 0.0373, Val Loss: 0.0467\n",
            "Epoch 96/100, Train Loss: 0.0372, Val Loss: 0.0467\n",
            "Epoch 97/100, Train Loss: 0.0370, Val Loss: 0.0467\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0390, Val Loss: 0.0467\n",
            "Epoch 99/100, Train Loss: 0.0370, Val Loss: 0.0467\n",
            "Epoch 100/100, Train Loss: 0.0377, Val Loss: 0.0467\n",
            "\n",
            "Total training time for hybrid_lstmUnit512_gruUnit16_drop0.2_dense32_batch4_epochs100: 195.55 seconds (3.26 minutes)\n",
            "\n",
            "**************************************************\n",
            "Starting walk-forward validation with parameters:\n",
            "Model Type: hybrid, Units: L512_G16, Dropout: 0.2, Dense Units: 64\n",
            "Batch Size: 4, Epochs: 100\n",
            "Device: cuda\n",
            "Total test samples: 14\n",
            "**************************************************\n",
            "\n",
            "\n",
            "Test iteration for date: 2019-01-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1386, Val Loss: 0.2716\n",
            "Epoch 2/100, Train Loss: 0.1234, Val Loss: 0.3591\n",
            "Epoch 3/100, Train Loss: 0.1185, Val Loss: 0.3598\n",
            "Epoch 4/100, Train Loss: 0.1058, Val Loss: 0.3323\n",
            "Epoch 5/100, Train Loss: 0.1094, Val Loss: 0.2647\n",
            "Epoch 6/100, Train Loss: 0.0721, Val Loss: 0.1006\n",
            "Epoch 7/100, Train Loss: 0.0715, Val Loss: 0.0974\n",
            "Epoch 8/100, Train Loss: 0.0718, Val Loss: 0.1233\n",
            "Epoch 9/100, Train Loss: 0.0592, Val Loss: 0.0639\n",
            "Epoch 10/100, Train Loss: 0.0562, Val Loss: 0.0669\n",
            "Epoch 11/100, Train Loss: 0.0584, Val Loss: 0.0499\n",
            "Epoch 12/100, Train Loss: 0.0589, Val Loss: 0.0461\n",
            "Epoch 13/100, Train Loss: 0.0617, Val Loss: 0.0451\n",
            "Epoch 14/100, Train Loss: 0.0563, Val Loss: 0.0415\n",
            "Epoch 15/100, Train Loss: 0.0505, Val Loss: 0.0534\n",
            "Epoch 16/100, Train Loss: 0.0523, Val Loss: 0.0362\n",
            "Epoch 17/100, Train Loss: 0.0471, Val Loss: 0.0300\n",
            "Epoch 18/100, Train Loss: 0.0537, Val Loss: 0.0578\n",
            "Epoch 19/100, Train Loss: 0.0473, Val Loss: 0.0531\n",
            "Epoch 20/100, Train Loss: 0.0439, Val Loss: 0.0366\n",
            "Epoch 21/100, Train Loss: 0.0422, Val Loss: 0.0281\n",
            "Epoch 22/100, Train Loss: 0.0412, Val Loss: 0.0446\n",
            "Epoch 23/100, Train Loss: 0.0515, Val Loss: 0.0302\n",
            "Epoch 24/100, Train Loss: 0.0472, Val Loss: 0.0258\n",
            "Epoch 25/100, Train Loss: 0.0476, Val Loss: 0.0764\n",
            "Epoch 26/100, Train Loss: 0.0403, Val Loss: 0.0275\n",
            "Epoch 27/100, Train Loss: 0.0390, Val Loss: 0.0253\n",
            "Epoch 28/100, Train Loss: 0.0490, Val Loss: 0.0405\n",
            "Epoch 29/100, Train Loss: 0.0405, Val Loss: 0.0269\n",
            "Epoch 30/100, Train Loss: 0.0491, Val Loss: 0.0237\n",
            "Epoch 31/100, Train Loss: 0.0360, Val Loss: 0.0364\n",
            "Epoch 32/100, Train Loss: 0.0353, Val Loss: 0.0205\n",
            "Epoch 33/100, Train Loss: 0.0396, Val Loss: 0.0259\n",
            "Epoch 34/100, Train Loss: 0.0393, Val Loss: 0.0291\n",
            "Epoch 35/100, Train Loss: 0.0393, Val Loss: 0.0272\n",
            "Epoch 36/100, Train Loss: 0.0439, Val Loss: 0.0301\n",
            "Epoch 37/100, Train Loss: 0.0352, Val Loss: 0.0338\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0350, Val Loss: 0.0318\n",
            "Epoch 39/100, Train Loss: 0.0339, Val Loss: 0.0292\n",
            "Epoch 40/100, Train Loss: 0.0331, Val Loss: 0.0280\n",
            "Epoch 41/100, Train Loss: 0.0347, Val Loss: 0.0277\n",
            "Epoch 42/100, Train Loss: 0.0331, Val Loss: 0.0271\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0344, Val Loss: 0.0271\n",
            "Epoch 44/100, Train Loss: 0.0344, Val Loss: 0.0270\n",
            "Epoch 45/100, Train Loss: 0.0335, Val Loss: 0.0270\n",
            "Epoch 46/100, Train Loss: 0.0322, Val Loss: 0.0270\n",
            "Epoch 47/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 49/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 50/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 51/100, Train Loss: 0.0331, Val Loss: 0.0270\n",
            "Epoch 52/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Epoch 54/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 55/100, Train Loss: 0.0333, Val Loss: 0.0270\n",
            "Epoch 56/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 57/100, Train Loss: 0.0333, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "Epoch 59/100, Train Loss: 0.0326, Val Loss: 0.0270\n",
            "Epoch 60/100, Train Loss: 0.0337, Val Loss: 0.0270\n",
            "Epoch 61/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 62/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0331, Val Loss: 0.0270\n",
            "Epoch 64/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 65/100, Train Loss: 0.0312, Val Loss: 0.0270\n",
            "Epoch 66/100, Train Loss: 0.0313, Val Loss: 0.0270\n",
            "Epoch 67/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 69/100, Train Loss: 0.0312, Val Loss: 0.0270\n",
            "Epoch 70/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 71/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 72/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0332, Val Loss: 0.0270\n",
            "Epoch 74/100, Train Loss: 0.0319, Val Loss: 0.0270\n",
            "Epoch 75/100, Train Loss: 0.0341, Val Loss: 0.0270\n",
            "Epoch 76/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 77/100, Train Loss: 0.0313, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0449, Val Loss: 0.0270\n",
            "Epoch 79/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Epoch 80/100, Train Loss: 0.0310, Val Loss: 0.0270\n",
            "Epoch 81/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 82/100, Train Loss: 0.0319, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0374, Val Loss: 0.0270\n",
            "Epoch 84/100, Train Loss: 0.0330, Val Loss: 0.0270\n",
            "Epoch 85/100, Train Loss: 0.0326, Val Loss: 0.0270\n",
            "Epoch 86/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Epoch 87/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0322, Val Loss: 0.0270\n",
            "Epoch 89/100, Train Loss: 0.0328, Val Loss: 0.0270\n",
            "Epoch 90/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 91/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 92/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 94/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Epoch 95/100, Train Loss: 0.0356, Val Loss: 0.0270\n",
            "Epoch 96/100, Train Loss: 0.0340, Val Loss: 0.0270\n",
            "Epoch 97/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0338, Val Loss: 0.0270\n",
            "Epoch 99/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 100/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "\n",
            "Test iteration for date: 2019-02-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1386, Val Loss: 0.2716\n",
            "Epoch 2/100, Train Loss: 0.1234, Val Loss: 0.3591\n",
            "Epoch 3/100, Train Loss: 0.1185, Val Loss: 0.3598\n",
            "Epoch 4/100, Train Loss: 0.1058, Val Loss: 0.3323\n",
            "Epoch 5/100, Train Loss: 0.1094, Val Loss: 0.2647\n",
            "Epoch 6/100, Train Loss: 0.0721, Val Loss: 0.1006\n",
            "Epoch 7/100, Train Loss: 0.0715, Val Loss: 0.0974\n",
            "Epoch 8/100, Train Loss: 0.0718, Val Loss: 0.1233\n",
            "Epoch 9/100, Train Loss: 0.0592, Val Loss: 0.0639\n",
            "Epoch 10/100, Train Loss: 0.0562, Val Loss: 0.0669\n",
            "Epoch 11/100, Train Loss: 0.0584, Val Loss: 0.0499\n",
            "Epoch 12/100, Train Loss: 0.0589, Val Loss: 0.0461\n",
            "Epoch 13/100, Train Loss: 0.0617, Val Loss: 0.0451\n",
            "Epoch 14/100, Train Loss: 0.0563, Val Loss: 0.0415\n",
            "Epoch 15/100, Train Loss: 0.0505, Val Loss: 0.0534\n",
            "Epoch 16/100, Train Loss: 0.0523, Val Loss: 0.0362\n",
            "Epoch 17/100, Train Loss: 0.0471, Val Loss: 0.0300\n",
            "Epoch 18/100, Train Loss: 0.0537, Val Loss: 0.0578\n",
            "Epoch 19/100, Train Loss: 0.0473, Val Loss: 0.0531\n",
            "Epoch 20/100, Train Loss: 0.0439, Val Loss: 0.0366\n",
            "Epoch 21/100, Train Loss: 0.0422, Val Loss: 0.0281\n",
            "Epoch 22/100, Train Loss: 0.0412, Val Loss: 0.0446\n",
            "Epoch 23/100, Train Loss: 0.0515, Val Loss: 0.0302\n",
            "Epoch 24/100, Train Loss: 0.0472, Val Loss: 0.0258\n",
            "Epoch 25/100, Train Loss: 0.0476, Val Loss: 0.0764\n",
            "Epoch 26/100, Train Loss: 0.0403, Val Loss: 0.0275\n",
            "Epoch 27/100, Train Loss: 0.0390, Val Loss: 0.0253\n",
            "Epoch 28/100, Train Loss: 0.0490, Val Loss: 0.0405\n",
            "Epoch 29/100, Train Loss: 0.0405, Val Loss: 0.0269\n",
            "Epoch 30/100, Train Loss: 0.0491, Val Loss: 0.0237\n",
            "Epoch 31/100, Train Loss: 0.0360, Val Loss: 0.0364\n",
            "Epoch 32/100, Train Loss: 0.0353, Val Loss: 0.0205\n",
            "Epoch 33/100, Train Loss: 0.0396, Val Loss: 0.0259\n",
            "Epoch 34/100, Train Loss: 0.0393, Val Loss: 0.0291\n",
            "Epoch 35/100, Train Loss: 0.0393, Val Loss: 0.0272\n",
            "Epoch 36/100, Train Loss: 0.0439, Val Loss: 0.0301\n",
            "Epoch 37/100, Train Loss: 0.0352, Val Loss: 0.0338\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0350, Val Loss: 0.0318\n",
            "Epoch 39/100, Train Loss: 0.0339, Val Loss: 0.0292\n",
            "Epoch 40/100, Train Loss: 0.0331, Val Loss: 0.0280\n",
            "Epoch 41/100, Train Loss: 0.0347, Val Loss: 0.0277\n",
            "Epoch 42/100, Train Loss: 0.0331, Val Loss: 0.0271\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0344, Val Loss: 0.0271\n",
            "Epoch 44/100, Train Loss: 0.0344, Val Loss: 0.0270\n",
            "Epoch 45/100, Train Loss: 0.0335, Val Loss: 0.0270\n",
            "Epoch 46/100, Train Loss: 0.0322, Val Loss: 0.0270\n",
            "Epoch 47/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 49/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 50/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 51/100, Train Loss: 0.0331, Val Loss: 0.0270\n",
            "Epoch 52/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Epoch 54/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 55/100, Train Loss: 0.0333, Val Loss: 0.0270\n",
            "Epoch 56/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 57/100, Train Loss: 0.0333, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "Epoch 59/100, Train Loss: 0.0326, Val Loss: 0.0270\n",
            "Epoch 60/100, Train Loss: 0.0337, Val Loss: 0.0270\n",
            "Epoch 61/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 62/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0331, Val Loss: 0.0270\n",
            "Epoch 64/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 65/100, Train Loss: 0.0312, Val Loss: 0.0270\n",
            "Epoch 66/100, Train Loss: 0.0313, Val Loss: 0.0270\n",
            "Epoch 67/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 69/100, Train Loss: 0.0312, Val Loss: 0.0270\n",
            "Epoch 70/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 71/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 72/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0332, Val Loss: 0.0270\n",
            "Epoch 74/100, Train Loss: 0.0319, Val Loss: 0.0270\n",
            "Epoch 75/100, Train Loss: 0.0341, Val Loss: 0.0270\n",
            "Epoch 76/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 77/100, Train Loss: 0.0313, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0449, Val Loss: 0.0270\n",
            "Epoch 79/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Epoch 80/100, Train Loss: 0.0310, Val Loss: 0.0270\n",
            "Epoch 81/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 82/100, Train Loss: 0.0319, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0374, Val Loss: 0.0270\n",
            "Epoch 84/100, Train Loss: 0.0330, Val Loss: 0.0270\n",
            "Epoch 85/100, Train Loss: 0.0326, Val Loss: 0.0270\n",
            "Epoch 86/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Epoch 87/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0322, Val Loss: 0.0270\n",
            "Epoch 89/100, Train Loss: 0.0328, Val Loss: 0.0270\n",
            "Epoch 90/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 91/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 92/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 94/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Epoch 95/100, Train Loss: 0.0356, Val Loss: 0.0270\n",
            "Epoch 96/100, Train Loss: 0.0340, Val Loss: 0.0270\n",
            "Epoch 97/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0338, Val Loss: 0.0270\n",
            "Epoch 99/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 100/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "\n",
            "Test iteration for date: 2019-03-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1386, Val Loss: 0.2716\n",
            "Epoch 2/100, Train Loss: 0.1234, Val Loss: 0.3591\n",
            "Epoch 3/100, Train Loss: 0.1185, Val Loss: 0.3598\n",
            "Epoch 4/100, Train Loss: 0.1058, Val Loss: 0.3323\n",
            "Epoch 5/100, Train Loss: 0.1094, Val Loss: 0.2647\n",
            "Epoch 6/100, Train Loss: 0.0721, Val Loss: 0.1006\n",
            "Epoch 7/100, Train Loss: 0.0715, Val Loss: 0.0974\n",
            "Epoch 8/100, Train Loss: 0.0718, Val Loss: 0.1233\n",
            "Epoch 9/100, Train Loss: 0.0592, Val Loss: 0.0639\n",
            "Epoch 10/100, Train Loss: 0.0562, Val Loss: 0.0669\n",
            "Epoch 11/100, Train Loss: 0.0584, Val Loss: 0.0499\n",
            "Epoch 12/100, Train Loss: 0.0589, Val Loss: 0.0461\n",
            "Epoch 13/100, Train Loss: 0.0617, Val Loss: 0.0451\n",
            "Epoch 14/100, Train Loss: 0.0563, Val Loss: 0.0415\n",
            "Epoch 15/100, Train Loss: 0.0505, Val Loss: 0.0534\n",
            "Epoch 16/100, Train Loss: 0.0523, Val Loss: 0.0362\n",
            "Epoch 17/100, Train Loss: 0.0471, Val Loss: 0.0300\n",
            "Epoch 18/100, Train Loss: 0.0537, Val Loss: 0.0578\n",
            "Epoch 19/100, Train Loss: 0.0473, Val Loss: 0.0531\n",
            "Epoch 20/100, Train Loss: 0.0439, Val Loss: 0.0366\n",
            "Epoch 21/100, Train Loss: 0.0422, Val Loss: 0.0281\n",
            "Epoch 22/100, Train Loss: 0.0412, Val Loss: 0.0446\n",
            "Epoch 23/100, Train Loss: 0.0515, Val Loss: 0.0302\n",
            "Epoch 24/100, Train Loss: 0.0472, Val Loss: 0.0258\n",
            "Epoch 25/100, Train Loss: 0.0476, Val Loss: 0.0764\n",
            "Epoch 26/100, Train Loss: 0.0403, Val Loss: 0.0275\n",
            "Epoch 27/100, Train Loss: 0.0390, Val Loss: 0.0253\n",
            "Epoch 28/100, Train Loss: 0.0490, Val Loss: 0.0405\n",
            "Epoch 29/100, Train Loss: 0.0405, Val Loss: 0.0269\n",
            "Epoch 30/100, Train Loss: 0.0491, Val Loss: 0.0237\n",
            "Epoch 31/100, Train Loss: 0.0360, Val Loss: 0.0364\n",
            "Epoch 32/100, Train Loss: 0.0353, Val Loss: 0.0205\n",
            "Epoch 33/100, Train Loss: 0.0396, Val Loss: 0.0259\n",
            "Epoch 34/100, Train Loss: 0.0393, Val Loss: 0.0291\n",
            "Epoch 35/100, Train Loss: 0.0393, Val Loss: 0.0272\n",
            "Epoch 36/100, Train Loss: 0.0439, Val Loss: 0.0301\n",
            "Epoch 37/100, Train Loss: 0.0352, Val Loss: 0.0338\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0350, Val Loss: 0.0318\n",
            "Epoch 39/100, Train Loss: 0.0339, Val Loss: 0.0292\n",
            "Epoch 40/100, Train Loss: 0.0331, Val Loss: 0.0280\n",
            "Epoch 41/100, Train Loss: 0.0347, Val Loss: 0.0277\n",
            "Epoch 42/100, Train Loss: 0.0331, Val Loss: 0.0271\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0344, Val Loss: 0.0271\n",
            "Epoch 44/100, Train Loss: 0.0344, Val Loss: 0.0270\n",
            "Epoch 45/100, Train Loss: 0.0335, Val Loss: 0.0270\n",
            "Epoch 46/100, Train Loss: 0.0322, Val Loss: 0.0270\n",
            "Epoch 47/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 49/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 50/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 51/100, Train Loss: 0.0331, Val Loss: 0.0270\n",
            "Epoch 52/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Epoch 54/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 55/100, Train Loss: 0.0333, Val Loss: 0.0270\n",
            "Epoch 56/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 57/100, Train Loss: 0.0333, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "Epoch 59/100, Train Loss: 0.0326, Val Loss: 0.0270\n",
            "Epoch 60/100, Train Loss: 0.0337, Val Loss: 0.0270\n",
            "Epoch 61/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 62/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0331, Val Loss: 0.0270\n",
            "Epoch 64/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 65/100, Train Loss: 0.0312, Val Loss: 0.0270\n",
            "Epoch 66/100, Train Loss: 0.0313, Val Loss: 0.0270\n",
            "Epoch 67/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 69/100, Train Loss: 0.0312, Val Loss: 0.0270\n",
            "Epoch 70/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 71/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 72/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0332, Val Loss: 0.0270\n",
            "Epoch 74/100, Train Loss: 0.0319, Val Loss: 0.0270\n",
            "Epoch 75/100, Train Loss: 0.0341, Val Loss: 0.0270\n",
            "Epoch 76/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 77/100, Train Loss: 0.0313, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0449, Val Loss: 0.0270\n",
            "Epoch 79/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Epoch 80/100, Train Loss: 0.0310, Val Loss: 0.0270\n",
            "Epoch 81/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 82/100, Train Loss: 0.0319, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0374, Val Loss: 0.0270\n",
            "Epoch 84/100, Train Loss: 0.0330, Val Loss: 0.0270\n",
            "Epoch 85/100, Train Loss: 0.0326, Val Loss: 0.0270\n",
            "Epoch 86/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Epoch 87/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0322, Val Loss: 0.0270\n",
            "Epoch 89/100, Train Loss: 0.0328, Val Loss: 0.0270\n",
            "Epoch 90/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 91/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 92/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 94/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Epoch 95/100, Train Loss: 0.0356, Val Loss: 0.0270\n",
            "Epoch 96/100, Train Loss: 0.0340, Val Loss: 0.0270\n",
            "Epoch 97/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0338, Val Loss: 0.0270\n",
            "Epoch 99/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 100/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "\n",
            "Test iteration for date: 2019-04-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1386, Val Loss: 0.2716\n",
            "Epoch 2/100, Train Loss: 0.1234, Val Loss: 0.3591\n",
            "Epoch 3/100, Train Loss: 0.1185, Val Loss: 0.3598\n",
            "Epoch 4/100, Train Loss: 0.1058, Val Loss: 0.3323\n",
            "Epoch 5/100, Train Loss: 0.1094, Val Loss: 0.2647\n",
            "Epoch 6/100, Train Loss: 0.0721, Val Loss: 0.1006\n",
            "Epoch 7/100, Train Loss: 0.0715, Val Loss: 0.0974\n",
            "Epoch 8/100, Train Loss: 0.0718, Val Loss: 0.1233\n",
            "Epoch 9/100, Train Loss: 0.0592, Val Loss: 0.0639\n",
            "Epoch 10/100, Train Loss: 0.0562, Val Loss: 0.0669\n",
            "Epoch 11/100, Train Loss: 0.0584, Val Loss: 0.0499\n",
            "Epoch 12/100, Train Loss: 0.0589, Val Loss: 0.0461\n",
            "Epoch 13/100, Train Loss: 0.0617, Val Loss: 0.0451\n",
            "Epoch 14/100, Train Loss: 0.0563, Val Loss: 0.0415\n",
            "Epoch 15/100, Train Loss: 0.0505, Val Loss: 0.0534\n",
            "Epoch 16/100, Train Loss: 0.0523, Val Loss: 0.0362\n",
            "Epoch 17/100, Train Loss: 0.0471, Val Loss: 0.0300\n",
            "Epoch 18/100, Train Loss: 0.0537, Val Loss: 0.0578\n",
            "Epoch 19/100, Train Loss: 0.0473, Val Loss: 0.0531\n",
            "Epoch 20/100, Train Loss: 0.0439, Val Loss: 0.0366\n",
            "Epoch 21/100, Train Loss: 0.0422, Val Loss: 0.0281\n",
            "Epoch 22/100, Train Loss: 0.0412, Val Loss: 0.0446\n",
            "Epoch 23/100, Train Loss: 0.0515, Val Loss: 0.0302\n",
            "Epoch 24/100, Train Loss: 0.0472, Val Loss: 0.0258\n",
            "Epoch 25/100, Train Loss: 0.0476, Val Loss: 0.0764\n",
            "Epoch 26/100, Train Loss: 0.0403, Val Loss: 0.0275\n",
            "Epoch 27/100, Train Loss: 0.0390, Val Loss: 0.0253\n",
            "Epoch 28/100, Train Loss: 0.0490, Val Loss: 0.0405\n",
            "Epoch 29/100, Train Loss: 0.0405, Val Loss: 0.0269\n",
            "Epoch 30/100, Train Loss: 0.0491, Val Loss: 0.0237\n",
            "Epoch 31/100, Train Loss: 0.0360, Val Loss: 0.0364\n",
            "Epoch 32/100, Train Loss: 0.0353, Val Loss: 0.0205\n",
            "Epoch 33/100, Train Loss: 0.0396, Val Loss: 0.0259\n",
            "Epoch 34/100, Train Loss: 0.0393, Val Loss: 0.0291\n",
            "Epoch 35/100, Train Loss: 0.0393, Val Loss: 0.0272\n",
            "Epoch 36/100, Train Loss: 0.0439, Val Loss: 0.0301\n",
            "Epoch 37/100, Train Loss: 0.0352, Val Loss: 0.0338\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0350, Val Loss: 0.0318\n",
            "Epoch 39/100, Train Loss: 0.0339, Val Loss: 0.0292\n",
            "Epoch 40/100, Train Loss: 0.0331, Val Loss: 0.0280\n",
            "Epoch 41/100, Train Loss: 0.0347, Val Loss: 0.0277\n",
            "Epoch 42/100, Train Loss: 0.0331, Val Loss: 0.0271\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0344, Val Loss: 0.0271\n",
            "Epoch 44/100, Train Loss: 0.0344, Val Loss: 0.0270\n",
            "Epoch 45/100, Train Loss: 0.0335, Val Loss: 0.0270\n",
            "Epoch 46/100, Train Loss: 0.0322, Val Loss: 0.0270\n",
            "Epoch 47/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 49/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 50/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 51/100, Train Loss: 0.0331, Val Loss: 0.0270\n",
            "Epoch 52/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Epoch 54/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 55/100, Train Loss: 0.0333, Val Loss: 0.0270\n",
            "Epoch 56/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 57/100, Train Loss: 0.0333, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "Epoch 59/100, Train Loss: 0.0326, Val Loss: 0.0270\n",
            "Epoch 60/100, Train Loss: 0.0337, Val Loss: 0.0270\n",
            "Epoch 61/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 62/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0331, Val Loss: 0.0270\n",
            "Epoch 64/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 65/100, Train Loss: 0.0312, Val Loss: 0.0270\n",
            "Epoch 66/100, Train Loss: 0.0313, Val Loss: 0.0270\n",
            "Epoch 67/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 69/100, Train Loss: 0.0312, Val Loss: 0.0270\n",
            "Epoch 70/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 71/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 72/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0332, Val Loss: 0.0270\n",
            "Epoch 74/100, Train Loss: 0.0319, Val Loss: 0.0270\n",
            "Epoch 75/100, Train Loss: 0.0341, Val Loss: 0.0270\n",
            "Epoch 76/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 77/100, Train Loss: 0.0313, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0449, Val Loss: 0.0270\n",
            "Epoch 79/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Epoch 80/100, Train Loss: 0.0310, Val Loss: 0.0270\n",
            "Epoch 81/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 82/100, Train Loss: 0.0319, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0374, Val Loss: 0.0270\n",
            "Epoch 84/100, Train Loss: 0.0330, Val Loss: 0.0270\n",
            "Epoch 85/100, Train Loss: 0.0326, Val Loss: 0.0270\n",
            "Epoch 86/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Epoch 87/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0322, Val Loss: 0.0270\n",
            "Epoch 89/100, Train Loss: 0.0328, Val Loss: 0.0270\n",
            "Epoch 90/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 91/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 92/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 94/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Epoch 95/100, Train Loss: 0.0356, Val Loss: 0.0270\n",
            "Epoch 96/100, Train Loss: 0.0340, Val Loss: 0.0270\n",
            "Epoch 97/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0338, Val Loss: 0.0270\n",
            "Epoch 99/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 100/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "\n",
            "Test iteration for date: 2019-05-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1386, Val Loss: 0.2716\n",
            "Epoch 2/100, Train Loss: 0.1234, Val Loss: 0.3591\n",
            "Epoch 3/100, Train Loss: 0.1185, Val Loss: 0.3598\n",
            "Epoch 4/100, Train Loss: 0.1058, Val Loss: 0.3323\n",
            "Epoch 5/100, Train Loss: 0.1094, Val Loss: 0.2647\n",
            "Epoch 6/100, Train Loss: 0.0721, Val Loss: 0.1006\n",
            "Epoch 7/100, Train Loss: 0.0715, Val Loss: 0.0974\n",
            "Epoch 8/100, Train Loss: 0.0718, Val Loss: 0.1233\n",
            "Epoch 9/100, Train Loss: 0.0592, Val Loss: 0.0639\n",
            "Epoch 10/100, Train Loss: 0.0562, Val Loss: 0.0669\n",
            "Epoch 11/100, Train Loss: 0.0584, Val Loss: 0.0499\n",
            "Epoch 12/100, Train Loss: 0.0589, Val Loss: 0.0461\n",
            "Epoch 13/100, Train Loss: 0.0617, Val Loss: 0.0451\n",
            "Epoch 14/100, Train Loss: 0.0563, Val Loss: 0.0415\n",
            "Epoch 15/100, Train Loss: 0.0505, Val Loss: 0.0534\n",
            "Epoch 16/100, Train Loss: 0.0523, Val Loss: 0.0362\n",
            "Epoch 17/100, Train Loss: 0.0471, Val Loss: 0.0300\n",
            "Epoch 18/100, Train Loss: 0.0537, Val Loss: 0.0578\n",
            "Epoch 19/100, Train Loss: 0.0473, Val Loss: 0.0531\n",
            "Epoch 20/100, Train Loss: 0.0439, Val Loss: 0.0366\n",
            "Epoch 21/100, Train Loss: 0.0422, Val Loss: 0.0281\n",
            "Epoch 22/100, Train Loss: 0.0412, Val Loss: 0.0446\n",
            "Epoch 23/100, Train Loss: 0.0515, Val Loss: 0.0302\n",
            "Epoch 24/100, Train Loss: 0.0472, Val Loss: 0.0258\n",
            "Epoch 25/100, Train Loss: 0.0476, Val Loss: 0.0764\n",
            "Epoch 26/100, Train Loss: 0.0403, Val Loss: 0.0275\n",
            "Epoch 27/100, Train Loss: 0.0390, Val Loss: 0.0253\n",
            "Epoch 28/100, Train Loss: 0.0490, Val Loss: 0.0405\n",
            "Epoch 29/100, Train Loss: 0.0405, Val Loss: 0.0269\n",
            "Epoch 30/100, Train Loss: 0.0491, Val Loss: 0.0237\n",
            "Epoch 31/100, Train Loss: 0.0360, Val Loss: 0.0364\n",
            "Epoch 32/100, Train Loss: 0.0353, Val Loss: 0.0205\n",
            "Epoch 33/100, Train Loss: 0.0396, Val Loss: 0.0259\n",
            "Epoch 34/100, Train Loss: 0.0393, Val Loss: 0.0291\n",
            "Epoch 35/100, Train Loss: 0.0393, Val Loss: 0.0272\n",
            "Epoch 36/100, Train Loss: 0.0439, Val Loss: 0.0301\n",
            "Epoch 37/100, Train Loss: 0.0352, Val Loss: 0.0338\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0350, Val Loss: 0.0318\n",
            "Epoch 39/100, Train Loss: 0.0339, Val Loss: 0.0292\n",
            "Epoch 40/100, Train Loss: 0.0331, Val Loss: 0.0280\n",
            "Epoch 41/100, Train Loss: 0.0347, Val Loss: 0.0277\n",
            "Epoch 42/100, Train Loss: 0.0331, Val Loss: 0.0271\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0344, Val Loss: 0.0271\n",
            "Epoch 44/100, Train Loss: 0.0344, Val Loss: 0.0270\n",
            "Epoch 45/100, Train Loss: 0.0335, Val Loss: 0.0270\n",
            "Epoch 46/100, Train Loss: 0.0322, Val Loss: 0.0270\n",
            "Epoch 47/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 49/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 50/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 51/100, Train Loss: 0.0331, Val Loss: 0.0270\n",
            "Epoch 52/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Epoch 54/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 55/100, Train Loss: 0.0333, Val Loss: 0.0270\n",
            "Epoch 56/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 57/100, Train Loss: 0.0333, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "Epoch 59/100, Train Loss: 0.0326, Val Loss: 0.0270\n",
            "Epoch 60/100, Train Loss: 0.0337, Val Loss: 0.0270\n",
            "Epoch 61/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 62/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0331, Val Loss: 0.0270\n",
            "Epoch 64/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 65/100, Train Loss: 0.0312, Val Loss: 0.0270\n",
            "Epoch 66/100, Train Loss: 0.0313, Val Loss: 0.0270\n",
            "Epoch 67/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 69/100, Train Loss: 0.0312, Val Loss: 0.0270\n",
            "Epoch 70/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 71/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 72/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0332, Val Loss: 0.0270\n",
            "Epoch 74/100, Train Loss: 0.0319, Val Loss: 0.0270\n",
            "Epoch 75/100, Train Loss: 0.0341, Val Loss: 0.0270\n",
            "Epoch 76/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 77/100, Train Loss: 0.0313, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0449, Val Loss: 0.0270\n",
            "Epoch 79/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Epoch 80/100, Train Loss: 0.0310, Val Loss: 0.0270\n",
            "Epoch 81/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 82/100, Train Loss: 0.0319, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0374, Val Loss: 0.0270\n",
            "Epoch 84/100, Train Loss: 0.0330, Val Loss: 0.0270\n",
            "Epoch 85/100, Train Loss: 0.0326, Val Loss: 0.0270\n",
            "Epoch 86/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Epoch 87/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0322, Val Loss: 0.0270\n",
            "Epoch 89/100, Train Loss: 0.0328, Val Loss: 0.0270\n",
            "Epoch 90/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 91/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 92/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 94/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Epoch 95/100, Train Loss: 0.0356, Val Loss: 0.0270\n",
            "Epoch 96/100, Train Loss: 0.0340, Val Loss: 0.0270\n",
            "Epoch 97/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0338, Val Loss: 0.0270\n",
            "Epoch 99/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 100/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "\n",
            "Test iteration for date: 2019-06-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1386, Val Loss: 0.2716\n",
            "Epoch 2/100, Train Loss: 0.1234, Val Loss: 0.3591\n",
            "Epoch 3/100, Train Loss: 0.1185, Val Loss: 0.3598\n",
            "Epoch 4/100, Train Loss: 0.1058, Val Loss: 0.3323\n",
            "Epoch 5/100, Train Loss: 0.1094, Val Loss: 0.2647\n",
            "Epoch 6/100, Train Loss: 0.0721, Val Loss: 0.1006\n",
            "Epoch 7/100, Train Loss: 0.0715, Val Loss: 0.0974\n",
            "Epoch 8/100, Train Loss: 0.0718, Val Loss: 0.1233\n",
            "Epoch 9/100, Train Loss: 0.0592, Val Loss: 0.0639\n",
            "Epoch 10/100, Train Loss: 0.0562, Val Loss: 0.0669\n",
            "Epoch 11/100, Train Loss: 0.0584, Val Loss: 0.0499\n",
            "Epoch 12/100, Train Loss: 0.0589, Val Loss: 0.0461\n",
            "Epoch 13/100, Train Loss: 0.0617, Val Loss: 0.0451\n",
            "Epoch 14/100, Train Loss: 0.0563, Val Loss: 0.0415\n",
            "Epoch 15/100, Train Loss: 0.0505, Val Loss: 0.0534\n",
            "Epoch 16/100, Train Loss: 0.0523, Val Loss: 0.0362\n",
            "Epoch 17/100, Train Loss: 0.0471, Val Loss: 0.0300\n",
            "Epoch 18/100, Train Loss: 0.0537, Val Loss: 0.0578\n",
            "Epoch 19/100, Train Loss: 0.0473, Val Loss: 0.0531\n",
            "Epoch 20/100, Train Loss: 0.0439, Val Loss: 0.0366\n",
            "Epoch 21/100, Train Loss: 0.0422, Val Loss: 0.0281\n",
            "Epoch 22/100, Train Loss: 0.0412, Val Loss: 0.0446\n",
            "Epoch 23/100, Train Loss: 0.0515, Val Loss: 0.0302\n",
            "Epoch 24/100, Train Loss: 0.0472, Val Loss: 0.0258\n",
            "Epoch 25/100, Train Loss: 0.0476, Val Loss: 0.0764\n",
            "Epoch 26/100, Train Loss: 0.0403, Val Loss: 0.0275\n",
            "Epoch 27/100, Train Loss: 0.0390, Val Loss: 0.0253\n",
            "Epoch 28/100, Train Loss: 0.0490, Val Loss: 0.0405\n",
            "Epoch 29/100, Train Loss: 0.0405, Val Loss: 0.0269\n",
            "Epoch 30/100, Train Loss: 0.0491, Val Loss: 0.0237\n",
            "Epoch 31/100, Train Loss: 0.0360, Val Loss: 0.0364\n",
            "Epoch 32/100, Train Loss: 0.0353, Val Loss: 0.0205\n",
            "Epoch 33/100, Train Loss: 0.0396, Val Loss: 0.0259\n",
            "Epoch 34/100, Train Loss: 0.0393, Val Loss: 0.0291\n",
            "Epoch 35/100, Train Loss: 0.0393, Val Loss: 0.0272\n",
            "Epoch 36/100, Train Loss: 0.0439, Val Loss: 0.0301\n",
            "Epoch 37/100, Train Loss: 0.0352, Val Loss: 0.0338\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0350, Val Loss: 0.0318\n",
            "Epoch 39/100, Train Loss: 0.0339, Val Loss: 0.0292\n",
            "Epoch 40/100, Train Loss: 0.0331, Val Loss: 0.0280\n",
            "Epoch 41/100, Train Loss: 0.0347, Val Loss: 0.0277\n",
            "Epoch 42/100, Train Loss: 0.0331, Val Loss: 0.0271\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0344, Val Loss: 0.0271\n",
            "Epoch 44/100, Train Loss: 0.0344, Val Loss: 0.0270\n",
            "Epoch 45/100, Train Loss: 0.0335, Val Loss: 0.0270\n",
            "Epoch 46/100, Train Loss: 0.0322, Val Loss: 0.0270\n",
            "Epoch 47/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 49/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 50/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 51/100, Train Loss: 0.0331, Val Loss: 0.0270\n",
            "Epoch 52/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Epoch 54/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 55/100, Train Loss: 0.0333, Val Loss: 0.0270\n",
            "Epoch 56/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 57/100, Train Loss: 0.0333, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "Epoch 59/100, Train Loss: 0.0326, Val Loss: 0.0270\n",
            "Epoch 60/100, Train Loss: 0.0337, Val Loss: 0.0270\n",
            "Epoch 61/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 62/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0331, Val Loss: 0.0270\n",
            "Epoch 64/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 65/100, Train Loss: 0.0312, Val Loss: 0.0270\n",
            "Epoch 66/100, Train Loss: 0.0313, Val Loss: 0.0270\n",
            "Epoch 67/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 69/100, Train Loss: 0.0312, Val Loss: 0.0270\n",
            "Epoch 70/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 71/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 72/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0332, Val Loss: 0.0270\n",
            "Epoch 74/100, Train Loss: 0.0319, Val Loss: 0.0270\n",
            "Epoch 75/100, Train Loss: 0.0341, Val Loss: 0.0270\n",
            "Epoch 76/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 77/100, Train Loss: 0.0313, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0449, Val Loss: 0.0270\n",
            "Epoch 79/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Epoch 80/100, Train Loss: 0.0310, Val Loss: 0.0270\n",
            "Epoch 81/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 82/100, Train Loss: 0.0319, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0374, Val Loss: 0.0270\n",
            "Epoch 84/100, Train Loss: 0.0330, Val Loss: 0.0270\n",
            "Epoch 85/100, Train Loss: 0.0326, Val Loss: 0.0270\n",
            "Epoch 86/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Epoch 87/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0322, Val Loss: 0.0270\n",
            "Epoch 89/100, Train Loss: 0.0328, Val Loss: 0.0270\n",
            "Epoch 90/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 91/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 92/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 94/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Epoch 95/100, Train Loss: 0.0356, Val Loss: 0.0270\n",
            "Epoch 96/100, Train Loss: 0.0340, Val Loss: 0.0270\n",
            "Epoch 97/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0338, Val Loss: 0.0270\n",
            "Epoch 99/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 100/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "\n",
            "Test iteration for date: 2019-07-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1386, Val Loss: 0.2716\n",
            "Epoch 2/100, Train Loss: 0.1234, Val Loss: 0.3591\n",
            "Epoch 3/100, Train Loss: 0.1185, Val Loss: 0.3598\n",
            "Epoch 4/100, Train Loss: 0.1058, Val Loss: 0.3323\n",
            "Epoch 5/100, Train Loss: 0.1094, Val Loss: 0.2647\n",
            "Epoch 6/100, Train Loss: 0.0721, Val Loss: 0.1006\n",
            "Epoch 7/100, Train Loss: 0.0715, Val Loss: 0.0974\n",
            "Epoch 8/100, Train Loss: 0.0718, Val Loss: 0.1233\n",
            "Epoch 9/100, Train Loss: 0.0592, Val Loss: 0.0639\n",
            "Epoch 10/100, Train Loss: 0.0562, Val Loss: 0.0669\n",
            "Epoch 11/100, Train Loss: 0.0584, Val Loss: 0.0499\n",
            "Epoch 12/100, Train Loss: 0.0589, Val Loss: 0.0461\n",
            "Epoch 13/100, Train Loss: 0.0617, Val Loss: 0.0451\n",
            "Epoch 14/100, Train Loss: 0.0563, Val Loss: 0.0415\n",
            "Epoch 15/100, Train Loss: 0.0505, Val Loss: 0.0534\n",
            "Epoch 16/100, Train Loss: 0.0523, Val Loss: 0.0362\n",
            "Epoch 17/100, Train Loss: 0.0471, Val Loss: 0.0300\n",
            "Epoch 18/100, Train Loss: 0.0537, Val Loss: 0.0578\n",
            "Epoch 19/100, Train Loss: 0.0473, Val Loss: 0.0531\n",
            "Epoch 20/100, Train Loss: 0.0439, Val Loss: 0.0366\n",
            "Epoch 21/100, Train Loss: 0.0422, Val Loss: 0.0281\n",
            "Epoch 22/100, Train Loss: 0.0412, Val Loss: 0.0446\n",
            "Epoch 23/100, Train Loss: 0.0515, Val Loss: 0.0302\n",
            "Epoch 24/100, Train Loss: 0.0472, Val Loss: 0.0258\n",
            "Epoch 25/100, Train Loss: 0.0476, Val Loss: 0.0764\n",
            "Epoch 26/100, Train Loss: 0.0403, Val Loss: 0.0275\n",
            "Epoch 27/100, Train Loss: 0.0390, Val Loss: 0.0253\n",
            "Epoch 28/100, Train Loss: 0.0490, Val Loss: 0.0405\n",
            "Epoch 29/100, Train Loss: 0.0405, Val Loss: 0.0269\n",
            "Epoch 30/100, Train Loss: 0.0491, Val Loss: 0.0237\n",
            "Epoch 31/100, Train Loss: 0.0360, Val Loss: 0.0364\n",
            "Epoch 32/100, Train Loss: 0.0353, Val Loss: 0.0205\n",
            "Epoch 33/100, Train Loss: 0.0396, Val Loss: 0.0259\n",
            "Epoch 34/100, Train Loss: 0.0393, Val Loss: 0.0291\n",
            "Epoch 35/100, Train Loss: 0.0393, Val Loss: 0.0272\n",
            "Epoch 36/100, Train Loss: 0.0439, Val Loss: 0.0301\n",
            "Epoch 37/100, Train Loss: 0.0352, Val Loss: 0.0338\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0350, Val Loss: 0.0318\n",
            "Epoch 39/100, Train Loss: 0.0339, Val Loss: 0.0292\n",
            "Epoch 40/100, Train Loss: 0.0331, Val Loss: 0.0280\n",
            "Epoch 41/100, Train Loss: 0.0347, Val Loss: 0.0277\n",
            "Epoch 42/100, Train Loss: 0.0331, Val Loss: 0.0271\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0344, Val Loss: 0.0271\n",
            "Epoch 44/100, Train Loss: 0.0344, Val Loss: 0.0270\n",
            "Epoch 45/100, Train Loss: 0.0335, Val Loss: 0.0270\n",
            "Epoch 46/100, Train Loss: 0.0322, Val Loss: 0.0270\n",
            "Epoch 47/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 49/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 50/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 51/100, Train Loss: 0.0331, Val Loss: 0.0270\n",
            "Epoch 52/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Epoch 54/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 55/100, Train Loss: 0.0333, Val Loss: 0.0270\n",
            "Epoch 56/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 57/100, Train Loss: 0.0333, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "Epoch 59/100, Train Loss: 0.0326, Val Loss: 0.0270\n",
            "Epoch 60/100, Train Loss: 0.0337, Val Loss: 0.0270\n",
            "Epoch 61/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 62/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0331, Val Loss: 0.0270\n",
            "Epoch 64/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 65/100, Train Loss: 0.0312, Val Loss: 0.0270\n",
            "Epoch 66/100, Train Loss: 0.0313, Val Loss: 0.0270\n",
            "Epoch 67/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 69/100, Train Loss: 0.0312, Val Loss: 0.0270\n",
            "Epoch 70/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 71/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 72/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0332, Val Loss: 0.0270\n",
            "Epoch 74/100, Train Loss: 0.0319, Val Loss: 0.0270\n",
            "Epoch 75/100, Train Loss: 0.0341, Val Loss: 0.0270\n",
            "Epoch 76/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 77/100, Train Loss: 0.0313, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0449, Val Loss: 0.0270\n",
            "Epoch 79/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Epoch 80/100, Train Loss: 0.0310, Val Loss: 0.0270\n",
            "Epoch 81/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 82/100, Train Loss: 0.0319, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0374, Val Loss: 0.0270\n",
            "Epoch 84/100, Train Loss: 0.0330, Val Loss: 0.0270\n",
            "Epoch 85/100, Train Loss: 0.0326, Val Loss: 0.0270\n",
            "Epoch 86/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Epoch 87/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0322, Val Loss: 0.0270\n",
            "Epoch 89/100, Train Loss: 0.0328, Val Loss: 0.0270\n",
            "Epoch 90/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 91/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 92/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 94/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Epoch 95/100, Train Loss: 0.0356, Val Loss: 0.0270\n",
            "Epoch 96/100, Train Loss: 0.0340, Val Loss: 0.0270\n",
            "Epoch 97/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0338, Val Loss: 0.0270\n",
            "Epoch 99/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 100/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "\n",
            "Test iteration for date: 2019-08-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1386, Val Loss: 0.2716\n",
            "Epoch 2/100, Train Loss: 0.1234, Val Loss: 0.3591\n",
            "Epoch 3/100, Train Loss: 0.1185, Val Loss: 0.3598\n",
            "Epoch 4/100, Train Loss: 0.1058, Val Loss: 0.3323\n",
            "Epoch 5/100, Train Loss: 0.1094, Val Loss: 0.2647\n",
            "Epoch 6/100, Train Loss: 0.0721, Val Loss: 0.1006\n",
            "Epoch 7/100, Train Loss: 0.0715, Val Loss: 0.0974\n",
            "Epoch 8/100, Train Loss: 0.0718, Val Loss: 0.1233\n",
            "Epoch 9/100, Train Loss: 0.0592, Val Loss: 0.0639\n",
            "Epoch 10/100, Train Loss: 0.0562, Val Loss: 0.0669\n",
            "Epoch 11/100, Train Loss: 0.0584, Val Loss: 0.0499\n",
            "Epoch 12/100, Train Loss: 0.0589, Val Loss: 0.0461\n",
            "Epoch 13/100, Train Loss: 0.0617, Val Loss: 0.0451\n",
            "Epoch 14/100, Train Loss: 0.0563, Val Loss: 0.0415\n",
            "Epoch 15/100, Train Loss: 0.0505, Val Loss: 0.0534\n",
            "Epoch 16/100, Train Loss: 0.0523, Val Loss: 0.0362\n",
            "Epoch 17/100, Train Loss: 0.0471, Val Loss: 0.0300\n",
            "Epoch 18/100, Train Loss: 0.0537, Val Loss: 0.0578\n",
            "Epoch 19/100, Train Loss: 0.0473, Val Loss: 0.0531\n",
            "Epoch 20/100, Train Loss: 0.0439, Val Loss: 0.0366\n",
            "Epoch 21/100, Train Loss: 0.0422, Val Loss: 0.0281\n",
            "Epoch 22/100, Train Loss: 0.0412, Val Loss: 0.0446\n",
            "Epoch 23/100, Train Loss: 0.0515, Val Loss: 0.0302\n",
            "Epoch 24/100, Train Loss: 0.0472, Val Loss: 0.0258\n",
            "Epoch 25/100, Train Loss: 0.0476, Val Loss: 0.0764\n",
            "Epoch 26/100, Train Loss: 0.0403, Val Loss: 0.0275\n",
            "Epoch 27/100, Train Loss: 0.0390, Val Loss: 0.0253\n",
            "Epoch 28/100, Train Loss: 0.0490, Val Loss: 0.0405\n",
            "Epoch 29/100, Train Loss: 0.0405, Val Loss: 0.0269\n",
            "Epoch 30/100, Train Loss: 0.0491, Val Loss: 0.0237\n",
            "Epoch 31/100, Train Loss: 0.0360, Val Loss: 0.0364\n",
            "Epoch 32/100, Train Loss: 0.0353, Val Loss: 0.0205\n",
            "Epoch 33/100, Train Loss: 0.0396, Val Loss: 0.0259\n",
            "Epoch 34/100, Train Loss: 0.0393, Val Loss: 0.0291\n",
            "Epoch 35/100, Train Loss: 0.0393, Val Loss: 0.0272\n",
            "Epoch 36/100, Train Loss: 0.0439, Val Loss: 0.0301\n",
            "Epoch 37/100, Train Loss: 0.0352, Val Loss: 0.0338\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0350, Val Loss: 0.0318\n",
            "Epoch 39/100, Train Loss: 0.0339, Val Loss: 0.0292\n",
            "Epoch 40/100, Train Loss: 0.0331, Val Loss: 0.0280\n",
            "Epoch 41/100, Train Loss: 0.0347, Val Loss: 0.0277\n",
            "Epoch 42/100, Train Loss: 0.0331, Val Loss: 0.0271\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0344, Val Loss: 0.0271\n",
            "Epoch 44/100, Train Loss: 0.0344, Val Loss: 0.0270\n",
            "Epoch 45/100, Train Loss: 0.0335, Val Loss: 0.0270\n",
            "Epoch 46/100, Train Loss: 0.0322, Val Loss: 0.0270\n",
            "Epoch 47/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 49/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 50/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 51/100, Train Loss: 0.0331, Val Loss: 0.0270\n",
            "Epoch 52/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Epoch 54/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 55/100, Train Loss: 0.0333, Val Loss: 0.0270\n",
            "Epoch 56/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 57/100, Train Loss: 0.0333, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "Epoch 59/100, Train Loss: 0.0326, Val Loss: 0.0270\n",
            "Epoch 60/100, Train Loss: 0.0337, Val Loss: 0.0270\n",
            "Epoch 61/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 62/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0331, Val Loss: 0.0270\n",
            "Epoch 64/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 65/100, Train Loss: 0.0312, Val Loss: 0.0270\n",
            "Epoch 66/100, Train Loss: 0.0313, Val Loss: 0.0270\n",
            "Epoch 67/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 69/100, Train Loss: 0.0312, Val Loss: 0.0270\n",
            "Epoch 70/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 71/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 72/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0332, Val Loss: 0.0270\n",
            "Epoch 74/100, Train Loss: 0.0319, Val Loss: 0.0270\n",
            "Epoch 75/100, Train Loss: 0.0341, Val Loss: 0.0270\n",
            "Epoch 76/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 77/100, Train Loss: 0.0313, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0449, Val Loss: 0.0270\n",
            "Epoch 79/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Epoch 80/100, Train Loss: 0.0310, Val Loss: 0.0270\n",
            "Epoch 81/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 82/100, Train Loss: 0.0319, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0374, Val Loss: 0.0270\n",
            "Epoch 84/100, Train Loss: 0.0330, Val Loss: 0.0270\n",
            "Epoch 85/100, Train Loss: 0.0326, Val Loss: 0.0270\n",
            "Epoch 86/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Epoch 87/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0322, Val Loss: 0.0270\n",
            "Epoch 89/100, Train Loss: 0.0328, Val Loss: 0.0270\n",
            "Epoch 90/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 91/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 92/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 94/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Epoch 95/100, Train Loss: 0.0356, Val Loss: 0.0270\n",
            "Epoch 96/100, Train Loss: 0.0340, Val Loss: 0.0270\n",
            "Epoch 97/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0338, Val Loss: 0.0270\n",
            "Epoch 99/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 100/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "\n",
            "Test iteration for date: 2019-09-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1386, Val Loss: 0.2716\n",
            "Epoch 2/100, Train Loss: 0.1234, Val Loss: 0.3591\n",
            "Epoch 3/100, Train Loss: 0.1185, Val Loss: 0.3598\n",
            "Epoch 4/100, Train Loss: 0.1058, Val Loss: 0.3323\n",
            "Epoch 5/100, Train Loss: 0.1094, Val Loss: 0.2647\n",
            "Epoch 6/100, Train Loss: 0.0721, Val Loss: 0.1006\n",
            "Epoch 7/100, Train Loss: 0.0715, Val Loss: 0.0974\n",
            "Epoch 8/100, Train Loss: 0.0718, Val Loss: 0.1233\n",
            "Epoch 9/100, Train Loss: 0.0592, Val Loss: 0.0639\n",
            "Epoch 10/100, Train Loss: 0.0562, Val Loss: 0.0669\n",
            "Epoch 11/100, Train Loss: 0.0584, Val Loss: 0.0499\n",
            "Epoch 12/100, Train Loss: 0.0589, Val Loss: 0.0461\n",
            "Epoch 13/100, Train Loss: 0.0617, Val Loss: 0.0451\n",
            "Epoch 14/100, Train Loss: 0.0563, Val Loss: 0.0415\n",
            "Epoch 15/100, Train Loss: 0.0505, Val Loss: 0.0534\n",
            "Epoch 16/100, Train Loss: 0.0523, Val Loss: 0.0362\n",
            "Epoch 17/100, Train Loss: 0.0471, Val Loss: 0.0300\n",
            "Epoch 18/100, Train Loss: 0.0537, Val Loss: 0.0578\n",
            "Epoch 19/100, Train Loss: 0.0473, Val Loss: 0.0531\n",
            "Epoch 20/100, Train Loss: 0.0439, Val Loss: 0.0366\n",
            "Epoch 21/100, Train Loss: 0.0422, Val Loss: 0.0281\n",
            "Epoch 22/100, Train Loss: 0.0412, Val Loss: 0.0446\n",
            "Epoch 23/100, Train Loss: 0.0515, Val Loss: 0.0302\n",
            "Epoch 24/100, Train Loss: 0.0472, Val Loss: 0.0258\n",
            "Epoch 25/100, Train Loss: 0.0476, Val Loss: 0.0764\n",
            "Epoch 26/100, Train Loss: 0.0403, Val Loss: 0.0275\n",
            "Epoch 27/100, Train Loss: 0.0390, Val Loss: 0.0253\n",
            "Epoch 28/100, Train Loss: 0.0490, Val Loss: 0.0405\n",
            "Epoch 29/100, Train Loss: 0.0405, Val Loss: 0.0269\n",
            "Epoch 30/100, Train Loss: 0.0491, Val Loss: 0.0237\n",
            "Epoch 31/100, Train Loss: 0.0360, Val Loss: 0.0364\n",
            "Epoch 32/100, Train Loss: 0.0353, Val Loss: 0.0205\n",
            "Epoch 33/100, Train Loss: 0.0396, Val Loss: 0.0259\n",
            "Epoch 34/100, Train Loss: 0.0393, Val Loss: 0.0291\n",
            "Epoch 35/100, Train Loss: 0.0393, Val Loss: 0.0272\n",
            "Epoch 36/100, Train Loss: 0.0439, Val Loss: 0.0301\n",
            "Epoch 37/100, Train Loss: 0.0352, Val Loss: 0.0338\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0350, Val Loss: 0.0318\n",
            "Epoch 39/100, Train Loss: 0.0339, Val Loss: 0.0292\n",
            "Epoch 40/100, Train Loss: 0.0331, Val Loss: 0.0280\n",
            "Epoch 41/100, Train Loss: 0.0347, Val Loss: 0.0277\n",
            "Epoch 42/100, Train Loss: 0.0331, Val Loss: 0.0271\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0344, Val Loss: 0.0271\n",
            "Epoch 44/100, Train Loss: 0.0344, Val Loss: 0.0270\n",
            "Epoch 45/100, Train Loss: 0.0335, Val Loss: 0.0270\n",
            "Epoch 46/100, Train Loss: 0.0322, Val Loss: 0.0270\n",
            "Epoch 47/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 49/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 50/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 51/100, Train Loss: 0.0331, Val Loss: 0.0270\n",
            "Epoch 52/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Epoch 54/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 55/100, Train Loss: 0.0333, Val Loss: 0.0270\n",
            "Epoch 56/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 57/100, Train Loss: 0.0333, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "Epoch 59/100, Train Loss: 0.0326, Val Loss: 0.0270\n",
            "Epoch 60/100, Train Loss: 0.0337, Val Loss: 0.0270\n",
            "Epoch 61/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 62/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0331, Val Loss: 0.0270\n",
            "Epoch 64/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 65/100, Train Loss: 0.0312, Val Loss: 0.0270\n",
            "Epoch 66/100, Train Loss: 0.0313, Val Loss: 0.0270\n",
            "Epoch 67/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 69/100, Train Loss: 0.0312, Val Loss: 0.0270\n",
            "Epoch 70/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 71/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 72/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0332, Val Loss: 0.0270\n",
            "Epoch 74/100, Train Loss: 0.0319, Val Loss: 0.0270\n",
            "Epoch 75/100, Train Loss: 0.0341, Val Loss: 0.0270\n",
            "Epoch 76/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 77/100, Train Loss: 0.0313, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0449, Val Loss: 0.0270\n",
            "Epoch 79/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Epoch 80/100, Train Loss: 0.0310, Val Loss: 0.0270\n",
            "Epoch 81/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 82/100, Train Loss: 0.0319, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0374, Val Loss: 0.0270\n",
            "Epoch 84/100, Train Loss: 0.0330, Val Loss: 0.0270\n",
            "Epoch 85/100, Train Loss: 0.0326, Val Loss: 0.0270\n",
            "Epoch 86/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Epoch 87/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0322, Val Loss: 0.0270\n",
            "Epoch 89/100, Train Loss: 0.0328, Val Loss: 0.0270\n",
            "Epoch 90/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 91/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 92/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 94/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Epoch 95/100, Train Loss: 0.0356, Val Loss: 0.0270\n",
            "Epoch 96/100, Train Loss: 0.0340, Val Loss: 0.0270\n",
            "Epoch 97/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0338, Val Loss: 0.0270\n",
            "Epoch 99/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 100/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "\n",
            "Test iteration for date: 2019-10-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1386, Val Loss: 0.2716\n",
            "Epoch 2/100, Train Loss: 0.1234, Val Loss: 0.3591\n",
            "Epoch 3/100, Train Loss: 0.1185, Val Loss: 0.3598\n",
            "Epoch 4/100, Train Loss: 0.1058, Val Loss: 0.3323\n",
            "Epoch 5/100, Train Loss: 0.1094, Val Loss: 0.2647\n",
            "Epoch 6/100, Train Loss: 0.0721, Val Loss: 0.1006\n",
            "Epoch 7/100, Train Loss: 0.0715, Val Loss: 0.0974\n",
            "Epoch 8/100, Train Loss: 0.0718, Val Loss: 0.1233\n",
            "Epoch 9/100, Train Loss: 0.0592, Val Loss: 0.0639\n",
            "Epoch 10/100, Train Loss: 0.0562, Val Loss: 0.0669\n",
            "Epoch 11/100, Train Loss: 0.0584, Val Loss: 0.0499\n",
            "Epoch 12/100, Train Loss: 0.0589, Val Loss: 0.0461\n",
            "Epoch 13/100, Train Loss: 0.0617, Val Loss: 0.0451\n",
            "Epoch 14/100, Train Loss: 0.0563, Val Loss: 0.0415\n",
            "Epoch 15/100, Train Loss: 0.0505, Val Loss: 0.0534\n",
            "Epoch 16/100, Train Loss: 0.0523, Val Loss: 0.0362\n",
            "Epoch 17/100, Train Loss: 0.0471, Val Loss: 0.0300\n",
            "Epoch 18/100, Train Loss: 0.0537, Val Loss: 0.0578\n",
            "Epoch 19/100, Train Loss: 0.0473, Val Loss: 0.0531\n",
            "Epoch 20/100, Train Loss: 0.0439, Val Loss: 0.0366\n",
            "Epoch 21/100, Train Loss: 0.0422, Val Loss: 0.0281\n",
            "Epoch 22/100, Train Loss: 0.0412, Val Loss: 0.0446\n",
            "Epoch 23/100, Train Loss: 0.0515, Val Loss: 0.0302\n",
            "Epoch 24/100, Train Loss: 0.0472, Val Loss: 0.0258\n",
            "Epoch 25/100, Train Loss: 0.0476, Val Loss: 0.0764\n",
            "Epoch 26/100, Train Loss: 0.0403, Val Loss: 0.0275\n",
            "Epoch 27/100, Train Loss: 0.0390, Val Loss: 0.0253\n",
            "Epoch 28/100, Train Loss: 0.0490, Val Loss: 0.0405\n",
            "Epoch 29/100, Train Loss: 0.0405, Val Loss: 0.0269\n",
            "Epoch 30/100, Train Loss: 0.0491, Val Loss: 0.0237\n",
            "Epoch 31/100, Train Loss: 0.0360, Val Loss: 0.0364\n",
            "Epoch 32/100, Train Loss: 0.0353, Val Loss: 0.0205\n",
            "Epoch 33/100, Train Loss: 0.0396, Val Loss: 0.0259\n",
            "Epoch 34/100, Train Loss: 0.0393, Val Loss: 0.0291\n",
            "Epoch 35/100, Train Loss: 0.0393, Val Loss: 0.0272\n",
            "Epoch 36/100, Train Loss: 0.0439, Val Loss: 0.0301\n",
            "Epoch 37/100, Train Loss: 0.0352, Val Loss: 0.0338\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0350, Val Loss: 0.0318\n",
            "Epoch 39/100, Train Loss: 0.0339, Val Loss: 0.0292\n",
            "Epoch 40/100, Train Loss: 0.0331, Val Loss: 0.0280\n",
            "Epoch 41/100, Train Loss: 0.0347, Val Loss: 0.0277\n",
            "Epoch 42/100, Train Loss: 0.0331, Val Loss: 0.0271\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0344, Val Loss: 0.0271\n",
            "Epoch 44/100, Train Loss: 0.0344, Val Loss: 0.0270\n",
            "Epoch 45/100, Train Loss: 0.0335, Val Loss: 0.0270\n",
            "Epoch 46/100, Train Loss: 0.0322, Val Loss: 0.0270\n",
            "Epoch 47/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 49/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 50/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 51/100, Train Loss: 0.0331, Val Loss: 0.0270\n",
            "Epoch 52/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Epoch 54/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 55/100, Train Loss: 0.0333, Val Loss: 0.0270\n",
            "Epoch 56/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 57/100, Train Loss: 0.0333, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "Epoch 59/100, Train Loss: 0.0326, Val Loss: 0.0270\n",
            "Epoch 60/100, Train Loss: 0.0337, Val Loss: 0.0270\n",
            "Epoch 61/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 62/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0331, Val Loss: 0.0270\n",
            "Epoch 64/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 65/100, Train Loss: 0.0312, Val Loss: 0.0270\n",
            "Epoch 66/100, Train Loss: 0.0313, Val Loss: 0.0270\n",
            "Epoch 67/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 69/100, Train Loss: 0.0312, Val Loss: 0.0270\n",
            "Epoch 70/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 71/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 72/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0332, Val Loss: 0.0270\n",
            "Epoch 74/100, Train Loss: 0.0319, Val Loss: 0.0270\n",
            "Epoch 75/100, Train Loss: 0.0341, Val Loss: 0.0270\n",
            "Epoch 76/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 77/100, Train Loss: 0.0313, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0449, Val Loss: 0.0270\n",
            "Epoch 79/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Epoch 80/100, Train Loss: 0.0310, Val Loss: 0.0270\n",
            "Epoch 81/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 82/100, Train Loss: 0.0319, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0374, Val Loss: 0.0270\n",
            "Epoch 84/100, Train Loss: 0.0330, Val Loss: 0.0270\n",
            "Epoch 85/100, Train Loss: 0.0326, Val Loss: 0.0270\n",
            "Epoch 86/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Epoch 87/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0322, Val Loss: 0.0270\n",
            "Epoch 89/100, Train Loss: 0.0328, Val Loss: 0.0270\n",
            "Epoch 90/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 91/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 92/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 94/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Epoch 95/100, Train Loss: 0.0356, Val Loss: 0.0270\n",
            "Epoch 96/100, Train Loss: 0.0340, Val Loss: 0.0270\n",
            "Epoch 97/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0338, Val Loss: 0.0270\n",
            "Epoch 99/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 100/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "\n",
            "Test iteration for date: 2019-11-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1386, Val Loss: 0.2716\n",
            "Epoch 2/100, Train Loss: 0.1234, Val Loss: 0.3591\n",
            "Epoch 3/100, Train Loss: 0.1185, Val Loss: 0.3598\n",
            "Epoch 4/100, Train Loss: 0.1058, Val Loss: 0.3323\n",
            "Epoch 5/100, Train Loss: 0.1094, Val Loss: 0.2647\n",
            "Epoch 6/100, Train Loss: 0.0721, Val Loss: 0.1006\n",
            "Epoch 7/100, Train Loss: 0.0715, Val Loss: 0.0974\n",
            "Epoch 8/100, Train Loss: 0.0718, Val Loss: 0.1233\n",
            "Epoch 9/100, Train Loss: 0.0592, Val Loss: 0.0639\n",
            "Epoch 10/100, Train Loss: 0.0562, Val Loss: 0.0669\n",
            "Epoch 11/100, Train Loss: 0.0584, Val Loss: 0.0499\n",
            "Epoch 12/100, Train Loss: 0.0589, Val Loss: 0.0461\n",
            "Epoch 13/100, Train Loss: 0.0617, Val Loss: 0.0451\n",
            "Epoch 14/100, Train Loss: 0.0563, Val Loss: 0.0415\n",
            "Epoch 15/100, Train Loss: 0.0505, Val Loss: 0.0534\n",
            "Epoch 16/100, Train Loss: 0.0523, Val Loss: 0.0362\n",
            "Epoch 17/100, Train Loss: 0.0471, Val Loss: 0.0300\n",
            "Epoch 18/100, Train Loss: 0.0537, Val Loss: 0.0578\n",
            "Epoch 19/100, Train Loss: 0.0473, Val Loss: 0.0531\n",
            "Epoch 20/100, Train Loss: 0.0439, Val Loss: 0.0366\n",
            "Epoch 21/100, Train Loss: 0.0422, Val Loss: 0.0281\n",
            "Epoch 22/100, Train Loss: 0.0412, Val Loss: 0.0446\n",
            "Epoch 23/100, Train Loss: 0.0515, Val Loss: 0.0302\n",
            "Epoch 24/100, Train Loss: 0.0472, Val Loss: 0.0258\n",
            "Epoch 25/100, Train Loss: 0.0476, Val Loss: 0.0764\n",
            "Epoch 26/100, Train Loss: 0.0403, Val Loss: 0.0275\n",
            "Epoch 27/100, Train Loss: 0.0390, Val Loss: 0.0253\n",
            "Epoch 28/100, Train Loss: 0.0490, Val Loss: 0.0405\n",
            "Epoch 29/100, Train Loss: 0.0405, Val Loss: 0.0269\n",
            "Epoch 30/100, Train Loss: 0.0491, Val Loss: 0.0237\n",
            "Epoch 31/100, Train Loss: 0.0360, Val Loss: 0.0364\n",
            "Epoch 32/100, Train Loss: 0.0353, Val Loss: 0.0205\n",
            "Epoch 33/100, Train Loss: 0.0396, Val Loss: 0.0259\n",
            "Epoch 34/100, Train Loss: 0.0393, Val Loss: 0.0291\n",
            "Epoch 35/100, Train Loss: 0.0393, Val Loss: 0.0272\n",
            "Epoch 36/100, Train Loss: 0.0439, Val Loss: 0.0301\n",
            "Epoch 37/100, Train Loss: 0.0352, Val Loss: 0.0338\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0350, Val Loss: 0.0318\n",
            "Epoch 39/100, Train Loss: 0.0339, Val Loss: 0.0292\n",
            "Epoch 40/100, Train Loss: 0.0331, Val Loss: 0.0280\n",
            "Epoch 41/100, Train Loss: 0.0347, Val Loss: 0.0277\n",
            "Epoch 42/100, Train Loss: 0.0331, Val Loss: 0.0271\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0344, Val Loss: 0.0271\n",
            "Epoch 44/100, Train Loss: 0.0344, Val Loss: 0.0270\n",
            "Epoch 45/100, Train Loss: 0.0335, Val Loss: 0.0270\n",
            "Epoch 46/100, Train Loss: 0.0322, Val Loss: 0.0270\n",
            "Epoch 47/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 49/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 50/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 51/100, Train Loss: 0.0331, Val Loss: 0.0270\n",
            "Epoch 52/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Epoch 54/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 55/100, Train Loss: 0.0333, Val Loss: 0.0270\n",
            "Epoch 56/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 57/100, Train Loss: 0.0333, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "Epoch 59/100, Train Loss: 0.0326, Val Loss: 0.0270\n",
            "Epoch 60/100, Train Loss: 0.0337, Val Loss: 0.0270\n",
            "Epoch 61/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 62/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0331, Val Loss: 0.0270\n",
            "Epoch 64/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 65/100, Train Loss: 0.0312, Val Loss: 0.0270\n",
            "Epoch 66/100, Train Loss: 0.0313, Val Loss: 0.0270\n",
            "Epoch 67/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 69/100, Train Loss: 0.0312, Val Loss: 0.0270\n",
            "Epoch 70/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 71/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 72/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0332, Val Loss: 0.0270\n",
            "Epoch 74/100, Train Loss: 0.0319, Val Loss: 0.0270\n",
            "Epoch 75/100, Train Loss: 0.0341, Val Loss: 0.0270\n",
            "Epoch 76/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 77/100, Train Loss: 0.0313, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0449, Val Loss: 0.0270\n",
            "Epoch 79/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Epoch 80/100, Train Loss: 0.0310, Val Loss: 0.0270\n",
            "Epoch 81/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 82/100, Train Loss: 0.0319, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0374, Val Loss: 0.0270\n",
            "Epoch 84/100, Train Loss: 0.0330, Val Loss: 0.0270\n",
            "Epoch 85/100, Train Loss: 0.0326, Val Loss: 0.0270\n",
            "Epoch 86/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Epoch 87/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0322, Val Loss: 0.0270\n",
            "Epoch 89/100, Train Loss: 0.0328, Val Loss: 0.0270\n",
            "Epoch 90/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 91/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 92/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 94/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Epoch 95/100, Train Loss: 0.0356, Val Loss: 0.0270\n",
            "Epoch 96/100, Train Loss: 0.0340, Val Loss: 0.0270\n",
            "Epoch 97/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0338, Val Loss: 0.0270\n",
            "Epoch 99/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 100/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "\n",
            "Test iteration for date: 2019-12-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1386, Val Loss: 0.2716\n",
            "Epoch 2/100, Train Loss: 0.1234, Val Loss: 0.3591\n",
            "Epoch 3/100, Train Loss: 0.1185, Val Loss: 0.3598\n",
            "Epoch 4/100, Train Loss: 0.1058, Val Loss: 0.3323\n",
            "Epoch 5/100, Train Loss: 0.1094, Val Loss: 0.2647\n",
            "Epoch 6/100, Train Loss: 0.0721, Val Loss: 0.1006\n",
            "Epoch 7/100, Train Loss: 0.0715, Val Loss: 0.0974\n",
            "Epoch 8/100, Train Loss: 0.0718, Val Loss: 0.1233\n",
            "Epoch 9/100, Train Loss: 0.0592, Val Loss: 0.0639\n",
            "Epoch 10/100, Train Loss: 0.0562, Val Loss: 0.0669\n",
            "Epoch 11/100, Train Loss: 0.0584, Val Loss: 0.0499\n",
            "Epoch 12/100, Train Loss: 0.0589, Val Loss: 0.0461\n",
            "Epoch 13/100, Train Loss: 0.0617, Val Loss: 0.0451\n",
            "Epoch 14/100, Train Loss: 0.0563, Val Loss: 0.0415\n",
            "Epoch 15/100, Train Loss: 0.0505, Val Loss: 0.0534\n",
            "Epoch 16/100, Train Loss: 0.0523, Val Loss: 0.0362\n",
            "Epoch 17/100, Train Loss: 0.0471, Val Loss: 0.0300\n",
            "Epoch 18/100, Train Loss: 0.0537, Val Loss: 0.0578\n",
            "Epoch 19/100, Train Loss: 0.0473, Val Loss: 0.0531\n",
            "Epoch 20/100, Train Loss: 0.0439, Val Loss: 0.0366\n",
            "Epoch 21/100, Train Loss: 0.0422, Val Loss: 0.0281\n",
            "Epoch 22/100, Train Loss: 0.0412, Val Loss: 0.0446\n",
            "Epoch 23/100, Train Loss: 0.0515, Val Loss: 0.0302\n",
            "Epoch 24/100, Train Loss: 0.0472, Val Loss: 0.0258\n",
            "Epoch 25/100, Train Loss: 0.0476, Val Loss: 0.0764\n",
            "Epoch 26/100, Train Loss: 0.0403, Val Loss: 0.0275\n",
            "Epoch 27/100, Train Loss: 0.0390, Val Loss: 0.0253\n",
            "Epoch 28/100, Train Loss: 0.0490, Val Loss: 0.0405\n",
            "Epoch 29/100, Train Loss: 0.0405, Val Loss: 0.0269\n",
            "Epoch 30/100, Train Loss: 0.0491, Val Loss: 0.0237\n",
            "Epoch 31/100, Train Loss: 0.0360, Val Loss: 0.0364\n",
            "Epoch 32/100, Train Loss: 0.0353, Val Loss: 0.0205\n",
            "Epoch 33/100, Train Loss: 0.0396, Val Loss: 0.0259\n",
            "Epoch 34/100, Train Loss: 0.0393, Val Loss: 0.0291\n",
            "Epoch 35/100, Train Loss: 0.0393, Val Loss: 0.0272\n",
            "Epoch 36/100, Train Loss: 0.0439, Val Loss: 0.0301\n",
            "Epoch 37/100, Train Loss: 0.0352, Val Loss: 0.0338\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 38/100, Train Loss: 0.0350, Val Loss: 0.0318\n",
            "Epoch 39/100, Train Loss: 0.0339, Val Loss: 0.0292\n",
            "Epoch 40/100, Train Loss: 0.0331, Val Loss: 0.0280\n",
            "Epoch 41/100, Train Loss: 0.0347, Val Loss: 0.0277\n",
            "Epoch 42/100, Train Loss: 0.0331, Val Loss: 0.0271\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 43/100, Train Loss: 0.0344, Val Loss: 0.0271\n",
            "Epoch 44/100, Train Loss: 0.0344, Val Loss: 0.0270\n",
            "Epoch 45/100, Train Loss: 0.0335, Val Loss: 0.0270\n",
            "Epoch 46/100, Train Loss: 0.0322, Val Loss: 0.0270\n",
            "Epoch 47/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 48/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 49/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 50/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 51/100, Train Loss: 0.0331, Val Loss: 0.0270\n",
            "Epoch 52/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 53/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Epoch 54/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 55/100, Train Loss: 0.0333, Val Loss: 0.0270\n",
            "Epoch 56/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 57/100, Train Loss: 0.0333, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 58/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "Epoch 59/100, Train Loss: 0.0326, Val Loss: 0.0270\n",
            "Epoch 60/100, Train Loss: 0.0337, Val Loss: 0.0270\n",
            "Epoch 61/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 62/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 63/100, Train Loss: 0.0331, Val Loss: 0.0270\n",
            "Epoch 64/100, Train Loss: 0.0324, Val Loss: 0.0270\n",
            "Epoch 65/100, Train Loss: 0.0312, Val Loss: 0.0270\n",
            "Epoch 66/100, Train Loss: 0.0313, Val Loss: 0.0270\n",
            "Epoch 67/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 68/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 69/100, Train Loss: 0.0312, Val Loss: 0.0270\n",
            "Epoch 70/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 71/100, Train Loss: 0.0320, Val Loss: 0.0270\n",
            "Epoch 72/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 73/100, Train Loss: 0.0332, Val Loss: 0.0270\n",
            "Epoch 74/100, Train Loss: 0.0319, Val Loss: 0.0270\n",
            "Epoch 75/100, Train Loss: 0.0341, Val Loss: 0.0270\n",
            "Epoch 76/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 77/100, Train Loss: 0.0313, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 78/100, Train Loss: 0.0449, Val Loss: 0.0270\n",
            "Epoch 79/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Epoch 80/100, Train Loss: 0.0310, Val Loss: 0.0270\n",
            "Epoch 81/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 82/100, Train Loss: 0.0319, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 83/100, Train Loss: 0.0374, Val Loss: 0.0270\n",
            "Epoch 84/100, Train Loss: 0.0330, Val Loss: 0.0270\n",
            "Epoch 85/100, Train Loss: 0.0326, Val Loss: 0.0270\n",
            "Epoch 86/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Epoch 87/100, Train Loss: 0.0321, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 88/100, Train Loss: 0.0322, Val Loss: 0.0270\n",
            "Epoch 89/100, Train Loss: 0.0328, Val Loss: 0.0270\n",
            "Epoch 90/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 91/100, Train Loss: 0.0323, Val Loss: 0.0270\n",
            "Epoch 92/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 93/100, Train Loss: 0.0334, Val Loss: 0.0270\n",
            "Epoch 94/100, Train Loss: 0.0316, Val Loss: 0.0270\n",
            "Epoch 95/100, Train Loss: 0.0356, Val Loss: 0.0270\n",
            "Epoch 96/100, Train Loss: 0.0340, Val Loss: 0.0270\n",
            "Epoch 97/100, Train Loss: 0.0327, Val Loss: 0.0270\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 98/100, Train Loss: 0.0338, Val Loss: 0.0270\n",
            "Epoch 99/100, Train Loss: 0.0318, Val Loss: 0.0270\n",
            "Epoch 100/100, Train Loss: 0.0325, Val Loss: 0.0270\n",
            "\n",
            "Test iteration for date: 2020-06-01 00:00:00\n",
            "Current training set size: 125 samples\n",
            "Epoch 1/100, Train Loss: 0.1483, Val Loss: 0.2699\n",
            "Epoch 2/100, Train Loss: 0.1313, Val Loss: 0.3646\n",
            "Epoch 3/100, Train Loss: 0.1223, Val Loss: 0.3673\n",
            "Epoch 4/100, Train Loss: 0.1074, Val Loss: 0.2011\n",
            "Epoch 5/100, Train Loss: 0.1135, Val Loss: 0.2848\n",
            "Epoch 6/100, Train Loss: 0.0951, Val Loss: 0.0914\n",
            "Epoch 7/100, Train Loss: 0.0716, Val Loss: 0.0625\n",
            "Epoch 8/100, Train Loss: 0.0726, Val Loss: 0.0689\n",
            "Epoch 9/100, Train Loss: 0.0625, Val Loss: 0.0574\n",
            "Epoch 10/100, Train Loss: 0.0570, Val Loss: 0.0651\n",
            "Epoch 11/100, Train Loss: 0.0568, Val Loss: 0.0606\n",
            "Epoch 12/100, Train Loss: 0.0604, Val Loss: 0.0487\n",
            "Epoch 13/100, Train Loss: 0.0663, Val Loss: 0.0674\n",
            "Epoch 14/100, Train Loss: 0.0514, Val Loss: 0.0591\n",
            "Epoch 15/100, Train Loss: 0.0512, Val Loss: 0.0488\n",
            "Epoch 16/100, Train Loss: 0.0537, Val Loss: 0.0687\n",
            "Epoch 17/100, Train Loss: 0.0468, Val Loss: 0.0527\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 18/100, Train Loss: 0.0422, Val Loss: 0.0508\n",
            "Epoch 19/100, Train Loss: 0.0405, Val Loss: 0.0489\n",
            "Epoch 20/100, Train Loss: 0.0410, Val Loss: 0.0476\n",
            "Epoch 21/100, Train Loss: 0.0390, Val Loss: 0.0465\n",
            "Epoch 22/100, Train Loss: 0.0410, Val Loss: 0.0460\n",
            "Epoch 23/100, Train Loss: 0.0408, Val Loss: 0.0447\n",
            "Epoch 24/100, Train Loss: 0.0402, Val Loss: 0.0448\n",
            "Epoch 25/100, Train Loss: 0.0403, Val Loss: 0.0445\n",
            "Epoch 26/100, Train Loss: 0.0394, Val Loss: 0.0439\n",
            "Epoch 27/100, Train Loss: 0.0401, Val Loss: 0.0432\n",
            "Epoch 28/100, Train Loss: 0.0516, Val Loss: 0.0432\n",
            "Epoch 29/100, Train Loss: 0.0406, Val Loss: 0.0421\n",
            "Epoch 30/100, Train Loss: 0.0390, Val Loss: 0.0423\n",
            "Epoch 31/100, Train Loss: 0.0407, Val Loss: 0.0424\n",
            "Epoch 32/100, Train Loss: 0.0397, Val Loss: 0.0414\n",
            "Epoch 33/100, Train Loss: 0.0399, Val Loss: 0.0416\n",
            "Epoch 34/100, Train Loss: 0.0405, Val Loss: 0.0411\n",
            "Epoch 35/100, Train Loss: 0.0398, Val Loss: 0.0411\n",
            "Epoch 36/100, Train Loss: 0.0399, Val Loss: 0.0420\n",
            "Epoch 37/100, Train Loss: 0.0410, Val Loss: 0.0424\n",
            "Epoch 38/100, Train Loss: 0.0396, Val Loss: 0.0417\n",
            "Epoch 39/100, Train Loss: 0.0389, Val Loss: 0.0409\n",
            "Epoch 40/100, Train Loss: 0.0395, Val Loss: 0.0409\n",
            "Epoch 41/100, Train Loss: 0.0396, Val Loss: 0.0404\n",
            "Epoch 42/100, Train Loss: 0.0404, Val Loss: 0.0406\n",
            "Epoch 43/100, Train Loss: 0.0398, Val Loss: 0.0406\n",
            "Epoch 44/100, Train Loss: 0.0410, Val Loss: 0.0402\n",
            "Epoch 45/100, Train Loss: 0.0406, Val Loss: 0.0403\n",
            "Epoch 46/100, Train Loss: 0.0400, Val Loss: 0.0403\n",
            "Epoch 47/100, Train Loss: 0.0398, Val Loss: 0.0405\n",
            "Epoch 48/100, Train Loss: 0.0394, Val Loss: 0.0405\n",
            "Epoch 49/100, Train Loss: 0.0400, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 50/100, Train Loss: 0.0402, Val Loss: 0.0403\n",
            "Epoch 51/100, Train Loss: 0.0383, Val Loss: 0.0403\n",
            "Epoch 52/100, Train Loss: 0.0388, Val Loss: 0.0403\n",
            "Epoch 53/100, Train Loss: 0.0404, Val Loss: 0.0403\n",
            "Epoch 54/100, Train Loss: 0.0385, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 55/100, Train Loss: 0.0388, Val Loss: 0.0403\n",
            "Epoch 56/100, Train Loss: 0.0403, Val Loss: 0.0403\n",
            "Epoch 57/100, Train Loss: 0.0398, Val Loss: 0.0403\n",
            "Epoch 58/100, Train Loss: 0.0390, Val Loss: 0.0403\n",
            "Epoch 59/100, Train Loss: 0.0403, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 60/100, Train Loss: 0.0408, Val Loss: 0.0403\n",
            "Epoch 61/100, Train Loss: 0.0402, Val Loss: 0.0403\n",
            "Epoch 62/100, Train Loss: 0.0387, Val Loss: 0.0403\n",
            "Epoch 63/100, Train Loss: 0.0388, Val Loss: 0.0403\n",
            "Epoch 64/100, Train Loss: 0.0396, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 65/100, Train Loss: 0.0387, Val Loss: 0.0403\n",
            "Epoch 66/100, Train Loss: 0.0400, Val Loss: 0.0403\n",
            "Epoch 67/100, Train Loss: 0.0380, Val Loss: 0.0403\n",
            "Epoch 68/100, Train Loss: 0.0393, Val Loss: 0.0403\n",
            "Epoch 69/100, Train Loss: 0.0387, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 70/100, Train Loss: 0.0390, Val Loss: 0.0403\n",
            "Epoch 71/100, Train Loss: 0.0391, Val Loss: 0.0403\n",
            "Epoch 72/100, Train Loss: 0.0408, Val Loss: 0.0403\n",
            "Epoch 73/100, Train Loss: 0.0397, Val Loss: 0.0403\n",
            "Epoch 74/100, Train Loss: 0.0392, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 75/100, Train Loss: 0.0400, Val Loss: 0.0403\n",
            "Epoch 76/100, Train Loss: 0.0412, Val Loss: 0.0403\n",
            "Epoch 77/100, Train Loss: 0.0393, Val Loss: 0.0403\n",
            "Epoch 78/100, Train Loss: 0.0540, Val Loss: 0.0403\n",
            "Epoch 79/100, Train Loss: 0.0400, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 80/100, Train Loss: 0.0388, Val Loss: 0.0403\n",
            "Epoch 81/100, Train Loss: 0.0400, Val Loss: 0.0403\n",
            "Epoch 82/100, Train Loss: 0.0396, Val Loss: 0.0403\n",
            "Epoch 83/100, Train Loss: 0.0453, Val Loss: 0.0403\n",
            "Epoch 84/100, Train Loss: 0.0406, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 85/100, Train Loss: 0.0382, Val Loss: 0.0403\n",
            "Epoch 86/100, Train Loss: 0.0393, Val Loss: 0.0403\n",
            "Epoch 87/100, Train Loss: 0.0387, Val Loss: 0.0403\n",
            "Epoch 88/100, Train Loss: 0.0389, Val Loss: 0.0403\n",
            "Epoch 89/100, Train Loss: 0.0405, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 90/100, Train Loss: 0.0392, Val Loss: 0.0403\n",
            "Epoch 91/100, Train Loss: 0.0404, Val Loss: 0.0403\n",
            "Epoch 92/100, Train Loss: 0.0390, Val Loss: 0.0403\n",
            "Epoch 93/100, Train Loss: 0.0404, Val Loss: 0.0403\n",
            "Epoch 94/100, Train Loss: 0.0390, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 95/100, Train Loss: 0.0417, Val Loss: 0.0403\n",
            "Epoch 96/100, Train Loss: 0.0414, Val Loss: 0.0403\n",
            "Epoch 97/100, Train Loss: 0.0394, Val Loss: 0.0403\n",
            "Epoch 98/100, Train Loss: 0.0407, Val Loss: 0.0403\n",
            "Epoch 99/100, Train Loss: 0.0390, Val Loss: 0.0403\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 100/100, Train Loss: 0.0384, Val Loss: 0.0403\n",
            "\n",
            "Test iteration for date: 2020-07-01 00:00:00\n",
            "Current training set size: 126 samples\n",
            "Epoch 1/100, Train Loss: 0.1563, Val Loss: 0.3789\n",
            "Epoch 2/100, Train Loss: 0.1324, Val Loss: 0.2834\n",
            "Epoch 3/100, Train Loss: 0.1064, Val Loss: 0.1642\n",
            "Epoch 4/100, Train Loss: 0.1137, Val Loss: 0.2891\n",
            "Epoch 5/100, Train Loss: 0.0990, Val Loss: 0.1648\n",
            "Epoch 6/100, Train Loss: 0.0842, Val Loss: 0.0607\n",
            "Epoch 7/100, Train Loss: 0.0713, Val Loss: 0.0580\n",
            "Epoch 8/100, Train Loss: 0.0798, Val Loss: 0.0877\n",
            "Epoch 9/100, Train Loss: 0.0689, Val Loss: 0.0574\n",
            "Epoch 10/100, Train Loss: 0.0664, Val Loss: 0.0746\n",
            "Epoch 11/100, Train Loss: 0.0626, Val Loss: 0.1244\n",
            "Epoch 12/100, Train Loss: 0.0611, Val Loss: 0.0485\n",
            "Epoch 13/100, Train Loss: 0.0582, Val Loss: 0.0527\n",
            "Epoch 14/100, Train Loss: 0.0539, Val Loss: 0.0516\n",
            "Epoch 15/100, Train Loss: 0.0557, Val Loss: 0.0885\n",
            "Epoch 16/100, Train Loss: 0.0528, Val Loss: 0.0454\n",
            "Epoch 17/100, Train Loss: 0.0447, Val Loss: 0.0467\n",
            "Epoch 18/100, Train Loss: 0.0555, Val Loss: 0.0480\n",
            "Epoch 19/100, Train Loss: 0.0474, Val Loss: 0.0648\n",
            "Epoch 20/100, Train Loss: 0.0434, Val Loss: 0.0530\n",
            "Epoch 21/100, Train Loss: 0.0424, Val Loss: 0.0525\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 22/100, Train Loss: 0.0429, Val Loss: 0.0527\n",
            "Epoch 23/100, Train Loss: 0.0422, Val Loss: 0.0526\n",
            "Epoch 24/100, Train Loss: 0.0393, Val Loss: 0.0522\n",
            "Epoch 25/100, Train Loss: 0.0396, Val Loss: 0.0527\n",
            "Epoch 26/100, Train Loss: 0.0406, Val Loss: 0.0524\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 27/100, Train Loss: 0.0391, Val Loss: 0.0524\n",
            "Epoch 28/100, Train Loss: 0.0402, Val Loss: 0.0524\n",
            "Epoch 29/100, Train Loss: 0.0405, Val Loss: 0.0524\n",
            "Epoch 30/100, Train Loss: 0.0392, Val Loss: 0.0524\n",
            "Epoch 31/100, Train Loss: 0.0404, Val Loss: 0.0524\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 32/100, Train Loss: 0.0392, Val Loss: 0.0524\n",
            "Epoch 33/100, Train Loss: 0.0396, Val Loss: 0.0524\n",
            "Epoch 34/100, Train Loss: 0.0398, Val Loss: 0.0524\n",
            "Epoch 35/100, Train Loss: 0.0389, Val Loss: 0.0524\n",
            "Epoch 36/100, Train Loss: 0.0391, Val Loss: 0.0524\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 37/100, Train Loss: 0.0390, Val Loss: 0.0524\n",
            "Epoch 38/100, Train Loss: 0.0397, Val Loss: 0.0524\n",
            "Epoch 39/100, Train Loss: 0.0395, Val Loss: 0.0524\n",
            "Epoch 40/100, Train Loss: 0.0391, Val Loss: 0.0524\n",
            "Epoch 41/100, Train Loss: 0.0391, Val Loss: 0.0524\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 42/100, Train Loss: 0.0394, Val Loss: 0.0524\n",
            "Epoch 43/100, Train Loss: 0.0385, Val Loss: 0.0524\n",
            "Epoch 44/100, Train Loss: 0.0403, Val Loss: 0.0524\n",
            "Epoch 45/100, Train Loss: 0.0403, Val Loss: 0.0524\n",
            "Epoch 46/100, Train Loss: 0.0383, Val Loss: 0.0524\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 47/100, Train Loss: 0.0395, Val Loss: 0.0524\n",
            "Epoch 48/100, Train Loss: 0.0395, Val Loss: 0.0524\n",
            "Epoch 49/100, Train Loss: 0.0385, Val Loss: 0.0524\n",
            "Epoch 50/100, Train Loss: 0.0385, Val Loss: 0.0524\n",
            "Epoch 51/100, Train Loss: 0.0404, Val Loss: 0.0524\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 52/100, Train Loss: 0.0390, Val Loss: 0.0524\n",
            "Epoch 53/100, Train Loss: 0.0404, Val Loss: 0.0524\n",
            "Epoch 54/100, Train Loss: 0.0391, Val Loss: 0.0524\n",
            "Epoch 55/100, Train Loss: 0.0386, Val Loss: 0.0524\n",
            "Epoch 56/100, Train Loss: 0.0386, Val Loss: 0.0524\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 57/100, Train Loss: 0.0392, Val Loss: 0.0524\n",
            "Epoch 58/100, Train Loss: 0.0374, Val Loss: 0.0524\n",
            "Epoch 59/100, Train Loss: 0.0390, Val Loss: 0.0524\n",
            "Epoch 60/100, Train Loss: 0.0398, Val Loss: 0.0524\n",
            "Epoch 61/100, Train Loss: 0.0402, Val Loss: 0.0524\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 62/100, Train Loss: 0.0392, Val Loss: 0.0524\n",
            "Epoch 63/100, Train Loss: 0.0384, Val Loss: 0.0524\n",
            "Epoch 64/100, Train Loss: 0.0388, Val Loss: 0.0524\n",
            "Epoch 65/100, Train Loss: 0.0402, Val Loss: 0.0524\n",
            "Epoch 66/100, Train Loss: 0.0386, Val Loss: 0.0524\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 67/100, Train Loss: 0.0395, Val Loss: 0.0524\n",
            "Epoch 68/100, Train Loss: 0.0398, Val Loss: 0.0524\n",
            "Epoch 69/100, Train Loss: 0.0389, Val Loss: 0.0524\n",
            "Epoch 70/100, Train Loss: 0.0396, Val Loss: 0.0524\n",
            "Epoch 71/100, Train Loss: 0.0405, Val Loss: 0.0524\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 72/100, Train Loss: 0.0389, Val Loss: 0.0524\n",
            "Epoch 73/100, Train Loss: 0.0391, Val Loss: 0.0524\n",
            "Epoch 74/100, Train Loss: 0.0390, Val Loss: 0.0524\n",
            "Epoch 75/100, Train Loss: 0.0396, Val Loss: 0.0524\n",
            "Epoch 76/100, Train Loss: 0.0391, Val Loss: 0.0524\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 77/100, Train Loss: 0.0394, Val Loss: 0.0524\n",
            "Epoch 78/100, Train Loss: 0.0373, Val Loss: 0.0524\n",
            "Epoch 79/100, Train Loss: 0.0384, Val Loss: 0.0524\n",
            "Epoch 80/100, Train Loss: 0.0390, Val Loss: 0.0524\n",
            "Epoch 81/100, Train Loss: 0.0392, Val Loss: 0.0524\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 82/100, Train Loss: 0.0397, Val Loss: 0.0524\n",
            "Epoch 83/100, Train Loss: 0.0400, Val Loss: 0.0524\n",
            "Epoch 84/100, Train Loss: 0.0390, Val Loss: 0.0524\n",
            "Epoch 85/100, Train Loss: 0.0398, Val Loss: 0.0524\n",
            "Epoch 86/100, Train Loss: 0.0396, Val Loss: 0.0524\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 87/100, Train Loss: 0.0380, Val Loss: 0.0524\n",
            "Epoch 88/100, Train Loss: 0.0390, Val Loss: 0.0524\n",
            "Epoch 89/100, Train Loss: 0.0379, Val Loss: 0.0524\n",
            "Epoch 90/100, Train Loss: 0.0384, Val Loss: 0.0524\n",
            "Epoch 91/100, Train Loss: 0.0385, Val Loss: 0.0524\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 92/100, Train Loss: 0.0389, Val Loss: 0.0524\n",
            "Epoch 93/100, Train Loss: 0.0397, Val Loss: 0.0524\n",
            "Epoch 94/100, Train Loss: 0.0387, Val Loss: 0.0524\n",
            "Epoch 95/100, Train Loss: 0.0393, Val Loss: 0.0524\n",
            "Epoch 96/100, Train Loss: 0.0391, Val Loss: 0.0524\n",
            "Reducing learning rate by factor of 0.01\n",
            "Epoch 97/100, Train Loss: 0.0384, Val Loss: 0.0524\n",
            "Epoch 98/100, Train Loss: 0.0410, Val Loss: 0.0524\n",
            "Epoch 99/100, Train Loss: 0.0381, Val Loss: 0.0524\n",
            "Epoch 100/100, Train Loss: 0.0392, Val Loss: 0.0524\n",
            "\n",
            "Total training time for hybrid_lstmUnit512_gruUnit16_drop0.2_dense64_batch4_epochs100: 194.44 seconds (3.24 minutes)\n",
            "\n",
            "Training Times Summary:\n",
            "lstm_unit256_drop0.1_dense32_batch4_epochs100: 167.30 seconds (2.79 minutes)\n",
            "lstm_unit256_drop0.1_dense64_batch4_epochs100: 160.84 seconds (2.68 minutes)\n",
            "lstm_unit256_drop0.2_dense32_batch4_epochs100: 160.82 seconds (2.68 minutes)\n",
            "lstm_unit256_drop0.2_dense64_batch4_epochs100: 160.80 seconds (2.68 minutes)\n",
            "lstm_unit512_drop0.1_dense32_batch4_epochs100: 159.40 seconds (2.66 minutes)\n",
            "lstm_unit512_drop0.1_dense64_batch4_epochs100: 159.11 seconds (2.65 minutes)\n",
            "lstm_unit512_drop0.2_dense32_batch4_epochs100: 159.33 seconds (2.66 minutes)\n",
            "lstm_unit512_drop0.2_dense64_batch4_epochs100: 158.90 seconds (2.65 minutes)\n",
            "gru_unit8_drop0.1_dense32_batch4_epochs100: 137.41 seconds (2.29 minutes)\n",
            "gru_unit8_drop0.1_dense64_batch4_epochs100: 137.63 seconds (2.29 minutes)\n",
            "gru_unit8_drop0.2_dense32_batch4_epochs100: 137.46 seconds (2.29 minutes)\n",
            "gru_unit8_drop0.2_dense64_batch4_epochs100: 136.87 seconds (2.28 minutes)\n",
            "gru_unit16_drop0.1_dense32_batch4_epochs100: 136.93 seconds (2.28 minutes)\n",
            "gru_unit16_drop0.1_dense64_batch4_epochs100: 137.58 seconds (2.29 minutes)\n",
            "gru_unit16_drop0.2_dense32_batch4_epochs100: 137.15 seconds (2.29 minutes)\n",
            "gru_unit16_drop0.2_dense64_batch4_epochs100: 136.99 seconds (2.28 minutes)\n",
            "hybrid_lstmUnit256_gruUnit8_drop0.1_dense32_batch4_epochs100: 197.44 seconds (3.29 minutes)\n",
            "hybrid_lstmUnit256_gruUnit8_drop0.1_dense64_batch4_epochs100: 196.83 seconds (3.28 minutes)\n",
            "hybrid_lstmUnit256_gruUnit8_drop0.2_dense32_batch4_epochs100: 197.36 seconds (3.29 minutes)\n",
            "hybrid_lstmUnit256_gruUnit8_drop0.2_dense64_batch4_epochs100: 197.59 seconds (3.29 minutes)\n",
            "hybrid_lstmUnit256_gruUnit16_drop0.1_dense32_batch4_epochs100: 197.37 seconds (3.29 minutes)\n",
            "hybrid_lstmUnit256_gruUnit16_drop0.1_dense64_batch4_epochs100: 197.23 seconds (3.29 minutes)\n",
            "hybrid_lstmUnit256_gruUnit16_drop0.2_dense32_batch4_epochs100: 197.43 seconds (3.29 minutes)\n",
            "hybrid_lstmUnit256_gruUnit16_drop0.2_dense64_batch4_epochs100: 197.54 seconds (3.29 minutes)\n",
            "hybrid_lstmUnit512_gruUnit8_drop0.1_dense32_batch4_epochs100: 195.74 seconds (3.26 minutes)\n",
            "hybrid_lstmUnit512_gruUnit8_drop0.1_dense64_batch4_epochs100: 195.49 seconds (3.26 minutes)\n",
            "hybrid_lstmUnit512_gruUnit8_drop0.2_dense32_batch4_epochs100: 195.88 seconds (3.26 minutes)\n",
            "hybrid_lstmUnit512_gruUnit8_drop0.2_dense64_batch4_epochs100: 195.53 seconds (3.26 minutes)\n",
            "hybrid_lstmUnit512_gruUnit16_drop0.1_dense32_batch4_epochs100: 195.59 seconds (3.26 minutes)\n",
            "hybrid_lstmUnit512_gruUnit16_drop0.1_dense64_batch4_epochs100: 195.41 seconds (3.26 minutes)\n",
            "hybrid_lstmUnit512_gruUnit16_drop0.2_dense32_batch4_epochs100: 195.55 seconds (3.26 minutes)\n",
            "hybrid_lstmUnit512_gruUnit16_drop0.2_dense64_batch4_epochs100: 194.44 seconds (3.24 minutes)\n",
            "\n",
            "Saving results with training times...\n",
            "Results:\n",
            "             RMSE            MAE       MAPE model_type     units  drop_rate  \\\n",
            "0   182253.718184  147104.125000  11.074097       lstm       256        0.1   \n",
            "1   123565.964505   93664.953125   7.049941       lstm       256        0.1   \n",
            "2   101792.513045   77620.203125   5.938788       lstm       256        0.2   \n",
            "3   113857.785259   85579.351562   6.451847       lstm       256        0.2   \n",
            "4   105087.576126   78026.804688   5.912697       lstm       512        0.1   \n",
            "5   127416.883779   98962.289062   7.480051       lstm       512        0.1   \n",
            "6   109556.438642   85183.257812   6.588063       lstm       512        0.2   \n",
            "7   123701.698485   93392.453125   7.061137       lstm       512        0.2   \n",
            "8   197476.633230  154682.281250  11.442617        gru         8        0.1   \n",
            "9   210967.309392  168735.859375  12.508427        gru         8        0.1   \n",
            "10  129609.650999   93619.578125   6.959952        gru         8        0.2   \n",
            "11  151597.493172  114421.562500   8.492849        gru         8        0.2   \n",
            "12  174313.361232  137852.171875  10.292980        gru        16        0.1   \n",
            "13  106403.604149   80614.515625   6.138186        gru        16        0.1   \n",
            "14  136101.887481  100902.671875   7.521171        gru        16        0.2   \n",
            "15  110505.447612   83498.429688   6.332117        gru        16        0.2   \n",
            "16  134470.267346  103107.273438   7.728670     hybrid   L256_G8        0.1   \n",
            "17  186427.364172  147878.750000  10.986444     hybrid   L256_G8        0.1   \n",
            "18  154147.783221  123833.875000   9.268191     hybrid   L256_G8        0.2   \n",
            "19  104515.914195   82462.273438   6.431404     hybrid   L256_G8        0.2   \n",
            "20  117010.199897   87352.187500   6.595997     hybrid  L256_G16        0.1   \n",
            "21  111372.169450   84013.945312   6.359159     hybrid  L256_G16        0.1   \n",
            "22  113273.595299   86226.914062   6.520490     hybrid  L256_G16        0.2   \n",
            "23   90284.854101   71326.007812   5.638931     hybrid  L256_G16        0.2   \n",
            "24  183880.888055  148421.859375  11.222830     hybrid   L512_G8        0.1   \n",
            "25  101068.386828   77833.765625   5.963865     hybrid   L512_G8        0.1   \n",
            "26  114118.447378   89291.851562   6.883733     hybrid   L512_G8        0.2   \n",
            "27  136175.164373  108543.937500   8.245108     hybrid   L512_G8        0.2   \n",
            "28  157048.475777  125588.398438   9.495052     hybrid  L512_G16        0.1   \n",
            "29  121550.419366   91940.546875   6.965878     hybrid  L512_G16        0.1   \n",
            "30  163936.940071  131267.218750   9.904110     hybrid  L512_G16        0.2   \n",
            "31   91303.175323   72204.109375   5.592945     hybrid  L512_G16        0.2   \n",
            "\n",
            "    dense_unit  batch_size  epochs  training_time  \n",
            "0           32           4     100     167.300521  \n",
            "1           64           4     100     160.835905  \n",
            "2           32           4     100     160.822445  \n",
            "3           64           4     100     160.804775  \n",
            "4           32           4     100     159.396672  \n",
            "5           64           4     100     159.106784  \n",
            "6           32           4     100     159.333252  \n",
            "7           64           4     100     158.900290  \n",
            "8           32           4     100     137.411107  \n",
            "9           64           4     100     137.632552  \n",
            "10          32           4     100     137.459421  \n",
            "11          64           4     100     136.871357  \n",
            "12          32           4     100     136.933894  \n",
            "13          64           4     100     137.576897  \n",
            "14          32           4     100     137.150626  \n",
            "15          64           4     100     136.987139  \n",
            "16          32           4     100     197.441017  \n",
            "17          64           4     100     196.825622  \n",
            "18          32           4     100     197.358603  \n",
            "19          64           4     100     197.592206  \n",
            "20          32           4     100     197.368425  \n",
            "21          64           4     100     197.230048  \n",
            "22          32           4     100     197.427649  \n",
            "23          64           4     100     197.535923  \n",
            "24          32           4     100     195.740386  \n",
            "25          64           4     100     195.489981  \n",
            "26          32           4     100     195.880128  \n",
            "27          64           4     100     195.533581  \n",
            "28          32           4     100     195.585089  \n",
            "29          64           4     100     195.410639  \n",
            "30          32           4     100     195.552592  \n",
            "31          64           4     100     194.435508  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result"
      ],
      "metadata": {
        "id": "1V_8xkppJHwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adjusted_valuelist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mTdShHIopGJW",
        "outputId": "0af720dc-c42b-44c2-d143-db458c9e0cc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             RMSE            MAE       MAPE model_type     units  drop_rate  \\\n",
              "0   182253.718184  147104.125000  11.074097       lstm       256        0.1   \n",
              "1   123565.964505   93664.953125   7.049941       lstm       256        0.1   \n",
              "2   101792.513045   77620.203125   5.938788       lstm       256        0.2   \n",
              "3   113857.785259   85579.351562   6.451847       lstm       256        0.2   \n",
              "4   105087.576126   78026.804688   5.912697       lstm       512        0.1   \n",
              "5   127416.883779   98962.289062   7.480051       lstm       512        0.1   \n",
              "6   109556.438642   85183.257812   6.588063       lstm       512        0.2   \n",
              "7   123701.698485   93392.453125   7.061137       lstm       512        0.2   \n",
              "8   197476.633230  154682.281250  11.442617        gru         8        0.1   \n",
              "9   210967.309392  168735.859375  12.508427        gru         8        0.1   \n",
              "10  129609.650999   93619.578125   6.959952        gru         8        0.2   \n",
              "11  151597.493172  114421.562500   8.492849        gru         8        0.2   \n",
              "12  174313.361232  137852.171875  10.292980        gru        16        0.1   \n",
              "13  106403.604149   80614.515625   6.138186        gru        16        0.1   \n",
              "14  136101.887481  100902.671875   7.521171        gru        16        0.2   \n",
              "15  110505.447612   83498.429688   6.332117        gru        16        0.2   \n",
              "16  134470.267346  103107.273438   7.728670     hybrid   L256_G8        0.1   \n",
              "17  186427.364172  147878.750000  10.986444     hybrid   L256_G8        0.1   \n",
              "18  154147.783221  123833.875000   9.268191     hybrid   L256_G8        0.2   \n",
              "19  104515.914195   82462.273438   6.431404     hybrid   L256_G8        0.2   \n",
              "20  117010.199897   87352.187500   6.595997     hybrid  L256_G16        0.1   \n",
              "21  111372.169450   84013.945312   6.359159     hybrid  L256_G16        0.1   \n",
              "22  113273.595299   86226.914062   6.520490     hybrid  L256_G16        0.2   \n",
              "23   90284.854101   71326.007812   5.638931     hybrid  L256_G16        0.2   \n",
              "24  183880.888055  148421.859375  11.222830     hybrid   L512_G8        0.1   \n",
              "25  101068.386828   77833.765625   5.963865     hybrid   L512_G8        0.1   \n",
              "26  114118.447378   89291.851562   6.883733     hybrid   L512_G8        0.2   \n",
              "27  136175.164373  108543.937500   8.245108     hybrid   L512_G8        0.2   \n",
              "28  157048.475777  125588.398438   9.495052     hybrid  L512_G16        0.1   \n",
              "29  121550.419366   91940.546875   6.965878     hybrid  L512_G16        0.1   \n",
              "30  163936.940071  131267.218750   9.904110     hybrid  L512_G16        0.2   \n",
              "31   91303.175323   72204.109375   5.592945     hybrid  L512_G16        0.2   \n",
              "\n",
              "    dense_unit  batch_size  epochs  training_time  \n",
              "0           32           4     100     167.300521  \n",
              "1           64           4     100     160.835905  \n",
              "2           32           4     100     160.822445  \n",
              "3           64           4     100     160.804775  \n",
              "4           32           4     100     159.396672  \n",
              "5           64           4     100     159.106784  \n",
              "6           32           4     100     159.333252  \n",
              "7           64           4     100     158.900290  \n",
              "8           32           4     100     137.411107  \n",
              "9           64           4     100     137.632552  \n",
              "10          32           4     100     137.459421  \n",
              "11          64           4     100     136.871357  \n",
              "12          32           4     100     136.933894  \n",
              "13          64           4     100     137.576897  \n",
              "14          32           4     100     137.150626  \n",
              "15          64           4     100     136.987139  \n",
              "16          32           4     100     197.441017  \n",
              "17          64           4     100     196.825622  \n",
              "18          32           4     100     197.358603  \n",
              "19          64           4     100     197.592206  \n",
              "20          32           4     100     197.368425  \n",
              "21          64           4     100     197.230048  \n",
              "22          32           4     100     197.427649  \n",
              "23          64           4     100     197.535923  \n",
              "24          32           4     100     195.740386  \n",
              "25          64           4     100     195.489981  \n",
              "26          32           4     100     195.880128  \n",
              "27          64           4     100     195.533581  \n",
              "28          32           4     100     195.585089  \n",
              "29          64           4     100     195.410639  \n",
              "30          32           4     100     195.552592  \n",
              "31          64           4     100     194.435508  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d8bede95-e058-4ed7-912a-fc94dc361f3f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAE</th>\n",
              "      <th>MAPE</th>\n",
              "      <th>model_type</th>\n",
              "      <th>units</th>\n",
              "      <th>drop_rate</th>\n",
              "      <th>dense_unit</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>epochs</th>\n",
              "      <th>training_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>182253.718184</td>\n",
              "      <td>147104.125000</td>\n",
              "      <td>11.074097</td>\n",
              "      <td>lstm</td>\n",
              "      <td>256</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>167.300521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>123565.964505</td>\n",
              "      <td>93664.953125</td>\n",
              "      <td>7.049941</td>\n",
              "      <td>lstm</td>\n",
              "      <td>256</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>160.835905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>101792.513045</td>\n",
              "      <td>77620.203125</td>\n",
              "      <td>5.938788</td>\n",
              "      <td>lstm</td>\n",
              "      <td>256</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>160.822445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>113857.785259</td>\n",
              "      <td>85579.351562</td>\n",
              "      <td>6.451847</td>\n",
              "      <td>lstm</td>\n",
              "      <td>256</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>160.804775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>105087.576126</td>\n",
              "      <td>78026.804688</td>\n",
              "      <td>5.912697</td>\n",
              "      <td>lstm</td>\n",
              "      <td>512</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>159.396672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>127416.883779</td>\n",
              "      <td>98962.289062</td>\n",
              "      <td>7.480051</td>\n",
              "      <td>lstm</td>\n",
              "      <td>512</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>159.106784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>109556.438642</td>\n",
              "      <td>85183.257812</td>\n",
              "      <td>6.588063</td>\n",
              "      <td>lstm</td>\n",
              "      <td>512</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>159.333252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>123701.698485</td>\n",
              "      <td>93392.453125</td>\n",
              "      <td>7.061137</td>\n",
              "      <td>lstm</td>\n",
              "      <td>512</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>158.900290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>197476.633230</td>\n",
              "      <td>154682.281250</td>\n",
              "      <td>11.442617</td>\n",
              "      <td>gru</td>\n",
              "      <td>8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>137.411107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>210967.309392</td>\n",
              "      <td>168735.859375</td>\n",
              "      <td>12.508427</td>\n",
              "      <td>gru</td>\n",
              "      <td>8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>137.632552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>129609.650999</td>\n",
              "      <td>93619.578125</td>\n",
              "      <td>6.959952</td>\n",
              "      <td>gru</td>\n",
              "      <td>8</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>137.459421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>151597.493172</td>\n",
              "      <td>114421.562500</td>\n",
              "      <td>8.492849</td>\n",
              "      <td>gru</td>\n",
              "      <td>8</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>136.871357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>174313.361232</td>\n",
              "      <td>137852.171875</td>\n",
              "      <td>10.292980</td>\n",
              "      <td>gru</td>\n",
              "      <td>16</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>136.933894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>106403.604149</td>\n",
              "      <td>80614.515625</td>\n",
              "      <td>6.138186</td>\n",
              "      <td>gru</td>\n",
              "      <td>16</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>137.576897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>136101.887481</td>\n",
              "      <td>100902.671875</td>\n",
              "      <td>7.521171</td>\n",
              "      <td>gru</td>\n",
              "      <td>16</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>137.150626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>110505.447612</td>\n",
              "      <td>83498.429688</td>\n",
              "      <td>6.332117</td>\n",
              "      <td>gru</td>\n",
              "      <td>16</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>136.987139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>134470.267346</td>\n",
              "      <td>103107.273438</td>\n",
              "      <td>7.728670</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L256_G8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>197.441017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>186427.364172</td>\n",
              "      <td>147878.750000</td>\n",
              "      <td>10.986444</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L256_G8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>196.825622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>154147.783221</td>\n",
              "      <td>123833.875000</td>\n",
              "      <td>9.268191</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L256_G8</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>197.358603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>104515.914195</td>\n",
              "      <td>82462.273438</td>\n",
              "      <td>6.431404</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L256_G8</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>197.592206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>117010.199897</td>\n",
              "      <td>87352.187500</td>\n",
              "      <td>6.595997</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L256_G16</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>197.368425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>111372.169450</td>\n",
              "      <td>84013.945312</td>\n",
              "      <td>6.359159</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L256_G16</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>197.230048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>113273.595299</td>\n",
              "      <td>86226.914062</td>\n",
              "      <td>6.520490</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L256_G16</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>197.427649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>90284.854101</td>\n",
              "      <td>71326.007812</td>\n",
              "      <td>5.638931</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L256_G16</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>197.535923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>183880.888055</td>\n",
              "      <td>148421.859375</td>\n",
              "      <td>11.222830</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L512_G8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>195.740386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>101068.386828</td>\n",
              "      <td>77833.765625</td>\n",
              "      <td>5.963865</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L512_G8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>195.489981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>114118.447378</td>\n",
              "      <td>89291.851562</td>\n",
              "      <td>6.883733</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L512_G8</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>195.880128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>136175.164373</td>\n",
              "      <td>108543.937500</td>\n",
              "      <td>8.245108</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L512_G8</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>195.533581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>157048.475777</td>\n",
              "      <td>125588.398438</td>\n",
              "      <td>9.495052</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L512_G16</td>\n",
              "      <td>0.1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>195.585089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>121550.419366</td>\n",
              "      <td>91940.546875</td>\n",
              "      <td>6.965878</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L512_G16</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>195.410639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>163936.940071</td>\n",
              "      <td>131267.218750</td>\n",
              "      <td>9.904110</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L512_G16</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>195.552592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>91303.175323</td>\n",
              "      <td>72204.109375</td>\n",
              "      <td>5.592945</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>L512_G16</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>194.435508</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d8bede95-e058-4ed7-912a-fc94dc361f3f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d8bede95-e058-4ed7-912a-fc94dc361f3f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d8bede95-e058-4ed7-912a-fc94dc361f3f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-576c0382-dc25-4467-8049-00b6ed20c189\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-576c0382-dc25-4467-8049-00b6ed20c189')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-576c0382-dc25-4467-8049-00b6ed20c189 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_08f52a17-260c-436f-aeaf-85e7ed7e9f03\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('adjusted_valuelist')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_08f52a17-260c-436f-aeaf-85e7ed7e9f03 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('adjusted_valuelist');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "adjusted_valuelist",
              "summary": "{\n  \"name\": \"adjusted_valuelist\",\n  \"rows\": 32,\n  \"fields\": [\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 32741.20125548066,\n        \"min\": 90284.85410078481,\n        \"max\": 210967.30939176335,\n        \"num_unique_values\": 32,\n        \"samples\": [\n          121550.4193657924,\n          110505.44761232362,\n          183880.8880552843\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 27456.265656469477,\n        \"min\": 71326.0078125,\n        \"max\": 168735.859375,\n        \"num_unique_values\": 32,\n        \"samples\": [\n          91940.546875,\n          83498.4296875,\n          148421.859375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.992100455530909,\n        \"min\": 5.592945218086243,\n        \"max\": 12.50842660665512,\n        \"num_unique_values\": 32,\n        \"samples\": [\n          6.965877860784531,\n          6.332116574048996,\n          11.222829669713974\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"lstm\",\n          \"gru\",\n          \"hybrid\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"units\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          512,\n          \"L256_G16\",\n          256\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"drop_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.050800050800076206,\n        \"min\": 0.1,\n        \"max\": 0.2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.2,\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dense_unit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16,\n        \"min\": 32,\n        \"max\": 64,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          64,\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 4,\n        \"max\": 4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"epochs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 100,\n        \"max\": 100,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          100\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"training_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 25.551617311814816,\n        \"min\": 136.87135672569275,\n        \"max\": 197.5922064781189,\n        \"num_unique_values\": 32,\n        \"samples\": [\n          195.41063904762268\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_adjusted_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmGOVsRopIci",
        "outputId": "0cacdc6a-f05f-48d5-e20e-33ac72594400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lstm_unit256_drop0.1_dense32_batch4_epochs100': array([1457751.1, 1414111. , 1310062. , 1202888.6, 1162393. , 1196769.6,\n",
              "        1232553.2, 1279402.2, 1303493.9, 1359491.1, 1472966.1, 1579900.5,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'lstm_unit256_drop0.1_dense64_batch4_epochs100': array([1515680.8, 1498410.6, 1428545.6, 1325597.2, 1265521.2, 1262136.4,\n",
              "        1286336.6, 1328971.6, 1366635.2, 1436741.6, 1553205.4, 1647428.8,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'lstm_unit256_drop0.2_dense32_batch4_epochs100': array([1518744.6, 1501224.4, 1441179.8, 1371678.1, 1326203. , 1335538. ,\n",
              "        1358408.1, 1398536.6, 1434133.4, 1496476.4, 1576867.1, 1644279.6,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'lstm_unit256_drop0.2_dense64_batch4_epochs100': array([1521142.9, 1494560.6, 1414871.4, 1316949.9, 1269541.4, 1282476.6,\n",
              "        1316659.9, 1364574.9, 1397952.9, 1461431.6, 1565235.8, 1649003.9,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'lstm_unit512_drop0.1_dense32_batch4_epochs100': array([1509005.5, 1470298.9, 1407896.1, 1334291.5, 1293204.5, 1299009.8,\n",
              "        1314375.4, 1368645.4, 1415789.5, 1488509. , 1590683.5, 1666115.2,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'lstm_unit512_drop0.1_dense64_batch4_epochs100': array([1493309.8, 1439587.8, 1371579. , 1307585.1, 1276668.8, 1277207.2,\n",
              "        1285381.2, 1327598.5, 1375100.8, 1443069.4, 1559161.6, 1650394.4,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'lstm_unit512_drop0.2_dense32_batch4_epochs100': array([1569708.1, 1525029.6, 1391617.5, 1259587.4, 1215469.2, 1280787.8,\n",
              "        1341021.6, 1394500.4, 1420195. , 1488406.4, 1603428. , 1703303.4,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'lstm_unit512_drop0.2_dense64_batch4_epochs100': array([1504510.4, 1463485.6, 1388352.6, 1314926.9, 1271288.6, 1288592. ,\n",
              "        1302106.1, 1355258.2, 1365792.9, 1434936.1, 1560919.4, 1662710.5,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'gru_unit8_drop0.1_dense32_batch4_epochs100': array([1293710.9, 1319149.1, 1337033.8, 1338151.2, 1315107.5, 1310974.8,\n",
              "        1331316.1, 1342270.6, 1357414. , 1380134.8, 1397692.5, 1402191.6,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'gru_unit8_drop0.1_dense64_batch4_epochs100': array([1272259. , 1290627.6, 1296518.5, 1307834.1, 1295840.9, 1297457.1,\n",
              "        1302735.1, 1319948.6, 1338427.4, 1375191.1, 1385868.8, 1387605.1,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'gru_unit8_drop0.2_dense32_batch4_epochs100': array([1517779.2, 1492041.9, 1422644.2, 1366091.5, 1304770.4, 1290912.5,\n",
              "        1316998.2, 1357213.4, 1362183.1, 1407568. , 1516535.4, 1613467.5,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'gru_unit8_drop0.2_dense64_batch4_epochs100': array([1471432. , 1446936.9, 1386662.4, 1341677.6, 1289200.5, 1275463.1,\n",
              "        1290089.2, 1323917.1, 1332341.6, 1381473.8, 1477408.9, 1570122.2,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'gru_unit16_drop0.1_dense32_batch4_epochs100': array([1457917. , 1433074.8, 1355155. , 1267381.8, 1211046. , 1213526.8,\n",
              "        1237853.6, 1284646. , 1303418.8, 1349096.8, 1458807.6, 1562900. ,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'gru_unit16_drop0.1_dense64_batch4_epochs100': array([1538800.6, 1515062.9, 1452618.9, 1373954.4, 1329844. , 1323390.9,\n",
              "        1338910.9, 1381413.9, 1412984.9, 1466127.8, 1571048.9, 1663244.4,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'gru_unit16_drop0.2_dense32_batch4_epochs100': array([1498451. , 1478202.1, 1403690.1, 1325576.5, 1271400.9, 1277238. ,\n",
              "        1300077.9, 1348019.5, 1364246.9, 1405526.5, 1506622.5, 1600487.8,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'gru_unit16_drop0.2_dense64_batch4_epochs100': array([1540270.5, 1517446.2, 1440744.1, 1360340. , 1313739.1, 1317734.9,\n",
              "        1337331.9, 1385849.9, 1408410.2, 1452907.5, 1557254.8, 1653366.6,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit8_drop0.1_dense32_batch4_epochs100': array([1484868.5, 1463040.9, 1397769.5, 1305247.5, 1257707. , 1263376.9,\n",
              "        1289658.1, 1326281. , 1362608.6, 1425374.5, 1529383.4, 1615637. ,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit8_drop0.1_dense64_batch4_epochs100': array([1304191.2, 1312556.1, 1326796.9, 1323544.8, 1316735. , 1314467.9,\n",
              "        1326906.6, 1346104. , 1374656.1, 1401628.6, 1427077.4, 1443898.9,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit8_drop0.2_dense32_batch4_epochs100': array([1442941.1, 1427581.6, 1369894. , 1283919.8, 1232590.8, 1228576.4,\n",
              "        1253882.6, 1298401.9, 1341409.4, 1405274.9, 1500348.9, 1576358.2,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit8_drop0.2_dense64_batch4_epochs100': array([1585692.8, 1550785.5, 1440713.4, 1310433.9, 1271300. , 1326705. ,\n",
              "        1385777.4, 1430483.6, 1435564.9, 1475378.4, 1591564.6, 1701116.8,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit16_drop0.1_dense32_batch4_epochs100': array([1502297.8, 1475998.2, 1413779.4, 1332000.2, 1298452.9, 1310828.4,\n",
              "        1335049.2, 1366609.8, 1401508.1, 1458283. , 1553951.4, 1634431.5,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit16_drop0.1_dense64_batch4_epochs100': array([1509653.4, 1479510.5, 1416412. , 1329400.2, 1294443.4, 1304567.9,\n",
              "        1334885.4, 1377543.9, 1410547.6, 1465598.4, 1567807.5, 1646682.9,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit16_drop0.2_dense32_batch4_epochs100': array([1491384.1, 1466530.6, 1408656.4, 1328984. , 1294396.9, 1307940.8,\n",
              "        1339582.9, 1374866.1, 1416789.5, 1476005.1, 1561161.8, 1627689.4,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit16_drop0.2_dense64_batch4_epochs100': array([1556130.1, 1529689.2, 1463142.9, 1380754. , 1346321.2, 1363428. ,\n",
              "        1395959. , 1438592.2, 1474426.5, 1529247.4, 1616769. , 1685813.8,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit8_drop0.1_dense32_batch4_epochs100': array([1462071.6, 1410845.5, 1288229.1, 1181955.8, 1149954.5, 1205194. ,\n",
              "        1250113.8, 1276448.4, 1290582.8, 1364856.5, 1490517.2, 1599413.9,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit8_drop0.1_dense64_batch4_epochs100': array([1549290.1, 1518110. , 1449744.4, 1350666.1, 1294708.6, 1303364. ,\n",
              "        1338680. , 1395968.8, 1411922.4, 1468462.4, 1594094.9, 1699683.6,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit8_drop0.2_dense32_batch4_epochs100': array([1562017.8, 1536730.2, 1415935.1, 1296744.2, 1242895.5, 1307662.5,\n",
              "        1372377.6, 1407791.8, 1395704.8, 1458270.8, 1572186.1, 1679794.8,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit8_drop0.2_dense64_batch4_epochs100': array([1480102.4, 1423565.2, 1343562. , 1289731.8, 1266460. , 1285878.9,\n",
              "        1287988.2, 1307089.6, 1349497.8, 1449814.9, 1562003.2, 1644149.6,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit16_drop0.1_dense32_batch4_epochs100': array([1475989.9, 1430337.2, 1324454.8, 1242354.2, 1218972. , 1267513. ,\n",
              "        1296229. , 1308616.9, 1320183. , 1401016.1, 1516427.4, 1612395.9,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit16_drop0.1_dense64_batch4_epochs100': array([1531469. , 1510150.4, 1407733.1, 1290349.5, 1229376.4, 1270438.5,\n",
              "        1327808.4, 1364132.2, 1376456.1, 1445264.4, 1558883.4, 1662787.8,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit16_drop0.2_dense32_batch4_epochs100': array([1472893. , 1432469.1, 1330817.4, 1236311.5, 1189597.2, 1220003.4,\n",
              "        1252596.9, 1291262.1, 1308104.8, 1385047.6, 1510452.8, 1610410.9,\n",
              "              0. ,       0. ], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit16_drop0.2_dense64_batch4_epochs100': array([1561364.4, 1524802. , 1435777.4, 1338904.9, 1274832. , 1303154.1,\n",
              "        1345908. , 1414297.8, 1433454. , 1507681.4, 1620072.5, 1703512.9,\n",
              "              0. ,       0. ], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Pf0g-G_pJ0s",
        "outputId": "c0d7ca5d-5250-44f7-a59f-f0c4a9cb9517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lstm_unit256_drop0.1_dense32_batch4_epochs100': array([1457751.1 , 1414111.  , 1310062.  , 1202888.6 , 1162393.  ,\n",
              "        1196769.6 , 1232553.2 , 1279402.2 , 1303493.9 , 1359491.1 ,\n",
              "        1472966.1 , 1579900.5 ,  565184.75,   80532.74], dtype=float32),\n",
              " 'lstm_unit256_drop0.1_dense64_batch4_epochs100': array([1515680.8  , 1498410.6  , 1428545.6  , 1325597.2  , 1265521.2  ,\n",
              "        1262136.4  , 1286336.6  , 1328971.6  , 1366635.2  , 1436741.6  ,\n",
              "        1553205.4  , 1647428.8  ,  100009.63 ,   42122.082], dtype=float32),\n",
              " 'lstm_unit256_drop0.2_dense32_batch4_epochs100': array([1518744.6 , 1501224.4 , 1441179.8 , 1371678.1 , 1326203.  ,\n",
              "        1335538.  , 1358408.1 , 1398536.6 , 1434133.4 , 1496476.4 ,\n",
              "        1576867.1 , 1644279.6 ,  674244.6 ,   82374.77], dtype=float32),\n",
              " 'lstm_unit256_drop0.2_dense64_batch4_epochs100': array([1521142.9  , 1494560.6  , 1414871.4  , 1316949.9  , 1269541.4  ,\n",
              "        1282476.6  , 1316659.9  , 1364574.9  , 1397952.9  , 1461431.6  ,\n",
              "        1565235.8  , 1649003.9  ,  458266.6  ,   59564.082], dtype=float32),\n",
              " 'lstm_unit512_drop0.1_dense32_batch4_epochs100': array([1509005.5 , 1470298.9 , 1407896.1 , 1334291.5 , 1293204.5 ,\n",
              "        1299009.8 , 1314375.4 , 1368645.4 , 1415789.5 , 1488509.  ,\n",
              "        1590683.5 , 1666115.2 , 1059255.  ,   55282.96], dtype=float32),\n",
              " 'lstm_unit512_drop0.1_dense64_batch4_epochs100': array([1493309.8  , 1439587.8  , 1371579.   , 1307585.1  , 1276668.8  ,\n",
              "        1277207.2  , 1285381.2  , 1327598.5  , 1375100.8  , 1443069.4  ,\n",
              "        1559161.6  , 1650394.4  ,  213446.33 ,   39554.133], dtype=float32),\n",
              " 'lstm_unit512_drop0.2_dense32_batch4_epochs100': array([1569708.1  , 1525029.6  , 1391617.5  , 1259587.4  , 1215469.2  ,\n",
              "        1280787.8  , 1341021.6  , 1394500.4  , 1420195.   , 1488406.4  ,\n",
              "        1603428.   , 1703303.4  ,  338579.78 ,   79197.375], dtype=float32),\n",
              " 'lstm_unit512_drop0.2_dense64_batch4_epochs100': array([1504510.4  , 1463485.6  , 1388352.6  , 1314926.9  , 1271288.6  ,\n",
              "        1288592.   , 1302106.1  , 1355258.2  , 1365792.9  , 1434936.1  ,\n",
              "        1560919.4  , 1662710.5  ,   96666.055,   19156.521], dtype=float32),\n",
              " 'gru_unit8_drop0.1_dense32_batch4_epochs100': array([1293710.9  , 1319149.1  , 1337033.8  , 1338151.2  , 1315107.5  ,\n",
              "        1310974.8  , 1331316.1  , 1342270.6  , 1357414.   , 1380134.8  ,\n",
              "        1397692.5  , 1402191.6  ,  493619.97 ,   36851.367], dtype=float32),\n",
              " 'gru_unit8_drop0.1_dense64_batch4_epochs100': array([1272259.  , 1290627.6 , 1296518.5 , 1307834.1 , 1295840.9 ,\n",
              "        1297457.1 , 1302735.1 , 1319948.6 , 1338427.4 , 1375191.1 ,\n",
              "        1385868.8 , 1387605.1 ,  592535.5 ,  592309.94], dtype=float32),\n",
              " 'gru_unit8_drop0.2_dense32_batch4_epochs100': array([1517779.2, 1492041.9, 1422644.2, 1366091.5, 1304770.4, 1290912.5,\n",
              "        1316998.2, 1357213.4, 1362183.1, 1407568. , 1516535.4, 1613467.5,\n",
              "         571435.9,  263547.3], dtype=float32),\n",
              " 'gru_unit8_drop0.2_dense64_batch4_epochs100': array([1471432.  , 1446936.9 , 1386662.4 , 1341677.6 , 1289200.5 ,\n",
              "        1275463.1 , 1290089.2 , 1323917.1 , 1332341.6 , 1381473.8 ,\n",
              "        1477408.9 , 1570122.2 ,  659262.06,  382729.94], dtype=float32),\n",
              " 'gru_unit16_drop0.1_dense32_batch4_epochs100': array([1457917. , 1433074.8, 1355155. , 1267381.8, 1211046. , 1213526.8,\n",
              "        1237853.6, 1284646. , 1303418.8, 1349096.8, 1458807.6, 1562900. ,\n",
              "        1256169.8,  878781.3], dtype=float32),\n",
              " 'gru_unit16_drop0.1_dense64_batch4_epochs100': array([1538800.6 , 1515062.9 , 1452618.9 , 1373954.4 , 1329844.  ,\n",
              "        1323390.9 , 1338910.9 , 1381413.9 , 1412984.9 , 1466127.8 ,\n",
              "        1571048.9 , 1663244.4 ,  559822.2 ,  773373.56], dtype=float32),\n",
              " 'gru_unit16_drop0.2_dense32_batch4_epochs100': array([1498451.  , 1478202.1 , 1403690.1 , 1325576.5 , 1271400.9 ,\n",
              "        1277238.  , 1300077.9 , 1348019.5 , 1364246.9 , 1405526.5 ,\n",
              "        1506622.5 , 1600487.8 , 1280434.1 ,   59804.43], dtype=float32),\n",
              " 'gru_unit16_drop0.2_dense64_batch4_epochs100': array([1540270.5 , 1517446.2 , 1440744.1 , 1360340.  , 1313739.1 ,\n",
              "        1317734.9 , 1337331.9 , 1385849.9 , 1408410.2 , 1452907.5 ,\n",
              "        1557254.8 , 1653366.6 ,  618849.8 ,  130979.44], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit8_drop0.1_dense32_batch4_epochs100': array([1484868.5 , 1463040.9 , 1397769.5 , 1305247.5 , 1257707.  ,\n",
              "        1263376.9 , 1289658.1 , 1326281.  , 1362608.6 , 1425374.5 ,\n",
              "        1529383.4 , 1615637.  ,  235578.28,   76576.53], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit8_drop0.1_dense64_batch4_epochs100': array([1304191.2  , 1312556.1  , 1326796.9  , 1323544.8  , 1316735.   ,\n",
              "        1314467.9  , 1326906.6  , 1346104.   , 1374656.1  , 1401628.6  ,\n",
              "        1427077.4  , 1443898.9  ,  149923.36 ,   59581.535], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit8_drop0.2_dense32_batch4_epochs100': array([1442941.1 , 1427581.6 , 1369894.  , 1283919.8 , 1232590.8 ,\n",
              "        1228576.4 , 1253882.6 , 1298401.9 , 1341409.4 , 1405274.9 ,\n",
              "        1500348.9 , 1576358.2 ,  211703.31,  113318.96], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit8_drop0.2_dense64_batch4_epochs100': array([1585692.8, 1550785.5, 1440713.4, 1310433.9, 1271300. , 1326705. ,\n",
              "        1385777.4, 1430483.6, 1435564.9, 1475378.4, 1591564.6, 1701116.8,\n",
              "         426070.2,   31622.7], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit16_drop0.1_dense32_batch4_epochs100': array([1502297.8  , 1475998.2  , 1413779.4  , 1332000.2  , 1298452.9  ,\n",
              "        1310828.4  , 1335049.2  , 1366609.8  , 1401508.1  , 1458283.   ,\n",
              "        1553951.4  , 1634431.5  ,  156202.66 ,   59907.434], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit16_drop0.1_dense64_batch4_epochs100': array([1509653.4  , 1479510.5  , 1416412.   , 1329400.2  , 1294443.4  ,\n",
              "        1304567.9  , 1334885.4  , 1377543.9  , 1410547.6  , 1465598.4  ,\n",
              "        1567807.5  , 1646682.9  ,  248966.39 ,   53030.934], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit16_drop0.2_dense32_batch4_epochs100': array([1491384.1 , 1466530.6 , 1408656.4 , 1328984.  , 1294396.9 ,\n",
              "        1307940.8 , 1339582.9 , 1374866.1 , 1416789.5 , 1476005.1 ,\n",
              "        1561161.8 , 1627689.4 ,  124758.37,   92648.94], dtype=float32),\n",
              " 'hybrid_lstmUnit256_gruUnit16_drop0.2_dense64_batch4_epochs100': array([1556130.1 , 1529689.2 , 1463142.9 , 1380754.  , 1346321.2 ,\n",
              "        1363428.  , 1395959.  , 1438592.2 , 1474426.5 , 1529247.4 ,\n",
              "        1616769.  , 1685813.8 ,  139504.56,   80983.96], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit8_drop0.1_dense32_batch4_epochs100': array([1462071.6  , 1410845.5  , 1288229.1  , 1181955.8  , 1149954.5  ,\n",
              "        1205194.   , 1250113.8  , 1276448.4  , 1290582.8  , 1364856.5  ,\n",
              "        1490517.2  , 1599413.9  ,  309950.2  ,   57961.477], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit8_drop0.1_dense64_batch4_epochs100': array([1549290.1 , 1518110.  , 1449744.4 , 1350666.1 , 1294708.6 ,\n",
              "        1303364.  , 1338680.  , 1395968.8 , 1411922.4 , 1468462.4 ,\n",
              "        1594094.9 , 1699683.6 ,  228074.52,   32824.52], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit8_drop0.2_dense32_batch4_epochs100': array([1562017.8 , 1536730.2 , 1415935.1 , 1296744.2 , 1242895.5 ,\n",
              "        1307662.5 , 1372377.6 , 1407791.8 , 1395704.8 , 1458270.8 ,\n",
              "        1572186.1 , 1679794.8 ,  320478.06,   78387.93], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit8_drop0.2_dense64_batch4_epochs100': array([1480102.4  , 1423565.2  , 1343562.   , 1289731.8  , 1266460.   ,\n",
              "        1285878.9  , 1287988.2  , 1307089.6  , 1349497.8  , 1449814.9  ,\n",
              "        1562003.2  , 1644149.6  ,   97249.6  ,   48941.477], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit16_drop0.1_dense32_batch4_epochs100': array([1475989.9 , 1430337.2 , 1324454.8 , 1242354.2 , 1218972.  ,\n",
              "        1267513.  , 1296229.  , 1308616.9 , 1320183.  , 1401016.1 ,\n",
              "        1516427.4 , 1612395.9 ,  159766.61,  303116.22], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit16_drop0.1_dense64_batch4_epochs100': array([1531469.   , 1510150.4  , 1407733.1  , 1290349.5  , 1229376.4  ,\n",
              "        1270438.5  , 1327808.4  , 1364132.2  , 1376456.1  , 1445264.4  ,\n",
              "        1558883.4  , 1662787.8  ,  217101.8  ,   30474.465], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit16_drop0.2_dense32_batch4_epochs100': array([1472893.  , 1432469.1 , 1330817.4 , 1236311.5 , 1189597.2 ,\n",
              "        1220003.4 , 1252596.9 , 1291262.1 , 1308104.8 , 1385047.6 ,\n",
              "        1510452.8 , 1610410.9 ,  251071.89,   93802.  ], dtype=float32),\n",
              " 'hybrid_lstmUnit512_gruUnit16_drop0.2_dense64_batch4_epochs100': array([1561364.4 , 1524802.  , 1435777.4 , 1338904.9 , 1274832.  ,\n",
              "        1303154.1 , 1345908.  , 1414297.8 , 1433454.  , 1507681.4 ,\n",
              "        1620072.5 , 1703512.9 ,  188267.19,   78622.01], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}